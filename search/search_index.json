{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Welcome to GigaSpatial","text":"<p>GigaSpatial is a powerful Python package designed for spatial data analysis and processing, providing efficient tools and utilities for handling geographic information systems (GIS) data.</p>"},{"location":"#features","title":"Features","text":"<ul> <li>Efficient Data Processing: Fast and memory-efficient processing of large spatial datasets</li> <li>Multiple Format Support: Support for various spatial data formats</li> <li>Advanced Analysis Tools: Comprehensive set of spatial analysis tools</li> <li>Easy Integration: Seamless integration with popular GIS and data science libraries</li> <li>Scalable Solutions: Designed to handle both small and large-scale spatial data processing tasks</li> </ul>"},{"location":"#quick-links","title":"Quick Links","text":"<ul> <li>Installation Guide</li> <li>Quick Start Tutorial</li> <li>API Reference</li> <li>Example Gallery</li> </ul>"},{"location":"#getting-help","title":"Getting Help","text":"<p>If you need help using GigaSpatial, please check out our:</p> <ul> <li>User Guide for detailed usage instructions</li> <li>API Reference for detailed function and class documentation</li> <li>GitHub Issues for bug reports and feature requests</li> <li>Contributing Guide for guidelines on how to contribute to the project</li> </ul>"},{"location":"#license","title":"License","text":"<p>GigaSpatial is released under the AGPL-3.0 License. See the LICENSE file for more details. </p>"},{"location":"changelog/","title":"Changelog","text":"<p>All notable changes to this project will be documented in this file.</p>"},{"location":"changelog/#v041-2025-04-17","title":"[v0.4.1] - 2025-04-17","text":""},{"location":"changelog/#added","title":"Added","text":"<ul> <li>Documentation:<ul> <li>Added API Reference documentation for all modules, classes, and functions.</li> <li>Added a Configuration Guide to explain how to set up paths, API keys, and other.</li> </ul> </li> <li>TifProcessor: added new to_dataframe method.</li> <li>config: added set_path method for dynamic path management.</li> </ul>"},{"location":"changelog/#changed","title":"Changed","text":"<ul> <li>Documentation:<ul> <li>Restructured the <code>docs/</code> directory to improve organization and navigation.</li> <li>Updated the <code>index.md</code> for the User Guide to provide a clear overview of available documentation.</li> <li>Updated Examples for downloading, processing, and storing geospatial data - more to come.</li> </ul> </li> <li>README:<ul> <li>Updated the README with a clear description of the package\u2019s purpose and key features.</li> <li>Added a section on View Generators to explain spatial context enrichment and mapping to grid or POI locations.</li> <li>Included a Supported Datasets section with an image of dataset provider logos.</li> </ul> </li> </ul>"},{"location":"changelog/#fixed","title":"Fixed","text":"<ul> <li>Handled errors when processing nodes, relations, and ways in OSMLocationFetcher.</li> <li>Made <code>admin1</code> and <code>admin1_id_giga</code> optional in GigaEntity instances for countries with no admin level 1 divisions.</li> </ul>"},{"location":"changelog/#v040-2025-04-01","title":"[v0.4.0] - 2025-04-01","text":""},{"location":"changelog/#added_1","title":"Added","text":"<ul> <li>POI View Generators: Introduced a new module, generators, containing a base class for POI view generation.</li> <li>Expanded POI Support: Added new classes for generating POI views from:<ul> <li>Google Open Buildings</li> <li>Microsoft Global Buildings</li> <li>GHSL Settlement Model</li> <li>GHSL Built Surface</li> </ul> </li> <li>New Reader: Added read_gzipped_json_or_csv to handle compressed JSON/CSV files.</li> </ul>"},{"location":"changelog/#changed_1","title":"Changed","text":"<ul> <li>ADLSDataStore Enhancements: Updated methods to match LocalDataStore for improved consistency.</li> <li>Geo Processing Updates:<ul> <li>Improved convert_to_dataframe for more efficient data conversion.</li> <li>Enhanced annotate_with_admin_regions to improve spatial joins.</li> </ul> </li> <li>New TifProcessor Methods:<ul> <li>sample_by_polygons for polygon-based raster sampling.</li> <li>sample_multiple_tifs_by_coordinates &amp; sample_multiple_tifs_by_polygons to manage multi-raster sampling.</li> </ul> </li> <li>Fixed Global Config Handling: Resolved issues with handling configurations inside classes.</li> </ul>"},{"location":"changelog/#v032-2025-03-21","title":"[v0.3.2] - 2025-03-21","text":""},{"location":"changelog/#added_2","title":"Added","text":"<ul> <li>Added a method to efficiently assign unique IDs to features.</li> </ul>"},{"location":"changelog/#changed_2","title":"Changed","text":"<ul> <li>Enhanced logging for better debugging and clarity.</li> </ul>"},{"location":"changelog/#fixed_1","title":"Fixed","text":"<ul> <li>Minor bug fix in config.py</li> </ul>"},{"location":"changelog/#031-2025-03-20","title":"[0.3.1] - 2025-03-20","text":""},{"location":"changelog/#added_3","title":"Added","text":"<ul> <li>Enhanced AdminBoundaries handler with improved error handling for cases where administrative level data is unavailable for a country.</li> <li>Added pyproject.toml and setup.py, enabling pip install support for the package.</li> <li>Introduced a new method annotate_with_admin_regions in geo.py to perform spatial joins between input points and administrative boundaries (levels 1 and 2), handling conflicts where points intersect multiple admin regions.</li> </ul>"},{"location":"changelog/#removed","title":"Removed","text":"<ul> <li>Removed the utils module containing logger.py and integrated LOG_FORMAT and get_logger into config.py for a more streamlined logging approach.</li> </ul>"},{"location":"changelog/#030-2025-03-18","title":"[0.3.0] - 2025-03-18","text":""},{"location":"changelog/#added_4","title":"Added","text":"<ul> <li>Compression support in readers for improved efficiency  </li> <li>New GHSL data handler to manage GHSL dataset downloads  </li> </ul>"},{"location":"changelog/#fixed_2","title":"Fixed","text":"<ul> <li>Small fixes/improvements in Microsoft Buildings, Maxar, and Overture handlers  </li> </ul>"},{"location":"changelog/#v022-2025-03-12","title":"[v0.2.2] - 2025-03-12","text":"<ul> <li> <p>Refactored Handlers: Improved structure and performance of maxar_image.py, osm.py and overture.py to enhance geospatial data handling.</p> </li> <li> <p>Documentation Improvements:</p> <ul> <li>Updated index.md, advanced.md, and use-cases.md for better clarity.</li> <li>Added installation.md under docs/getting-started for setup guidance.</li> <li>Refined API documentation in docs/api/index.md.</li> </ul> </li> <li> <p>Configuration &amp; Setup Enhancements:     \u2022   Improved .gitignore to exclude unnecessary files.     \u2022   Updated mkdocs.yml for better documentation structuring.</p> </li> <li>Bug Fixes &amp; Minor Optimizations: Small fixes and improvements across the codebase for stability and maintainability.</li> </ul>"},{"location":"changelog/#v021-2025-02-28","title":"[v0.2.1] - 2025-02-28","text":""},{"location":"changelog/#added_5","title":"Added","text":"<ul> <li>Introduced WorldPopDownloader feature to handlers</li> <li>Refactored TifProcessor class for better performance</li> </ul>"},{"location":"changelog/#fixed_3","title":"Fixed","text":"<ul> <li>Minor bug fixes and performance improvements</li> </ul>"},{"location":"changelog/#v020-maxarimagedownloader-bug-fixes-2025-02-24","title":"[v0.2.0] - MaxarImageDownloader &amp; Bug Fixes - 2025-02-24","text":"<ul> <li>New Handler: MaxarImageDownloader for downloading Maxar images.</li> <li>Bug Fixes: Various improvements and bug fixes.</li> <li>Enhancements: Minor optimizations in handlers.</li> </ul>"},{"location":"changelog/#v011-2025-02-24","title":"[v0.1.1] - 2025-02-24","text":""},{"location":"changelog/#added_6","title":"Added","text":"<ul> <li>Local Data Store: Introduced a new local data store alongside ADLS to improve data storage and read/write functionality.</li> <li>Boundaries Handler: Added <code>boundaries.py</code>, a new handler that allows to read administrative boundaries from GADM.</li> </ul>"},{"location":"changelog/#changed_3","title":"Changed","text":"<ul> <li>Handler Refactoring: Refactored existing handlers to improve modularity and data handling.</li> <li>Configuration Management: Added <code>config.py</code> to manage paths, runtime settings, and environment variables.</li> </ul>"},{"location":"changelog/#removed_1","title":"Removed","text":"<ul> <li>Administrative Schema: Removed <code>administrative.py</code> since its functionality is now handled by the <code>boundaries</code> handler.</li> <li>Globals Module: Removed <code>globals.py</code> and replaced it with <code>config.py</code> for better configuration management.</li> </ul>"},{"location":"changelog/#updated-files","title":"Updated Files","text":"<ul> <li><code>config.py</code></li> <li><code>boundaries.py</code></li> <li><code>google_open_buildings.py</code></li> <li><code>mapbox_image.py</code></li> <li><code>microsoft_global_buildings.py</code></li> <li><code>ookla_speedtest.py</code></li> <li><code>mercator_tiles.py</code></li> <li><code>adls_data_store.py</code></li> <li><code>data_store.py</code></li> <li><code>local_data_store.py</code></li> <li><code>readers.py</code></li> <li><code>writers.py</code></li> <li><code>entity.py</code></li> </ul>"},{"location":"changelog/#v010-2025-02-07","title":"[v0.1.0] - 2025-02-07","text":""},{"location":"changelog/#added_7","title":"Added","text":"<ul> <li>New data handlers: <code>google_open_buildings.py</code>, <code>microsoft_global_buildings.py</code>, <code>overture.py</code>, <code>mapbox_image.py</code>, <code>osm.py</code></li> <li>Processing functions in <code>tif_processor.py</code>, <code>geo.py</code> and <code>transform.py</code></li> <li>Grid generation modules: <code>h3_tiles.py</code>, <code>mercator_tiles.py</code></li> <li>View managers: <code>grid_view.py</code> and <code>national_view.py</code></li> <li>Schemas: <code>administrative.py</code></li> </ul>"},{"location":"changelog/#changed_4","title":"Changed","text":"<ul> <li>Updated <code>requirements.txt</code> with new dependencies</li> <li>Improved logging and data storage mechanisms</li> </ul>"},{"location":"changelog/#removed_2","title":"Removed","text":"<ul> <li>Deprecated views: <code>h3_view.py</code>, <code>mercator_view.py</code></li> </ul>"},{"location":"contributing/","title":"Contribution Guidelines","text":"<p>Thank you for considering contributing to Giga! We value your input and aim to make the contribution process as accessible and transparent as possible. Whether you're interested in reporting bugs, discussing code, submitting fixes, proposing features, becoming a maintainer, or engaging with the Giga community, we welcome your involvement.</p>"},{"location":"contributing/#how-to-contribute-to-our-giga-project","title":"How to Contribute to our Giga Project?","text":"<ol> <li>Familiarize Yourself: Before contributing, familiarize yourself with the project by reviewing the README, code of conduct, and existing issues or pull requests.</li> <li>Issues and Feature Requests: Check the issue tracker for existing issues or create a new one to report bugs, suggest improvements, or propose new features.</li> <li>Fork and Branch: Fork the repository and create a branch for your contribution. Branch names should be descriptive (e.g., feature/add-new-functionality, bugfix/issue-description).</li> <li>Code Changes: Make changes or additions following our coding conventions and style guide. Ensure to write clear commit messages that explain the purpose of each commit.</li> <li>Testing: If applicable, include tests for the changes made to ensure code reliability. Ensure existing tests pass.</li> <li>Documentation: Update relevant documentation, including README files or any other necessary guides.</li> <li>Pull Request: Open a pull request (PR) against the main branch. Clearly describe the changes introduced, referencing any related issues.</li> </ol>"},{"location":"contributing/#report-a-bug-or-suggestion","title":"Report a Bug or Suggestion","text":"<ul> <li>Bug Reports: Help us understand and address issues by submitting detailed bug reports via GitHub issues. Include as many relevant details as possible in the provided template to expedite resolutions.</li> <li>Suggestions: Share your ideas, feedback, or stay updated on Giga by joining our Discord channel.</li> </ul>"},{"location":"contributing/#making-changes-and-pull-requests","title":"Making Changes and Pull Requests","text":"<p>To contribute code changes:</p> <ol> <li>Fork the repository and create a new branch for your contribution  <p><code>git checkout -b 'my-contribution'</code>.</p> </li> <li>Make your changes on the created branch.</li> <li>Commit with clear messages describing the updates.</li> <li>Submit a pull request in the main repository, ensuring the following:</li> <li>Clear use case or demonstration of bug fix/new feature.</li> <li>Inclusion of relevant tests (unit, functional, and fuzz tests where applicable).</li> <li>Adherence to code style guidelines.</li> <li>No breaking changes to the existing test suite.</li> <li>Bug fixes accompanied by tests to prevent regression.</li> <li>Update of relevant comments and documentation reflecting code behavior changes.</li> </ol>"},{"location":"contributing/#contributing-with-an-issue","title":"Contributing with an Issue","text":"<p>If you encounter a bug but aren't sure how to fix it or submit a pull request, you can create an issue. Issues serve as avenues for bug reports, feedback, and general discussions within the GigaSpatial GitHub repository.</p>"},{"location":"contributing/#other-ways-to-contribute","title":"Other Ways to Contribute","text":"<p>Beyond code contributions:</p> <ul> <li>Feedback and Insights: Share your expertise and experiences related to cash transfer by contacting us at giga@unicef.org.</li> <li>Documentation: Contribute to our journey by sharing reports, case studies, articles, blogs, or surveys. Contact us to contribute and learn more via giga@unicef.org.</li> <li>Designs: If you're passionate about UI/UX, animations, graphics, tutorials, etc., contact us to create visuals for the Giga community via giga@unicef.org.</li> </ul>"},{"location":"contributing/#connect-with-giga-contributors","title":"Connect with Giga Contributors","text":"<p>Connect with fellow contributors via our Discord channel to engage with the Giga community: Click</p>"},{"location":"license/","title":"License","text":"<pre><code>                GNU AFFERO GENERAL PUBLIC LICENSE\n                   Version 3, 19 November 2007\n</code></pre> <p>Copyright (C) 2007 Free Software Foundation, Inc. https://fsf.org/  Everyone is permitted to copy and distribute verbatim copies  of this license document, but changing it is not allowed.</p> <pre><code>                        Preamble\n</code></pre> <p>The GNU Affero General Public License is a free, copyleft license for software and other kinds of works, specifically designed to ensure cooperation with the community in the case of network server software.</p> <p>The licenses for most software and other practical works are designed to take away your freedom to share and change the works.  By contrast, our General Public Licenses are intended to guarantee your freedom to share and change all versions of a program--to make sure it remains free software for all its users.</p> <p>When we speak of free software, we are referring to freedom, not price.  Our General Public Licenses are designed to make sure that you have the freedom to distribute copies of free software (and charge for them if you wish), that you receive source code or can get it if you want it, that you can change the software or use pieces of it in new free programs, and that you know you can do these things.</p> <p>Developers that use our General Public Licenses protect your rights with two steps: (1) assert copyright on the software, and (2) offer you this License which gives you legal permission to copy, distribute and/or modify the software.</p> <p>A secondary benefit of defending all users' freedom is that improvements made in alternate versions of the program, if they receive widespread use, become available for other developers to incorporate.  Many developers of free software are heartened and encouraged by the resulting cooperation.  However, in the case of software used on network servers, this result may fail to come about. The GNU General Public License permits making a modified version and letting the public access it on a server without ever releasing its source code to the public.</p> <p>The GNU Affero General Public License is designed specifically to ensure that, in such cases, the modified source code becomes available to the community.  It requires the operator of a network server to provide the source code of the modified version running there to the users of that server.  Therefore, public use of a modified version, on a publicly accessible server, gives the public access to the source code of the modified version.</p> <p>An older license, called the Affero General Public License and published by Affero, was designed to accomplish similar goals.  This is a different license, not a version of the Affero GPL, but Affero has released a new version of the Affero GPL which permits relicensing under this license.</p> <p>The precise terms and conditions for copying, distribution and modification follow.</p> <pre><code>                   TERMS AND CONDITIONS\n</code></pre> <ol> <li>Definitions.</li> </ol> <p>\"This License\" refers to version 3 of the GNU Affero General Public License.</p> <p>\"Copyright\" also means copyright-like laws that apply to other kinds of works, such as semiconductor masks.</p> <p>\"The Program\" refers to any copyrightable work licensed under this License.  Each licensee is addressed as \"you\".  \"Licensees\" and \"recipients\" may be individuals or organizations.</p> <p>To \"modify\" a work means to copy from or adapt all or part of the work in a fashion requiring copyright permission, other than the making of an exact copy.  The resulting work is called a \"modified version\" of the earlier work or a work \"based on\" the earlier work.</p> <p>A \"covered work\" means either the unmodified Program or a work based on the Program.</p> <p>To \"propagate\" a work means to do anything with it that, without permission, would make you directly or secondarily liable for infringement under applicable copyright law, except executing it on a computer or modifying a private copy.  Propagation includes copying, distribution (with or without modification), making available to the public, and in some countries other activities as well.</p> <p>To \"convey\" a work means any kind of propagation that enables other parties to make or receive copies.  Mere interaction with a user through a computer network, with no transfer of a copy, is not conveying.</p> <p>An interactive user interface displays \"Appropriate Legal Notices\" to the extent that it includes a convenient and prominently visible feature that (1) displays an appropriate copyright notice, and (2) tells the user that there is no warranty for the work (except to the extent that warranties are provided), that licensees may convey the work under this License, and how to view a copy of this License.  If the interface presents a list of user commands or options, such as a menu, a prominent item in the list meets this criterion.</p> <ol> <li>Source Code.</li> </ol> <p>The \"source code\" for a work means the preferred form of the work for making modifications to it.  \"Object code\" means any non-source form of a work.</p> <p>A \"Standard Interface\" means an interface that either is an official standard defined by a recognized standards body, or, in the case of interfaces specified for a particular programming language, one that is widely used among developers working in that language.</p> <p>The \"System Libraries\" of an executable work include anything, other than the work as a whole, that (a) is included in the normal form of packaging a Major Component, but which is not part of that Major Component, and (b) serves only to enable use of the work with that Major Component, or to implement a Standard Interface for which an implementation is available to the public in source code form.  A \"Major Component\", in this context, means a major essential component (kernel, window system, and so on) of the specific operating system (if any) on which the executable work runs, or a compiler used to produce the work, or an object code interpreter used to run it.</p> <p>The \"Corresponding Source\" for a work in object code form means all the source code needed to generate, install, and (for an executable work) run the object code and to modify the work, including scripts to control those activities.  However, it does not include the work's System Libraries, or general-purpose tools or generally available free programs which are used unmodified in performing those activities but which are not part of the work.  For example, Corresponding Source includes interface definition files associated with source files for the work, and the source code for shared libraries and dynamically linked subprograms that the work is specifically designed to require, such as by intimate data communication or control flow between those subprograms and other parts of the work.</p> <p>The Corresponding Source need not include anything that users can regenerate automatically from other parts of the Corresponding Source.</p> <p>The Corresponding Source for a work in source code form is that same work.</p> <ol> <li>Basic Permissions.</li> </ol> <p>All rights granted under this License are granted for the term of copyright on the Program, and are irrevocable provided the stated conditions are met.  This License explicitly affirms your unlimited permission to run the unmodified Program.  The output from running a covered work is covered by this License only if the output, given its content, constitutes a covered work.  This License acknowledges your rights of fair use or other equivalent, as provided by copyright law.</p> <p>You may make, run and propagate covered works that you do not convey, without conditions so long as your license otherwise remains in force.  You may convey covered works to others for the sole purpose of having them make modifications exclusively for you, or provide you with facilities for running those works, provided that you comply with the terms of this License in conveying all material for which you do not control copyright.  Those thus making or running the covered works for you must do so exclusively on your behalf, under your direction and control, on terms that prohibit them from making any copies of your copyrighted material outside their relationship with you.</p> <p>Conveying under any other circumstances is permitted solely under the conditions stated below.  Sublicensing is not allowed; section 10 makes it unnecessary.</p> <ol> <li>Protecting Users' Legal Rights From Anti-Circumvention Law.</li> </ol> <p>No covered work shall be deemed part of an effective technological measure under any applicable law fulfilling obligations under article 11 of the WIPO copyright treaty adopted on 20 December 1996, or similar laws prohibiting or restricting circumvention of such measures.</p> <p>When you convey a covered work, you waive any legal power to forbid circumvention of technological measures to the extent such circumvention is effected by exercising rights under this License with respect to the covered work, and you disclaim any intention to limit operation or modification of the work as a means of enforcing, against the work's users, your or third parties' legal rights to forbid circumvention of technological measures.</p> <ol> <li>Conveying Verbatim Copies.</li> </ol> <p>You may convey verbatim copies of the Program's source code as you receive it, in any medium, provided that you conspicuously and appropriately publish on each copy an appropriate copyright notice; keep intact all notices stating that this License and any non-permissive terms added in accord with section 7 apply to the code; keep intact all notices of the absence of any warranty; and give all recipients a copy of this License along with the Program.</p> <p>You may charge any price or no price for each copy that you convey, and you may offer support or warranty protection for a fee.</p> <ol> <li>Conveying Modified Source Versions.</li> </ol> <p>You may convey a work based on the Program, or the modifications to produce it from the Program, in the form of source code under the terms of section 4, provided that you also meet all of these conditions:</p> <pre><code>a) The work must carry prominent notices stating that you modified\nit, and giving a relevant date.\n\nb) The work must carry prominent notices stating that it is\nreleased under this License and any conditions added under section\n7.  This requirement modifies the requirement in section 4 to\n\"keep intact all notices\".\n\nc) You must license the entire work, as a whole, under this\nLicense to anyone who comes into possession of a copy.  This\nLicense will therefore apply, along with any applicable section 7\nadditional terms, to the whole of the work, and all its parts,\nregardless of how they are packaged.  This License gives no\npermission to license the work in any other way, but it does not\ninvalidate such permission if you have separately received it.\n\nd) If the work has interactive user interfaces, each must display\nAppropriate Legal Notices; however, if the Program has interactive\ninterfaces that do not display Appropriate Legal Notices, your\nwork need not make them do so.\n</code></pre> <p>A compilation of a covered work with other separate and independent works, which are not by their nature extensions of the covered work, and which are not combined with it such as to form a larger program, in or on a volume of a storage or distribution medium, is called an \"aggregate\" if the compilation and its resulting copyright are not used to limit the access or legal rights of the compilation's users beyond what the individual works permit.  Inclusion of a covered work in an aggregate does not cause this License to apply to the other parts of the aggregate.</p> <ol> <li>Conveying Non-Source Forms.</li> </ol> <p>You may convey a covered work in object code form under the terms of sections 4 and 5, provided that you also convey the machine-readable Corresponding Source under the terms of this License, in one of these ways:</p> <pre><code>a) Convey the object code in, or embodied in, a physical product\n(including a physical distribution medium), accompanied by the\nCorresponding Source fixed on a durable physical medium\ncustomarily used for software interchange.\n\nb) Convey the object code in, or embodied in, a physical product\n(including a physical distribution medium), accompanied by a\nwritten offer, valid for at least three years and valid for as\nlong as you offer spare parts or customer support for that product\nmodel, to give anyone who possesses the object code either (1) a\ncopy of the Corresponding Source for all the software in the\nproduct that is covered by this License, on a durable physical\nmedium customarily used for software interchange, for a price no\nmore than your reasonable cost of physically performing this\nconveying of source, or (2) access to copy the\nCorresponding Source from a network server at no charge.\n\nc) Convey individual copies of the object code with a copy of the\nwritten offer to provide the Corresponding Source.  This\nalternative is allowed only occasionally and noncommercially, and\nonly if you received the object code with such an offer, in accord\nwith subsection 6b.\n\nd) Convey the object code by offering access from a designated\nplace (gratis or for a charge), and offer equivalent access to the\nCorresponding Source in the same way through the same place at no\nfurther charge.  You need not require recipients to copy the\nCorresponding Source along with the object code.  If the place to\ncopy the object code is a network server, the Corresponding Source\nmay be on a different server (operated by you or a third party)\nthat supports equivalent copying facilities, provided you maintain\nclear directions next to the object code saying where to find the\nCorresponding Source.  Regardless of what server hosts the\nCorresponding Source, you remain obligated to ensure that it is\navailable for as long as needed to satisfy these requirements.\n\ne) Convey the object code using peer-to-peer transmission, provided\nyou inform other peers where the object code and Corresponding\nSource of the work are being offered to the general public at no\ncharge under subsection 6d.\n</code></pre> <p>A separable portion of the object code, whose source code is excluded from the Corresponding Source as a System Library, need not be included in conveying the object code work.</p> <p>A \"User Product\" is either (1) a \"consumer product\", which means any tangible personal property which is normally used for personal, family, or household purposes, or (2) anything designed or sold for incorporation into a dwelling.  In determining whether a product is a consumer product, doubtful cases shall be resolved in favor of coverage.  For a particular product received by a particular user, \"normally used\" refers to a typical or common use of that class of product, regardless of the status of the particular user or of the way in which the particular user actually uses, or expects or is expected to use, the product.  A product is a consumer product regardless of whether the product has substantial commercial, industrial or non-consumer uses, unless such uses represent the only significant mode of use of the product.</p> <p>\"Installation Information\" for a User Product means any methods, procedures, authorization keys, or other information required to install and execute modified versions of a covered work in that User Product from a modified version of its Corresponding Source.  The information must suffice to ensure that the continued functioning of the modified object code is in no case prevented or interfered with solely because modification has been made.</p> <p>If you convey an object code work under this section in, or with, or specifically for use in, a User Product, and the conveying occurs as part of a transaction in which the right of possession and use of the User Product is transferred to the recipient in perpetuity or for a fixed term (regardless of how the transaction is characterized), the Corresponding Source conveyed under this section must be accompanied by the Installation Information.  But this requirement does not apply if neither you nor any third party retains the ability to install modified object code on the User Product (for example, the work has been installed in ROM).</p> <p>The requirement to provide Installation Information does not include a requirement to continue to provide support service, warranty, or updates for a work that has been modified or installed by the recipient, or for the User Product in which it has been modified or installed.  Access to a network may be denied when the modification itself materially and adversely affects the operation of the network or violates the rules and protocols for communication across the network.</p> <p>Corresponding Source conveyed, and Installation Information provided, in accord with this section must be in a format that is publicly documented (and with an implementation available to the public in source code form), and must require no special password or key for unpacking, reading or copying.</p> <ol> <li>Additional Terms.</li> </ol> <p>\"Additional permissions\" are terms that supplement the terms of this License by making exceptions from one or more of its conditions. Additional permissions that are applicable to the entire Program shall be treated as though they were included in this License, to the extent that they are valid under applicable law.  If additional permissions apply only to part of the Program, that part may be used separately under those permissions, but the entire Program remains governed by this License without regard to the additional permissions.</p> <p>When you convey a copy of a covered work, you may at your option remove any additional permissions from that copy, or from any part of it.  (Additional permissions may be written to require their own removal in certain cases when you modify the work.)  You may place additional permissions on material, added by you to a covered work, for which you have or can give appropriate copyright permission.</p> <p>Notwithstanding any other provision of this License, for material you add to a covered work, you may (if authorized by the copyright holders of that material) supplement the terms of this License with terms:</p> <pre><code>a) Disclaiming warranty or limiting liability differently from the\nterms of sections 15 and 16 of this License; or\n\nb) Requiring preservation of specified reasonable legal notices or\nauthor attributions in that material or in the Appropriate Legal\nNotices displayed by works containing it; or\n\nc) Prohibiting misrepresentation of the origin of that material, or\nrequiring that modified versions of such material be marked in\nreasonable ways as different from the original version; or\n\nd) Limiting the use for publicity purposes of names of licensors or\nauthors of the material; or\n\ne) Declining to grant rights under trademark law for use of some\ntrade names, trademarks, or service marks; or\n\nf) Requiring indemnification of licensors and authors of that\nmaterial by anyone who conveys the material (or modified versions of\nit) with contractual assumptions of liability to the recipient, for\nany liability that these contractual assumptions directly impose on\nthose licensors and authors.\n</code></pre> <p>All other non-permissive additional terms are considered \"further restrictions\" within the meaning of section 10.  If the Program as you received it, or any part of it, contains a notice stating that it is governed by this License along with a term that is a further restriction, you may remove that term.  If a license document contains a further restriction but permits relicensing or conveying under this License, you may add to a covered work material governed by the terms of that license document, provided that the further restriction does not survive such relicensing or conveying.</p> <p>If you add terms to a covered work in accord with this section, you must place, in the relevant source files, a statement of the additional terms that apply to those files, or a notice indicating where to find the applicable terms.</p> <p>Additional terms, permissive or non-permissive, may be stated in the form of a separately written license, or stated as exceptions; the above requirements apply either way.</p> <ol> <li>Termination.</li> </ol> <p>You may not propagate or modify a covered work except as expressly provided under this License.  Any attempt otherwise to propagate or modify it is void, and will automatically terminate your rights under this License (including any patent licenses granted under the third paragraph of section 11).</p> <p>However, if you cease all violation of this License, then your license from a particular copyright holder is reinstated (a) provisionally, unless and until the copyright holder explicitly and finally terminates your license, and (b) permanently, if the copyright holder fails to notify you of the violation by some reasonable means prior to 60 days after the cessation.</p> <p>Moreover, your license from a particular copyright holder is reinstated permanently if the copyright holder notifies you of the violation by some reasonable means, this is the first time you have received notice of violation of this License (for any work) from that copyright holder, and you cure the violation prior to 30 days after your receipt of the notice.</p> <p>Termination of your rights under this section does not terminate the licenses of parties who have received copies or rights from you under this License.  If your rights have been terminated and not permanently reinstated, you do not qualify to receive new licenses for the same material under section 10.</p> <ol> <li>Acceptance Not Required for Having Copies.</li> </ol> <p>You are not required to accept this License in order to receive or run a copy of the Program.  Ancillary propagation of a covered work occurring solely as a consequence of using peer-to-peer transmission to receive a copy likewise does not require acceptance.  However, nothing other than this License grants you permission to propagate or modify any covered work.  These actions infringe copyright if you do not accept this License.  Therefore, by modifying or propagating a covered work, you indicate your acceptance of this License to do so.</p> <ol> <li>Automatic Licensing of Downstream Recipients.</li> </ol> <p>Each time you convey a covered work, the recipient automatically receives a license from the original licensors, to run, modify and propagate that work, subject to this License.  You are not responsible for enforcing compliance by third parties with this License.</p> <p>An \"entity transaction\" is a transaction transferring control of an organization, or substantially all assets of one, or subdividing an organization, or merging organizations.  If propagation of a covered work results from an entity transaction, each party to that transaction who receives a copy of the work also receives whatever licenses to the work the party's predecessor in interest had or could give under the previous paragraph, plus a right to possession of the Corresponding Source of the work from the predecessor in interest, if the predecessor has it or can get it with reasonable efforts.</p> <p>You may not impose any further restrictions on the exercise of the rights granted or affirmed under this License.  For example, you may not impose a license fee, royalty, or other charge for exercise of rights granted under this License, and you may not initiate litigation (including a cross-claim or counterclaim in a lawsuit) alleging that any patent claim is infringed by making, using, selling, offering for sale, or importing the Program or any portion of it.</p> <ol> <li>Patents.</li> </ol> <p>A \"contributor\" is a copyright holder who authorizes use under this License of the Program or a work on which the Program is based.  The work thus licensed is called the contributor's \"contributor version\".</p> <p>A contributor's \"essential patent claims\" are all patent claims owned or controlled by the contributor, whether already acquired or hereafter acquired, that would be infringed by some manner, permitted by this License, of making, using, or selling its contributor version, but do not include claims that would be infringed only as a consequence of further modification of the contributor version.  For purposes of this definition, \"control\" includes the right to grant patent sublicenses in a manner consistent with the requirements of this License.</p> <p>Each contributor grants you a non-exclusive, worldwide, royalty-free patent license under the contributor's essential patent claims, to make, use, sell, offer for sale, import and otherwise run, modify and propagate the contents of its contributor version.</p> <p>In the following three paragraphs, a \"patent license\" is any express agreement or commitment, however denominated, not to enforce a patent (such as an express permission to practice a patent or covenant not to sue for patent infringement).  To \"grant\" such a patent license to a party means to make such an agreement or commitment not to enforce a patent against the party.</p> <p>If you convey a covered work, knowingly relying on a patent license, and the Corresponding Source of the work is not available for anyone to copy, free of charge and under the terms of this License, through a publicly available network server or other readily accessible means, then you must either (1) cause the Corresponding Source to be so available, or (2) arrange to deprive yourself of the benefit of the patent license for this particular work, or (3) arrange, in a manner consistent with the requirements of this License, to extend the patent license to downstream recipients.  \"Knowingly relying\" means you have actual knowledge that, but for the patent license, your conveying the covered work in a country, or your recipient's use of the covered work in a country, would infringe one or more identifiable patents in that country that you have reason to believe are valid.</p> <p>If, pursuant to or in connection with a single transaction or arrangement, you convey, or propagate by procuring conveyance of, a covered work, and grant a patent license to some of the parties receiving the covered work authorizing them to use, propagate, modify or convey a specific copy of the covered work, then the patent license you grant is automatically extended to all recipients of the covered work and works based on it.</p> <p>A patent license is \"discriminatory\" if it does not include within the scope of its coverage, prohibits the exercise of, or is conditioned on the non-exercise of one or more of the rights that are specifically granted under this License.  You may not convey a covered work if you are a party to an arrangement with a third party that is in the business of distributing software, under which you make payment to the third party based on the extent of your activity of conveying the work, and under which the third party grants, to any of the parties who would receive the covered work from you, a discriminatory patent license (a) in connection with copies of the covered work conveyed by you (or copies made from those copies), or (b) primarily for and in connection with specific products or compilations that contain the covered work, unless you entered into that arrangement, or that patent license was granted, prior to 28 March 2007.</p> <p>Nothing in this License shall be construed as excluding or limiting any implied license or other defenses to infringement that may otherwise be available to you under applicable patent law.</p> <ol> <li>No Surrender of Others' Freedom.</li> </ol> <p>If conditions are imposed on you (whether by court order, agreement or otherwise) that contradict the conditions of this License, they do not excuse you from the conditions of this License.  If you cannot convey a covered work so as to satisfy simultaneously your obligations under this License and any other pertinent obligations, then as a consequence you may not convey it at all.  For example, if you agree to terms that obligate you to collect a royalty for further conveying from those to whom you convey the Program, the only way you could satisfy both those terms and this License would be to refrain entirely from conveying the Program.</p> <ol> <li>Remote Network Interaction; Use with the GNU General Public License.</li> </ol> <p>Notwithstanding any other provision of this License, if you modify the Program, your modified version must prominently offer all users interacting with it remotely through a computer network (if your version supports such interaction) an opportunity to receive the Corresponding Source of your version by providing access to the Corresponding Source from a network server at no charge, through some standard or customary means of facilitating copying of software.  This Corresponding Source shall include the Corresponding Source for any work covered by version 3 of the GNU General Public License that is incorporated pursuant to the following paragraph.</p> <p>Notwithstanding any other provision of this License, you have permission to link or combine any covered work with a work licensed under version 3 of the GNU General Public License into a single combined work, and to convey the resulting work.  The terms of this License will continue to apply to the part which is the covered work, but the work with which it is combined will remain governed by version 3 of the GNU General Public License.</p> <ol> <li>Revised Versions of this License.</li> </ol> <p>The Free Software Foundation may publish revised and/or new versions of the GNU Affero General Public License from time to time.  Such new versions will be similar in spirit to the present version, but may differ in detail to address new problems or concerns.</p> <p>Each version is given a distinguishing version number.  If the Program specifies that a certain numbered version of the GNU Affero General Public License \"or any later version\" applies to it, you have the option of following the terms and conditions either of that numbered version or of any later version published by the Free Software Foundation.  If the Program does not specify a version number of the GNU Affero General Public License, you may choose any version ever published by the Free Software Foundation.</p> <p>If the Program specifies that a proxy can decide which future versions of the GNU Affero General Public License can be used, that proxy's public statement of acceptance of a version permanently authorizes you to choose that version for the Program.</p> <p>Later license versions may give you additional or different permissions.  However, no additional obligations are imposed on any author or copyright holder as a result of your choosing to follow a later version.</p> <ol> <li>Disclaimer of Warranty.</li> </ol> <p>THERE IS NO WARRANTY FOR THE PROGRAM, TO THE EXTENT PERMITTED BY APPLICABLE LAW.  EXCEPT WHEN OTHERWISE STATED IN WRITING THE COPYRIGHT HOLDERS AND/OR OTHER PARTIES PROVIDE THE PROGRAM \"AS IS\" WITHOUT WARRANTY OF ANY KIND, EITHER EXPRESSED OR IMPLIED, INCLUDING, BUT NOT LIMITED TO, THE IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE.  THE ENTIRE RISK AS TO THE QUALITY AND PERFORMANCE OF THE PROGRAM IS WITH YOU.  SHOULD THE PROGRAM PROVE DEFECTIVE, YOU ASSUME THE COST OF ALL NECESSARY SERVICING, REPAIR OR CORRECTION.</p> <ol> <li>Limitation of Liability.</li> </ol> <p>IN NO EVENT UNLESS REQUIRED BY APPLICABLE LAW OR AGREED TO IN WRITING WILL ANY COPYRIGHT HOLDER, OR ANY OTHER PARTY WHO MODIFIES AND/OR CONVEYS THE PROGRAM AS PERMITTED ABOVE, BE LIABLE TO YOU FOR DAMAGES, INCLUDING ANY GENERAL, SPECIAL, INCIDENTAL OR CONSEQUENTIAL DAMAGES ARISING OUT OF THE USE OR INABILITY TO USE THE PROGRAM (INCLUDING BUT NOT LIMITED TO LOSS OF DATA OR DATA BEING RENDERED INACCURATE OR LOSSES SUSTAINED BY YOU OR THIRD PARTIES OR A FAILURE OF THE PROGRAM TO OPERATE WITH ANY OTHER PROGRAMS), EVEN IF SUCH HOLDER OR OTHER PARTY HAS BEEN ADVISED OF THE POSSIBILITY OF SUCH DAMAGES.</p> <ol> <li>Interpretation of Sections 15 and 16.</li> </ol> <p>If the disclaimer of warranty and limitation of liability provided above cannot be given local legal effect according to their terms, reviewing courts shall apply local law that most closely approximates an absolute waiver of all civil liability in connection with the Program, unless a warranty or assumption of liability accompanies a copy of the Program in return for a fee.</p> <pre><code>                 END OF TERMS AND CONDITIONS\n\n        How to Apply These Terms to Your New Programs\n</code></pre> <p>If you develop a new program, and you want it to be of the greatest possible use to the public, the best way to achieve this is to make it free software which everyone can redistribute and change under these terms.</p> <p>To do so, attach the following notices to the program.  It is safest to attach them to the start of each source file to most effectively state the exclusion of warranty; and each file should have at least the \"copyright\" line and a pointer to where the full notice is found.</p> <pre><code>&lt;one line to give the program's name and a brief idea of what it does.&gt;\nCopyright (C) &lt;year&gt;  &lt;name of author&gt;\n\nThis program is free software: you can redistribute it and/or modify\nit under the terms of the GNU Affero General Public License as published\nby the Free Software Foundation, either version 3 of the License, or\n(at your option) any later version.\n\nThis program is distributed in the hope that it will be useful,\nbut WITHOUT ANY WARRANTY; without even the implied warranty of\nMERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the\nGNU Affero General Public License for more details.\n\nYou should have received a copy of the GNU Affero General Public License\nalong with this program.  If not, see &lt;https://www.gnu.org/licenses/&gt;.\n</code></pre> <p>Also add information on how to contact you by electronic and paper mail.</p> <p>If your software can interact with users remotely through a computer network, you should also make sure that it provides a way for users to get its source.  For example, if your program is a web application, its interface could display a \"Source\" link that leads users to an archive of the code.  There are many ways you could offer source, and different solutions will be better for different programs; see section 13 for the specific requirements.</p> <p>You should also get your employer (if you work as a programmer) or school, if any, to sign a \"copyright disclaimer\" for the program, if necessary. For more information on this, and how to apply and follow the GNU AGPL, see https://www.gnu.org/licenses/.</p>"},{"location":"api/","title":"API Reference","text":"<p>Welcome to the API reference for the <code>gigaspatial</code> package. This documentation provides detailed information about the modules, classes, and functions available in the package.</p>"},{"location":"api/#modules","title":"Modules","text":"<p>The <code>gigaspatial</code> package is organized into several modules, each serving a specific purpose:</p>"},{"location":"api/#1-handlers","title":"1. Handlers","text":"<p>The <code>handlers</code> module contains classes for downloading and processing geospatial data from various sources, such as OpenStreetMap (OSM) and the Global Human Settlement Layer (GHSL).</p> <ul> <li>OSMLocationFetcher: Fetches and processes location data from OpenStreetMap.</li> <li>GHSLDataDownloader: Downloads and processes data from the Global Human Settlement Layer.</li> </ul> <p>Learn more about the Handlers module</p>"},{"location":"api/#2-processing","title":"2. Processing","text":"<p>The <code>processing</code> module provides tools for processing geospatial data, such as GeoTIFF files.</p> <ul> <li>TifProcessor: Processes GeoTIFF files and extracts relevant data.</li> </ul> <p>Learn more about the Processing module</p>"},{"location":"api/#3-core","title":"3. Core","text":"<p>The <code>core</code> module contains essential utilities and base classes used throughout the package.</p> <ul> <li>DataStore: Handles the storage and retrieval of geospatial data in various formats.</li> <li>Config: Manages configuration settings, such as paths and API keys.</li> </ul> <p>Learn more about the Core module</p>"},{"location":"api/#4-generators","title":"4. Generators","text":"<p>The <code>generators</code> module includes tools for generating geospatial data, such as grids and synthetic datasets.</p> <p>Learn more about the Generators module</p>"},{"location":"api/#5-grid","title":"5. Grid","text":"<p>The <code>grid</code> module provides utilities for working with geospatial grids, such as creating and manipulating grid-based data.</p> <p>Learn more about the Grid module</p>"},{"location":"api/#getting-started","title":"Getting Started","text":"<p>To get started with the <code>gigaspatial</code> package, follow the Quick Start Guide.</p>"},{"location":"api/#additional-resources","title":"Additional Resources","text":"<ul> <li>Examples: Real-world examples and use cases.</li> <li>Changelog: Information about the latest updates and changes.</li> <li>Contributing: Guidelines for contributing to the project.</li> </ul>"},{"location":"api/#support","title":"Support","text":"<p>If you encounter any issues or have questions, feel free to open an issue or join our Discord community.</p>"},{"location":"api/core/","title":"Core Module","text":""},{"location":"api/core/#gigaspatial.core","title":"<code>gigaspatial.core</code>","text":""},{"location":"api/core/#gigaspatial.core.io","title":"<code>io</code>","text":""},{"location":"api/core/#gigaspatial.core.io.adls_data_store","title":"<code>adls_data_store</code>","text":""},{"location":"api/core/#gigaspatial.core.io.adls_data_store.ADLSDataStore","title":"<code>ADLSDataStore</code>","text":"<p>               Bases: <code>DataStore</code></p> <p>An implementation of DataStore for Azure Data Lake Storage.</p> Source code in <code>gigaspatial/core/io/adls_data_store.py</code> <pre><code>class ADLSDataStore(DataStore):\n    \"\"\"\n    An implementation of DataStore for Azure Data Lake Storage.\n    \"\"\"\n\n    def __init__(\n        self,\n        container: str = config.ADLS_CONTAINER_NAME,\n        connection_string: str = config.ADLS_CONNECTION_STRING,\n    ):\n        \"\"\"\n        Create a new instance of ADLSDataStore\n        :param container: The name of the container in ADLS to interact with.\n        \"\"\"\n        self.blob_service_client = BlobServiceClient.from_connection_string(\n            connection_string\n        )\n        self.container_client = self.blob_service_client.get_container_client(\n            container=container\n        )\n        self.container = container\n\n    def read_file(self, path: str, encoding: Optional[str] = None) -&gt; Union[str, bytes]:\n        \"\"\"\n        Read file with flexible encoding support.\n\n        :param path: Path to the file in blob storage\n        :param encoding: File encoding (optional)\n        :return: File contents as string or bytes\n        \"\"\"\n        try:\n            blob_client = self.container_client.get_blob_client(path)\n            blob_data = blob_client.download_blob().readall()\n\n            # If no encoding specified, return raw bytes\n            if encoding is None:\n                return blob_data\n\n            # If encoding is specified, decode the bytes\n            return blob_data.decode(encoding)\n\n        except Exception as e:\n            raise IOError(f\"Error reading file {path}: {e}\")\n\n    def write_file(self, path: str, data) -&gt; None:\n        \"\"\"\n        Write file with support for content type and improved type handling.\n\n        :param path: Destination path in blob storage\n        :param data: File contents\n        \"\"\"\n        blob_client = self.blob_service_client.get_blob_client(\n            container=self.container, blob=path, snapshot=None\n        )\n\n        if isinstance(data, str):\n            binary_data = data.encode()\n        elif isinstance(data, bytes):\n            binary_data = data\n        else:\n            raise Exception(f'Unsupported data type. Only \"bytes\" or \"string\" accepted')\n\n        blob_client.upload_blob(binary_data, overwrite=True)\n\n    def upload_file(self, file_path, blob_path):\n        \"\"\"Uploads a single file to Azure Blob Storage.\"\"\"\n        try:\n            blob_client = self.container_client.get_blob_client(blob_path)\n            with open(file_path, \"rb\") as data:\n                blob_client.upload_blob(data, overwrite=True)\n            print(f\"Uploaded {file_path} to {blob_path}\")\n        except Exception as e:\n            print(f\"Failed to upload {file_path}: {e}\")\n\n    def upload_directory(self, dir_path, blob_dir_path):\n        \"\"\"Uploads all files from a directory to Azure Blob Storage.\"\"\"\n        for root, dirs, files in os.walk(dir_path):\n            for file in files:\n                local_file_path = os.path.join(root, file)\n                relative_path = os.path.relpath(local_file_path, dir_path)\n                blob_file_path = os.path.join(blob_dir_path, relative_path).replace(\n                    \"\\\\\", \"/\"\n                )\n\n                self.upload_file(local_file_path, blob_file_path)\n\n    def download_directory(self, blob_dir_path: str, local_dir_path: str):\n        \"\"\"Downloads all files from a directory in Azure Blob Storage to a local directory.\"\"\"\n        try:\n            # Ensure the local directory exists\n            os.makedirs(local_dir_path, exist_ok=True)\n\n            # List all files in the blob directory\n            blob_items = self.container_client.list_blobs(\n                name_starts_with=blob_dir_path\n            )\n\n            for blob_item in blob_items:\n                # Get the relative path of the blob file\n                relative_path = os.path.relpath(blob_item.name, blob_dir_path)\n                # Construct the local file path\n                local_file_path = os.path.join(local_dir_path, relative_path)\n                # Create directories if needed\n                os.makedirs(os.path.dirname(local_file_path), exist_ok=True)\n\n                # Download the blob to the local file\n                blob_client = self.container_client.get_blob_client(blob_item.name)\n                with open(local_file_path, \"wb\") as file:\n                    file.write(blob_client.download_blob().readall())\n\n            print(f\"Downloaded directory {blob_dir_path} to {local_dir_path}\")\n        except Exception as e:\n            print(f\"Failed to download directory {blob_dir_path}: {e}\")\n\n    def copy_directory(self, source_dir: str, destination_dir: str):\n        \"\"\"\n        Copies all files from a source directory to a destination directory within the same container.\n\n        :param source_dir: The source directory path in the blob storage\n        :param destination_dir: The destination directory path in the blob storage\n        \"\"\"\n        try:\n            # Ensure source directory path ends with a trailing slash\n            source_dir = source_dir.rstrip(\"/\") + \"/\"\n            destination_dir = destination_dir.rstrip(\"/\") + \"/\"\n\n            # List all blobs in the source directory\n            source_blobs = self.container_client.list_blobs(name_starts_with=source_dir)\n\n            for blob in source_blobs:\n                # Get the relative path of the blob\n                relative_path = os.path.relpath(blob.name, source_dir)\n\n                # Construct the new blob path\n                new_blob_path = os.path.join(destination_dir, relative_path).replace(\n                    \"\\\\\", \"/\"\n                )\n\n                # Create a source blob client\n                source_blob_client = self.container_client.get_blob_client(blob.name)\n\n                # Create a destination blob client\n                destination_blob_client = self.container_client.get_blob_client(\n                    new_blob_path\n                )\n\n                # Start the copy operation\n                destination_blob_client.start_copy_from_url(source_blob_client.url)\n\n            print(f\"Copied directory from {source_dir} to {destination_dir}\")\n        except Exception as e:\n            print(f\"Failed to copy directory {source_dir}: {e}\")\n\n    def exists(self, path: str) -&gt; bool:\n        blob_client = self.blob_service_client.get_blob_client(\n            container=self.container, blob=path, snapshot=None\n        )\n        return blob_client.exists()\n\n    def file_exists(self, path: str) -&gt; bool:\n        return self.exists(path) and not self.is_dir(path)\n\n    def file_size(self, path: str) -&gt; float:\n        blob_client = self.blob_service_client.get_blob_client(\n            container=self.container, blob=path, snapshot=None\n        )\n        properties = blob_client.get_blob_properties()\n\n        # The size is in bytes, convert it to kilobytes\n        size_in_bytes = properties.size\n        size_in_kb = size_in_bytes / 1024.0\n        return size_in_kb\n\n    def list_files(self, path: str):\n        blob_items = self.container_client.list_blobs(name_starts_with=path)\n        return [item[\"name\"] for item in blob_items]\n\n    def walk(self, top: str):\n        top = top.rstrip(\"/\") + \"/\"\n        blob_items = self.container_client.list_blobs(name_starts_with=top)\n        blobs = [item[\"name\"] for item in blob_items]\n        for blob in blobs:\n            dirpath, filename = os.path.split(blob)\n            yield (dirpath, [], [filename])\n\n    @contextlib.contextmanager\n    def open(self, path: str, mode: str = \"r\"):\n        \"\"\"\n        Context manager for file operations with enhanced mode support.\n\n        :param path: File path in blob storage\n        :param mode: File open mode (r, rb, w, wb)\n        \"\"\"\n        if mode == \"w\":\n            file = io.StringIO()\n            yield file\n            self.write_file(path, file.getvalue())\n\n        elif mode == \"wb\":\n            file = io.BytesIO()\n            yield file\n            self.write_file(path, file.getvalue())\n\n        elif mode == \"r\":\n            data = self.read_file(path, encoding=\"UTF-8\")\n            file = io.StringIO(data)\n            yield file\n\n        elif mode == \"rb\":\n            data = self.read_file(path)\n            file = io.BytesIO(data)\n            yield file\n\n    def get_file_metadata(self, path: str) -&gt; dict:\n        \"\"\"\n        Retrieve comprehensive file metadata.\n\n        :param path: File path in blob storage\n        :return: File metadata dictionary\n        \"\"\"\n        blob_client = self.container_client.get_blob_client(path)\n        properties = blob_client.get_blob_properties()\n\n        return {\n            \"name\": path,\n            \"size_bytes\": properties.size,\n            \"content_type\": properties.content_settings.content_type,\n            \"last_modified\": properties.last_modified,\n            \"etag\": properties.etag,\n        }\n\n    def is_file(self, path: str) -&gt; bool:\n        return self.file_exists(path)\n\n    def is_dir(self, path: str) -&gt; bool:\n        dir_path = path.rstrip(\"/\") + \"/\"\n\n        existing_blobs = self.list_files(dir_path)\n\n        if len(existing_blobs) &gt; 1:\n            return True\n        elif len(existing_blobs) == 1:\n            if existing_blobs[0] != path.rstrip(\"/\"):\n                return True\n\n        return False\n\n    def rmdir(self, dir: str) -&gt; None:\n        blobs = self.list_files(dir)\n        self.container_client.delete_blobs(*blobs)\n\n    def mkdir(self, path: str, exist_ok: bool = False) -&gt; None:\n        \"\"\"\n        Create a directory in Azure Blob Storage.\n\n        In ADLS, directories are conceptual and created by adding a placeholder blob.\n\n        :param path: Path of the directory to create\n        :param exist_ok: If False, raise an error if the directory already exists\n        \"\"\"\n        dir_path = path.rstrip(\"/\") + \"/\"\n\n        existing_blobs = list(self.list_files(dir_path))\n\n        if existing_blobs and not exist_ok:\n            raise FileExistsError(f\"Directory {path} already exists\")\n\n        # Create a placeholder blob to represent the directory\n        placeholder_blob_path = os.path.join(dir_path, \".placeholder\")\n\n        # Only create placeholder if it doesn't already exist\n        if not self.file_exists(placeholder_blob_path):\n            placeholder_content = (\n                b\"This is a placeholder blob to represent a directory.\"\n            )\n            blob_client = self.blob_service_client.get_blob_client(\n                container=self.container, blob=placeholder_blob_path\n            )\n            blob_client.upload_blob(placeholder_content, overwrite=True)\n\n    def remove(self, path: str) -&gt; None:\n        blob_client = self.blob_service_client.get_blob_client(\n            container=self.container, blob=path, snapshot=None\n        )\n        if blob_client.exists():\n            blob_client.delete_blob()\n</code></pre>"},{"location":"api/core/#gigaspatial.core.io.adls_data_store.ADLSDataStore.__init__","title":"<code>__init__(container=config.ADLS_CONTAINER_NAME, connection_string=config.ADLS_CONNECTION_STRING)</code>","text":"<p>Create a new instance of ADLSDataStore :param container: The name of the container in ADLS to interact with.</p> Source code in <code>gigaspatial/core/io/adls_data_store.py</code> <pre><code>def __init__(\n    self,\n    container: str = config.ADLS_CONTAINER_NAME,\n    connection_string: str = config.ADLS_CONNECTION_STRING,\n):\n    \"\"\"\n    Create a new instance of ADLSDataStore\n    :param container: The name of the container in ADLS to interact with.\n    \"\"\"\n    self.blob_service_client = BlobServiceClient.from_connection_string(\n        connection_string\n    )\n    self.container_client = self.blob_service_client.get_container_client(\n        container=container\n    )\n    self.container = container\n</code></pre>"},{"location":"api/core/#gigaspatial.core.io.adls_data_store.ADLSDataStore.copy_directory","title":"<code>copy_directory(source_dir, destination_dir)</code>","text":"<p>Copies all files from a source directory to a destination directory within the same container.</p> <p>:param source_dir: The source directory path in the blob storage :param destination_dir: The destination directory path in the blob storage</p> Source code in <code>gigaspatial/core/io/adls_data_store.py</code> <pre><code>def copy_directory(self, source_dir: str, destination_dir: str):\n    \"\"\"\n    Copies all files from a source directory to a destination directory within the same container.\n\n    :param source_dir: The source directory path in the blob storage\n    :param destination_dir: The destination directory path in the blob storage\n    \"\"\"\n    try:\n        # Ensure source directory path ends with a trailing slash\n        source_dir = source_dir.rstrip(\"/\") + \"/\"\n        destination_dir = destination_dir.rstrip(\"/\") + \"/\"\n\n        # List all blobs in the source directory\n        source_blobs = self.container_client.list_blobs(name_starts_with=source_dir)\n\n        for blob in source_blobs:\n            # Get the relative path of the blob\n            relative_path = os.path.relpath(blob.name, source_dir)\n\n            # Construct the new blob path\n            new_blob_path = os.path.join(destination_dir, relative_path).replace(\n                \"\\\\\", \"/\"\n            )\n\n            # Create a source blob client\n            source_blob_client = self.container_client.get_blob_client(blob.name)\n\n            # Create a destination blob client\n            destination_blob_client = self.container_client.get_blob_client(\n                new_blob_path\n            )\n\n            # Start the copy operation\n            destination_blob_client.start_copy_from_url(source_blob_client.url)\n\n        print(f\"Copied directory from {source_dir} to {destination_dir}\")\n    except Exception as e:\n        print(f\"Failed to copy directory {source_dir}: {e}\")\n</code></pre>"},{"location":"api/core/#gigaspatial.core.io.adls_data_store.ADLSDataStore.download_directory","title":"<code>download_directory(blob_dir_path, local_dir_path)</code>","text":"<p>Downloads all files from a directory in Azure Blob Storage to a local directory.</p> Source code in <code>gigaspatial/core/io/adls_data_store.py</code> <pre><code>def download_directory(self, blob_dir_path: str, local_dir_path: str):\n    \"\"\"Downloads all files from a directory in Azure Blob Storage to a local directory.\"\"\"\n    try:\n        # Ensure the local directory exists\n        os.makedirs(local_dir_path, exist_ok=True)\n\n        # List all files in the blob directory\n        blob_items = self.container_client.list_blobs(\n            name_starts_with=blob_dir_path\n        )\n\n        for blob_item in blob_items:\n            # Get the relative path of the blob file\n            relative_path = os.path.relpath(blob_item.name, blob_dir_path)\n            # Construct the local file path\n            local_file_path = os.path.join(local_dir_path, relative_path)\n            # Create directories if needed\n            os.makedirs(os.path.dirname(local_file_path), exist_ok=True)\n\n            # Download the blob to the local file\n            blob_client = self.container_client.get_blob_client(blob_item.name)\n            with open(local_file_path, \"wb\") as file:\n                file.write(blob_client.download_blob().readall())\n\n        print(f\"Downloaded directory {blob_dir_path} to {local_dir_path}\")\n    except Exception as e:\n        print(f\"Failed to download directory {blob_dir_path}: {e}\")\n</code></pre>"},{"location":"api/core/#gigaspatial.core.io.adls_data_store.ADLSDataStore.get_file_metadata","title":"<code>get_file_metadata(path)</code>","text":"<p>Retrieve comprehensive file metadata.</p> <p>:param path: File path in blob storage :return: File metadata dictionary</p> Source code in <code>gigaspatial/core/io/adls_data_store.py</code> <pre><code>def get_file_metadata(self, path: str) -&gt; dict:\n    \"\"\"\n    Retrieve comprehensive file metadata.\n\n    :param path: File path in blob storage\n    :return: File metadata dictionary\n    \"\"\"\n    blob_client = self.container_client.get_blob_client(path)\n    properties = blob_client.get_blob_properties()\n\n    return {\n        \"name\": path,\n        \"size_bytes\": properties.size,\n        \"content_type\": properties.content_settings.content_type,\n        \"last_modified\": properties.last_modified,\n        \"etag\": properties.etag,\n    }\n</code></pre>"},{"location":"api/core/#gigaspatial.core.io.adls_data_store.ADLSDataStore.mkdir","title":"<code>mkdir(path, exist_ok=False)</code>","text":"<p>Create a directory in Azure Blob Storage.</p> <p>In ADLS, directories are conceptual and created by adding a placeholder blob.</p> <p>:param path: Path of the directory to create :param exist_ok: If False, raise an error if the directory already exists</p> Source code in <code>gigaspatial/core/io/adls_data_store.py</code> <pre><code>def mkdir(self, path: str, exist_ok: bool = False) -&gt; None:\n    \"\"\"\n    Create a directory in Azure Blob Storage.\n\n    In ADLS, directories are conceptual and created by adding a placeholder blob.\n\n    :param path: Path of the directory to create\n    :param exist_ok: If False, raise an error if the directory already exists\n    \"\"\"\n    dir_path = path.rstrip(\"/\") + \"/\"\n\n    existing_blobs = list(self.list_files(dir_path))\n\n    if existing_blobs and not exist_ok:\n        raise FileExistsError(f\"Directory {path} already exists\")\n\n    # Create a placeholder blob to represent the directory\n    placeholder_blob_path = os.path.join(dir_path, \".placeholder\")\n\n    # Only create placeholder if it doesn't already exist\n    if not self.file_exists(placeholder_blob_path):\n        placeholder_content = (\n            b\"This is a placeholder blob to represent a directory.\"\n        )\n        blob_client = self.blob_service_client.get_blob_client(\n            container=self.container, blob=placeholder_blob_path\n        )\n        blob_client.upload_blob(placeholder_content, overwrite=True)\n</code></pre>"},{"location":"api/core/#gigaspatial.core.io.adls_data_store.ADLSDataStore.open","title":"<code>open(path, mode='r')</code>","text":"<p>Context manager for file operations with enhanced mode support.</p> <p>:param path: File path in blob storage :param mode: File open mode (r, rb, w, wb)</p> Source code in <code>gigaspatial/core/io/adls_data_store.py</code> <pre><code>@contextlib.contextmanager\ndef open(self, path: str, mode: str = \"r\"):\n    \"\"\"\n    Context manager for file operations with enhanced mode support.\n\n    :param path: File path in blob storage\n    :param mode: File open mode (r, rb, w, wb)\n    \"\"\"\n    if mode == \"w\":\n        file = io.StringIO()\n        yield file\n        self.write_file(path, file.getvalue())\n\n    elif mode == \"wb\":\n        file = io.BytesIO()\n        yield file\n        self.write_file(path, file.getvalue())\n\n    elif mode == \"r\":\n        data = self.read_file(path, encoding=\"UTF-8\")\n        file = io.StringIO(data)\n        yield file\n\n    elif mode == \"rb\":\n        data = self.read_file(path)\n        file = io.BytesIO(data)\n        yield file\n</code></pre>"},{"location":"api/core/#gigaspatial.core.io.adls_data_store.ADLSDataStore.read_file","title":"<code>read_file(path, encoding=None)</code>","text":"<p>Read file with flexible encoding support.</p> <p>:param path: Path to the file in blob storage :param encoding: File encoding (optional) :return: File contents as string or bytes</p> Source code in <code>gigaspatial/core/io/adls_data_store.py</code> <pre><code>def read_file(self, path: str, encoding: Optional[str] = None) -&gt; Union[str, bytes]:\n    \"\"\"\n    Read file with flexible encoding support.\n\n    :param path: Path to the file in blob storage\n    :param encoding: File encoding (optional)\n    :return: File contents as string or bytes\n    \"\"\"\n    try:\n        blob_client = self.container_client.get_blob_client(path)\n        blob_data = blob_client.download_blob().readall()\n\n        # If no encoding specified, return raw bytes\n        if encoding is None:\n            return blob_data\n\n        # If encoding is specified, decode the bytes\n        return blob_data.decode(encoding)\n\n    except Exception as e:\n        raise IOError(f\"Error reading file {path}: {e}\")\n</code></pre>"},{"location":"api/core/#gigaspatial.core.io.adls_data_store.ADLSDataStore.upload_directory","title":"<code>upload_directory(dir_path, blob_dir_path)</code>","text":"<p>Uploads all files from a directory to Azure Blob Storage.</p> Source code in <code>gigaspatial/core/io/adls_data_store.py</code> <pre><code>def upload_directory(self, dir_path, blob_dir_path):\n    \"\"\"Uploads all files from a directory to Azure Blob Storage.\"\"\"\n    for root, dirs, files in os.walk(dir_path):\n        for file in files:\n            local_file_path = os.path.join(root, file)\n            relative_path = os.path.relpath(local_file_path, dir_path)\n            blob_file_path = os.path.join(blob_dir_path, relative_path).replace(\n                \"\\\\\", \"/\"\n            )\n\n            self.upload_file(local_file_path, blob_file_path)\n</code></pre>"},{"location":"api/core/#gigaspatial.core.io.adls_data_store.ADLSDataStore.upload_file","title":"<code>upload_file(file_path, blob_path)</code>","text":"<p>Uploads a single file to Azure Blob Storage.</p> Source code in <code>gigaspatial/core/io/adls_data_store.py</code> <pre><code>def upload_file(self, file_path, blob_path):\n    \"\"\"Uploads a single file to Azure Blob Storage.\"\"\"\n    try:\n        blob_client = self.container_client.get_blob_client(blob_path)\n        with open(file_path, \"rb\") as data:\n            blob_client.upload_blob(data, overwrite=True)\n        print(f\"Uploaded {file_path} to {blob_path}\")\n    except Exception as e:\n        print(f\"Failed to upload {file_path}: {e}\")\n</code></pre>"},{"location":"api/core/#gigaspatial.core.io.adls_data_store.ADLSDataStore.write_file","title":"<code>write_file(path, data)</code>","text":"<p>Write file with support for content type and improved type handling.</p> <p>:param path: Destination path in blob storage :param data: File contents</p> Source code in <code>gigaspatial/core/io/adls_data_store.py</code> <pre><code>def write_file(self, path: str, data) -&gt; None:\n    \"\"\"\n    Write file with support for content type and improved type handling.\n\n    :param path: Destination path in blob storage\n    :param data: File contents\n    \"\"\"\n    blob_client = self.blob_service_client.get_blob_client(\n        container=self.container, blob=path, snapshot=None\n    )\n\n    if isinstance(data, str):\n        binary_data = data.encode()\n    elif isinstance(data, bytes):\n        binary_data = data\n    else:\n        raise Exception(f'Unsupported data type. Only \"bytes\" or \"string\" accepted')\n\n    blob_client.upload_blob(binary_data, overwrite=True)\n</code></pre>"},{"location":"api/core/#gigaspatial.core.io.data_api","title":"<code>data_api</code>","text":""},{"location":"api/core/#gigaspatial.core.io.data_api.GigaDataAPI","title":"<code>GigaDataAPI</code>","text":"Source code in <code>gigaspatial/core/io/data_api.py</code> <pre><code>class GigaDataAPI:\n\n    def __init__(\n        self,\n        profile_file: Union[str, Path] = config.API_PROFILE_FILE_PATH,\n        share_name: str = config.API_SHARE_NAME,\n        schema_name: str = config.API_SCHEMA_NAME,\n    ):\n        \"\"\"\n        Initialize the GigaDataAPI class with the profile file, share name, and schema name.\n\n        profile_file: Path to the delta-sharing profile file.\n        share_name: Name of the share (e.g., \"gold\").\n        schema_name: Name of the schema (e.g., \"school-master\").\n        \"\"\"\n        self.profile_file = profile_file\n        self.share_name = share_name\n        self.schema_name = schema_name\n        self.client = delta_sharing.SharingClient(profile_file)\n\n        self._cache = {}\n\n    def get_country_list(self, sort=True):\n        \"\"\"\n        Retrieve a list of available countries in the dataset.\n\n        :param sort: Whether to sort the country list alphabetically (default is True).\n        \"\"\"\n        country_list = [\n            t.name for t in self.client.list_all_tables() if t.share == self.share_name\n        ]\n        if sort:\n            country_list.sort()\n        return country_list\n\n    def load_country_data(self, country, filters=None, use_cache=True):\n        \"\"\"\n        Load the dataset for the specified country with optional filtering and caching.\n\n        country: The country code (e.g., \"MWI\").\n        filters: A dictionary with column names as keys and filter values as values.\n        use_cache: Whether to use cached data if available (default is True).\n        \"\"\"\n        # Check if data is cached\n        if use_cache and country in self._cache:\n            df_country = self._cache[country]\n        else:\n            # Load data from the API\n            table_url = (\n                f\"{self.profile_file}#{self.share_name}.{self.schema_name}.{country}\"\n            )\n            df_country = delta_sharing.load_as_pandas(table_url)\n            self._cache[country] = df_country  # Cache the data\n\n        # Apply filters if provided\n        if filters:\n            for column, value in filters.items():\n                df_country = df_country[df_country[column] == value]\n\n        return df_country\n\n    def load_multiple_countries(self, countries):\n        \"\"\"\n        Load data for multiple countries and combine them into a single DataFrame.\n\n        countries: A list of country codes.\n        \"\"\"\n        df_list = []\n        for country in countries:\n            df_list.append(self.load_country_data(country))\n        return pd.concat(df_list, ignore_index=True)\n\n    def get_country_metadata(self, country):\n        \"\"\"\n        Retrieve metadata (e.g., column names and data types) for a country's dataset.\n\n        country: The country code (e.g., \"MWI\").\n        \"\"\"\n        df_country = self.load_country_data(country)\n        metadata = {\n            \"columns\": df_country.columns.tolist(),\n            \"data_types\": df_country.dtypes.to_dict(),\n            \"num_records\": len(df_country),\n        }\n        return metadata\n\n    def get_all_cached_data_as_dict(self):\n        \"\"\"\n        Retrieve all cached data in a dictionary format, where each key is a country code,\n        and the value is the DataFrame of that country.\n        \"\"\"\n        return self._cache if self._cache else {}\n\n    def get_all_cached_data_as_json(self):\n        \"\"\"\n        Retrieve all cached data in a JSON-like format. Each country is represented as a key,\n        and the value is a list of records (i.e., the DataFrame's `to_dict(orient='records')` format).\n        \"\"\"\n        if not self._cache:\n            return {}\n\n        # Convert each DataFrame in the cache to a JSON-like format (list of records)\n        return {\n            country: df.to_dict(orient=\"records\") for country, df in self._cache.items()\n        }\n</code></pre>"},{"location":"api/core/#gigaspatial.core.io.data_api.GigaDataAPI.__init__","title":"<code>__init__(profile_file=config.API_PROFILE_FILE_PATH, share_name=config.API_SHARE_NAME, schema_name=config.API_SCHEMA_NAME)</code>","text":"<p>Initialize the GigaDataAPI class with the profile file, share name, and schema name.</p> <p>profile_file: Path to the delta-sharing profile file. share_name: Name of the share (e.g., \"gold\"). schema_name: Name of the schema (e.g., \"school-master\").</p> Source code in <code>gigaspatial/core/io/data_api.py</code> <pre><code>def __init__(\n    self,\n    profile_file: Union[str, Path] = config.API_PROFILE_FILE_PATH,\n    share_name: str = config.API_SHARE_NAME,\n    schema_name: str = config.API_SCHEMA_NAME,\n):\n    \"\"\"\n    Initialize the GigaDataAPI class with the profile file, share name, and schema name.\n\n    profile_file: Path to the delta-sharing profile file.\n    share_name: Name of the share (e.g., \"gold\").\n    schema_name: Name of the schema (e.g., \"school-master\").\n    \"\"\"\n    self.profile_file = profile_file\n    self.share_name = share_name\n    self.schema_name = schema_name\n    self.client = delta_sharing.SharingClient(profile_file)\n\n    self._cache = {}\n</code></pre>"},{"location":"api/core/#gigaspatial.core.io.data_api.GigaDataAPI.get_all_cached_data_as_dict","title":"<code>get_all_cached_data_as_dict()</code>","text":"<p>Retrieve all cached data in a dictionary format, where each key is a country code, and the value is the DataFrame of that country.</p> Source code in <code>gigaspatial/core/io/data_api.py</code> <pre><code>def get_all_cached_data_as_dict(self):\n    \"\"\"\n    Retrieve all cached data in a dictionary format, where each key is a country code,\n    and the value is the DataFrame of that country.\n    \"\"\"\n    return self._cache if self._cache else {}\n</code></pre>"},{"location":"api/core/#gigaspatial.core.io.data_api.GigaDataAPI.get_all_cached_data_as_json","title":"<code>get_all_cached_data_as_json()</code>","text":"<p>Retrieve all cached data in a JSON-like format. Each country is represented as a key, and the value is a list of records (i.e., the DataFrame's <code>to_dict(orient='records')</code> format).</p> Source code in <code>gigaspatial/core/io/data_api.py</code> <pre><code>def get_all_cached_data_as_json(self):\n    \"\"\"\n    Retrieve all cached data in a JSON-like format. Each country is represented as a key,\n    and the value is a list of records (i.e., the DataFrame's `to_dict(orient='records')` format).\n    \"\"\"\n    if not self._cache:\n        return {}\n\n    # Convert each DataFrame in the cache to a JSON-like format (list of records)\n    return {\n        country: df.to_dict(orient=\"records\") for country, df in self._cache.items()\n    }\n</code></pre>"},{"location":"api/core/#gigaspatial.core.io.data_api.GigaDataAPI.get_country_list","title":"<code>get_country_list(sort=True)</code>","text":"<p>Retrieve a list of available countries in the dataset.</p> <p>:param sort: Whether to sort the country list alphabetically (default is True).</p> Source code in <code>gigaspatial/core/io/data_api.py</code> <pre><code>def get_country_list(self, sort=True):\n    \"\"\"\n    Retrieve a list of available countries in the dataset.\n\n    :param sort: Whether to sort the country list alphabetically (default is True).\n    \"\"\"\n    country_list = [\n        t.name for t in self.client.list_all_tables() if t.share == self.share_name\n    ]\n    if sort:\n        country_list.sort()\n    return country_list\n</code></pre>"},{"location":"api/core/#gigaspatial.core.io.data_api.GigaDataAPI.get_country_metadata","title":"<code>get_country_metadata(country)</code>","text":"<p>Retrieve metadata (e.g., column names and data types) for a country's dataset.</p> <p>country: The country code (e.g., \"MWI\").</p> Source code in <code>gigaspatial/core/io/data_api.py</code> <pre><code>def get_country_metadata(self, country):\n    \"\"\"\n    Retrieve metadata (e.g., column names and data types) for a country's dataset.\n\n    country: The country code (e.g., \"MWI\").\n    \"\"\"\n    df_country = self.load_country_data(country)\n    metadata = {\n        \"columns\": df_country.columns.tolist(),\n        \"data_types\": df_country.dtypes.to_dict(),\n        \"num_records\": len(df_country),\n    }\n    return metadata\n</code></pre>"},{"location":"api/core/#gigaspatial.core.io.data_api.GigaDataAPI.load_country_data","title":"<code>load_country_data(country, filters=None, use_cache=True)</code>","text":"<p>Load the dataset for the specified country with optional filtering and caching.</p> <p>country: The country code (e.g., \"MWI\"). filters: A dictionary with column names as keys and filter values as values. use_cache: Whether to use cached data if available (default is True).</p> Source code in <code>gigaspatial/core/io/data_api.py</code> <pre><code>def load_country_data(self, country, filters=None, use_cache=True):\n    \"\"\"\n    Load the dataset for the specified country with optional filtering and caching.\n\n    country: The country code (e.g., \"MWI\").\n    filters: A dictionary with column names as keys and filter values as values.\n    use_cache: Whether to use cached data if available (default is True).\n    \"\"\"\n    # Check if data is cached\n    if use_cache and country in self._cache:\n        df_country = self._cache[country]\n    else:\n        # Load data from the API\n        table_url = (\n            f\"{self.profile_file}#{self.share_name}.{self.schema_name}.{country}\"\n        )\n        df_country = delta_sharing.load_as_pandas(table_url)\n        self._cache[country] = df_country  # Cache the data\n\n    # Apply filters if provided\n    if filters:\n        for column, value in filters.items():\n            df_country = df_country[df_country[column] == value]\n\n    return df_country\n</code></pre>"},{"location":"api/core/#gigaspatial.core.io.data_api.GigaDataAPI.load_multiple_countries","title":"<code>load_multiple_countries(countries)</code>","text":"<p>Load data for multiple countries and combine them into a single DataFrame.</p> <p>countries: A list of country codes.</p> Source code in <code>gigaspatial/core/io/data_api.py</code> <pre><code>def load_multiple_countries(self, countries):\n    \"\"\"\n    Load data for multiple countries and combine them into a single DataFrame.\n\n    countries: A list of country codes.\n    \"\"\"\n    df_list = []\n    for country in countries:\n        df_list.append(self.load_country_data(country))\n    return pd.concat(df_list, ignore_index=True)\n</code></pre>"},{"location":"api/core/#gigaspatial.core.io.data_store","title":"<code>data_store</code>","text":""},{"location":"api/core/#gigaspatial.core.io.data_store.DataStore","title":"<code>DataStore</code>","text":"<p>               Bases: <code>ABC</code></p> <p>Abstract base class defining the interface for data store implementations. This class serves as a parent for both local and cloud-based storage solutions.</p> Source code in <code>gigaspatial/core/io/data_store.py</code> <pre><code>class DataStore(ABC):\n    \"\"\"\n    Abstract base class defining the interface for data store implementations.\n    This class serves as a parent for both local and cloud-based storage solutions.\n    \"\"\"\n\n    @abstractmethod\n    def read_file(self, path: str) -&gt; Any:\n        \"\"\"\n        Read contents of a file from the data store.\n\n        Args:\n            path: Path to the file to read\n\n        Returns:\n            Contents of the file\n\n        Raises:\n            IOError: If file cannot be read\n        \"\"\"\n        pass\n\n    @abstractmethod\n    def write_file(self, path: str, data: Any) -&gt; None:\n        \"\"\"\n        Write data to a file in the data store.\n\n        Args:\n            path: Path where to write the file\n            data: Data to write to the file\n\n        Raises:\n            IOError: If file cannot be written\n        \"\"\"\n        pass\n\n    @abstractmethod\n    def file_exists(self, path: str) -&gt; bool:\n        \"\"\"\n        Check if a file exists in the data store.\n\n        Args:\n            path: Path to check\n\n        Returns:\n            True if file exists, False otherwise\n        \"\"\"\n        pass\n\n    @abstractmethod\n    def list_files(self, path: str) -&gt; List[str]:\n        \"\"\"\n        List all files in a directory.\n\n        Args:\n            path: Directory path to list\n\n        Returns:\n            List of file paths in the directory\n        \"\"\"\n        pass\n\n    @abstractmethod\n    def walk(self, top: str) -&gt; Generator:\n        \"\"\"\n        Walk through directory tree, similar to os.walk().\n\n        Args:\n            top: Starting directory for the walk\n\n        Returns:\n            Generator yielding tuples of (dirpath, dirnames, filenames)\n        \"\"\"\n        pass\n\n    @abstractmethod\n    def open(self, file: str, mode: str = \"r\") -&gt; Union[str, bytes]:\n        \"\"\"\n        Context manager for file operations.\n\n        Args:\n            file: Path to the file\n            mode: File mode ('r', 'w', 'rb', 'wb')\n\n        Yields:\n            File-like object\n\n        Raises:\n            IOError: If file cannot be opened\n        \"\"\"\n        pass\n\n    @abstractmethod\n    def is_file(self, path: str) -&gt; bool:\n        \"\"\"\n        Check if path points to a file.\n\n        Args:\n            path: Path to check\n\n        Returns:\n            True if path is a file, False otherwise\n        \"\"\"\n        pass\n\n    @abstractmethod\n    def is_dir(self, path: str) -&gt; bool:\n        \"\"\"\n        Check if path points to a directory.\n\n        Args:\n            path: Path to check\n\n        Returns:\n            True if path is a directory, False otherwise\n        \"\"\"\n        pass\n\n    @abstractmethod\n    def remove(self, path: str) -&gt; None:\n        \"\"\"\n        Remove a file.\n\n        Args:\n            path: Path to the file to remove\n\n        Raises:\n            IOError: If file cannot be removed\n        \"\"\"\n        pass\n\n    @abstractmethod\n    def rmdir(self, dir: str) -&gt; None:\n        \"\"\"\n        Remove a directory and all its contents.\n\n        Args:\n            dir: Path to the directory to remove\n\n        Raises:\n            IOError: If directory cannot be removed\n        \"\"\"\n        pass\n</code></pre>"},{"location":"api/core/#gigaspatial.core.io.data_store.DataStore.file_exists","title":"<code>file_exists(path)</code>  <code>abstractmethod</code>","text":"<p>Check if a file exists in the data store.</p> <p>Parameters:</p> Name Type Description Default <code>path</code> <code>str</code> <p>Path to check</p> required <p>Returns:</p> Type Description <code>bool</code> <p>True if file exists, False otherwise</p> Source code in <code>gigaspatial/core/io/data_store.py</code> <pre><code>@abstractmethod\ndef file_exists(self, path: str) -&gt; bool:\n    \"\"\"\n    Check if a file exists in the data store.\n\n    Args:\n        path: Path to check\n\n    Returns:\n        True if file exists, False otherwise\n    \"\"\"\n    pass\n</code></pre>"},{"location":"api/core/#gigaspatial.core.io.data_store.DataStore.is_dir","title":"<code>is_dir(path)</code>  <code>abstractmethod</code>","text":"<p>Check if path points to a directory.</p> <p>Parameters:</p> Name Type Description Default <code>path</code> <code>str</code> <p>Path to check</p> required <p>Returns:</p> Type Description <code>bool</code> <p>True if path is a directory, False otherwise</p> Source code in <code>gigaspatial/core/io/data_store.py</code> <pre><code>@abstractmethod\ndef is_dir(self, path: str) -&gt; bool:\n    \"\"\"\n    Check if path points to a directory.\n\n    Args:\n        path: Path to check\n\n    Returns:\n        True if path is a directory, False otherwise\n    \"\"\"\n    pass\n</code></pre>"},{"location":"api/core/#gigaspatial.core.io.data_store.DataStore.is_file","title":"<code>is_file(path)</code>  <code>abstractmethod</code>","text":"<p>Check if path points to a file.</p> <p>Parameters:</p> Name Type Description Default <code>path</code> <code>str</code> <p>Path to check</p> required <p>Returns:</p> Type Description <code>bool</code> <p>True if path is a file, False otherwise</p> Source code in <code>gigaspatial/core/io/data_store.py</code> <pre><code>@abstractmethod\ndef is_file(self, path: str) -&gt; bool:\n    \"\"\"\n    Check if path points to a file.\n\n    Args:\n        path: Path to check\n\n    Returns:\n        True if path is a file, False otherwise\n    \"\"\"\n    pass\n</code></pre>"},{"location":"api/core/#gigaspatial.core.io.data_store.DataStore.list_files","title":"<code>list_files(path)</code>  <code>abstractmethod</code>","text":"<p>List all files in a directory.</p> <p>Parameters:</p> Name Type Description Default <code>path</code> <code>str</code> <p>Directory path to list</p> required <p>Returns:</p> Type Description <code>List[str]</code> <p>List of file paths in the directory</p> Source code in <code>gigaspatial/core/io/data_store.py</code> <pre><code>@abstractmethod\ndef list_files(self, path: str) -&gt; List[str]:\n    \"\"\"\n    List all files in a directory.\n\n    Args:\n        path: Directory path to list\n\n    Returns:\n        List of file paths in the directory\n    \"\"\"\n    pass\n</code></pre>"},{"location":"api/core/#gigaspatial.core.io.data_store.DataStore.open","title":"<code>open(file, mode='r')</code>  <code>abstractmethod</code>","text":"<p>Context manager for file operations.</p> <p>Parameters:</p> Name Type Description Default <code>file</code> <code>str</code> <p>Path to the file</p> required <code>mode</code> <code>str</code> <p>File mode ('r', 'w', 'rb', 'wb')</p> <code>'r'</code> <p>Yields:</p> Type Description <code>Union[str, bytes]</code> <p>File-like object</p> <p>Raises:</p> Type Description <code>IOError</code> <p>If file cannot be opened</p> Source code in <code>gigaspatial/core/io/data_store.py</code> <pre><code>@abstractmethod\ndef open(self, file: str, mode: str = \"r\") -&gt; Union[str, bytes]:\n    \"\"\"\n    Context manager for file operations.\n\n    Args:\n        file: Path to the file\n        mode: File mode ('r', 'w', 'rb', 'wb')\n\n    Yields:\n        File-like object\n\n    Raises:\n        IOError: If file cannot be opened\n    \"\"\"\n    pass\n</code></pre>"},{"location":"api/core/#gigaspatial.core.io.data_store.DataStore.read_file","title":"<code>read_file(path)</code>  <code>abstractmethod</code>","text":"<p>Read contents of a file from the data store.</p> <p>Parameters:</p> Name Type Description Default <code>path</code> <code>str</code> <p>Path to the file to read</p> required <p>Returns:</p> Type Description <code>Any</code> <p>Contents of the file</p> <p>Raises:</p> Type Description <code>IOError</code> <p>If file cannot be read</p> Source code in <code>gigaspatial/core/io/data_store.py</code> <pre><code>@abstractmethod\ndef read_file(self, path: str) -&gt; Any:\n    \"\"\"\n    Read contents of a file from the data store.\n\n    Args:\n        path: Path to the file to read\n\n    Returns:\n        Contents of the file\n\n    Raises:\n        IOError: If file cannot be read\n    \"\"\"\n    pass\n</code></pre>"},{"location":"api/core/#gigaspatial.core.io.data_store.DataStore.remove","title":"<code>remove(path)</code>  <code>abstractmethod</code>","text":"<p>Remove a file.</p> <p>Parameters:</p> Name Type Description Default <code>path</code> <code>str</code> <p>Path to the file to remove</p> required <p>Raises:</p> Type Description <code>IOError</code> <p>If file cannot be removed</p> Source code in <code>gigaspatial/core/io/data_store.py</code> <pre><code>@abstractmethod\ndef remove(self, path: str) -&gt; None:\n    \"\"\"\n    Remove a file.\n\n    Args:\n        path: Path to the file to remove\n\n    Raises:\n        IOError: If file cannot be removed\n    \"\"\"\n    pass\n</code></pre>"},{"location":"api/core/#gigaspatial.core.io.data_store.DataStore.rmdir","title":"<code>rmdir(dir)</code>  <code>abstractmethod</code>","text":"<p>Remove a directory and all its contents.</p> <p>Parameters:</p> Name Type Description Default <code>dir</code> <code>str</code> <p>Path to the directory to remove</p> required <p>Raises:</p> Type Description <code>IOError</code> <p>If directory cannot be removed</p> Source code in <code>gigaspatial/core/io/data_store.py</code> <pre><code>@abstractmethod\ndef rmdir(self, dir: str) -&gt; None:\n    \"\"\"\n    Remove a directory and all its contents.\n\n    Args:\n        dir: Path to the directory to remove\n\n    Raises:\n        IOError: If directory cannot be removed\n    \"\"\"\n    pass\n</code></pre>"},{"location":"api/core/#gigaspatial.core.io.data_store.DataStore.walk","title":"<code>walk(top)</code>  <code>abstractmethod</code>","text":"<p>Walk through directory tree, similar to os.walk().</p> <p>Parameters:</p> Name Type Description Default <code>top</code> <code>str</code> <p>Starting directory for the walk</p> required <p>Returns:</p> Type Description <code>Generator</code> <p>Generator yielding tuples of (dirpath, dirnames, filenames)</p> Source code in <code>gigaspatial/core/io/data_store.py</code> <pre><code>@abstractmethod\ndef walk(self, top: str) -&gt; Generator:\n    \"\"\"\n    Walk through directory tree, similar to os.walk().\n\n    Args:\n        top: Starting directory for the walk\n\n    Returns:\n        Generator yielding tuples of (dirpath, dirnames, filenames)\n    \"\"\"\n    pass\n</code></pre>"},{"location":"api/core/#gigaspatial.core.io.data_store.DataStore.write_file","title":"<code>write_file(path, data)</code>  <code>abstractmethod</code>","text":"<p>Write data to a file in the data store.</p> <p>Parameters:</p> Name Type Description Default <code>path</code> <code>str</code> <p>Path where to write the file</p> required <code>data</code> <code>Any</code> <p>Data to write to the file</p> required <p>Raises:</p> Type Description <code>IOError</code> <p>If file cannot be written</p> Source code in <code>gigaspatial/core/io/data_store.py</code> <pre><code>@abstractmethod\ndef write_file(self, path: str, data: Any) -&gt; None:\n    \"\"\"\n    Write data to a file in the data store.\n\n    Args:\n        path: Path where to write the file\n        data: Data to write to the file\n\n    Raises:\n        IOError: If file cannot be written\n    \"\"\"\n    pass\n</code></pre>"},{"location":"api/core/#gigaspatial.core.io.local_data_store","title":"<code>local_data_store</code>","text":""},{"location":"api/core/#gigaspatial.core.io.local_data_store.LocalDataStore","title":"<code>LocalDataStore</code>","text":"<p>               Bases: <code>DataStore</code></p> <p>Implementation for local filesystem storage.</p> Source code in <code>gigaspatial/core/io/local_data_store.py</code> <pre><code>class LocalDataStore(DataStore):\n    \"\"\"Implementation for local filesystem storage.\"\"\"\n\n    def __init__(self, base_path: Union[str, Path] = \"\"):\n        super().__init__()\n        self.base_path = Path(base_path).resolve()\n\n    def _resolve_path(self, path: str) -&gt; Path:\n        \"\"\"Resolve path relative to base directory.\"\"\"\n        return self.base_path / path\n\n    def read_file(self, path: str) -&gt; bytes:\n        full_path = self._resolve_path(path)\n        with open(full_path, \"rb\") as f:\n            return f.read()\n\n    def write_file(self, path: str, data: Union[bytes, str]) -&gt; None:\n        full_path = self._resolve_path(path)\n        self.mkdir(str(full_path.parent), exist_ok=True)\n\n        if isinstance(data, str):\n            mode = \"w\"\n            encoding = \"utf-8\"\n        else:\n            mode = \"wb\"\n            encoding = None\n\n        with open(full_path, mode, encoding=encoding) as f:\n            f.write(data)\n\n    def file_exists(self, path: str) -&gt; bool:\n        return self._resolve_path(path).is_file()\n\n    def list_files(self, path: str) -&gt; List[str]:\n        full_path = self._resolve_path(path)\n        return [\n            str(f.relative_to(self.base_path))\n            for f in full_path.iterdir()\n            if f.is_file()\n        ]\n\n    def walk(self, top: str) -&gt; Generator[Tuple[str, List[str], List[str]], None, None]:\n        full_path = self._resolve_path(top)\n        for root, dirs, files in os.walk(full_path):\n            rel_root = str(Path(root).relative_to(self.base_path))\n            yield rel_root, dirs, files\n\n    def open(self, path: str, mode: str = \"r\") -&gt; IO:\n        full_path = self._resolve_path(path)\n        self.mkdir(str(full_path.parent), exist_ok=True)\n        return open(full_path, mode)\n\n    def is_file(self, path: str) -&gt; bool:\n        return self._resolve_path(path).is_file()\n\n    def is_dir(self, path: str) -&gt; bool:\n        return self._resolve_path(path).is_dir()\n\n    def remove(self, path: str) -&gt; None:\n        full_path = self._resolve_path(path)\n        if full_path.is_file():\n            os.remove(full_path)\n\n    def rmdir(self, directory: str) -&gt; None:\n        full_path = self._resolve_path(directory)\n        if full_path.is_dir():\n            os.rmdir(full_path)\n\n    def mkdir(self, path: str, exist_ok: bool = False) -&gt; None:\n        full_path = self._resolve_path(path)\n        full_path.mkdir(parents=True, exist_ok=exist_ok)\n\n    def exists(self, path: str) -&gt; bool:\n        return self._resolve_path(path).exists()\n</code></pre>"},{"location":"api/core/#gigaspatial.core.io.readers","title":"<code>readers</code>","text":""},{"location":"api/core/#gigaspatial.core.io.readers.read_dataset","title":"<code>read_dataset(data_store, path, compression=None, **kwargs)</code>","text":"<p>Read data from various file formats stored in both local and cloud-based storage.</p>"},{"location":"api/core/#gigaspatial.core.io.readers.read_dataset--parameters","title":"Parameters:","text":"<p>data_store : DataStore     Instance of DataStore for accessing data storage. path : str, Path     Path to the file in data storage. **kwargs : dict     Additional arguments passed to the specific reader function.</p>"},{"location":"api/core/#gigaspatial.core.io.readers.read_dataset--returns","title":"Returns:","text":"<p>pandas.DataFrame or geopandas.GeoDataFrame     The data read from the file.</p>"},{"location":"api/core/#gigaspatial.core.io.readers.read_dataset--raises","title":"Raises:","text":"<p>FileNotFoundError     If the file doesn't exist in blob storage. ValueError     If the file type is unsupported or if there's an error reading the file.</p> Source code in <code>gigaspatial/core/io/readers.py</code> <pre><code>def read_dataset(data_store: DataStore, path: str, compression: str = None, **kwargs):\n    \"\"\"\n    Read data from various file formats stored in both local and cloud-based storage.\n\n    Parameters:\n    ----------\n    data_store : DataStore\n        Instance of DataStore for accessing data storage.\n    path : str, Path\n        Path to the file in data storage.\n    **kwargs : dict\n        Additional arguments passed to the specific reader function.\n\n    Returns:\n    -------\n    pandas.DataFrame or geopandas.GeoDataFrame\n        The data read from the file.\n\n    Raises:\n    ------\n    FileNotFoundError\n        If the file doesn't exist in blob storage.\n    ValueError\n        If the file type is unsupported or if there's an error reading the file.\n    \"\"\"\n\n    # Define supported file formats and their readers\n    BINARY_FORMATS = {\n        \".shp\",\n        \".zip\",\n        \".parquet\",\n        \".gpkg\",\n        \".xlsx\",\n        \".xls\",\n        \".kmz\",\n        \".gz\",\n    }\n\n    PANDAS_READERS = {\n        \".csv\": pd.read_csv,\n        \".xlsx\": lambda f, **kw: pd.read_excel(f, engine=\"openpyxl\", **kw),\n        \".xls\": lambda f, **kw: pd.read_excel(f, engine=\"xlrd\", **kw),\n        \".json\": pd.read_json,\n        # \".gz\": lambda f, **kw: pd.read_csv(f, compression=\"gzip\", **kw),\n    }\n\n    GEO_READERS = {\n        \".shp\": gpd.read_file,\n        \".zip\": gpd.read_file,\n        \".geojson\": gpd.read_file,\n        \".gpkg\": gpd.read_file,\n        \".parquet\": gpd.read_parquet,\n        \".kmz\": read_kmz,\n    }\n\n    COMPRESSION_FORMATS = {\n        \".gz\": \"gzip\",\n        \".bz2\": \"bz2\",\n        \".zip\": \"zip\",\n        \".xz\": \"xz\",\n    }\n\n    try:\n        # Check if file exists\n        if not data_store.file_exists(path):\n            raise FileNotFoundError(f\"File '{path}' not found in blob storage\")\n\n        path_obj = Path(path)\n        suffixes = path_obj.suffixes\n        file_extension = suffixes[-1].lower() if suffixes else \"\"\n\n        if compression is None and file_extension in COMPRESSION_FORMATS:\n            compression_format = COMPRESSION_FORMATS[file_extension]\n\n            # if file has multiple extensions (e.g., .csv.gz), get the inner format\n            if len(suffixes) &gt; 1:\n                inner_extension = suffixes[-2].lower()\n\n                if inner_extension == \".tar\":\n                    raise ValueError(\n                        \"Tar archives (.tar.gz) are not directly supported\"\n                    )\n\n                if inner_extension in PANDAS_READERS:\n                    try:\n                        with data_store.open(path, \"rb\") as f:\n                            return PANDAS_READERS[inner_extension](\n                                f, compression=compression_format, **kwargs\n                            )\n                    except Exception as e:\n                        raise ValueError(f\"Error reading compressed file: {str(e)}\")\n                elif inner_extension in GEO_READERS:\n                    try:\n                        with data_store.open(path, \"rb\") as f:\n                            if compression_format == \"gzip\":\n                                import gzip\n\n                                decompressed_data = gzip.decompress(f.read())\n                                import io\n\n                                return GEO_READERS[inner_extension](\n                                    io.BytesIO(decompressed_data), **kwargs\n                                )\n                            else:\n                                raise ValueError(\n                                    f\"Compression format {compression_format} not supported for geo data\"\n                                )\n                    except Exception as e:\n                        raise ValueError(f\"Error reading compressed geo file: {str(e)}\")\n            else:\n                # if just .gz without clear inner type, assume csv\n                try:\n                    with data_store.open(path, \"rb\") as f:\n                        return pd.read_csv(f, compression=compression_format, **kwargs)\n                except Exception as e:\n                    raise ValueError(\n                        f\"Error reading compressed file as CSV: {str(e)}. \"\n                        f\"If not a CSV, specify the format in the filename (e.g., .json.gz)\"\n                    )\n\n        # Special handling for compressed files\n        if file_extension == \".zip\":\n            # For zip files, we need to use binary mode\n            with data_store.open(path, \"rb\") as f:\n                return gpd.read_file(f)\n\n        # Determine if we need binary mode based on file type\n        mode = \"rb\" if file_extension in BINARY_FORMATS else \"r\"\n\n        # Try reading with appropriate reader\n        if file_extension in PANDAS_READERS:\n            try:\n                with data_store.open(path, mode) as f:\n                    return PANDAS_READERS[file_extension](f, **kwargs)\n            except Exception as e:\n                raise ValueError(f\"Error reading file with pandas: {str(e)}\")\n\n        if file_extension in GEO_READERS:\n            try:\n                with data_store.open(path, \"rb\") as f:\n                    return GEO_READERS[file_extension](f, **kwargs)\n            except Exception as e:\n                # For parquet files, try pandas reader if geopandas fails\n                if file_extension == \".parquet\":\n                    try:\n                        with data_store.open(path, \"rb\") as f:\n                            return pd.read_parquet(f, **kwargs)\n                    except Exception as e2:\n                        raise ValueError(\n                            f\"Failed to read parquet with both geopandas ({str(e)}) \"\n                            f\"and pandas ({str(e2)})\"\n                        )\n                raise ValueError(f\"Error reading file with geopandas: {str(e)}\")\n\n        # If we get here, the file type is unsupported\n        supported_formats = sorted(set(PANDAS_READERS.keys()) | set(GEO_READERS.keys()))\n        supported_compressions = sorted(COMPRESSION_FORMATS.keys())\n        raise ValueError(\n            f\"Unsupported file type: {file_extension}\\n\"\n            f\"Supported formats: {', '.join(supported_formats)}\"\n            f\"Supported compressions: {', '.join(supported_compressions)}\"\n        )\n\n    except Exception as e:\n        if isinstance(e, (FileNotFoundError, ValueError)):\n            raise\n        raise RuntimeError(f\"Unexpected error reading dataset: {str(e)}\")\n</code></pre>"},{"location":"api/core/#gigaspatial.core.io.readers.read_datasets","title":"<code>read_datasets(data_store, paths, **kwargs)</code>","text":"<p>Read multiple datasets from data storage at once.</p>"},{"location":"api/core/#gigaspatial.core.io.readers.read_datasets--parameters","title":"Parameters:","text":"<p>data_store : DataStore     Instance of DataStore for accessing data storage. paths : list of str     Paths to files in data storage. **kwargs : dict     Additional arguments passed to read_dataset.</p>"},{"location":"api/core/#gigaspatial.core.io.readers.read_datasets--returns","title":"Returns:","text":"<p>dict     Dictionary mapping paths to their corresponding DataFrames/GeoDataFrames.</p> Source code in <code>gigaspatial/core/io/readers.py</code> <pre><code>def read_datasets(data_store: DataStore, paths, **kwargs):\n    \"\"\"\n    Read multiple datasets from data storage at once.\n\n    Parameters:\n    ----------\n    data_store : DataStore\n        Instance of DataStore for accessing data storage.\n    paths : list of str\n        Paths to files in data storage.\n    **kwargs : dict\n        Additional arguments passed to read_dataset.\n\n    Returns:\n    -------\n    dict\n        Dictionary mapping paths to their corresponding DataFrames/GeoDataFrames.\n    \"\"\"\n    results = {}\n    errors = {}\n\n    for path in paths:\n        try:\n            results[path] = read_dataset(data_store, path, **kwargs)\n        except Exception as e:\n            errors[path] = str(e)\n\n    if errors:\n        error_msg = \"\\n\".join(f\"- {path}: {error}\" for path, error in errors.items())\n        raise ValueError(f\"Errors reading datasets:\\n{error_msg}\")\n\n    return results\n</code></pre>"},{"location":"api/core/#gigaspatial.core.io.readers.read_gzipped_json_or_csv","title":"<code>read_gzipped_json_or_csv(file_path, data_store)</code>","text":"<p>Reads a gzipped file, attempting to parse it as JSON (lines=True) or CSV.</p> Source code in <code>gigaspatial/core/io/readers.py</code> <pre><code>def read_gzipped_json_or_csv(file_path, data_store):\n    \"\"\"Reads a gzipped file, attempting to parse it as JSON (lines=True) or CSV.\"\"\"\n\n    with data_store.open(file_path, \"rb\") as f:\n        g = gzip.GzipFile(fileobj=f)\n        text = g.read().decode(\"utf-8\")\n        try:\n            df = pd.read_json(io.StringIO(text), lines=True)\n            return df\n        except json.JSONDecodeError:\n            try:\n                df = pd.read_csv(io.StringIO(text))\n                return df\n            except pd.errors.ParserError:\n                print(f\"Error: Could not parse {file_path} as JSON or CSV.\")\n                return None\n</code></pre>"},{"location":"api/core/#gigaspatial.core.io.readers.read_kmz","title":"<code>read_kmz(file_obj, **kwargs)</code>","text":"<p>Helper function to read KMZ files and return a GeoDataFrame.</p> Source code in <code>gigaspatial/core/io/readers.py</code> <pre><code>def read_kmz(file_obj, **kwargs):\n    \"\"\"Helper function to read KMZ files and return a GeoDataFrame.\"\"\"\n    try:\n        with zipfile.ZipFile(file_obj) as kmz:\n            # Find the KML file in the archive (usually doc.kml)\n            kml_filename = next(\n                name for name in kmz.namelist() if name.endswith(\".kml\")\n            )\n\n            # Read the KML content\n            kml_content = io.BytesIO(kmz.read(kml_filename))\n\n            gdf = gpd.read_file(kml_content)\n\n            # Validate the GeoDataFrame\n            if gdf.empty:\n                raise ValueError(\n                    \"The KML file is empty or does not contain valid geospatial data.\"\n                )\n\n        return gdf\n\n    except zipfile.BadZipFile:\n        raise ValueError(\"The provided file is not a valid KMZ file.\")\n    except StopIteration:\n        raise ValueError(\"No KML file found in the KMZ archive.\")\n    except Exception as e:\n        raise RuntimeError(f\"An error occurred: {e}\")\n</code></pre>"},{"location":"api/core/#gigaspatial.core.io.writers","title":"<code>writers</code>","text":""},{"location":"api/core/#gigaspatial.core.io.writers.write_dataset","title":"<code>write_dataset(data, data_store, path, **kwargs)</code>","text":"<p>Write DataFrame or GeoDataFrame to various file formats in Azure Blob Storage.</p>"},{"location":"api/core/#gigaspatial.core.io.writers.write_dataset--parameters","title":"Parameters:","text":"<p>data : pandas.DataFrame or geopandas.GeoDataFrame     The data to write to blob storage. data_store : DataStore     Instance of DataStore for accessing data storage. path : str     Path where the file will be written in data storage. **kwargs : dict     Additional arguments passed to the specific writer function.</p>"},{"location":"api/core/#gigaspatial.core.io.writers.write_dataset--raises","title":"Raises:","text":"<p>ValueError     If the file type is unsupported or if there's an error writing the file. TypeError     If input data is not a DataFrame or GeoDataFrame.</p> Source code in <code>gigaspatial/core/io/writers.py</code> <pre><code>def write_dataset(data, data_store: DataStore, path, **kwargs):\n    \"\"\"\n    Write DataFrame or GeoDataFrame to various file formats in Azure Blob Storage.\n\n    Parameters:\n    ----------\n    data : pandas.DataFrame or geopandas.GeoDataFrame\n        The data to write to blob storage.\n    data_store : DataStore\n        Instance of DataStore for accessing data storage.\n    path : str\n        Path where the file will be written in data storage.\n    **kwargs : dict\n        Additional arguments passed to the specific writer function.\n\n    Raises:\n    ------\n    ValueError\n        If the file type is unsupported or if there's an error writing the file.\n    TypeError\n        If input data is not a DataFrame or GeoDataFrame.\n    \"\"\"\n\n    # Define supported file formats and their writers\n    BINARY_FORMATS = {\".shp\", \".zip\", \".parquet\", \".gpkg\", \".xlsx\", \".xls\"}\n\n    PANDAS_WRITERS = {\n        \".csv\": lambda df, buf, **kw: df.to_csv(buf, **kw),\n        \".xlsx\": lambda df, buf, **kw: df.to_excel(buf, engine=\"openpyxl\", **kw),\n        \".json\": lambda df, buf, **kw: df.to_json(buf, **kw),\n        \".parquet\": lambda df, buf, **kw: df.to_parquet(buf, **kw),\n    }\n\n    GEO_WRITERS = {\n        \".geojson\": lambda gdf, buf, **kw: gdf.to_file(buf, driver=\"GeoJSON\", **kw),\n        \".gpkg\": lambda gdf, buf, **kw: gdf.to_file(buf, driver=\"GPKG\", **kw),\n        \".parquet\": lambda gdf, buf, **kw: gdf.to_parquet(buf, **kw),\n    }\n\n    try:\n        # Input validation\n        if not isinstance(data, (pd.DataFrame, gpd.GeoDataFrame)):\n            raise TypeError(\"Input data must be a pandas DataFrame or GeoDataFrame\")\n\n        # Get file suffix and ensure it's lowercase\n        suffix = Path(path).suffix.lower()\n\n        # Determine if we need binary mode based on file type\n        mode = \"wb\" if suffix in BINARY_FORMATS else \"w\"\n\n        # Handle different data types and formats\n        if isinstance(data, gpd.GeoDataFrame):\n            if suffix not in GEO_WRITERS:\n                supported_formats = sorted(GEO_WRITERS.keys())\n                raise ValueError(\n                    f\"Unsupported file type for GeoDataFrame: {suffix}\\n\"\n                    f\"Supported formats: {', '.join(supported_formats)}\"\n                )\n\n            try:\n                with data_store.open(path, \"wb\") as f:\n                    GEO_WRITERS[suffix](data, f, **kwargs)\n            except Exception as e:\n                raise ValueError(f\"Error writing GeoDataFrame: {str(e)}\")\n\n        else:  # pandas DataFrame\n            if suffix not in PANDAS_WRITERS:\n                supported_formats = sorted(PANDAS_WRITERS.keys())\n                raise ValueError(\n                    f\"Unsupported file type for DataFrame: {suffix}\\n\"\n                    f\"Supported formats: {', '.join(supported_formats)}\"\n                )\n\n            try:\n                with data_store.open(path, mode) as f:\n                    PANDAS_WRITERS[suffix](data, f, **kwargs)\n            except Exception as e:\n                raise ValueError(f\"Error writing DataFrame: {str(e)}\")\n\n    except Exception as e:\n        if isinstance(e, (TypeError, ValueError)):\n            raise\n        raise RuntimeError(f\"Unexpected error writing dataset: {str(e)}\")\n</code></pre>"},{"location":"api/core/#gigaspatial.core.io.writers.write_datasets","title":"<code>write_datasets(data_dict, data_store, **kwargs)</code>","text":"<p>Write multiple datasets to data storage at once.</p>"},{"location":"api/core/#gigaspatial.core.io.writers.write_datasets--parameters","title":"Parameters:","text":"<p>data_dict : dict     Dictionary mapping paths to DataFrames/GeoDataFrames. data_store : DataStore     Instance of DataStore for accessing data storage. **kwargs : dict     Additional arguments passed to write_dataset.</p>"},{"location":"api/core/#gigaspatial.core.io.writers.write_datasets--raises","title":"Raises:","text":"<p>ValueError     If there are any errors writing the datasets.</p> Source code in <code>gigaspatial/core/io/writers.py</code> <pre><code>def write_datasets(data_dict, data_store: DataStore, **kwargs):\n    \"\"\"\n    Write multiple datasets to data storage at once.\n\n    Parameters:\n    ----------\n    data_dict : dict\n        Dictionary mapping paths to DataFrames/GeoDataFrames.\n    data_store : DataStore\n        Instance of DataStore for accessing data storage.\n    **kwargs : dict\n        Additional arguments passed to write_dataset.\n\n    Raises:\n    ------\n    ValueError\n        If there are any errors writing the datasets.\n    \"\"\"\n    errors = {}\n\n    for path, data in data_dict.items():\n        try:\n            write_dataset(data, data_store, path, **kwargs)\n        except Exception as e:\n            errors[path] = str(e)\n\n    if errors:\n        error_msg = \"\\n\".join(f\"- {path}: {error}\" for path, error in errors.items())\n        raise ValueError(f\"Errors writing datasets:\\n{error_msg}\")\n</code></pre>"},{"location":"api/core/#gigaspatial.core.schemas","title":"<code>schemas</code>","text":""},{"location":"api/core/#gigaspatial.core.schemas.entity","title":"<code>entity</code>","text":""},{"location":"api/core/#gigaspatial.core.schemas.entity.BaseGigaEntity","title":"<code>BaseGigaEntity</code>","text":"<p>               Bases: <code>BaseModel</code></p> <p>Base class for all Giga entities with common fields.</p> Source code in <code>gigaspatial/core/schemas/entity.py</code> <pre><code>class BaseGigaEntity(BaseModel):\n    \"\"\"Base class for all Giga entities with common fields.\"\"\"\n\n    source: Optional[str] = Field(None, max_length=100, description=\"Source reference\")\n    source_detail: Optional[str] = None\n\n    @property\n    def id(self) -&gt; str:\n        \"\"\"Abstract property that must be implemented by subclasses.\"\"\"\n        raise NotImplementedError(\"Subclasses must implement id property\")\n</code></pre>"},{"location":"api/core/#gigaspatial.core.schemas.entity.BaseGigaEntity.id","title":"<code>id: str</code>  <code>property</code>","text":"<p>Abstract property that must be implemented by subclasses.</p>"},{"location":"api/core/#gigaspatial.core.schemas.entity.EntityTable","title":"<code>EntityTable</code>","text":"<p>               Bases: <code>BaseModel</code>, <code>Generic[E]</code></p> Source code in <code>gigaspatial/core/schemas/entity.py</code> <pre><code>class EntityTable(BaseModel, Generic[E]):\n    entities: List[E] = Field(default_factory=list)\n    _cached_kdtree: Optional[cKDTree] = PrivateAttr(\n        default=None\n    )  # Internal cache for the KDTree\n\n    @classmethod\n    def from_file(\n        cls: Type[\"EntityTable\"],\n        file_path: Union[str, Path],\n        entity_class: Type[E],\n        data_store: Optional[DataStore] = None,\n        **kwargs,\n    ) -&gt; \"EntityTable\":\n        \"\"\"\n        Create an EntityTable instance from a file.\n\n        Args:\n            file_path: Path to the dataset file\n            entity_class: The entity class for validation\n\n        Returns:\n            EntityTable instance\n\n        Raises:\n            ValidationError: If any row fails validation\n            FileNotFoundError: If the file doesn't exist\n        \"\"\"\n        data_store = data_store or LocalDataStore()\n        file_path = Path(file_path)\n        if not file_path.exists():\n            raise FileNotFoundError(f\"File not found: {file_path}\")\n\n        df = read_dataset(data_store, file_path, **kwargs)\n        try:\n            entities = [entity_class(**row) for row in df.to_dict(orient=\"records\")]\n            return cls(entities=entities)\n        except ValidationError as e:\n            raise ValueError(f\"Validation error in input data: {e}\")\n        except Exception as e:\n            raise ValueError(f\"Error reading or processing the file: {e}\")\n\n    def _check_has_location(self, method_name: str) -&gt; bool:\n        \"\"\"Helper method to check if entities have location data.\"\"\"\n        if not self.entities:\n            return False\n        if not isinstance(self.entities[0], GigaEntity):\n            raise ValueError(\n                f\"Cannot perform {method_name}: entities of type {type(self.entities[0]).__name__} \"\n                \"do not have location data (latitude/longitude)\"\n            )\n        return True\n\n    def to_dataframe(self) -&gt; pd.DataFrame:\n        \"\"\"Convert the entity table to a pandas DataFrame.\"\"\"\n        return pd.DataFrame([e.model_dump() for e in self.entities])\n\n    def to_geodataframe(self) -&gt; gpd.GeoDataFrame:\n        \"\"\"Convert the entity table to a GeoDataFrame.\"\"\"\n        if not self._check_has_location(\"to_geodataframe\"):\n            raise ValueError(\"Cannot create GeoDataFrame: no entities available\")\n        df = self.to_dataframe()\n        return gpd.GeoDataFrame(\n            df,\n            geometry=gpd.points_from_xy(df[\"longitude\"], df[\"latitude\"]),\n            crs=\"EPSG:4326\",\n        )\n\n    def to_coordinate_vector(self) -&gt; np.ndarray:\n        \"\"\"Transforms the entity table into a numpy vector of coordinates\"\"\"\n        if not self.entities:\n            return np.zeros((0, 2))\n\n        if not self._check_has_location(\"to_coordinate_vector\"):\n            return np.zeros((0, 2))\n\n        return np.array([[e.latitude, e.longitude] for e in self.entities])\n\n    def get_lat_array(self) -&gt; np.ndarray:\n        \"\"\"Get an array of latitude values.\"\"\"\n        if not self._check_has_location(\"get_lat_array\"):\n            return np.array([])\n        return np.array([e.latitude for e in self.entities])\n\n    def get_lon_array(self) -&gt; np.ndarray:\n        \"\"\"Get an array of longitude values.\"\"\"\n        if not self._check_has_location(\"get_lon_array\"):\n            return np.array([])\n        return np.array([e.longitude for e in self.entities])\n\n    def filter_by_admin1(self, admin1_id_giga: str) -&gt; \"EntityTable[E]\":\n        \"\"\"Filter entities by primary administrative division.\"\"\"\n        return self.__class__(\n            entities=[e for e in self.entities if e.admin1_id_giga == admin1_id_giga]\n        )\n\n    def filter_by_admin2(self, admin2_id_giga: str) -&gt; \"EntityTable[E]\":\n        \"\"\"Filter entities by secondary administrative division.\"\"\"\n        return self.__class__(\n            entities=[e for e in self.entities if e.admin2_id_giga == admin2_id_giga]\n        )\n\n    def filter_by_polygon(self, polygon: Polygon) -&gt; \"EntityTable[E]\":\n        \"\"\"Filter entities within a polygon\"\"\"\n        if not self._check_has_location(\"filter_by_polygon\"):\n            return self.__class__(entities=[])\n\n        filtered = [\n            e for e in self.entities if polygon.contains(Point(e.longitude, e.latitude))\n        ]\n        return self.__class__(entities=filtered)\n\n    def filter_by_bounds(\n        self, min_lat: float, max_lat: float, min_lon: float, max_lon: float\n    ) -&gt; \"EntityTable[E]\":\n        \"\"\"Filter entities whose coordinates fall within the given bounds.\"\"\"\n        if not self._check_has_location(\"filter_by_bounds\"):\n            return self.__class__(entities=[])\n\n        filtered = [\n            e\n            for e in self.entities\n            if min_lat &lt;= e.latitude &lt;= max_lat and min_lon &lt;= e.longitude &lt;= max_lon\n        ]\n        return self.__class__(entities=filtered)\n\n    def get_nearest_neighbors(\n        self, lat: float, lon: float, k: int = 5\n    ) -&gt; \"EntityTable[E]\":\n        \"\"\"Find k nearest neighbors to a point using a cached KDTree.\"\"\"\n        if not self._check_has_location(\"get_nearest_neighbors\"):\n            return self.__class__(entities=[])\n\n        if not self._cached_kdtree:\n            self._build_kdtree()  # Build the KDTree if not already cached\n\n        if not self._cached_kdtree:  # If still None after building\n            return self.__class__(entities=[])\n\n        _, indices = self._cached_kdtree.query([[lat, lon]], k=k)\n        return self.__class__(entities=[self.entities[i] for i in indices[0]])\n\n    def _build_kdtree(self):\n        \"\"\"Builds and caches the KDTree.\"\"\"\n        if not self._check_has_location(\"_build_kdtree\"):\n            self._cached_kdtree = None\n            return\n        coords = self.to_coordinate_vector()\n        if coords:\n            self._cached_kdtree = cKDTree(coords)\n\n    def clear_cache(self):\n        \"\"\"Clears the KDTree cache.\"\"\"\n        self._cached_kdtree = None\n\n    def to_file(\n        self,\n        file_path: Union[str, Path],\n        data_store: Optional[DataStore] = None,\n        **kwargs,\n    ) -&gt; None:\n        \"\"\"\n        Save the entity data to a file.\n\n        Args:\n            file_path: Path to save the file\n        \"\"\"\n        if not self.entities:\n            raise ValueError(\"Cannot write to a file: no entities available.\")\n\n        data_store = data_store or LocalDataStore()\n\n        write_dataset(self.to_dataframe(), data_store, file_path, **kwargs)\n\n    def __len__(self) -&gt; int:\n        return len(self.entities)\n\n    def __iter__(self):\n        return iter(self.entities)\n</code></pre>"},{"location":"api/core/#gigaspatial.core.schemas.entity.EntityTable.clear_cache","title":"<code>clear_cache()</code>","text":"<p>Clears the KDTree cache.</p> Source code in <code>gigaspatial/core/schemas/entity.py</code> <pre><code>def clear_cache(self):\n    \"\"\"Clears the KDTree cache.\"\"\"\n    self._cached_kdtree = None\n</code></pre>"},{"location":"api/core/#gigaspatial.core.schemas.entity.EntityTable.filter_by_admin1","title":"<code>filter_by_admin1(admin1_id_giga)</code>","text":"<p>Filter entities by primary administrative division.</p> Source code in <code>gigaspatial/core/schemas/entity.py</code> <pre><code>def filter_by_admin1(self, admin1_id_giga: str) -&gt; \"EntityTable[E]\":\n    \"\"\"Filter entities by primary administrative division.\"\"\"\n    return self.__class__(\n        entities=[e for e in self.entities if e.admin1_id_giga == admin1_id_giga]\n    )\n</code></pre>"},{"location":"api/core/#gigaspatial.core.schemas.entity.EntityTable.filter_by_admin2","title":"<code>filter_by_admin2(admin2_id_giga)</code>","text":"<p>Filter entities by secondary administrative division.</p> Source code in <code>gigaspatial/core/schemas/entity.py</code> <pre><code>def filter_by_admin2(self, admin2_id_giga: str) -&gt; \"EntityTable[E]\":\n    \"\"\"Filter entities by secondary administrative division.\"\"\"\n    return self.__class__(\n        entities=[e for e in self.entities if e.admin2_id_giga == admin2_id_giga]\n    )\n</code></pre>"},{"location":"api/core/#gigaspatial.core.schemas.entity.EntityTable.filter_by_bounds","title":"<code>filter_by_bounds(min_lat, max_lat, min_lon, max_lon)</code>","text":"<p>Filter entities whose coordinates fall within the given bounds.</p> Source code in <code>gigaspatial/core/schemas/entity.py</code> <pre><code>def filter_by_bounds(\n    self, min_lat: float, max_lat: float, min_lon: float, max_lon: float\n) -&gt; \"EntityTable[E]\":\n    \"\"\"Filter entities whose coordinates fall within the given bounds.\"\"\"\n    if not self._check_has_location(\"filter_by_bounds\"):\n        return self.__class__(entities=[])\n\n    filtered = [\n        e\n        for e in self.entities\n        if min_lat &lt;= e.latitude &lt;= max_lat and min_lon &lt;= e.longitude &lt;= max_lon\n    ]\n    return self.__class__(entities=filtered)\n</code></pre>"},{"location":"api/core/#gigaspatial.core.schemas.entity.EntityTable.filter_by_polygon","title":"<code>filter_by_polygon(polygon)</code>","text":"<p>Filter entities within a polygon</p> Source code in <code>gigaspatial/core/schemas/entity.py</code> <pre><code>def filter_by_polygon(self, polygon: Polygon) -&gt; \"EntityTable[E]\":\n    \"\"\"Filter entities within a polygon\"\"\"\n    if not self._check_has_location(\"filter_by_polygon\"):\n        return self.__class__(entities=[])\n\n    filtered = [\n        e for e in self.entities if polygon.contains(Point(e.longitude, e.latitude))\n    ]\n    return self.__class__(entities=filtered)\n</code></pre>"},{"location":"api/core/#gigaspatial.core.schemas.entity.EntityTable.from_file","title":"<code>from_file(file_path, entity_class, data_store=None, **kwargs)</code>  <code>classmethod</code>","text":"<p>Create an EntityTable instance from a file.</p> <p>Parameters:</p> Name Type Description Default <code>file_path</code> <code>Union[str, Path]</code> <p>Path to the dataset file</p> required <code>entity_class</code> <code>Type[E]</code> <p>The entity class for validation</p> required <p>Returns:</p> Type Description <code>EntityTable</code> <p>EntityTable instance</p> <p>Raises:</p> Type Description <code>ValidationError</code> <p>If any row fails validation</p> <code>FileNotFoundError</code> <p>If the file doesn't exist</p> Source code in <code>gigaspatial/core/schemas/entity.py</code> <pre><code>@classmethod\ndef from_file(\n    cls: Type[\"EntityTable\"],\n    file_path: Union[str, Path],\n    entity_class: Type[E],\n    data_store: Optional[DataStore] = None,\n    **kwargs,\n) -&gt; \"EntityTable\":\n    \"\"\"\n    Create an EntityTable instance from a file.\n\n    Args:\n        file_path: Path to the dataset file\n        entity_class: The entity class for validation\n\n    Returns:\n        EntityTable instance\n\n    Raises:\n        ValidationError: If any row fails validation\n        FileNotFoundError: If the file doesn't exist\n    \"\"\"\n    data_store = data_store or LocalDataStore()\n    file_path = Path(file_path)\n    if not file_path.exists():\n        raise FileNotFoundError(f\"File not found: {file_path}\")\n\n    df = read_dataset(data_store, file_path, **kwargs)\n    try:\n        entities = [entity_class(**row) for row in df.to_dict(orient=\"records\")]\n        return cls(entities=entities)\n    except ValidationError as e:\n        raise ValueError(f\"Validation error in input data: {e}\")\n    except Exception as e:\n        raise ValueError(f\"Error reading or processing the file: {e}\")\n</code></pre>"},{"location":"api/core/#gigaspatial.core.schemas.entity.EntityTable.get_lat_array","title":"<code>get_lat_array()</code>","text":"<p>Get an array of latitude values.</p> Source code in <code>gigaspatial/core/schemas/entity.py</code> <pre><code>def get_lat_array(self) -&gt; np.ndarray:\n    \"\"\"Get an array of latitude values.\"\"\"\n    if not self._check_has_location(\"get_lat_array\"):\n        return np.array([])\n    return np.array([e.latitude for e in self.entities])\n</code></pre>"},{"location":"api/core/#gigaspatial.core.schemas.entity.EntityTable.get_lon_array","title":"<code>get_lon_array()</code>","text":"<p>Get an array of longitude values.</p> Source code in <code>gigaspatial/core/schemas/entity.py</code> <pre><code>def get_lon_array(self) -&gt; np.ndarray:\n    \"\"\"Get an array of longitude values.\"\"\"\n    if not self._check_has_location(\"get_lon_array\"):\n        return np.array([])\n    return np.array([e.longitude for e in self.entities])\n</code></pre>"},{"location":"api/core/#gigaspatial.core.schemas.entity.EntityTable.get_nearest_neighbors","title":"<code>get_nearest_neighbors(lat, lon, k=5)</code>","text":"<p>Find k nearest neighbors to a point using a cached KDTree.</p> Source code in <code>gigaspatial/core/schemas/entity.py</code> <pre><code>def get_nearest_neighbors(\n    self, lat: float, lon: float, k: int = 5\n) -&gt; \"EntityTable[E]\":\n    \"\"\"Find k nearest neighbors to a point using a cached KDTree.\"\"\"\n    if not self._check_has_location(\"get_nearest_neighbors\"):\n        return self.__class__(entities=[])\n\n    if not self._cached_kdtree:\n        self._build_kdtree()  # Build the KDTree if not already cached\n\n    if not self._cached_kdtree:  # If still None after building\n        return self.__class__(entities=[])\n\n    _, indices = self._cached_kdtree.query([[lat, lon]], k=k)\n    return self.__class__(entities=[self.entities[i] for i in indices[0]])\n</code></pre>"},{"location":"api/core/#gigaspatial.core.schemas.entity.EntityTable.to_coordinate_vector","title":"<code>to_coordinate_vector()</code>","text":"<p>Transforms the entity table into a numpy vector of coordinates</p> Source code in <code>gigaspatial/core/schemas/entity.py</code> <pre><code>def to_coordinate_vector(self) -&gt; np.ndarray:\n    \"\"\"Transforms the entity table into a numpy vector of coordinates\"\"\"\n    if not self.entities:\n        return np.zeros((0, 2))\n\n    if not self._check_has_location(\"to_coordinate_vector\"):\n        return np.zeros((0, 2))\n\n    return np.array([[e.latitude, e.longitude] for e in self.entities])\n</code></pre>"},{"location":"api/core/#gigaspatial.core.schemas.entity.EntityTable.to_dataframe","title":"<code>to_dataframe()</code>","text":"<p>Convert the entity table to a pandas DataFrame.</p> Source code in <code>gigaspatial/core/schemas/entity.py</code> <pre><code>def to_dataframe(self) -&gt; pd.DataFrame:\n    \"\"\"Convert the entity table to a pandas DataFrame.\"\"\"\n    return pd.DataFrame([e.model_dump() for e in self.entities])\n</code></pre>"},{"location":"api/core/#gigaspatial.core.schemas.entity.EntityTable.to_file","title":"<code>to_file(file_path, data_store=None, **kwargs)</code>","text":"<p>Save the entity data to a file.</p> <p>Parameters:</p> Name Type Description Default <code>file_path</code> <code>Union[str, Path]</code> <p>Path to save the file</p> required Source code in <code>gigaspatial/core/schemas/entity.py</code> <pre><code>def to_file(\n    self,\n    file_path: Union[str, Path],\n    data_store: Optional[DataStore] = None,\n    **kwargs,\n) -&gt; None:\n    \"\"\"\n    Save the entity data to a file.\n\n    Args:\n        file_path: Path to save the file\n    \"\"\"\n    if not self.entities:\n        raise ValueError(\"Cannot write to a file: no entities available.\")\n\n    data_store = data_store or LocalDataStore()\n\n    write_dataset(self.to_dataframe(), data_store, file_path, **kwargs)\n</code></pre>"},{"location":"api/core/#gigaspatial.core.schemas.entity.EntityTable.to_geodataframe","title":"<code>to_geodataframe()</code>","text":"<p>Convert the entity table to a GeoDataFrame.</p> Source code in <code>gigaspatial/core/schemas/entity.py</code> <pre><code>def to_geodataframe(self) -&gt; gpd.GeoDataFrame:\n    \"\"\"Convert the entity table to a GeoDataFrame.\"\"\"\n    if not self._check_has_location(\"to_geodataframe\"):\n        raise ValueError(\"Cannot create GeoDataFrame: no entities available\")\n    df = self.to_dataframe()\n    return gpd.GeoDataFrame(\n        df,\n        geometry=gpd.points_from_xy(df[\"longitude\"], df[\"latitude\"]),\n        crs=\"EPSG:4326\",\n    )\n</code></pre>"},{"location":"api/core/#gigaspatial.core.schemas.entity.GigaEntity","title":"<code>GigaEntity</code>","text":"<p>               Bases: <code>BaseGigaEntity</code></p> <p>Entity with location data.</p> Source code in <code>gigaspatial/core/schemas/entity.py</code> <pre><code>class GigaEntity(BaseGigaEntity):\n    \"\"\"Entity with location data.\"\"\"\n\n    latitude: float = Field(\n        ..., ge=-90, le=90, description=\"Latitude coordinate of the entity\"\n    )\n    longitude: float = Field(\n        ..., ge=-180, le=180, description=\"Longitude coordinate of the entity\"\n    )\n    admin1: Optional[str] = Field(\n        \"Unknown\", max_length=100, description=\"Primary administrative division\"\n    )\n    admin1_id_giga: Optional[str] = Field(\n        None,\n        max_length=50,\n        description=\"Unique identifier for the primary administrative division\",\n    )\n    admin2: Optional[str] = Field(\n        \"Unknown\", max_length=100, description=\"Secondary administrative division\"\n    )\n    admin2_id_giga: Optional[str] = Field(\n        None,\n        max_length=50,\n        description=\"Unique identifier for the secondary administrative division\",\n    )\n</code></pre>"},{"location":"api/core/#gigaspatial.core.schemas.entity.GigaEntityNoLocation","title":"<code>GigaEntityNoLocation</code>","text":"<p>               Bases: <code>BaseGigaEntity</code></p> <p>Entity without location data.</p> Source code in <code>gigaspatial/core/schemas/entity.py</code> <pre><code>class GigaEntityNoLocation(BaseGigaEntity):\n    \"\"\"Entity without location data.\"\"\"\n\n    pass\n</code></pre>"},{"location":"api/generators/","title":"Generators Module","text":""},{"location":"api/generators/#gigaspatial.generators","title":"<code>gigaspatial.generators</code>","text":""},{"location":"api/generators/#gigaspatial.generators.poi","title":"<code>poi</code>","text":""},{"location":"api/generators/#gigaspatial.generators.poi.base","title":"<code>base</code>","text":""},{"location":"api/generators/#gigaspatial.generators.poi.base.PoiViewGenerator","title":"<code>PoiViewGenerator</code>","text":"<p>               Bases: <code>ABC</code></p> <p>Base class for generating views from Points of Interest (POI) datasets.</p> <p>This class provides the structure for processing downloaded data sources and mapping them to POI data. Concrete implementations should extend this class for specific data sources.</p> Source code in <code>gigaspatial/generators/poi/base.py</code> <pre><code>class PoiViewGenerator(ABC):\n    \"\"\"\n    Base class for generating views from Points of Interest (POI) datasets.\n\n    This class provides the structure for processing downloaded data sources\n    and mapping them to POI data. Concrete implementations should extend this\n    class for specific data sources.\n    \"\"\"\n\n    def __init__(\n        self,\n        data_config: Optional[Any] = None,\n        generator_config: Optional[\"PoiViewGeneratorConfig\"] = None,\n        data_store: Optional[\"DataStore\"] = None,\n    ):\n        \"\"\"\n        Initialize the POI View Generator.\n\n        Args:\n            generator_config: Configuration for the view generator\n            data_store: Data store for reading/writing data\n        \"\"\"\n        self.data_config = data_config\n        self.generator_config = generator_config or PoiViewGeneratorConfig()\n        self.data_store = data_store or LocalDataStore()\n        self.logger = config.get_logger(self.__class__.__name__)\n\n    def resolve_source_paths(\n        self,\n        poi_data: Union[pd.DataFrame, gpd.GeoDataFrame],\n        explicit_paths: Optional[Union[Path, str, List[Union[str, Path]]]] = None,\n        **kwargs,\n    ) -&gt; List[Union[str, Path]]:\n        \"\"\"\n        Resolve source data paths based on POI data or explicit paths.\n\n        This method allows generators to dynamically determine source paths\n        based on the POI data (e.g., by geographic intersection).\n\n        Args:\n            poi_data: POI data that may be used to determine relevant source paths\n            explicit_paths: Explicitly provided source paths, if any\n            **kwargs: Additional parameters for path resolution\n\n        Returns:\n            List of resolved source paths\n\n        Notes:\n            Default implementation returns explicit_paths if provided.\n            Subclasses should override this to implement dynamic path resolution.\n        \"\"\"\n        if explicit_paths is not None:\n            if isinstance(explicit_paths, (str, Path)):\n                return [explicit_paths]\n            return list(explicit_paths)\n\n        # Raises NotImplementedError if no explicit paths\n        # and subclass hasn't overridden this method\n        raise NotImplementedError(\n            \"This generator requires explicit source paths or a subclass \"\n            \"implementation of resolve_source_paths()\"\n        )\n\n    def _pre_load_hook(self, source_data_path, **kwargs) -&gt; Any:\n        \"\"\"Hook called before loading data\"\"\"\n        return source_data_path\n\n    def _post_load_hook(self, data, **kwargs) -&gt; Any:\n        \"\"\"Hook called after loading data\"\"\"\n        return data\n\n    @abstractmethod\n    def load_data(self, source_data_path: List[Union[str, Path]], **kwargs) -&gt; Any:\n        \"\"\"\n        Load source data for POI processing. This method handles diverse source data formats.\n\n        Args:\n            source_data_path: List of source paths\n            **kwargs: Additional parameters for data loading\n\n        Returns:\n            Data in its source format (DataFrame, GeoDataFrame, TifProcessor, etc.)\n        \"\"\"\n        pass\n\n    def process_data(self, data: Any, **kwargs) -&gt; Any:\n        \"\"\"Process the source data to prepare it for POI view generation.\"\"\"\n        return data\n        raise NotImplementedError(\"Subclasses must implement this method...\")\n\n    @abstractmethod\n    def map_to_poi(\n        self,\n        processed_data: Any,\n        poi_data: Union[pd.DataFrame, gpd.GeoDataFrame],\n        **kwargs,\n    ) -&gt; Union[pd.DataFrame, gpd.GeoDataFrame]:\n        \"\"\"\n        Map processed data to POI data.\n\n        Args:\n            processed_data: Processed source data as a GeoDataFrame\n            poi_data: POI data to map to\n            **kwargs: Additional mapping parameters\n\n        Returns:\n            (Geo)DataFrame with POI data mapped to source data\n        \"\"\"\n        pass\n\n    def generate_poi_view(\n        self,\n        poi_data: Union[pd.DataFrame, gpd.GeoDataFrame],\n        source_data_path: Optional[Union[Path, str, List[Union[str, Path]]]] = None,\n        custom_pipeline: bool = False,\n        **kwargs,\n    ) -&gt; Union[pd.DataFrame, gpd.GeoDataFrame]:\n        \"\"\"\n        Generate a POI view by running the complete pipeline.\n\n        This method has been updated to make source_data_path optional.\n        If not provided, it will use resolve_source_paths() to determine paths.\n\n        Args:\n            poi_data: POI data to map to\n            source_data_path: Optional explicit path(s) to the source data\n            **kwargs: Additional parameters for the pipeline\n\n        Returns:\n            DataFrame with the generated POI view\n        \"\"\"\n        if custom_pipeline:\n            return self._custom_pipeline(source_data_path, poi_data, **kwargs)\n\n        self.logger.info(\"Starting POI view generation pipeline\")\n\n        # Resolve source paths if not explicitly provided\n        resolved_paths = self.resolve_source_paths(poi_data, source_data_path, **kwargs)\n\n        if not resolved_paths:\n            self.logger.warning(\n                \"No source data paths resolved. Returning original POI data.\"\n            )\n            return poi_data\n\n        # load data from resolved sources\n        source_data = self.load_data(resolved_paths, **kwargs)\n        self.logger.info(\"Source data loaded successfully\")\n\n        # process the data\n        processed_data = self.process_data(source_data, **kwargs)\n        self.logger.info(\"Data processing completed\")\n\n        # map to POI\n        poi_view = self.map_to_poi(processed_data, poi_data, **kwargs)\n        self.logger.info(\"POI mapping completed\")\n\n        return poi_view\n\n    def save_poi_view(\n        self,\n        poi_view: Union[pd.DataFrame, gpd.GeoDataFrame],\n        output_path: Union[Path, str],\n        **kwargs,\n    ) -&gt; None:\n        \"\"\"\n        Save the generated POI view to the data store.\n\n        Args:\n            poi_view: The POI view DataFrame to save\n            output_path: Path where the POI view will be saved in DataStore\n            **kwargs: Additional parameters for saving\n        \"\"\"\n        self.logger.info(f\"Saving POI view to {output_path}\")\n        write_dataset(poi_view, self.data_store, output_path, **kwargs)\n</code></pre>"},{"location":"api/generators/#gigaspatial.generators.poi.base.PoiViewGenerator.__init__","title":"<code>__init__(data_config=None, generator_config=None, data_store=None)</code>","text":"<p>Initialize the POI View Generator.</p> <p>Parameters:</p> Name Type Description Default <code>generator_config</code> <code>Optional[PoiViewGeneratorConfig]</code> <p>Configuration for the view generator</p> <code>None</code> <code>data_store</code> <code>Optional[DataStore]</code> <p>Data store for reading/writing data</p> <code>None</code> Source code in <code>gigaspatial/generators/poi/base.py</code> <pre><code>def __init__(\n    self,\n    data_config: Optional[Any] = None,\n    generator_config: Optional[\"PoiViewGeneratorConfig\"] = None,\n    data_store: Optional[\"DataStore\"] = None,\n):\n    \"\"\"\n    Initialize the POI View Generator.\n\n    Args:\n        generator_config: Configuration for the view generator\n        data_store: Data store for reading/writing data\n    \"\"\"\n    self.data_config = data_config\n    self.generator_config = generator_config or PoiViewGeneratorConfig()\n    self.data_store = data_store or LocalDataStore()\n    self.logger = config.get_logger(self.__class__.__name__)\n</code></pre>"},{"location":"api/generators/#gigaspatial.generators.poi.base.PoiViewGenerator.generate_poi_view","title":"<code>generate_poi_view(poi_data, source_data_path=None, custom_pipeline=False, **kwargs)</code>","text":"<p>Generate a POI view by running the complete pipeline.</p> <p>This method has been updated to make source_data_path optional. If not provided, it will use resolve_source_paths() to determine paths.</p> <p>Parameters:</p> Name Type Description Default <code>poi_data</code> <code>Union[DataFrame, GeoDataFrame]</code> <p>POI data to map to</p> required <code>source_data_path</code> <code>Optional[Union[Path, str, List[Union[str, Path]]]]</code> <p>Optional explicit path(s) to the source data</p> <code>None</code> <code>**kwargs</code> <p>Additional parameters for the pipeline</p> <code>{}</code> <p>Returns:</p> Type Description <code>Union[DataFrame, GeoDataFrame]</code> <p>DataFrame with the generated POI view</p> Source code in <code>gigaspatial/generators/poi/base.py</code> <pre><code>def generate_poi_view(\n    self,\n    poi_data: Union[pd.DataFrame, gpd.GeoDataFrame],\n    source_data_path: Optional[Union[Path, str, List[Union[str, Path]]]] = None,\n    custom_pipeline: bool = False,\n    **kwargs,\n) -&gt; Union[pd.DataFrame, gpd.GeoDataFrame]:\n    \"\"\"\n    Generate a POI view by running the complete pipeline.\n\n    This method has been updated to make source_data_path optional.\n    If not provided, it will use resolve_source_paths() to determine paths.\n\n    Args:\n        poi_data: POI data to map to\n        source_data_path: Optional explicit path(s) to the source data\n        **kwargs: Additional parameters for the pipeline\n\n    Returns:\n        DataFrame with the generated POI view\n    \"\"\"\n    if custom_pipeline:\n        return self._custom_pipeline(source_data_path, poi_data, **kwargs)\n\n    self.logger.info(\"Starting POI view generation pipeline\")\n\n    # Resolve source paths if not explicitly provided\n    resolved_paths = self.resolve_source_paths(poi_data, source_data_path, **kwargs)\n\n    if not resolved_paths:\n        self.logger.warning(\n            \"No source data paths resolved. Returning original POI data.\"\n        )\n        return poi_data\n\n    # load data from resolved sources\n    source_data = self.load_data(resolved_paths, **kwargs)\n    self.logger.info(\"Source data loaded successfully\")\n\n    # process the data\n    processed_data = self.process_data(source_data, **kwargs)\n    self.logger.info(\"Data processing completed\")\n\n    # map to POI\n    poi_view = self.map_to_poi(processed_data, poi_data, **kwargs)\n    self.logger.info(\"POI mapping completed\")\n\n    return poi_view\n</code></pre>"},{"location":"api/generators/#gigaspatial.generators.poi.base.PoiViewGenerator.load_data","title":"<code>load_data(source_data_path, **kwargs)</code>  <code>abstractmethod</code>","text":"<p>Load source data for POI processing. This method handles diverse source data formats.</p> <p>Parameters:</p> Name Type Description Default <code>source_data_path</code> <code>List[Union[str, Path]]</code> <p>List of source paths</p> required <code>**kwargs</code> <p>Additional parameters for data loading</p> <code>{}</code> <p>Returns:</p> Type Description <code>Any</code> <p>Data in its source format (DataFrame, GeoDataFrame, TifProcessor, etc.)</p> Source code in <code>gigaspatial/generators/poi/base.py</code> <pre><code>@abstractmethod\ndef load_data(self, source_data_path: List[Union[str, Path]], **kwargs) -&gt; Any:\n    \"\"\"\n    Load source data for POI processing. This method handles diverse source data formats.\n\n    Args:\n        source_data_path: List of source paths\n        **kwargs: Additional parameters for data loading\n\n    Returns:\n        Data in its source format (DataFrame, GeoDataFrame, TifProcessor, etc.)\n    \"\"\"\n    pass\n</code></pre>"},{"location":"api/generators/#gigaspatial.generators.poi.base.PoiViewGenerator.map_to_poi","title":"<code>map_to_poi(processed_data, poi_data, **kwargs)</code>  <code>abstractmethod</code>","text":"<p>Map processed data to POI data.</p> <p>Parameters:</p> Name Type Description Default <code>processed_data</code> <code>Any</code> <p>Processed source data as a GeoDataFrame</p> required <code>poi_data</code> <code>Union[DataFrame, GeoDataFrame]</code> <p>POI data to map to</p> required <code>**kwargs</code> <p>Additional mapping parameters</p> <code>{}</code> <p>Returns:</p> Type Description <code>Union[DataFrame, GeoDataFrame]</code> <p>(Geo)DataFrame with POI data mapped to source data</p> Source code in <code>gigaspatial/generators/poi/base.py</code> <pre><code>@abstractmethod\ndef map_to_poi(\n    self,\n    processed_data: Any,\n    poi_data: Union[pd.DataFrame, gpd.GeoDataFrame],\n    **kwargs,\n) -&gt; Union[pd.DataFrame, gpd.GeoDataFrame]:\n    \"\"\"\n    Map processed data to POI data.\n\n    Args:\n        processed_data: Processed source data as a GeoDataFrame\n        poi_data: POI data to map to\n        **kwargs: Additional mapping parameters\n\n    Returns:\n        (Geo)DataFrame with POI data mapped to source data\n    \"\"\"\n    pass\n</code></pre>"},{"location":"api/generators/#gigaspatial.generators.poi.base.PoiViewGenerator.process_data","title":"<code>process_data(data, **kwargs)</code>","text":"<p>Process the source data to prepare it for POI view generation.</p> Source code in <code>gigaspatial/generators/poi/base.py</code> <pre><code>def process_data(self, data: Any, **kwargs) -&gt; Any:\n    \"\"\"Process the source data to prepare it for POI view generation.\"\"\"\n    return data\n    raise NotImplementedError(\"Subclasses must implement this method...\")\n</code></pre>"},{"location":"api/generators/#gigaspatial.generators.poi.base.PoiViewGenerator.resolve_source_paths","title":"<code>resolve_source_paths(poi_data, explicit_paths=None, **kwargs)</code>","text":"<p>Resolve source data paths based on POI data or explicit paths.</p> <p>This method allows generators to dynamically determine source paths based on the POI data (e.g., by geographic intersection).</p> <p>Parameters:</p> Name Type Description Default <code>poi_data</code> <code>Union[DataFrame, GeoDataFrame]</code> <p>POI data that may be used to determine relevant source paths</p> required <code>explicit_paths</code> <code>Optional[Union[Path, str, List[Union[str, Path]]]]</code> <p>Explicitly provided source paths, if any</p> <code>None</code> <code>**kwargs</code> <p>Additional parameters for path resolution</p> <code>{}</code> <p>Returns:</p> Type Description <code>List[Union[str, Path]]</code> <p>List of resolved source paths</p> Notes <p>Default implementation returns explicit_paths if provided. Subclasses should override this to implement dynamic path resolution.</p> Source code in <code>gigaspatial/generators/poi/base.py</code> <pre><code>def resolve_source_paths(\n    self,\n    poi_data: Union[pd.DataFrame, gpd.GeoDataFrame],\n    explicit_paths: Optional[Union[Path, str, List[Union[str, Path]]]] = None,\n    **kwargs,\n) -&gt; List[Union[str, Path]]:\n    \"\"\"\n    Resolve source data paths based on POI data or explicit paths.\n\n    This method allows generators to dynamically determine source paths\n    based on the POI data (e.g., by geographic intersection).\n\n    Args:\n        poi_data: POI data that may be used to determine relevant source paths\n        explicit_paths: Explicitly provided source paths, if any\n        **kwargs: Additional parameters for path resolution\n\n    Returns:\n        List of resolved source paths\n\n    Notes:\n        Default implementation returns explicit_paths if provided.\n        Subclasses should override this to implement dynamic path resolution.\n    \"\"\"\n    if explicit_paths is not None:\n        if isinstance(explicit_paths, (str, Path)):\n            return [explicit_paths]\n        return list(explicit_paths)\n\n    # Raises NotImplementedError if no explicit paths\n    # and subclass hasn't overridden this method\n    raise NotImplementedError(\n        \"This generator requires explicit source paths or a subclass \"\n        \"implementation of resolve_source_paths()\"\n    )\n</code></pre>"},{"location":"api/generators/#gigaspatial.generators.poi.base.PoiViewGenerator.save_poi_view","title":"<code>save_poi_view(poi_view, output_path, **kwargs)</code>","text":"<p>Save the generated POI view to the data store.</p> <p>Parameters:</p> Name Type Description Default <code>poi_view</code> <code>Union[DataFrame, GeoDataFrame]</code> <p>The POI view DataFrame to save</p> required <code>output_path</code> <code>Union[Path, str]</code> <p>Path where the POI view will be saved in DataStore</p> required <code>**kwargs</code> <p>Additional parameters for saving</p> <code>{}</code> Source code in <code>gigaspatial/generators/poi/base.py</code> <pre><code>def save_poi_view(\n    self,\n    poi_view: Union[pd.DataFrame, gpd.GeoDataFrame],\n    output_path: Union[Path, str],\n    **kwargs,\n) -&gt; None:\n    \"\"\"\n    Save the generated POI view to the data store.\n\n    Args:\n        poi_view: The POI view DataFrame to save\n        output_path: Path where the POI view will be saved in DataStore\n        **kwargs: Additional parameters for saving\n    \"\"\"\n    self.logger.info(f\"Saving POI view to {output_path}\")\n    write_dataset(poi_view, self.data_store, output_path, **kwargs)\n</code></pre>"},{"location":"api/generators/#gigaspatial.generators.poi.base.PoiViewGeneratorConfig","title":"<code>PoiViewGeneratorConfig</code>","text":"<p>Configuration for POI view generation.</p> Source code in <code>gigaspatial/generators/poi/base.py</code> <pre><code>@dataclass\nclass PoiViewGeneratorConfig:\n    \"\"\"Configuration for POI view generation.\"\"\"\n\n    base_path: Path = Field(default=config.get_path(\"poi\", \"views\"))\n    n_workers: int = 4\n</code></pre>"},{"location":"api/generators/#gigaspatial.generators.poi.ghsl_built_s","title":"<code>ghsl_built_s</code>","text":""},{"location":"api/generators/#gigaspatial.generators.poi.ghsl_built_s.GhslBuiltSurfacePoiViewGenerator","title":"<code>GhslBuiltSurfacePoiViewGenerator</code>","text":"<p>               Bases: <code>PoiViewGenerator</code></p> <p>Generate POI views from GHSL Built Surface.</p> Source code in <code>gigaspatial/generators/poi/ghsl_built_s.py</code> <pre><code>class GhslBuiltSurfacePoiViewGenerator(PoiViewGenerator):\n    \"\"\"Generate POI views from GHSL Built Surface.\"\"\"\n\n    def __init__(\n        self,\n        data_config: Optional[GHSLDataConfig] = None,\n        generator_config: Optional[PoiViewGeneratorConfig] = None,\n        data_store: Optional[DataStore] = None,\n    ):\n        super().__init__(generator_config=generator_config, data_store=data_store)\n        self.data_config = data_config or GHSLDataConfig(\n            product=\"GHS_BUILT_S\", year=2020, resolution=100, coord_system=4326\n        )\n\n    def _pre_load_hook(self, source_data_path, **kwargs) -&gt; Any:\n        \"\"\"Pre-processing before loading data files.\"\"\"\n\n        # Convert single path to list for consistent handling\n        if isinstance(source_data_path, (Path, str)):\n            source_data_path = [source_data_path]\n\n        # change source suffix, .zip, to .tif and paths to string\n        source_data_path = [\n            str(file_path.with_suffix(\".tif\")) for file_path in source_data_path\n        ]\n\n        # Validate all paths exist\n        for file_path in source_data_path:\n            if not self.data_store.file_exists(file_path):\n                raise RuntimeError(\n                    f\"Source raster does not exist in the data store: {file_path}\"\n                )\n\n        self.logger.info(\n            f\"Pre-loading validation complete for {len(source_data_path)} files\"\n        )\n        return source_data_path\n\n    def _post_load_hook(self, data, **kwargs) -&gt; Any:\n        \"\"\"Post-processing after loading data files.\"\"\"\n        if not data:\n            self.logger.warning(\"No data was loaded from the source files\")\n            return data\n\n        self.logger.info(\n            f\"Post-load processing complete. {len(data)} valid TifProcessors.\"\n        )\n        return data\n\n    def resolve_source_paths(\n        self,\n        poi_data: Union[pd.DataFrame, gpd.GeoDataFrame],\n        explicit_paths: Optional[Union[Path, str, List[Union[str, Path]]]] = None,\n        **kwargs,\n    ) -&gt; List[Union[str, Path]]:\n        \"\"\"\n        For GHSL Built Surface rasters, resolve source data paths based on POI data geography.\n\n        Returns:\n            List of paths to relevant GHSL BUILT S tile rasters\n        \"\"\"\n        if explicit_paths is not None:\n            if isinstance(explicit_paths, (str, Path)):\n                return [explicit_paths]\n            return list(explicit_paths)\n\n        # Convert to GeoDataFrame if needed\n        if isinstance(poi_data, pd.DataFrame):\n            poi_data = convert_to_geodataframe(poi_data)\n\n        # Find intersecting tiles\n        intersection_tiles = self.data_config.get_intersecting_tiles(\n            geometry=poi_data, crs=poi_data.crs\n        )\n\n        if not intersection_tiles:\n            self.logger.warning(\"There are no matching GHSL tiles for the POI data\")\n            return []\n\n        # Generate paths for each intersecting tile\n        source_data_paths = [\n            self.data_config.get_tile_path(tile_id=tile) for tile in intersection_tiles\n        ]\n\n        self.logger.info(f\"Resolved {len(source_data_paths)} tile paths for POI data\")\n        return source_data_paths\n\n    def load_data(\n        self, source_data_path: Union[Path, List[Path]], **kwargs\n    ) -&gt; List[TifProcessor]:\n        \"\"\"\n        Load GHSL Built Surface rasters into TifProcessors from paths.\n\n        Args:\n            source_data_path: Path(s) to the source data\n            **kwargs: Additional loading parameters\n\n        Returns:\n            List of TifProcessors with built surface data\n        \"\"\"\n\n        processed_paths = self._pre_load_hook(source_data_path, **kwargs)\n\n        tif_processors = [\n            TifProcessor(data_path, self.data_store, mode=\"single\")\n            for data_path in processed_paths\n        ]\n\n        return self._post_load_hook(tif_processors)\n\n    def map_to_poi(\n        self,\n        processed_data: List[TifProcessor],\n        poi_data: Union[pd.DataFrame, gpd.GeoDataFrame],\n        map_radius_meters: float,\n        **kwargs,\n    ) -&gt; pd.DataFrame:\n        \"\"\"\n        Map from TifProcessors to POI data.\n\n        Args:\n            processed_data: TifProcessors\n            poi_data: POI data to map to\n            **kwargs: Additional mapping parameters\n\n        Returns:\n            DataFrame with POI data and built surface information\n        \"\"\"\n\n        # Convert to GeoDataFrame if needed\n        if not isinstance(poi_data, gpd.GeoDataFrame):\n            gdf_points = convert_to_geodataframe(poi_data)\n        else:\n            gdf_points = poi_data\n\n        gdf_points = gdf_points.to_crs(self.data_config.crs)\n\n        polygon_list = buffer_geodataframe(\n            gdf_points, buffer_distance_meters=map_radius_meters, cap_style=\"round\"\n        ).geometry\n\n        sampled_values = sample_multiple_tifs_by_polygons(\n            tif_processors=processed_data, polygon_list=polygon_list, stat=\"sum\"\n        )\n\n        poi_data[\"built_surface_m2\"] = sampled_values\n\n        return poi_data\n\n    def generate_view(\n        self,\n        poi_data: Union[pd.DataFrame, gpd.GeoDataFrame],\n        source_data_path: Optional[Union[Path, List[Path]]] = None,\n        map_radius_meters: float = 150,\n        **kwargs,\n    ) -&gt; pd.DataFrame:\n        \"\"\"\n        Generate POI view from GHSL Built Surface.\n\n        Returns:\n            Enhanced POI data with Built Surface information\n        \"\"\"\n        self.logger.info(\"Generating GHSL Built Surface POI view\")\n\n        return self.generate_poi_view(\n            poi_data=poi_data,\n            source_data_path=source_data_path,\n            map_radius_meters=map_radius_meters,\n            **kwargs,\n        )\n</code></pre>"},{"location":"api/generators/#gigaspatial.generators.poi.ghsl_built_s.GhslBuiltSurfacePoiViewGenerator.generate_view","title":"<code>generate_view(poi_data, source_data_path=None, map_radius_meters=150, **kwargs)</code>","text":"<p>Generate POI view from GHSL Built Surface.</p> <p>Returns:</p> Type Description <code>DataFrame</code> <p>Enhanced POI data with Built Surface information</p> Source code in <code>gigaspatial/generators/poi/ghsl_built_s.py</code> <pre><code>def generate_view(\n    self,\n    poi_data: Union[pd.DataFrame, gpd.GeoDataFrame],\n    source_data_path: Optional[Union[Path, List[Path]]] = None,\n    map_radius_meters: float = 150,\n    **kwargs,\n) -&gt; pd.DataFrame:\n    \"\"\"\n    Generate POI view from GHSL Built Surface.\n\n    Returns:\n        Enhanced POI data with Built Surface information\n    \"\"\"\n    self.logger.info(\"Generating GHSL Built Surface POI view\")\n\n    return self.generate_poi_view(\n        poi_data=poi_data,\n        source_data_path=source_data_path,\n        map_radius_meters=map_radius_meters,\n        **kwargs,\n    )\n</code></pre>"},{"location":"api/generators/#gigaspatial.generators.poi.ghsl_built_s.GhslBuiltSurfacePoiViewGenerator.load_data","title":"<code>load_data(source_data_path, **kwargs)</code>","text":"<p>Load GHSL Built Surface rasters into TifProcessors from paths.</p> <p>Parameters:</p> Name Type Description Default <code>source_data_path</code> <code>Union[Path, List[Path]]</code> <p>Path(s) to the source data</p> required <code>**kwargs</code> <p>Additional loading parameters</p> <code>{}</code> <p>Returns:</p> Type Description <code>List[TifProcessor]</code> <p>List of TifProcessors with built surface data</p> Source code in <code>gigaspatial/generators/poi/ghsl_built_s.py</code> <pre><code>def load_data(\n    self, source_data_path: Union[Path, List[Path]], **kwargs\n) -&gt; List[TifProcessor]:\n    \"\"\"\n    Load GHSL Built Surface rasters into TifProcessors from paths.\n\n    Args:\n        source_data_path: Path(s) to the source data\n        **kwargs: Additional loading parameters\n\n    Returns:\n        List of TifProcessors with built surface data\n    \"\"\"\n\n    processed_paths = self._pre_load_hook(source_data_path, **kwargs)\n\n    tif_processors = [\n        TifProcessor(data_path, self.data_store, mode=\"single\")\n        for data_path in processed_paths\n    ]\n\n    return self._post_load_hook(tif_processors)\n</code></pre>"},{"location":"api/generators/#gigaspatial.generators.poi.ghsl_built_s.GhslBuiltSurfacePoiViewGenerator.map_to_poi","title":"<code>map_to_poi(processed_data, poi_data, map_radius_meters, **kwargs)</code>","text":"<p>Map from TifProcessors to POI data.</p> <p>Parameters:</p> Name Type Description Default <code>processed_data</code> <code>List[TifProcessor]</code> <p>TifProcessors</p> required <code>poi_data</code> <code>Union[DataFrame, GeoDataFrame]</code> <p>POI data to map to</p> required <code>**kwargs</code> <p>Additional mapping parameters</p> <code>{}</code> <p>Returns:</p> Type Description <code>DataFrame</code> <p>DataFrame with POI data and built surface information</p> Source code in <code>gigaspatial/generators/poi/ghsl_built_s.py</code> <pre><code>def map_to_poi(\n    self,\n    processed_data: List[TifProcessor],\n    poi_data: Union[pd.DataFrame, gpd.GeoDataFrame],\n    map_radius_meters: float,\n    **kwargs,\n) -&gt; pd.DataFrame:\n    \"\"\"\n    Map from TifProcessors to POI data.\n\n    Args:\n        processed_data: TifProcessors\n        poi_data: POI data to map to\n        **kwargs: Additional mapping parameters\n\n    Returns:\n        DataFrame with POI data and built surface information\n    \"\"\"\n\n    # Convert to GeoDataFrame if needed\n    if not isinstance(poi_data, gpd.GeoDataFrame):\n        gdf_points = convert_to_geodataframe(poi_data)\n    else:\n        gdf_points = poi_data\n\n    gdf_points = gdf_points.to_crs(self.data_config.crs)\n\n    polygon_list = buffer_geodataframe(\n        gdf_points, buffer_distance_meters=map_radius_meters, cap_style=\"round\"\n    ).geometry\n\n    sampled_values = sample_multiple_tifs_by_polygons(\n        tif_processors=processed_data, polygon_list=polygon_list, stat=\"sum\"\n    )\n\n    poi_data[\"built_surface_m2\"] = sampled_values\n\n    return poi_data\n</code></pre>"},{"location":"api/generators/#gigaspatial.generators.poi.ghsl_built_s.GhslBuiltSurfacePoiViewGenerator.resolve_source_paths","title":"<code>resolve_source_paths(poi_data, explicit_paths=None, **kwargs)</code>","text":"<p>For GHSL Built Surface rasters, resolve source data paths based on POI data geography.</p> <p>Returns:</p> Type Description <code>List[Union[str, Path]]</code> <p>List of paths to relevant GHSL BUILT S tile rasters</p> Source code in <code>gigaspatial/generators/poi/ghsl_built_s.py</code> <pre><code>def resolve_source_paths(\n    self,\n    poi_data: Union[pd.DataFrame, gpd.GeoDataFrame],\n    explicit_paths: Optional[Union[Path, str, List[Union[str, Path]]]] = None,\n    **kwargs,\n) -&gt; List[Union[str, Path]]:\n    \"\"\"\n    For GHSL Built Surface rasters, resolve source data paths based on POI data geography.\n\n    Returns:\n        List of paths to relevant GHSL BUILT S tile rasters\n    \"\"\"\n    if explicit_paths is not None:\n        if isinstance(explicit_paths, (str, Path)):\n            return [explicit_paths]\n        return list(explicit_paths)\n\n    # Convert to GeoDataFrame if needed\n    if isinstance(poi_data, pd.DataFrame):\n        poi_data = convert_to_geodataframe(poi_data)\n\n    # Find intersecting tiles\n    intersection_tiles = self.data_config.get_intersecting_tiles(\n        geometry=poi_data, crs=poi_data.crs\n    )\n\n    if not intersection_tiles:\n        self.logger.warning(\"There are no matching GHSL tiles for the POI data\")\n        return []\n\n    # Generate paths for each intersecting tile\n    source_data_paths = [\n        self.data_config.get_tile_path(tile_id=tile) for tile in intersection_tiles\n    ]\n\n    self.logger.info(f\"Resolved {len(source_data_paths)} tile paths for POI data\")\n    return source_data_paths\n</code></pre>"},{"location":"api/generators/#gigaspatial.generators.poi.ghsl_smod","title":"<code>ghsl_smod</code>","text":""},{"location":"api/generators/#gigaspatial.generators.poi.ghsl_smod.GhslSmodPoiViewGenerator","title":"<code>GhslSmodPoiViewGenerator</code>","text":"<p>               Bases: <code>PoiViewGenerator</code></p> <p>Generate POI views from GHSL Settlement Model (SMOD).</p> Source code in <code>gigaspatial/generators/poi/ghsl_smod.py</code> <pre><code>class GhslSmodPoiViewGenerator(PoiViewGenerator):\n    \"\"\"Generate POI views from GHSL Settlement Model (SMOD).\"\"\"\n\n    def __init__(\n        self,\n        data_config: Optional[GHSLDataConfig] = None,\n        generator_config: Optional[PoiViewGeneratorConfig] = None,\n        data_store: Optional[DataStore] = None,\n    ):\n        super().__init__(generator_config=generator_config, data_store=data_store)\n        self.data_config = data_config or GHSLDataConfig(\n            product=\"GHS_SMOD\", year=2020, resolution=1000, coord_system=54009\n        )\n\n    def _pre_load_hook(self, source_data_path, **kwargs) -&gt; Any:\n        \"\"\"Pre-processing before loading data files.\"\"\"\n\n        # Convert single path to list for consistent handling\n        if isinstance(source_data_path, (Path, str)):\n            source_data_path = [source_data_path]\n\n        # change source suffix, .zip, to .tif and paths to string\n        source_data_path = [\n            str(file_path.with_suffix(\".tif\")) for file_path in source_data_path\n        ]\n\n        # Validate all paths exist\n        for file_path in source_data_path:\n            if not self.data_store.file_exists(file_path):\n                raise RuntimeError(\n                    f\"Source raster does not exist in the data store: {file_path}\"\n                )\n\n        self.logger.info(\n            f\"Pre-loading validation complete for {len(source_data_path)} files\"\n        )\n        return source_data_path\n\n    def _post_load_hook(self, data, **kwargs) -&gt; Any:\n        \"\"\"Post-processing after loading data files.\"\"\"\n        if not data:\n            self.logger.warning(\"No data was loaded from the source files\")\n            return data\n\n        self.logger.info(\n            f\"Post-load processing complete. {len(data)} valid TifProcessors.\"\n        )\n        return data\n\n    def resolve_source_paths(\n        self,\n        poi_data: Union[pd.DataFrame, gpd.GeoDataFrame],\n        explicit_paths: Optional[Union[Path, str, List[Union[str, Path]]]] = None,\n        **kwargs,\n    ) -&gt; List[Union[str, Path]]:\n        \"\"\"\n        For GHSL SMOD rasters, resolve source data paths based on POI data geography.\n\n        Returns:\n            List of paths to relevant GHSL SMOD tile rasters\n        \"\"\"\n        if explicit_paths is not None:\n            if isinstance(explicit_paths, (str, Path)):\n                return [explicit_paths]\n            return list(explicit_paths)\n\n        # Convert to GeoDataFrame if needed\n        if isinstance(poi_data, pd.DataFrame):\n            poi_data = convert_to_geodataframe(poi_data)\n\n        # Find intersecting tiles\n        intersection_tiles = self.data_config.get_intersecting_tiles(\n            geometry=poi_data, crs=poi_data.crs\n        )\n\n        if not intersection_tiles:\n            self.logger.warning(\"There are no matching GHSL tiles for the POI data\")\n            return []\n\n        # Generate paths for each intersecting tile\n        source_data_paths = [\n            self.data_config.get_tile_path(tile_id=tile) for tile in intersection_tiles\n        ]\n\n        self.logger.info(f\"Resolved {len(source_data_paths)} tile paths for POI data\")\n        return source_data_paths\n\n    def load_data(\n        self, source_data_path: Union[Path, List[Path]], **kwargs\n    ) -&gt; List[TifProcessor]:\n        \"\"\"\n        Load GHSL SMOD rasters into TifProcessors from paths.\n\n        Args:\n            source_data_path: Path(s) to the source data\n            **kwargs: Additional loading parameters\n\n        Returns:\n            List of TifProcessors with settlement model data\n        \"\"\"\n\n        processed_paths = self._pre_load_hook(source_data_path, **kwargs)\n\n        tif_processors = [\n            TifProcessor(data_path, self.data_store, mode=\"single\")\n            for data_path in processed_paths\n        ]\n\n        return self._post_load_hook(tif_processors)\n\n    def map_to_poi(\n        self,\n        processed_data: List[TifProcessor],\n        poi_data: Union[pd.DataFrame, gpd.GeoDataFrame],\n        **kwargs,\n    ) -&gt; pd.DataFrame:\n        \"\"\"\n        Map from TifProcessors to POI data.\n\n        Args:\n            processed_data: TifProcessors\n            poi_data: POI data to map to\n            **kwargs: Additional mapping parameters\n\n        Returns:\n            DataFrame with POI data and SMOD classification information\n        \"\"\"\n\n        # Convert to GeoDataFrame if needed\n        if not isinstance(poi_data, gpd.GeoDataFrame):\n            gdf_points = convert_to_geodataframe(poi_data)\n        else:\n            gdf_points = poi_data\n\n        gdf_points = gdf_points.to_crs(self.data_config.crs)\n\n        coord_list = [\n            (x, y) for x, y in zip(gdf_points[\"geometry\"].x, gdf_points[\"geometry\"].y)\n        ]\n\n        sampled_values = sample_multiple_tifs_by_coordinates(\n            tif_processors=processed_data, coordinate_list=coord_list\n        )\n\n        poi_data[\"smod_class\"] = sampled_values\n\n        return poi_data\n\n    def generate_view(\n        self,\n        poi_data: Union[pd.DataFrame, gpd.GeoDataFrame],\n        source_data_path: Optional[Union[Path, List[Path]]] = None,\n        **kwargs,\n    ) -&gt; pd.DataFrame:\n        \"\"\"\n        Generate POI view from GHSL Settlement Model.\n\n        Returns:\n            Enhanced POI data with SMOD classification\n        \"\"\"\n        self.logger.info(\"Generating GHSL Settlement Model POI view\")\n\n        return self.generate_poi_view(\n            poi_data=poi_data,\n            source_data_path=source_data_path,\n            **kwargs,\n        )\n</code></pre>"},{"location":"api/generators/#gigaspatial.generators.poi.ghsl_smod.GhslSmodPoiViewGenerator.generate_view","title":"<code>generate_view(poi_data, source_data_path=None, **kwargs)</code>","text":"<p>Generate POI view from GHSL Settlement Model.</p> <p>Returns:</p> Type Description <code>DataFrame</code> <p>Enhanced POI data with SMOD classification</p> Source code in <code>gigaspatial/generators/poi/ghsl_smod.py</code> <pre><code>def generate_view(\n    self,\n    poi_data: Union[pd.DataFrame, gpd.GeoDataFrame],\n    source_data_path: Optional[Union[Path, List[Path]]] = None,\n    **kwargs,\n) -&gt; pd.DataFrame:\n    \"\"\"\n    Generate POI view from GHSL Settlement Model.\n\n    Returns:\n        Enhanced POI data with SMOD classification\n    \"\"\"\n    self.logger.info(\"Generating GHSL Settlement Model POI view\")\n\n    return self.generate_poi_view(\n        poi_data=poi_data,\n        source_data_path=source_data_path,\n        **kwargs,\n    )\n</code></pre>"},{"location":"api/generators/#gigaspatial.generators.poi.ghsl_smod.GhslSmodPoiViewGenerator.load_data","title":"<code>load_data(source_data_path, **kwargs)</code>","text":"<p>Load GHSL SMOD rasters into TifProcessors from paths.</p> <p>Parameters:</p> Name Type Description Default <code>source_data_path</code> <code>Union[Path, List[Path]]</code> <p>Path(s) to the source data</p> required <code>**kwargs</code> <p>Additional loading parameters</p> <code>{}</code> <p>Returns:</p> Type Description <code>List[TifProcessor]</code> <p>List of TifProcessors with settlement model data</p> Source code in <code>gigaspatial/generators/poi/ghsl_smod.py</code> <pre><code>def load_data(\n    self, source_data_path: Union[Path, List[Path]], **kwargs\n) -&gt; List[TifProcessor]:\n    \"\"\"\n    Load GHSL SMOD rasters into TifProcessors from paths.\n\n    Args:\n        source_data_path: Path(s) to the source data\n        **kwargs: Additional loading parameters\n\n    Returns:\n        List of TifProcessors with settlement model data\n    \"\"\"\n\n    processed_paths = self._pre_load_hook(source_data_path, **kwargs)\n\n    tif_processors = [\n        TifProcessor(data_path, self.data_store, mode=\"single\")\n        for data_path in processed_paths\n    ]\n\n    return self._post_load_hook(tif_processors)\n</code></pre>"},{"location":"api/generators/#gigaspatial.generators.poi.ghsl_smod.GhslSmodPoiViewGenerator.map_to_poi","title":"<code>map_to_poi(processed_data, poi_data, **kwargs)</code>","text":"<p>Map from TifProcessors to POI data.</p> <p>Parameters:</p> Name Type Description Default <code>processed_data</code> <code>List[TifProcessor]</code> <p>TifProcessors</p> required <code>poi_data</code> <code>Union[DataFrame, GeoDataFrame]</code> <p>POI data to map to</p> required <code>**kwargs</code> <p>Additional mapping parameters</p> <code>{}</code> <p>Returns:</p> Type Description <code>DataFrame</code> <p>DataFrame with POI data and SMOD classification information</p> Source code in <code>gigaspatial/generators/poi/ghsl_smod.py</code> <pre><code>def map_to_poi(\n    self,\n    processed_data: List[TifProcessor],\n    poi_data: Union[pd.DataFrame, gpd.GeoDataFrame],\n    **kwargs,\n) -&gt; pd.DataFrame:\n    \"\"\"\n    Map from TifProcessors to POI data.\n\n    Args:\n        processed_data: TifProcessors\n        poi_data: POI data to map to\n        **kwargs: Additional mapping parameters\n\n    Returns:\n        DataFrame with POI data and SMOD classification information\n    \"\"\"\n\n    # Convert to GeoDataFrame if needed\n    if not isinstance(poi_data, gpd.GeoDataFrame):\n        gdf_points = convert_to_geodataframe(poi_data)\n    else:\n        gdf_points = poi_data\n\n    gdf_points = gdf_points.to_crs(self.data_config.crs)\n\n    coord_list = [\n        (x, y) for x, y in zip(gdf_points[\"geometry\"].x, gdf_points[\"geometry\"].y)\n    ]\n\n    sampled_values = sample_multiple_tifs_by_coordinates(\n        tif_processors=processed_data, coordinate_list=coord_list\n    )\n\n    poi_data[\"smod_class\"] = sampled_values\n\n    return poi_data\n</code></pre>"},{"location":"api/generators/#gigaspatial.generators.poi.ghsl_smod.GhslSmodPoiViewGenerator.resolve_source_paths","title":"<code>resolve_source_paths(poi_data, explicit_paths=None, **kwargs)</code>","text":"<p>For GHSL SMOD rasters, resolve source data paths based on POI data geography.</p> <p>Returns:</p> Type Description <code>List[Union[str, Path]]</code> <p>List of paths to relevant GHSL SMOD tile rasters</p> Source code in <code>gigaspatial/generators/poi/ghsl_smod.py</code> <pre><code>def resolve_source_paths(\n    self,\n    poi_data: Union[pd.DataFrame, gpd.GeoDataFrame],\n    explicit_paths: Optional[Union[Path, str, List[Union[str, Path]]]] = None,\n    **kwargs,\n) -&gt; List[Union[str, Path]]:\n    \"\"\"\n    For GHSL SMOD rasters, resolve source data paths based on POI data geography.\n\n    Returns:\n        List of paths to relevant GHSL SMOD tile rasters\n    \"\"\"\n    if explicit_paths is not None:\n        if isinstance(explicit_paths, (str, Path)):\n            return [explicit_paths]\n        return list(explicit_paths)\n\n    # Convert to GeoDataFrame if needed\n    if isinstance(poi_data, pd.DataFrame):\n        poi_data = convert_to_geodataframe(poi_data)\n\n    # Find intersecting tiles\n    intersection_tiles = self.data_config.get_intersecting_tiles(\n        geometry=poi_data, crs=poi_data.crs\n    )\n\n    if not intersection_tiles:\n        self.logger.warning(\"There are no matching GHSL tiles for the POI data\")\n        return []\n\n    # Generate paths for each intersecting tile\n    source_data_paths = [\n        self.data_config.get_tile_path(tile_id=tile) for tile in intersection_tiles\n    ]\n\n    self.logger.info(f\"Resolved {len(source_data_paths)} tile paths for POI data\")\n    return source_data_paths\n</code></pre>"},{"location":"api/generators/#gigaspatial.generators.poi.google_open_buildings","title":"<code>google_open_buildings</code>","text":""},{"location":"api/generators/#gigaspatial.generators.poi.google_open_buildings.GoogleBuildingsPoiViewGenerator","title":"<code>GoogleBuildingsPoiViewGenerator</code>","text":"<p>               Bases: <code>PoiViewGenerator</code></p> <p>Generate POI views from Google Open Buildings data.</p> Source code in <code>gigaspatial/generators/poi/google_open_buildings.py</code> <pre><code>class GoogleBuildingsPoiViewGenerator(PoiViewGenerator):\n    \"\"\"Generate POI views from Google Open Buildings data.\"\"\"\n\n    def __init__(\n        self,\n        data_config: Optional[GoogleOpenBuildingsConfig] = None,\n        generator_config: Optional[PoiViewGeneratorConfig] = None,\n        data_store: Optional[DataStore] = None,\n    ):\n        super().__init__(generator_config=generator_config, data_store=data_store)\n        self.data_config = data_config or GoogleOpenBuildingsConfig()\n        self.handler = GoogleOpenBuildingsDownloader(\n            config=self.data_config, data_store=self.data_store\n        )\n\n    def _pre_load_hook(self, source_data_path, **kwargs) -&gt; Any:\n        \"\"\"Pre-processing before loading data files.\"\"\"\n\n        # Convert single path to list for consistent handling\n        if isinstance(source_data_path, (Path, str)):\n            source_data_path = [source_data_path]\n\n        source_data_path = [str(file_path) for file_path in source_data_path]\n\n        # Validate all paths exist\n        for file_path in source_data_path:\n            if not self.handler.data_store.file_exists(file_path):\n                raise RuntimeError(\n                    f\"Source buildings file does not exist in the data store: {file_path}\"\n                )\n\n        self.logger.info(\n            f\"Pre-loading validation complete for {len(source_data_path)} files\"\n        )\n        return source_data_path\n\n    def _post_load_hook(self, data, **kwargs) -&gt; Any:\n        \"\"\"Post-processing after loading data files.\"\"\"\n        if data.empty:\n            self.logger.warning(\"No data was loaded from the source files\")\n            return data\n\n        self.logger.info(\n            f\"Post-load processing complete. {len(data)} valid building records.\"\n        )\n        return data\n\n    def resolve_source_paths(\n        self,\n        poi_data: Union[pd.DataFrame, gpd.GeoDataFrame],\n        explicit_paths: Optional[Union[Path, str, List[Union[str, Path]]]] = None,\n        **kwargs,\n    ) -&gt; List[Union[str, Path]]:\n        \"\"\"\n        For Google Open Buildings, resolve source data paths based on POI data geography. This determines which tile files\n        intersect with the POI data's geographic extent.\n        \"\"\"\n        if explicit_paths is not None:\n            if isinstance(explicit_paths, (str, Path)):\n                return [explicit_paths]\n            return list(explicit_paths)\n\n        # Convert to GeoDataFrame if needed\n        if not isinstance(poi_data, gpd.GeoDataFrame):\n            poi_data = convert_to_geodataframe(poi_data)\n\n        # Find intersecting tiles\n        intersection_tiles = self.handler._get_intersecting_tiles(poi_data)\n\n        if intersection_tiles.empty:\n            self.logger.warning(\n                \"There are no matching Google buildings tiles for the POI data\"\n            )\n            return []\n\n        # Generate paths for each intersecting tile\n        source_data_paths = [\n            self.data_config.get_tile_path(tile_id=tile, data_type=\"points\")\n            for tile in intersection_tiles.tile_id\n        ]\n\n        self.logger.info(f\"Resolved {len(source_data_paths)} tile paths for POI data\")\n        return source_data_paths\n\n    def load_data(\n        self, source_data_path: Union[Path, List[Path]], **kwargs\n    ) -&gt; pd.DataFrame:\n        \"\"\"\n        Load building data from paths.\n\n        Args:\n            source_data_path: Path(s) to the source data\n            **kwargs: Additional loading parameters\n\n        Returns:\n            DataFrame containing building data\n        \"\"\"\n\n        processed_paths = self._pre_load_hook(source_data_path, **kwargs)\n\n        all_data = []\n        for file_path in processed_paths:\n            all_data.append(read_dataset(self.handler.data_store, file_path))\n\n        if not all_data:\n            return pd.DataFrame()\n\n        # Concatenate all tile data\n        result = pd.concat(all_data, ignore_index=True)\n\n        return self._post_load_hook(result)\n\n    def map_to_poi(\n        self, processed_data: pd.DataFrame, poi_data: pd.DataFrame, **kwargs\n    ) -&gt; pd.DataFrame:\n        \"\"\"\n        Map processed building data to POI data.\n\n        Args:\n            processed_data: Processed building data as GeoDataFrame\n            poi_data: POI data to map to\n            **kwargs: Additional mapping parameters\n\n        Returns:\n            DataFrame with POI data and nearest building information\n        \"\"\"\n\n        tree = cKDTree(processed_data[[\"latitude\", \"longitude\"]])\n\n        if \"latitude\" not in poi_data:\n            poi_lat_col, poi_lon_col = detect_coordinate_columns(poi_data)\n            df_points = poi_data.rename(\n                columns={poi_lat_col: \"latitude\", poi_lon_col: \"longitude\"}\n            )\n        else:\n            df_points = poi_data.copy()\n\n        _, idx = tree.query(df_points[[\"latitude\", \"longitude\"]], k=1)\n\n        df_nearest_buildings = processed_data.iloc[idx]\n\n        dist = calculate_distance(\n            lat1=df_points.latitude,\n            lon1=df_points.longitude,\n            lat2=df_nearest_buildings.latitude,\n            lon2=df_nearest_buildings.longitude,\n        )\n\n        poi_data[\"nearest_google_building_id\"] = (\n            df_nearest_buildings.full_plus_code.to_numpy()\n        )\n        poi_data[\"nearest_google_building_distance\"] = dist\n\n        return poi_data\n\n    def generate_view(\n        self,\n        poi_data: Union[pd.DataFrame, gpd.GeoDataFrame],\n        source_data_path: Optional[Union[Path, List[Path]]] = None,\n        **kwargs,\n    ) -&gt; pd.DataFrame:\n        \"\"\"\n        Generate POI view for Google Open Buildings.\n\n        Returns:\n            Enhanced POI data with nearest building information\n        \"\"\"\n        self.logger.info(\"Generating Google Open Buildings POI view\")\n\n        return self.generate_poi_view(\n            poi_data=poi_data,\n            source_data_path=source_data_path,\n            **kwargs,\n        )\n</code></pre>"},{"location":"api/generators/#gigaspatial.generators.poi.google_open_buildings.GoogleBuildingsPoiViewGenerator.generate_view","title":"<code>generate_view(poi_data, source_data_path=None, **kwargs)</code>","text":"<p>Generate POI view for Google Open Buildings.</p> <p>Returns:</p> Type Description <code>DataFrame</code> <p>Enhanced POI data with nearest building information</p> Source code in <code>gigaspatial/generators/poi/google_open_buildings.py</code> <pre><code>def generate_view(\n    self,\n    poi_data: Union[pd.DataFrame, gpd.GeoDataFrame],\n    source_data_path: Optional[Union[Path, List[Path]]] = None,\n    **kwargs,\n) -&gt; pd.DataFrame:\n    \"\"\"\n    Generate POI view for Google Open Buildings.\n\n    Returns:\n        Enhanced POI data with nearest building information\n    \"\"\"\n    self.logger.info(\"Generating Google Open Buildings POI view\")\n\n    return self.generate_poi_view(\n        poi_data=poi_data,\n        source_data_path=source_data_path,\n        **kwargs,\n    )\n</code></pre>"},{"location":"api/generators/#gigaspatial.generators.poi.google_open_buildings.GoogleBuildingsPoiViewGenerator.load_data","title":"<code>load_data(source_data_path, **kwargs)</code>","text":"<p>Load building data from paths.</p> <p>Parameters:</p> Name Type Description Default <code>source_data_path</code> <code>Union[Path, List[Path]]</code> <p>Path(s) to the source data</p> required <code>**kwargs</code> <p>Additional loading parameters</p> <code>{}</code> <p>Returns:</p> Type Description <code>DataFrame</code> <p>DataFrame containing building data</p> Source code in <code>gigaspatial/generators/poi/google_open_buildings.py</code> <pre><code>def load_data(\n    self, source_data_path: Union[Path, List[Path]], **kwargs\n) -&gt; pd.DataFrame:\n    \"\"\"\n    Load building data from paths.\n\n    Args:\n        source_data_path: Path(s) to the source data\n        **kwargs: Additional loading parameters\n\n    Returns:\n        DataFrame containing building data\n    \"\"\"\n\n    processed_paths = self._pre_load_hook(source_data_path, **kwargs)\n\n    all_data = []\n    for file_path in processed_paths:\n        all_data.append(read_dataset(self.handler.data_store, file_path))\n\n    if not all_data:\n        return pd.DataFrame()\n\n    # Concatenate all tile data\n    result = pd.concat(all_data, ignore_index=True)\n\n    return self._post_load_hook(result)\n</code></pre>"},{"location":"api/generators/#gigaspatial.generators.poi.google_open_buildings.GoogleBuildingsPoiViewGenerator.map_to_poi","title":"<code>map_to_poi(processed_data, poi_data, **kwargs)</code>","text":"<p>Map processed building data to POI data.</p> <p>Parameters:</p> Name Type Description Default <code>processed_data</code> <code>DataFrame</code> <p>Processed building data as GeoDataFrame</p> required <code>poi_data</code> <code>DataFrame</code> <p>POI data to map to</p> required <code>**kwargs</code> <p>Additional mapping parameters</p> <code>{}</code> <p>Returns:</p> Type Description <code>DataFrame</code> <p>DataFrame with POI data and nearest building information</p> Source code in <code>gigaspatial/generators/poi/google_open_buildings.py</code> <pre><code>def map_to_poi(\n    self, processed_data: pd.DataFrame, poi_data: pd.DataFrame, **kwargs\n) -&gt; pd.DataFrame:\n    \"\"\"\n    Map processed building data to POI data.\n\n    Args:\n        processed_data: Processed building data as GeoDataFrame\n        poi_data: POI data to map to\n        **kwargs: Additional mapping parameters\n\n    Returns:\n        DataFrame with POI data and nearest building information\n    \"\"\"\n\n    tree = cKDTree(processed_data[[\"latitude\", \"longitude\"]])\n\n    if \"latitude\" not in poi_data:\n        poi_lat_col, poi_lon_col = detect_coordinate_columns(poi_data)\n        df_points = poi_data.rename(\n            columns={poi_lat_col: \"latitude\", poi_lon_col: \"longitude\"}\n        )\n    else:\n        df_points = poi_data.copy()\n\n    _, idx = tree.query(df_points[[\"latitude\", \"longitude\"]], k=1)\n\n    df_nearest_buildings = processed_data.iloc[idx]\n\n    dist = calculate_distance(\n        lat1=df_points.latitude,\n        lon1=df_points.longitude,\n        lat2=df_nearest_buildings.latitude,\n        lon2=df_nearest_buildings.longitude,\n    )\n\n    poi_data[\"nearest_google_building_id\"] = (\n        df_nearest_buildings.full_plus_code.to_numpy()\n    )\n    poi_data[\"nearest_google_building_distance\"] = dist\n\n    return poi_data\n</code></pre>"},{"location":"api/generators/#gigaspatial.generators.poi.google_open_buildings.GoogleBuildingsPoiViewGenerator.resolve_source_paths","title":"<code>resolve_source_paths(poi_data, explicit_paths=None, **kwargs)</code>","text":"<p>For Google Open Buildings, resolve source data paths based on POI data geography. This determines which tile files intersect with the POI data's geographic extent.</p> Source code in <code>gigaspatial/generators/poi/google_open_buildings.py</code> <pre><code>def resolve_source_paths(\n    self,\n    poi_data: Union[pd.DataFrame, gpd.GeoDataFrame],\n    explicit_paths: Optional[Union[Path, str, List[Union[str, Path]]]] = None,\n    **kwargs,\n) -&gt; List[Union[str, Path]]:\n    \"\"\"\n    For Google Open Buildings, resolve source data paths based on POI data geography. This determines which tile files\n    intersect with the POI data's geographic extent.\n    \"\"\"\n    if explicit_paths is not None:\n        if isinstance(explicit_paths, (str, Path)):\n            return [explicit_paths]\n        return list(explicit_paths)\n\n    # Convert to GeoDataFrame if needed\n    if not isinstance(poi_data, gpd.GeoDataFrame):\n        poi_data = convert_to_geodataframe(poi_data)\n\n    # Find intersecting tiles\n    intersection_tiles = self.handler._get_intersecting_tiles(poi_data)\n\n    if intersection_tiles.empty:\n        self.logger.warning(\n            \"There are no matching Google buildings tiles for the POI data\"\n        )\n        return []\n\n    # Generate paths for each intersecting tile\n    source_data_paths = [\n        self.data_config.get_tile_path(tile_id=tile, data_type=\"points\")\n        for tile in intersection_tiles.tile_id\n    ]\n\n    self.logger.info(f\"Resolved {len(source_data_paths)} tile paths for POI data\")\n    return source_data_paths\n</code></pre>"},{"location":"api/generators/#gigaspatial.generators.poi.microsoft_global_buildings","title":"<code>microsoft_global_buildings</code>","text":""},{"location":"api/generators/#gigaspatial.generators.poi.microsoft_global_buildings.MSBuildingsPoiViewGenerator","title":"<code>MSBuildingsPoiViewGenerator</code>","text":"<p>               Bases: <code>PoiViewGenerator</code></p> <p>Generate POI views from Microsoft Global Buildings data.</p> Source code in <code>gigaspatial/generators/poi/microsoft_global_buildings.py</code> <pre><code>class MSBuildingsPoiViewGenerator(PoiViewGenerator):\n    \"\"\"Generate POI views from Microsoft Global Buildings data.\"\"\"\n\n    def __init__(\n        self,\n        data_config: Optional[MSBuildingsConfig] = None,\n        generator_config: Optional[PoiViewGeneratorConfig] = None,\n        data_store: Optional[DataStore] = None,\n    ):\n        super().__init__(generator_config=generator_config, data_store=data_store)\n        self.data_config = data_config or MSBuildingsConfig(data_store=self.data_store)\n\n    def _pre_load_hook(self, source_data_path, **kwargs) -&gt; Any:\n        \"\"\"Pre-processing before loading data files.\"\"\"\n\n        # Convert single path to list for consistent handling\n        if isinstance(source_data_path, (Path, str)):\n            source_data_path = [source_data_path]\n\n        source_data_path = [str(file_path) for file_path in source_data_path]\n\n        # Validate all paths exist\n        for file_path in source_data_path:\n            if not self.data_store.file_exists(file_path):\n                raise RuntimeError(\n                    f\"Source buildings file does not exist in the data store: {file_path}\"\n                )\n\n        self.logger.info(\n            f\"Pre-loading validation complete for {len(source_data_path)} files\"\n        )\n        return source_data_path\n\n    def _post_load_hook(self, data, **kwargs) -&gt; Any:\n        \"\"\"Post-processing after loading data files.\"\"\"\n        if data.empty:\n            self.logger.warning(\"No data was loaded from the source files\")\n            return data\n\n        self.logger.info(\n            f\"Post-load processing complete. {len(data)} valid building records.\"\n        )\n        return data\n\n    def resolve_source_paths(\n        self,\n        poi_data: Union[pd.DataFrame, gpd.GeoDataFrame],\n        explicit_paths: Optional[Union[Path, str, List[Union[str, Path]]]] = None,\n        **kwargs,\n    ) -&gt; List[Union[str, Path]]:\n        \"\"\"\n        For Microsoft Buildings, resolve source data paths based on POI data geography.\n\n        Returns:\n            List of paths to relevant Microsoft Buildings tile files\n        \"\"\"\n        # Return explicit paths if provided\n        if explicit_paths is not None:\n            if isinstance(explicit_paths, (str, Path)):\n                return [explicit_paths]\n            return list(explicit_paths)\n\n        if \"latitude\" not in poi_data:\n            poi_lat_col, poi_lon_col = detect_coordinate_columns(poi_data)\n        else:\n            poi_lat_col, poi_lon_col = (\"latitude\", \"longitude\")\n\n        points = poi_data[[poi_lat_col, poi_lon_col]].to_numpy()\n\n        # Find intersecting tiles\n        tiles = self.data_config.get_tiles_for_points(points)\n\n        if tiles.empty:\n            self.logger.warning(\n                \"There are no matching Microsoft Buildings tiles for the POI data\"\n            )\n            return []\n\n        # Generate paths for each intersecting tile\n        source_data_paths = [\n            self.data_config.get_tile_path(\n                quadkey=tile[\"quadkey\"],\n                location=tile[\"country\"] if tile[\"country\"] else tile[\"location\"],\n            )\n            for _, tile in tiles.iterrows()\n        ]\n\n        self.logger.info(f\"Resolved {len(source_data_paths)} tile paths for POI data\")\n        return source_data_paths\n\n    def load_data(\n        self, source_data_path: Union[Path, List[Path]], **kwargs\n    ) -&gt; gpd.GeoDataFrame:\n        \"\"\"\n        Load building data from Microsoft Buildings dataset.\n\n        Args:\n            source_data_path: Path(s) to the source data files\n            **kwargs: Additional loading parameters\n\n        Returns:\n            DataFrame containing building data\n        \"\"\"\n\n        def read_ms_dataset(data_store: DataStore, file_path: str):\n            df = read_gzipped_json_or_csv(file_path=file_path, data_store=data_store)\n            df[\"geometry\"] = df[\"geometry\"].apply(shape)\n            return gpd.GeoDataFrame(df, crs=4326)\n\n        processed_paths = self._pre_load_hook(source_data_path, **kwargs)\n\n        all_data = []\n        for file_path in processed_paths:\n            all_data.append(read_ms_dataset(self.data_store, file_path))\n\n        if not all_data:\n            return pd.DataFrame()\n\n        # Concatenate all tile data\n        result = pd.concat(all_data, ignore_index=True)\n\n        return self._post_load_hook(result)\n\n    def process_data(self, data: gpd.GeoDataFrame, **kwargs) -&gt; pd.DataFrame:\n        return data.get_coordinates()\n\n    def map_to_poi(\n        self, processed_data: pd.DataFrame, poi_data: pd.DataFrame, **kwargs\n    ) -&gt; pd.DataFrame:\n        \"\"\"\n        Map processed building data to POI data.\n\n        Args:\n            processed_data: Processed building data as GeoDataFrame\n            poi_data: POI data to map to\n            **kwargs: Additional mapping parameters\n\n        Returns:\n            DataFrame with POI data and nearest building information\n        \"\"\"\n\n        tree = cKDTree(processed_data[[\"y\", \"x\"]])\n\n        if \"latitude\" not in poi_data:\n            poi_lat_col, poi_lon_col = detect_coordinate_columns(poi_data)\n            df_points = poi_data.rename(\n                columns={poi_lat_col: \"latitude\", poi_lon_col: \"longitude\"}\n            )\n        else:\n            df_points = poi_data.copy()\n\n        _, idx = tree.query(df_points[[\"latitude\", \"longitude\"]], k=1)\n\n        df_nearest_buildings = processed_data.iloc[idx]\n\n        dist = calculate_distance(\n            lat1=df_points.latitude,\n            lon1=df_points.longitude,\n            lat2=df_nearest_buildings.y,\n            lon2=df_nearest_buildings.x,\n        )\n\n        poi_data[\"nearest_ms_building_id\"] = df_nearest_buildings.get(\n            \"building_id\", None\n        )\n        poi_data[\"nearest_ms_building_distance\"] = dist\n\n        return poi_data\n\n    def generate_view(\n        self,\n        poi_data: Union[pd.DataFrame, gpd.GeoDataFrame],\n        source_data_path: Optional[Union[Path, List[Path]]] = None,\n        **kwargs,\n    ) -&gt; pd.DataFrame:\n        \"\"\"\n        Generate POI view for Microsoft Global Buildings.\n\n        Returns:\n            Enhanced POI data with nearest building information\n        \"\"\"\n        self.logger.info(\"Generating MicrosoftBuildings POI view\")\n\n        return self.generate_poi_view(\n            poi_data=poi_data,\n            source_data_path=source_data_path,\n            **kwargs,\n        )\n</code></pre>"},{"location":"api/generators/#gigaspatial.generators.poi.microsoft_global_buildings.MSBuildingsPoiViewGenerator.generate_view","title":"<code>generate_view(poi_data, source_data_path=None, **kwargs)</code>","text":"<p>Generate POI view for Microsoft Global Buildings.</p> <p>Returns:</p> Type Description <code>DataFrame</code> <p>Enhanced POI data with nearest building information</p> Source code in <code>gigaspatial/generators/poi/microsoft_global_buildings.py</code> <pre><code>def generate_view(\n    self,\n    poi_data: Union[pd.DataFrame, gpd.GeoDataFrame],\n    source_data_path: Optional[Union[Path, List[Path]]] = None,\n    **kwargs,\n) -&gt; pd.DataFrame:\n    \"\"\"\n    Generate POI view for Microsoft Global Buildings.\n\n    Returns:\n        Enhanced POI data with nearest building information\n    \"\"\"\n    self.logger.info(\"Generating MicrosoftBuildings POI view\")\n\n    return self.generate_poi_view(\n        poi_data=poi_data,\n        source_data_path=source_data_path,\n        **kwargs,\n    )\n</code></pre>"},{"location":"api/generators/#gigaspatial.generators.poi.microsoft_global_buildings.MSBuildingsPoiViewGenerator.load_data","title":"<code>load_data(source_data_path, **kwargs)</code>","text":"<p>Load building data from Microsoft Buildings dataset.</p> <p>Parameters:</p> Name Type Description Default <code>source_data_path</code> <code>Union[Path, List[Path]]</code> <p>Path(s) to the source data files</p> required <code>**kwargs</code> <p>Additional loading parameters</p> <code>{}</code> <p>Returns:</p> Type Description <code>GeoDataFrame</code> <p>DataFrame containing building data</p> Source code in <code>gigaspatial/generators/poi/microsoft_global_buildings.py</code> <pre><code>def load_data(\n    self, source_data_path: Union[Path, List[Path]], **kwargs\n) -&gt; gpd.GeoDataFrame:\n    \"\"\"\n    Load building data from Microsoft Buildings dataset.\n\n    Args:\n        source_data_path: Path(s) to the source data files\n        **kwargs: Additional loading parameters\n\n    Returns:\n        DataFrame containing building data\n    \"\"\"\n\n    def read_ms_dataset(data_store: DataStore, file_path: str):\n        df = read_gzipped_json_or_csv(file_path=file_path, data_store=data_store)\n        df[\"geometry\"] = df[\"geometry\"].apply(shape)\n        return gpd.GeoDataFrame(df, crs=4326)\n\n    processed_paths = self._pre_load_hook(source_data_path, **kwargs)\n\n    all_data = []\n    for file_path in processed_paths:\n        all_data.append(read_ms_dataset(self.data_store, file_path))\n\n    if not all_data:\n        return pd.DataFrame()\n\n    # Concatenate all tile data\n    result = pd.concat(all_data, ignore_index=True)\n\n    return self._post_load_hook(result)\n</code></pre>"},{"location":"api/generators/#gigaspatial.generators.poi.microsoft_global_buildings.MSBuildingsPoiViewGenerator.map_to_poi","title":"<code>map_to_poi(processed_data, poi_data, **kwargs)</code>","text":"<p>Map processed building data to POI data.</p> <p>Parameters:</p> Name Type Description Default <code>processed_data</code> <code>DataFrame</code> <p>Processed building data as GeoDataFrame</p> required <code>poi_data</code> <code>DataFrame</code> <p>POI data to map to</p> required <code>**kwargs</code> <p>Additional mapping parameters</p> <code>{}</code> <p>Returns:</p> Type Description <code>DataFrame</code> <p>DataFrame with POI data and nearest building information</p> Source code in <code>gigaspatial/generators/poi/microsoft_global_buildings.py</code> <pre><code>def map_to_poi(\n    self, processed_data: pd.DataFrame, poi_data: pd.DataFrame, **kwargs\n) -&gt; pd.DataFrame:\n    \"\"\"\n    Map processed building data to POI data.\n\n    Args:\n        processed_data: Processed building data as GeoDataFrame\n        poi_data: POI data to map to\n        **kwargs: Additional mapping parameters\n\n    Returns:\n        DataFrame with POI data and nearest building information\n    \"\"\"\n\n    tree = cKDTree(processed_data[[\"y\", \"x\"]])\n\n    if \"latitude\" not in poi_data:\n        poi_lat_col, poi_lon_col = detect_coordinate_columns(poi_data)\n        df_points = poi_data.rename(\n            columns={poi_lat_col: \"latitude\", poi_lon_col: \"longitude\"}\n        )\n    else:\n        df_points = poi_data.copy()\n\n    _, idx = tree.query(df_points[[\"latitude\", \"longitude\"]], k=1)\n\n    df_nearest_buildings = processed_data.iloc[idx]\n\n    dist = calculate_distance(\n        lat1=df_points.latitude,\n        lon1=df_points.longitude,\n        lat2=df_nearest_buildings.y,\n        lon2=df_nearest_buildings.x,\n    )\n\n    poi_data[\"nearest_ms_building_id\"] = df_nearest_buildings.get(\n        \"building_id\", None\n    )\n    poi_data[\"nearest_ms_building_distance\"] = dist\n\n    return poi_data\n</code></pre>"},{"location":"api/generators/#gigaspatial.generators.poi.microsoft_global_buildings.MSBuildingsPoiViewGenerator.resolve_source_paths","title":"<code>resolve_source_paths(poi_data, explicit_paths=None, **kwargs)</code>","text":"<p>For Microsoft Buildings, resolve source data paths based on POI data geography.</p> <p>Returns:</p> Type Description <code>List[Union[str, Path]]</code> <p>List of paths to relevant Microsoft Buildings tile files</p> Source code in <code>gigaspatial/generators/poi/microsoft_global_buildings.py</code> <pre><code>def resolve_source_paths(\n    self,\n    poi_data: Union[pd.DataFrame, gpd.GeoDataFrame],\n    explicit_paths: Optional[Union[Path, str, List[Union[str, Path]]]] = None,\n    **kwargs,\n) -&gt; List[Union[str, Path]]:\n    \"\"\"\n    For Microsoft Buildings, resolve source data paths based on POI data geography.\n\n    Returns:\n        List of paths to relevant Microsoft Buildings tile files\n    \"\"\"\n    # Return explicit paths if provided\n    if explicit_paths is not None:\n        if isinstance(explicit_paths, (str, Path)):\n            return [explicit_paths]\n        return list(explicit_paths)\n\n    if \"latitude\" not in poi_data:\n        poi_lat_col, poi_lon_col = detect_coordinate_columns(poi_data)\n    else:\n        poi_lat_col, poi_lon_col = (\"latitude\", \"longitude\")\n\n    points = poi_data[[poi_lat_col, poi_lon_col]].to_numpy()\n\n    # Find intersecting tiles\n    tiles = self.data_config.get_tiles_for_points(points)\n\n    if tiles.empty:\n        self.logger.warning(\n            \"There are no matching Microsoft Buildings tiles for the POI data\"\n        )\n        return []\n\n    # Generate paths for each intersecting tile\n    source_data_paths = [\n        self.data_config.get_tile_path(\n            quadkey=tile[\"quadkey\"],\n            location=tile[\"country\"] if tile[\"country\"] else tile[\"location\"],\n        )\n        for _, tile in tiles.iterrows()\n    ]\n\n    self.logger.info(f\"Resolved {len(source_data_paths)} tile paths for POI data\")\n    return source_data_paths\n</code></pre>"},{"location":"api/grid/","title":"Grid Module","text":""},{"location":"api/grid/#gigaspatial.grid","title":"<code>gigaspatial.grid</code>","text":""},{"location":"api/grid/#gigaspatial.grid.mercator_tiles","title":"<code>mercator_tiles</code>","text":""},{"location":"api/grid/#gigaspatial.grid.mercator_tiles.CountryMercatorTiles","title":"<code>CountryMercatorTiles</code>","text":"<p>               Bases: <code>MercatorTiles</code></p> <p>MercatorTiles specialized for country-level operations.</p> <p>This class extends MercatorTiles to work specifically with country boundaries. It can only be instantiated through the create() classmethod.</p> Source code in <code>gigaspatial/grid/mercator_tiles.py</code> <pre><code>class CountryMercatorTiles(MercatorTiles):\n    \"\"\"MercatorTiles specialized for country-level operations.\n\n    This class extends MercatorTiles to work specifically with country boundaries.\n    It can only be instantiated through the create() classmethod.\n    \"\"\"\n\n    _country: str = PrivateAttr()\n\n    def __init__(self, *args, **kwargs):\n        raise TypeError(\n            \"CountryMercatorTiles cannot be instantiated directly. \"\n            \"Use CountryMercatorTiles.create() instead.\"\n        )\n\n    @classmethod\n    def create(\n        cls,\n        country: str,\n        zoom_level: int,\n        data_store: Optional[DataStore] = None,\n        country_geom_path: Optional[Union[str, Path]] = None,\n        predicate: str = \"intersects\",\n    ) -&gt; \"CountryMercatorTiles\":\n        \"\"\"Create CountryMercatorTiles for a specific country.\"\"\"\n        instance = super().__new__(cls)\n        super(CountryMercatorTiles, instance).__init__(\n            zoom_level=zoom_level,\n            quadkeys=[],\n            data_store=data_store or LocalDataStore(),\n        )\n\n        instance._country = country\n\n        country_geom = instance._load_country_geometry(\n            country, data_store, country_geom_path\n        )\n\n        tiles = (\n            MercatorTiles.from_multipolygon\n            if isinstance(country_geom, MultiPolygon)\n            else MercatorTiles.from_polygon\n        )(country_geom, zoom_level, predicate)\n\n        instance.quadkeys = tiles.quadkeys\n        return instance\n\n    @property\n    def country(self) -&gt; str:\n        \"\"\"Get country identifier.\"\"\"\n        return self._country\n\n    def _load_country_geometry(\n        self,\n        country,\n        data_store: Optional[DataStore] = None,\n        path: Optional[Union[str, Path]] = None,\n    ) -&gt; Union[Polygon, MultiPolygon]:\n        \"\"\"Load country boundary geometry from DataStore or GADM.\"\"\"\n\n        gdf_admin0 = AdminBoundaries.create(\n            country_code=country, admin_level=0, data_store=data_store, path=path\n        ).to_geodataframe()\n\n        return gdf_admin0.geometry.iloc[0]\n</code></pre>"},{"location":"api/grid/#gigaspatial.grid.mercator_tiles.CountryMercatorTiles.country","title":"<code>country: str</code>  <code>property</code>","text":"<p>Get country identifier.</p>"},{"location":"api/grid/#gigaspatial.grid.mercator_tiles.CountryMercatorTiles.create","title":"<code>create(country, zoom_level, data_store=None, country_geom_path=None, predicate='intersects')</code>  <code>classmethod</code>","text":"<p>Create CountryMercatorTiles for a specific country.</p> Source code in <code>gigaspatial/grid/mercator_tiles.py</code> <pre><code>@classmethod\ndef create(\n    cls,\n    country: str,\n    zoom_level: int,\n    data_store: Optional[DataStore] = None,\n    country_geom_path: Optional[Union[str, Path]] = None,\n    predicate: str = \"intersects\",\n) -&gt; \"CountryMercatorTiles\":\n    \"\"\"Create CountryMercatorTiles for a specific country.\"\"\"\n    instance = super().__new__(cls)\n    super(CountryMercatorTiles, instance).__init__(\n        zoom_level=zoom_level,\n        quadkeys=[],\n        data_store=data_store or LocalDataStore(),\n    )\n\n    instance._country = country\n\n    country_geom = instance._load_country_geometry(\n        country, data_store, country_geom_path\n    )\n\n    tiles = (\n        MercatorTiles.from_multipolygon\n        if isinstance(country_geom, MultiPolygon)\n        else MercatorTiles.from_polygon\n    )(country_geom, zoom_level, predicate)\n\n    instance.quadkeys = tiles.quadkeys\n    return instance\n</code></pre>"},{"location":"api/grid/#gigaspatial.grid.mercator_tiles.MercatorTiles","title":"<code>MercatorTiles</code>","text":"<p>               Bases: <code>BaseModel</code></p> Source code in <code>gigaspatial/grid/mercator_tiles.py</code> <pre><code>class MercatorTiles(BaseModel):\n    zoom_level: int = Field(..., ge=0, le=20)\n    quadkeys: List[str] = Field(default_factory=list)\n    data_store: DataStore = Field(default_factory=LocalDataStore, exclude=True)\n\n    class Config:\n        arbitrary_types_allowed = True\n\n    @classmethod\n    def from_quadkeys(cls, quadkeys: List[str]) -&gt; \"MercatorTiles\":\n        \"\"\"Create MercatorTiles from list of quadkeys.\"\"\"\n        return MercatorTiles(zoom_level=len(quadkeys[0]), quadkeys=quadkeys)\n\n    @classmethod\n    def from_points(\n        cls, points: List[Union[Point, Tuple[float, float]]], zoom_level: int\n    ) -&gt; \"MercatorTiles\":\n        \"\"\"Create MercatorTiles from a list of points or lat-lon pairs.\"\"\"\n        quadkeys = {\n            (\n                mercantile.quadkey(mercantile.tile(p.x, p.y, zoom_level))\n                if isinstance(p, Point)\n                else mercantile.quadkey(mercantile.tile(p[1], p[0], zoom_level))\n            )\n            for p in points\n        }\n        return cls(zoom_level=zoom_level, quadkeys=list(quadkeys))\n\n    @staticmethod\n    def from_bounds(\n        xmin: float, ymin: float, xmax: float, ymax: float, zoom_level: int\n    ) -&gt; \"MercatorTiles\":\n        \"\"\"Create MercatorTiles from boundary coordinates.\"\"\"\n        return MercatorTiles(\n            zoom_level=zoom_level,\n            quadkeys=[\n                mercantile.quadkey(tile)\n                for tile in mercantile.tiles(xmin, ymin, xmax, ymax, zoom_level)\n            ],\n        )\n\n    @staticmethod\n    def from_polygon(\n        polygon: Polygon, zoom_level: int, predicate: str = \"intersects\"\n    ) -&gt; \"MercatorTiles\":\n        \"\"\"Create MercatorTiles from a polygon.\"\"\"\n        tiles = list(mercantile.tiles(*polygon.bounds, zoom_level))\n        quadkeys_boxes = [\n            (mercantile.quadkey(t), box(*mercantile.bounds(t))) for t in tiles\n        ]\n        quadkeys, boxes = zip(*quadkeys_boxes) if quadkeys_boxes else ([], [])\n\n        if not boxes:\n            return MercatorTiles(zoom_level=zoom_level, quadkeys=[])\n\n        s = STRtree(boxes)\n        result = s.query(polygon, predicate=predicate)\n        return MercatorTiles(\n            zoom_level=zoom_level, quadkeys=[quadkeys[i] for i in result]\n        )\n\n    @staticmethod\n    def from_multipolygon(\n        multipolygon: MultiPolygon, zoom_level: int, predicate: str = \"intersects\"\n    ) -&gt; \"MercatorTiles\":\n        \"\"\"Create MercatorTiles from a MultiPolygon.\"\"\"\n        quadkeys = {\n            quadkey\n            for geom in multipolygon.geoms\n            for quadkey in MercatorTiles.from_polygon(\n                geom, zoom_level, predicate\n            ).quadkeys\n        }\n        return MercatorTiles(zoom_level=zoom_level, quadkeys=list(quadkeys))\n\n    @classmethod\n    def from_json(\n        cls, data_store: DataStore, file: Union[str, Path], **kwargs\n    ) -&gt; \"MercatorTiles\":\n        \"\"\"Load MercatorTiles from a JSON file.\"\"\"\n        with data_store.open(str(file), \"r\") as f:\n            data = json.load(f)\n            if isinstance(data, list):  # If file contains only quadkeys\n                data = {\n                    \"zoom_level\": len(data[0]) if data else 0,\n                    \"quadkeys\": data,\n                    **kwargs,\n                }\n            else:\n                data.update(kwargs)\n            instance = cls(**data)\n            instance.data_store = data_store\n            return instance\n\n    def filter_quadkeys(self, quadkeys: Iterable[str]) -&gt; \"MercatorTiles\":\n        \"\"\"Filter quadkeys by a given set of quadkeys.\"\"\"\n        return MercatorTiles(\n            zoom_level=self.zoom_level,\n            quadkeys=list(set(self.quadkeys) &amp; set(quadkeys)),\n        )\n\n    def to_dataframe(self) -&gt; pd.DataFrame:\n        \"\"\"Convert to pandas DataFrame with quadkey and centroid coordinates.\"\"\"\n        tiles_data = [mercantile.quadkey_to_tile(q) for q in self.quadkeys]\n        bounds_data = [mercantile.bounds(tile) for tile in tiles_data]\n\n        centroids = [\n            (\n                (bounds.south + bounds.north) / 2,  # latitude\n                (bounds.west + bounds.east) / 2,  # longitude\n            )\n            for bounds in bounds_data\n        ]\n\n        return pd.DataFrame(\n            {\n                \"quadkey\": self.quadkeys,\n                \"latitude\": [c[0] for c in centroids],\n                \"longitude\": [c[1] for c in centroids],\n            }\n        )\n\n    def to_geodataframe(self) -&gt; gpd.GeoDataFrame:\n        \"\"\"Convert to GeoPandas GeoDataFrame.\"\"\"\n        geoms = [\n            box(*mercantile.bounds(mercantile.quadkey_to_tile(q)))\n            for q in self.quadkeys\n        ]\n        return gpd.GeoDataFrame(\n            {\"quadkey\": self.quadkeys, \"geometry\": geoms}, crs=\"EPSG:4326\"\n        )\n\n    def save(self, file: Union[str, Path], format: str = \"json\") -&gt; None:\n        \"\"\"Save MercatorTiles to file in specified format.\"\"\"\n        with self.data_store.open(str(file), \"wb\" if format == \"parquet\" else \"w\") as f:\n            if format == \"parquet\":\n                self.to_geodataframe().to_parquet(f, index=False)\n            elif format == \"geojson\":\n                f.write(self.to_geodataframe().to_json(drop_id=True))\n            elif format == \"json\":\n                json.dump(self.quadkeys, f)\n            else:\n                raise ValueError(f\"Unsupported format: {format}\")\n\n    def __len__(self) -&gt; int:\n        return len(self.quadkeys)\n</code></pre>"},{"location":"api/grid/#gigaspatial.grid.mercator_tiles.MercatorTiles.filter_quadkeys","title":"<code>filter_quadkeys(quadkeys)</code>","text":"<p>Filter quadkeys by a given set of quadkeys.</p> Source code in <code>gigaspatial/grid/mercator_tiles.py</code> <pre><code>def filter_quadkeys(self, quadkeys: Iterable[str]) -&gt; \"MercatorTiles\":\n    \"\"\"Filter quadkeys by a given set of quadkeys.\"\"\"\n    return MercatorTiles(\n        zoom_level=self.zoom_level,\n        quadkeys=list(set(self.quadkeys) &amp; set(quadkeys)),\n    )\n</code></pre>"},{"location":"api/grid/#gigaspatial.grid.mercator_tiles.MercatorTiles.from_bounds","title":"<code>from_bounds(xmin, ymin, xmax, ymax, zoom_level)</code>  <code>staticmethod</code>","text":"<p>Create MercatorTiles from boundary coordinates.</p> Source code in <code>gigaspatial/grid/mercator_tiles.py</code> <pre><code>@staticmethod\ndef from_bounds(\n    xmin: float, ymin: float, xmax: float, ymax: float, zoom_level: int\n) -&gt; \"MercatorTiles\":\n    \"\"\"Create MercatorTiles from boundary coordinates.\"\"\"\n    return MercatorTiles(\n        zoom_level=zoom_level,\n        quadkeys=[\n            mercantile.quadkey(tile)\n            for tile in mercantile.tiles(xmin, ymin, xmax, ymax, zoom_level)\n        ],\n    )\n</code></pre>"},{"location":"api/grid/#gigaspatial.grid.mercator_tiles.MercatorTiles.from_json","title":"<code>from_json(data_store, file, **kwargs)</code>  <code>classmethod</code>","text":"<p>Load MercatorTiles from a JSON file.</p> Source code in <code>gigaspatial/grid/mercator_tiles.py</code> <pre><code>@classmethod\ndef from_json(\n    cls, data_store: DataStore, file: Union[str, Path], **kwargs\n) -&gt; \"MercatorTiles\":\n    \"\"\"Load MercatorTiles from a JSON file.\"\"\"\n    with data_store.open(str(file), \"r\") as f:\n        data = json.load(f)\n        if isinstance(data, list):  # If file contains only quadkeys\n            data = {\n                \"zoom_level\": len(data[0]) if data else 0,\n                \"quadkeys\": data,\n                **kwargs,\n            }\n        else:\n            data.update(kwargs)\n        instance = cls(**data)\n        instance.data_store = data_store\n        return instance\n</code></pre>"},{"location":"api/grid/#gigaspatial.grid.mercator_tiles.MercatorTiles.from_multipolygon","title":"<code>from_multipolygon(multipolygon, zoom_level, predicate='intersects')</code>  <code>staticmethod</code>","text":"<p>Create MercatorTiles from a MultiPolygon.</p> Source code in <code>gigaspatial/grid/mercator_tiles.py</code> <pre><code>@staticmethod\ndef from_multipolygon(\n    multipolygon: MultiPolygon, zoom_level: int, predicate: str = \"intersects\"\n) -&gt; \"MercatorTiles\":\n    \"\"\"Create MercatorTiles from a MultiPolygon.\"\"\"\n    quadkeys = {\n        quadkey\n        for geom in multipolygon.geoms\n        for quadkey in MercatorTiles.from_polygon(\n            geom, zoom_level, predicate\n        ).quadkeys\n    }\n    return MercatorTiles(zoom_level=zoom_level, quadkeys=list(quadkeys))\n</code></pre>"},{"location":"api/grid/#gigaspatial.grid.mercator_tiles.MercatorTiles.from_points","title":"<code>from_points(points, zoom_level)</code>  <code>classmethod</code>","text":"<p>Create MercatorTiles from a list of points or lat-lon pairs.</p> Source code in <code>gigaspatial/grid/mercator_tiles.py</code> <pre><code>@classmethod\ndef from_points(\n    cls, points: List[Union[Point, Tuple[float, float]]], zoom_level: int\n) -&gt; \"MercatorTiles\":\n    \"\"\"Create MercatorTiles from a list of points or lat-lon pairs.\"\"\"\n    quadkeys = {\n        (\n            mercantile.quadkey(mercantile.tile(p.x, p.y, zoom_level))\n            if isinstance(p, Point)\n            else mercantile.quadkey(mercantile.tile(p[1], p[0], zoom_level))\n        )\n        for p in points\n    }\n    return cls(zoom_level=zoom_level, quadkeys=list(quadkeys))\n</code></pre>"},{"location":"api/grid/#gigaspatial.grid.mercator_tiles.MercatorTiles.from_polygon","title":"<code>from_polygon(polygon, zoom_level, predicate='intersects')</code>  <code>staticmethod</code>","text":"<p>Create MercatorTiles from a polygon.</p> Source code in <code>gigaspatial/grid/mercator_tiles.py</code> <pre><code>@staticmethod\ndef from_polygon(\n    polygon: Polygon, zoom_level: int, predicate: str = \"intersects\"\n) -&gt; \"MercatorTiles\":\n    \"\"\"Create MercatorTiles from a polygon.\"\"\"\n    tiles = list(mercantile.tiles(*polygon.bounds, zoom_level))\n    quadkeys_boxes = [\n        (mercantile.quadkey(t), box(*mercantile.bounds(t))) for t in tiles\n    ]\n    quadkeys, boxes = zip(*quadkeys_boxes) if quadkeys_boxes else ([], [])\n\n    if not boxes:\n        return MercatorTiles(zoom_level=zoom_level, quadkeys=[])\n\n    s = STRtree(boxes)\n    result = s.query(polygon, predicate=predicate)\n    return MercatorTiles(\n        zoom_level=zoom_level, quadkeys=[quadkeys[i] for i in result]\n    )\n</code></pre>"},{"location":"api/grid/#gigaspatial.grid.mercator_tiles.MercatorTiles.from_quadkeys","title":"<code>from_quadkeys(quadkeys)</code>  <code>classmethod</code>","text":"<p>Create MercatorTiles from list of quadkeys.</p> Source code in <code>gigaspatial/grid/mercator_tiles.py</code> <pre><code>@classmethod\ndef from_quadkeys(cls, quadkeys: List[str]) -&gt; \"MercatorTiles\":\n    \"\"\"Create MercatorTiles from list of quadkeys.\"\"\"\n    return MercatorTiles(zoom_level=len(quadkeys[0]), quadkeys=quadkeys)\n</code></pre>"},{"location":"api/grid/#gigaspatial.grid.mercator_tiles.MercatorTiles.save","title":"<code>save(file, format='json')</code>","text":"<p>Save MercatorTiles to file in specified format.</p> Source code in <code>gigaspatial/grid/mercator_tiles.py</code> <pre><code>def save(self, file: Union[str, Path], format: str = \"json\") -&gt; None:\n    \"\"\"Save MercatorTiles to file in specified format.\"\"\"\n    with self.data_store.open(str(file), \"wb\" if format == \"parquet\" else \"w\") as f:\n        if format == \"parquet\":\n            self.to_geodataframe().to_parquet(f, index=False)\n        elif format == \"geojson\":\n            f.write(self.to_geodataframe().to_json(drop_id=True))\n        elif format == \"json\":\n            json.dump(self.quadkeys, f)\n        else:\n            raise ValueError(f\"Unsupported format: {format}\")\n</code></pre>"},{"location":"api/grid/#gigaspatial.grid.mercator_tiles.MercatorTiles.to_dataframe","title":"<code>to_dataframe()</code>","text":"<p>Convert to pandas DataFrame with quadkey and centroid coordinates.</p> Source code in <code>gigaspatial/grid/mercator_tiles.py</code> <pre><code>def to_dataframe(self) -&gt; pd.DataFrame:\n    \"\"\"Convert to pandas DataFrame with quadkey and centroid coordinates.\"\"\"\n    tiles_data = [mercantile.quadkey_to_tile(q) for q in self.quadkeys]\n    bounds_data = [mercantile.bounds(tile) for tile in tiles_data]\n\n    centroids = [\n        (\n            (bounds.south + bounds.north) / 2,  # latitude\n            (bounds.west + bounds.east) / 2,  # longitude\n        )\n        for bounds in bounds_data\n    ]\n\n    return pd.DataFrame(\n        {\n            \"quadkey\": self.quadkeys,\n            \"latitude\": [c[0] for c in centroids],\n            \"longitude\": [c[1] for c in centroids],\n        }\n    )\n</code></pre>"},{"location":"api/grid/#gigaspatial.grid.mercator_tiles.MercatorTiles.to_geodataframe","title":"<code>to_geodataframe()</code>","text":"<p>Convert to GeoPandas GeoDataFrame.</p> Source code in <code>gigaspatial/grid/mercator_tiles.py</code> <pre><code>def to_geodataframe(self) -&gt; gpd.GeoDataFrame:\n    \"\"\"Convert to GeoPandas GeoDataFrame.\"\"\"\n    geoms = [\n        box(*mercantile.bounds(mercantile.quadkey_to_tile(q)))\n        for q in self.quadkeys\n    ]\n    return gpd.GeoDataFrame(\n        {\"quadkey\": self.quadkeys, \"geometry\": geoms}, crs=\"EPSG:4326\"\n    )\n</code></pre>"},{"location":"api/handlers/","title":"Handlers Module","text":""},{"location":"api/handlers/#gigaspatial.handlers","title":"<code>gigaspatial.handlers</code>","text":""},{"location":"api/handlers/#gigaspatial.handlers.boundaries","title":"<code>boundaries</code>","text":""},{"location":"api/handlers/#gigaspatial.handlers.boundaries.AdminBoundaries","title":"<code>AdminBoundaries</code>","text":"<p>               Bases: <code>BaseModel</code></p> <p>Base class for administrative boundary data with flexible fields.</p> Source code in <code>gigaspatial/handlers/boundaries.py</code> <pre><code>class AdminBoundaries(BaseModel):\n    \"\"\"Base class for administrative boundary data with flexible fields.\"\"\"\n\n    boundaries: List[AdminBoundary] = Field(default_factory=list)\n    level: int = Field(\n        ...,\n        ge=0,\n        le=4,\n        description=\"Administrative level (e.g., 0=country, 1=state, etc.)\",\n    )\n\n    logger: ClassVar = config.get_logger(__name__)\n\n    _schema_config: ClassVar[Dict[str, Dict[str, str]]] = {\n        \"gadm\": {\n            \"country_code\": \"GID_0\",\n            \"id\": \"GID_{level}\",\n            \"name\": \"NAME_{level}\",\n            \"parent_id\": \"GID_{parent_level}\",\n        },\n        \"internal\": {\n            \"id\": \"admin{level}_id_giga\",\n            \"name\": \"name\",\n            \"name_en\": \"name_en\",\n            \"country_code\": \"iso_3166_1_alpha_3\",\n        },\n    }\n\n    @classmethod\n    def get_schema_config(cls) -&gt; Dict[str, Dict[str, str]]:\n        \"\"\"Return field mappings for different data sources\"\"\"\n        return cls._schema_config\n\n    @classmethod\n    def from_gadm(\n        cls, country_code: str, admin_level: int = 0, **kwargs\n    ) -&gt; \"AdminBoundaries\":\n        \"\"\"Load and create instance from GADM data.\"\"\"\n        url = f\"https://geodata.ucdavis.edu/gadm/gadm4.1/json/gadm41_{country_code}_{admin_level}.json\"\n\n        try:\n            gdf = gpd.read_file(url)\n\n            gdf = cls._map_fields(gdf, \"gadm\", admin_level)\n\n            if admin_level == 0:\n                gdf[\"country_code\"] = gdf[\"id\"]\n                gdf[\"name\"] = gdf[\"COUNTRY\"]\n            elif admin_level == 1:\n                gdf[\"country_code\"] = gdf[\"parent_id\"]\n\n            boundaries = [\n                AdminBoundary(**row_dict) for row_dict in gdf.to_dict(\"records\")\n            ]\n            return cls(\n                boundaries=boundaries, level=admin_level, country_code=country_code\n            )\n\n        except (ValueError, HTTPError, FileNotFoundError) as e:\n            cls.logger.warning(\n                f\"No data found for {country_code} at admin level {admin_level}: {str(e)}\"\n            )\n            return cls._create_empty_instance(country_code, admin_level, \"gadm\")\n\n    @classmethod\n    def from_data_store(\n        cls,\n        data_store: DataStore,\n        path: Union[str, \"Path\"],\n        admin_level: int = 0,\n        **kwargs,\n    ) -&gt; \"AdminBoundaries\":\n        \"\"\"Load and create instance from internal data store.\"\"\"\n        try:\n            gdf = read_dataset(data_store, str(path), **kwargs)\n\n            if gdf.empty:\n                return cls._create_empty_instance(None, admin_level, \"internal\")\n\n            gdf = cls._map_fields(gdf, \"internal\", admin_level)\n\n            if admin_level == 0:\n                gdf[\"id\"] = gdf[\"country_code\"]\n            else:\n                gdf[\"parent_id\"] = gdf[\"id\"].apply(lambda x: x[:-3])\n\n            boundaries = [\n                AdminBoundary(**row_dict) for row_dict in gdf.to_dict(\"records\")\n            ]\n            return cls(boundaries=boundaries, level=admin_level)\n\n        except (FileNotFoundError, KeyError) as e:\n            cls.logger.warning(\n                f\"No data found at {path} for admin level {admin_level}: {str(e)}\"\n            )\n            return cls._create_empty_instance(None, admin_level, \"internal\")\n\n    @classmethod\n    def create(\n        cls,\n        country_code: Optional[str] = None,\n        admin_level: int = 0,\n        data_store: Optional[DataStore] = None,\n        path: Optional[Union[str, \"Path\"]] = None,\n        **kwargs,\n    ) -&gt; \"AdminBoundaries\":\n        \"\"\"Factory method to create AdminBoundaries instance from either GADM or data store.\"\"\"\n        if data_store is not None:\n            if path is None:\n                if country_code is None:\n                    ValueError(\n                        \"If data_store is provided, path or country_code must also be specified.\"\n                    )\n                path = config.get_admin_path(\n                    country_code=country_code, admin_level=admin_level\n                )\n            return cls.from_data_store(data_store, path, admin_level, **kwargs)\n        elif country_code is not None:\n            return cls.from_gadm(country_code, admin_level, **kwargs)\n        else:\n            raise ValueError(\n                \"Either country_code or (data_store, path) must be provided.\"\n            )\n\n    @classmethod\n    def _create_empty_instance(\n        cls, country_code: Optional[str], admin_level: int, source_type: str\n    ) -&gt; \"AdminBoundaries\":\n        \"\"\"Create an empty instance with the required schema structure.\"\"\"\n        # for to_geodataframe() to use later\n        instance = cls(boundaries=[], level=admin_level, country_code=country_code)\n\n        schema_fields = set(cls.get_schema_config()[source_type].keys())\n        schema_fields.update([\"geometry\", \"country_code\", \"id\", \"name\", \"name_en\"])\n        if admin_level &gt; 0:\n            schema_fields.add(\"parent_id\")\n\n        instance._empty_schema = list(schema_fields)\n        return instance\n\n    @classmethod\n    def _map_fields(\n        cls,\n        gdf: gpd.GeoDataFrame,\n        source: str,\n        current_level: int,\n    ) -&gt; gpd.GeoDataFrame:\n        \"\"\"Map source fields to schema fields\"\"\"\n        config = cls.get_schema_config().get(source, {})\n        parent_level = current_level - 1\n\n        field_mapping = {}\n        for k, v in config.items():\n            if \"{parent_level}\" in v:\n                field_mapping[v.format(parent_level=parent_level)] = k\n            elif \"{level}\" in v:\n                field_mapping[v.format(level=current_level)] = k\n            else:\n                field_mapping[v] = k\n\n        return gdf.rename(columns=field_mapping)\n\n    def to_geodataframe(self) -&gt; gpd.GeoDataFrame:\n        \"\"\"Convert the AdminBoundaries to a GeoDataFrame.\"\"\"\n        if not self.boundaries:\n            if hasattr(self, \"_empty_schema\"):\n                columns = self._empty_schema\n            else:\n                columns = [\"id\", \"name\", \"country_code\", \"geometry\"]\n                if self.level &gt; 0:\n                    columns.append(\"parent_id\")\n\n            return gpd.GeoDataFrame(columns=columns, geometry=\"geometry\", crs=4326)\n\n        return gpd.GeoDataFrame(\n            [boundary.model_dump() for boundary in self.boundaries],\n            geometry=\"geometry\",\n            crs=4326,\n        )\n</code></pre>"},{"location":"api/handlers/#gigaspatial.handlers.boundaries.AdminBoundaries.create","title":"<code>create(country_code=None, admin_level=0, data_store=None, path=None, **kwargs)</code>  <code>classmethod</code>","text":"<p>Factory method to create AdminBoundaries instance from either GADM or data store.</p> Source code in <code>gigaspatial/handlers/boundaries.py</code> <pre><code>@classmethod\ndef create(\n    cls,\n    country_code: Optional[str] = None,\n    admin_level: int = 0,\n    data_store: Optional[DataStore] = None,\n    path: Optional[Union[str, \"Path\"]] = None,\n    **kwargs,\n) -&gt; \"AdminBoundaries\":\n    \"\"\"Factory method to create AdminBoundaries instance from either GADM or data store.\"\"\"\n    if data_store is not None:\n        if path is None:\n            if country_code is None:\n                ValueError(\n                    \"If data_store is provided, path or country_code must also be specified.\"\n                )\n            path = config.get_admin_path(\n                country_code=country_code, admin_level=admin_level\n            )\n        return cls.from_data_store(data_store, path, admin_level, **kwargs)\n    elif country_code is not None:\n        return cls.from_gadm(country_code, admin_level, **kwargs)\n    else:\n        raise ValueError(\n            \"Either country_code or (data_store, path) must be provided.\"\n        )\n</code></pre>"},{"location":"api/handlers/#gigaspatial.handlers.boundaries.AdminBoundaries.from_data_store","title":"<code>from_data_store(data_store, path, admin_level=0, **kwargs)</code>  <code>classmethod</code>","text":"<p>Load and create instance from internal data store.</p> Source code in <code>gigaspatial/handlers/boundaries.py</code> <pre><code>@classmethod\ndef from_data_store(\n    cls,\n    data_store: DataStore,\n    path: Union[str, \"Path\"],\n    admin_level: int = 0,\n    **kwargs,\n) -&gt; \"AdminBoundaries\":\n    \"\"\"Load and create instance from internal data store.\"\"\"\n    try:\n        gdf = read_dataset(data_store, str(path), **kwargs)\n\n        if gdf.empty:\n            return cls._create_empty_instance(None, admin_level, \"internal\")\n\n        gdf = cls._map_fields(gdf, \"internal\", admin_level)\n\n        if admin_level == 0:\n            gdf[\"id\"] = gdf[\"country_code\"]\n        else:\n            gdf[\"parent_id\"] = gdf[\"id\"].apply(lambda x: x[:-3])\n\n        boundaries = [\n            AdminBoundary(**row_dict) for row_dict in gdf.to_dict(\"records\")\n        ]\n        return cls(boundaries=boundaries, level=admin_level)\n\n    except (FileNotFoundError, KeyError) as e:\n        cls.logger.warning(\n            f\"No data found at {path} for admin level {admin_level}: {str(e)}\"\n        )\n        return cls._create_empty_instance(None, admin_level, \"internal\")\n</code></pre>"},{"location":"api/handlers/#gigaspatial.handlers.boundaries.AdminBoundaries.from_gadm","title":"<code>from_gadm(country_code, admin_level=0, **kwargs)</code>  <code>classmethod</code>","text":"<p>Load and create instance from GADM data.</p> Source code in <code>gigaspatial/handlers/boundaries.py</code> <pre><code>@classmethod\ndef from_gadm(\n    cls, country_code: str, admin_level: int = 0, **kwargs\n) -&gt; \"AdminBoundaries\":\n    \"\"\"Load and create instance from GADM data.\"\"\"\n    url = f\"https://geodata.ucdavis.edu/gadm/gadm4.1/json/gadm41_{country_code}_{admin_level}.json\"\n\n    try:\n        gdf = gpd.read_file(url)\n\n        gdf = cls._map_fields(gdf, \"gadm\", admin_level)\n\n        if admin_level == 0:\n            gdf[\"country_code\"] = gdf[\"id\"]\n            gdf[\"name\"] = gdf[\"COUNTRY\"]\n        elif admin_level == 1:\n            gdf[\"country_code\"] = gdf[\"parent_id\"]\n\n        boundaries = [\n            AdminBoundary(**row_dict) for row_dict in gdf.to_dict(\"records\")\n        ]\n        return cls(\n            boundaries=boundaries, level=admin_level, country_code=country_code\n        )\n\n    except (ValueError, HTTPError, FileNotFoundError) as e:\n        cls.logger.warning(\n            f\"No data found for {country_code} at admin level {admin_level}: {str(e)}\"\n        )\n        return cls._create_empty_instance(country_code, admin_level, \"gadm\")\n</code></pre>"},{"location":"api/handlers/#gigaspatial.handlers.boundaries.AdminBoundaries.get_schema_config","title":"<code>get_schema_config()</code>  <code>classmethod</code>","text":"<p>Return field mappings for different data sources</p> Source code in <code>gigaspatial/handlers/boundaries.py</code> <pre><code>@classmethod\ndef get_schema_config(cls) -&gt; Dict[str, Dict[str, str]]:\n    \"\"\"Return field mappings for different data sources\"\"\"\n    return cls._schema_config\n</code></pre>"},{"location":"api/handlers/#gigaspatial.handlers.boundaries.AdminBoundaries.to_geodataframe","title":"<code>to_geodataframe()</code>","text":"<p>Convert the AdminBoundaries to a GeoDataFrame.</p> Source code in <code>gigaspatial/handlers/boundaries.py</code> <pre><code>def to_geodataframe(self) -&gt; gpd.GeoDataFrame:\n    \"\"\"Convert the AdminBoundaries to a GeoDataFrame.\"\"\"\n    if not self.boundaries:\n        if hasattr(self, \"_empty_schema\"):\n            columns = self._empty_schema\n        else:\n            columns = [\"id\", \"name\", \"country_code\", \"geometry\"]\n            if self.level &gt; 0:\n                columns.append(\"parent_id\")\n\n        return gpd.GeoDataFrame(columns=columns, geometry=\"geometry\", crs=4326)\n\n    return gpd.GeoDataFrame(\n        [boundary.model_dump() for boundary in self.boundaries],\n        geometry=\"geometry\",\n        crs=4326,\n    )\n</code></pre>"},{"location":"api/handlers/#gigaspatial.handlers.boundaries.AdminBoundary","title":"<code>AdminBoundary</code>","text":"<p>               Bases: <code>BaseModel</code></p> <p>Base class for administrative boundary data with flexible fields.</p> Source code in <code>gigaspatial/handlers/boundaries.py</code> <pre><code>class AdminBoundary(BaseModel):\n    \"\"\"Base class for administrative boundary data with flexible fields.\"\"\"\n\n    id: str = Field(..., description=\"Unique identifier for the administrative unit\")\n    name: str = Field(..., description=\"Primary local name\")\n    geometry: Union[Polygon, MultiPolygon] = Field(\n        ..., description=\"Geometry of the administrative boundary\"\n    )\n\n    name_en: Optional[str] = Field(\n        None, description=\"English name if different from local name\"\n    )\n    parent_id: Optional[str] = Field(\n        None, description=\"ID of parent administrative unit\"\n    )\n    country_code: Optional[str] = Field(\n        None, min_length=3, max_length=3, description=\"ISO 3166-1 alpha-3 country code\"\n    )\n\n    class Config:\n        # extra = \"allow\"\n        arbitrary_types_allowed = True\n</code></pre>"},{"location":"api/handlers/#gigaspatial.handlers.ghsl","title":"<code>ghsl</code>","text":""},{"location":"api/handlers/#gigaspatial.handlers.ghsl.CoordSystem","title":"<code>CoordSystem</code>","text":"<p>               Bases: <code>int</code>, <code>Enum</code></p> <p>Enum for coordinate systems used by GHSL datasets.</p> Source code in <code>gigaspatial/handlers/ghsl.py</code> <pre><code>class CoordSystem(int, Enum):\n    \"\"\"Enum for coordinate systems used by GHSL datasets.\"\"\"\n\n    WGS84 = 4326\n    Mollweide = 54009\n</code></pre>"},{"location":"api/handlers/#gigaspatial.handlers.ghsl.GHSLDataConfig","title":"<code>GHSLDataConfig</code>","text":"Source code in <code>gigaspatial/handlers/ghsl.py</code> <pre><code>@dataclass(config=ConfigDict(arbitrary_types_allowed=True))\nclass GHSLDataConfig:\n    # constants\n    AVAILABLE_YEARS: List = Field(default=np.append(np.arange(1975, 2031, 5), 2018))\n    AVAILABLE_RESOLUTIONS: List = Field(default=[10, 100, 1000])\n\n    # base config\n    GHSL_DB_BASE_URL: HttpUrl = Field(\n        default=\"https://jeodpp.jrc.ec.europa.eu/ftp/jrc-opendata/GHSL/\"\n    )\n    TILES_URL: str = \"https://ghsl.jrc.ec.europa.eu/download/GHSL_data_{}_shapefile.zip\"\n\n    # user config\n    base_path: Path = Field(default=global_config.get_path(\"ghsl\", \"bronze\"))\n    coord_system: CoordSystem = CoordSystem.WGS84\n    release: str = \"R2023A\"\n\n    product: Literal[\n        \"GHS_BUILT_S\",\n        \"GHS_BUILT_H_AGBH\",\n        \"GHS_BUILT_H_ANBH\",\n        \"GHS_BUILT_V\",\n        \"GHS_POP\",\n        \"GHS_SMOD\",\n    ] = Field(...)\n    year: int = 2020\n    resolution: int = 100\n\n    logger: logging.Logger = global_config.get_logger(__name__)\n    n_workers: int = 4\n\n    def _load_tiles(self):\n        \"\"\"Load GHSL tiles from tiles shapefile.\"\"\"\n        try:\n            self.tiles_gdf = gpd.read_file(self.TILES_URL)\n        except Exception as e:\n            self.logger.error(f\"Failed to download tiles shapefile: {e}\")\n            raise ValueError(\n                f\"Could not download GHSL tiles from {self.TILES_URL}\"\n            ) from e\n\n    @field_validator(\"year\")\n    def validate_year(cls, value: str) -&gt; int:\n        if value in cls.AVAILABLE_YEARS:\n            return value\n        raise ValueError(\n            f\"No datasets found for the provided year: {value}\\nAvailable years are: {cls.AVAILABLE_YEARS}\"\n        )\n\n    @field_validator(\"resolution\")\n    def validate_resolution(cls, value: str) -&gt; int:\n        if value in cls.AVAILABLE_RESOLUTIONS:\n            return value\n        raise ValueError(\n            f\"No datasets found for the provided resolution: {value}\\nAvailable resolutions are: {cls.AVAILABLE_RESOLUTIONS}\"\n        )\n\n    @model_validator(mode=\"after\")\n    def validate_configuration(self):\n        \"\"\"\n        Validate that the configuration is valid based on dataset availability constraints.\n\n        Specific rules:\n        -\n        \"\"\"\n        if self.year == 2018 and self.product in [\"GHS_BUILT_V\", \"GHS_POP\", \"GHS_SMOD\"]:\n            raise ValueError(f\"{self.product} product is not available for 2018\")\n\n        if self.resolution == 10 and self.product != \"GHS_BUILT_H\":\n            raise ValueError(\n                f\"{self.product} product is not available at 10 (10m) resolution\"\n            )\n\n        if \"GHS_BUILT_H\" in self.product:\n            if self.year != 2018:\n                self.logger.warning(\n                    \"Building height product is only available for 2018, year is set as 2018\"\n                )\n                self.year = 2018\n\n        if self.product == \"GHS_BUILT_S\":\n            if self.year == 2018 and self.resolution != 10:\n                self.logger.warning(\n                    \"Built-up surface product for 2018 is only available at 10m resolution, resolution is set as 10m\"\n                )\n                self.resolution = 10\n\n            if self.resolution == 10 and self.year != 2018:\n                self.logger.warning(\n                    \"Built-up surface product at resolution 10 is only available for 2018, year is set as 2018\"\n                )\n                self.year = 2018\n\n            if self.resolution == 10 and self.coord_system != CoordSystem.Mollweide:\n                self.logger.warning(\n                    f\"Built-up surface product at resolution 10 is only available with Mollweide ({CoordSystem.Mollweide}) projection, coordinate system is set as Mollweide\"\n                )\n                self.coord_system = CoordSystem.Mollweide\n\n        if self.product == \"GHS_SMOD\":\n            if self.resolution != 1000:\n                self.logger.warning(\n                    f\"Settlement model (SMOD) product is only available at 1000 (1km) resolution, resolution is set as 1000\"\n                )\n                self.resolution = 1000\n\n            if self.coord_system != CoordSystem.Mollweide:\n                self.logger.warning(\n                    f\"Settlement model (SMOD) product is only available with Mollweide ({CoordSystem.Mollweide}) projection, coordinate system is set as Mollweide\"\n                )\n                self.coord_system = CoordSystem.Mollweide\n\n        self.TILES_URL = self.TILES_URL.format(self.coord_system)\n\n        self._load_tiles()\n\n        return self\n\n    @property\n    def crs(self) -&gt; str:\n        return \"EPSG:4326\" if self.coord_system == CoordSystem.WGS84 else \"ESRI:54009\"\n\n    def _get_product_info(self) -&gt; dict:\n        \"\"\"Generate and return common product information used in multiple methods.\"\"\"\n        resolution_str = (\n            str(self.resolution)\n            if self.coord_system == CoordSystem.Mollweide\n            else (\"3ss\" if self.resolution == 100 else \"30ss\")\n        )\n        product_folder = f\"{self.product}_GLOBE_{self.release}\"\n        product_name = f\"{self.product}_E{self.year}_GLOBE_{self.release}_{self.coord_system}_{resolution_str}\"\n        product_version = 2 if self.product == \"GHS_SMOD\" else 1\n\n        return {\n            \"resolution_str\": resolution_str,\n            \"product_folder\": product_folder,\n            \"product_name\": product_name,\n            \"product_version\": product_version,\n        }\n\n    def compute_dataset_url(self, tile_id=None) -&gt; str:\n        \"\"\"Compute the download URL for a GHSL dataset.\"\"\"\n        info = self._get_product_info()\n\n        path_segments = [\n            str(self.GHSL_DB_BASE_URL),\n            info[\"product_folder\"],\n            info[\"product_name\"],\n            f\"V{info['product_version']}-0\",\n            \"tiles\" if tile_id else \"\",\n            f\"{info['product_name']}_V{info['product_version']}_0\"\n            + (f\"_{tile_id}\" if tile_id else \"\")\n            + \".zip\",\n        ]\n\n        return \"/\".join(path_segments)\n\n    def get_country_tiles(\n        self,\n        country: str,\n        data_store: Optional[DataStore] = None,\n        country_geom_path: Optional[Union[Path, str]] = None,\n    ) -&gt; List:\n\n        def _load_country_geometry(\n            country: str,\n            data_store: Optional[DataStore] = None,\n            country_geom_path: Optional[Union[Path, str]] = None,\n        ) -&gt; Union[Polygon, MultiPolygon]:\n            \"\"\"Load country boundary geometry from DataStore or GADM.\"\"\"\n\n            gdf_admin0 = (\n                AdminBoundaries.create(\n                    country_code=pycountry.countries.lookup(country).alpha_3,\n                    admin_level=0,\n                    data_store=data_store,\n                    path=country_geom_path,\n                )\n                .to_geodataframe()\n                .to_crs(self.tiles_gdf.crs)\n            )\n\n            return gdf_admin0.geometry.iloc[0]\n\n        country_geom = _load_country_geometry(country, data_store, country_geom_path)\n\n        s = STRtree(self.tiles_gdf.geometry)\n        result = s.query(country_geom, predicate=\"intersects\")\n\n        intersection_tiles = self.tiles_gdf.iloc[result].reset_index(drop=True)\n\n        return [tile for tile in intersection_tiles.tile_id]\n\n    def get_intersecting_tiles(\n        self, geometry: Union[Polygon, MultiPolygon, gpd.GeoDataFrame], crs=4326\n    ) -&gt; List[str]:\n        \"\"\"\n        Find all GHSL tiles that intersect with the provided geometry.\n\n        Args:\n            geometry: A geometry or GeoDataFrame to check for intersection with GHSL tiles\n            crs: Coordinate reference system of the given geometry\n\n        Returns:\n            List of URLs for GHSL dataset tiles that intersect with the geometry\n        \"\"\"\n\n        if isinstance(geometry, gpd.GeoDataFrame):\n            search_geom = geometry.geometry.unary_union\n        else:\n            search_geom = geometry\n\n        search_geom = (\n            gpd.GeoDataFrame(geometry=[search_geom], crs=crs)\n            .to_crs(self.tiles_gdf.crs)\n            .geometry.iloc[0]\n        )\n\n        s = STRtree(self.tiles_gdf.geometry)\n        result = s.query(search_geom, predicate=\"intersects\")\n\n        intersection_tiles = self.tiles_gdf.iloc[result].reset_index(drop=True)\n\n        return [tile for tile in intersection_tiles.tile_id]\n\n    def get_tile_path(self, tile_id=None) -&gt; str:\n        \"\"\"Construct and return the path for the configured dataset.\"\"\"\n        info = self._get_product_info()\n\n        tile_path = (\n            self.base_path\n            / info[\"product_folder\"]\n            / (\n                f\"{info['product_name']}_V{info['product_version']}_0\"\n                + (f\"_{tile_id}\" if tile_id else \"\")\n                + \".zip\"\n            )\n        )\n\n        return tile_path\n\n    def __repr__(self) -&gt; str:\n        \"\"\"Return a string representation of the GHSL dataset configuration.\"\"\"\n        return (\n            f\"GHSLDataConfig(\"\n            f\"product='{self.product}', \"\n            f\"year={self.year}, \"\n            f\"resolution={self.resolution}, \"\n            f\"coord_system={self.coord_system.name}, \"\n            f\"release='{self.release}'\"\n            f\")\"\n        )\n</code></pre>"},{"location":"api/handlers/#gigaspatial.handlers.ghsl.GHSLDataConfig.__repr__","title":"<code>__repr__()</code>","text":"<p>Return a string representation of the GHSL dataset configuration.</p> Source code in <code>gigaspatial/handlers/ghsl.py</code> <pre><code>def __repr__(self) -&gt; str:\n    \"\"\"Return a string representation of the GHSL dataset configuration.\"\"\"\n    return (\n        f\"GHSLDataConfig(\"\n        f\"product='{self.product}', \"\n        f\"year={self.year}, \"\n        f\"resolution={self.resolution}, \"\n        f\"coord_system={self.coord_system.name}, \"\n        f\"release='{self.release}'\"\n        f\")\"\n    )\n</code></pre>"},{"location":"api/handlers/#gigaspatial.handlers.ghsl.GHSLDataConfig.compute_dataset_url","title":"<code>compute_dataset_url(tile_id=None)</code>","text":"<p>Compute the download URL for a GHSL dataset.</p> Source code in <code>gigaspatial/handlers/ghsl.py</code> <pre><code>def compute_dataset_url(self, tile_id=None) -&gt; str:\n    \"\"\"Compute the download URL for a GHSL dataset.\"\"\"\n    info = self._get_product_info()\n\n    path_segments = [\n        str(self.GHSL_DB_BASE_URL),\n        info[\"product_folder\"],\n        info[\"product_name\"],\n        f\"V{info['product_version']}-0\",\n        \"tiles\" if tile_id else \"\",\n        f\"{info['product_name']}_V{info['product_version']}_0\"\n        + (f\"_{tile_id}\" if tile_id else \"\")\n        + \".zip\",\n    ]\n\n    return \"/\".join(path_segments)\n</code></pre>"},{"location":"api/handlers/#gigaspatial.handlers.ghsl.GHSLDataConfig.get_intersecting_tiles","title":"<code>get_intersecting_tiles(geometry, crs=4326)</code>","text":"<p>Find all GHSL tiles that intersect with the provided geometry.</p> <p>Parameters:</p> Name Type Description Default <code>geometry</code> <code>Union[Polygon, MultiPolygon, GeoDataFrame]</code> <p>A geometry or GeoDataFrame to check for intersection with GHSL tiles</p> required <code>crs</code> <p>Coordinate reference system of the given geometry</p> <code>4326</code> <p>Returns:</p> Type Description <code>List[str]</code> <p>List of URLs for GHSL dataset tiles that intersect with the geometry</p> Source code in <code>gigaspatial/handlers/ghsl.py</code> <pre><code>def get_intersecting_tiles(\n    self, geometry: Union[Polygon, MultiPolygon, gpd.GeoDataFrame], crs=4326\n) -&gt; List[str]:\n    \"\"\"\n    Find all GHSL tiles that intersect with the provided geometry.\n\n    Args:\n        geometry: A geometry or GeoDataFrame to check for intersection with GHSL tiles\n        crs: Coordinate reference system of the given geometry\n\n    Returns:\n        List of URLs for GHSL dataset tiles that intersect with the geometry\n    \"\"\"\n\n    if isinstance(geometry, gpd.GeoDataFrame):\n        search_geom = geometry.geometry.unary_union\n    else:\n        search_geom = geometry\n\n    search_geom = (\n        gpd.GeoDataFrame(geometry=[search_geom], crs=crs)\n        .to_crs(self.tiles_gdf.crs)\n        .geometry.iloc[0]\n    )\n\n    s = STRtree(self.tiles_gdf.geometry)\n    result = s.query(search_geom, predicate=\"intersects\")\n\n    intersection_tiles = self.tiles_gdf.iloc[result].reset_index(drop=True)\n\n    return [tile for tile in intersection_tiles.tile_id]\n</code></pre>"},{"location":"api/handlers/#gigaspatial.handlers.ghsl.GHSLDataConfig.get_tile_path","title":"<code>get_tile_path(tile_id=None)</code>","text":"<p>Construct and return the path for the configured dataset.</p> Source code in <code>gigaspatial/handlers/ghsl.py</code> <pre><code>def get_tile_path(self, tile_id=None) -&gt; str:\n    \"\"\"Construct and return the path for the configured dataset.\"\"\"\n    info = self._get_product_info()\n\n    tile_path = (\n        self.base_path\n        / info[\"product_folder\"]\n        / (\n            f\"{info['product_name']}_V{info['product_version']}_0\"\n            + (f\"_{tile_id}\" if tile_id else \"\")\n            + \".zip\"\n        )\n    )\n\n    return tile_path\n</code></pre>"},{"location":"api/handlers/#gigaspatial.handlers.ghsl.GHSLDataConfig.validate_configuration","title":"<code>validate_configuration()</code>","text":"<p>Validate that the configuration is valid based on dataset availability constraints.</p>"},{"location":"api/handlers/#gigaspatial.handlers.ghsl.GHSLDataConfig.validate_configuration--specific-rules","title":"Specific rules:","text":"Source code in <code>gigaspatial/handlers/ghsl.py</code> <pre><code>@model_validator(mode=\"after\")\ndef validate_configuration(self):\n    \"\"\"\n    Validate that the configuration is valid based on dataset availability constraints.\n\n    Specific rules:\n    -\n    \"\"\"\n    if self.year == 2018 and self.product in [\"GHS_BUILT_V\", \"GHS_POP\", \"GHS_SMOD\"]:\n        raise ValueError(f\"{self.product} product is not available for 2018\")\n\n    if self.resolution == 10 and self.product != \"GHS_BUILT_H\":\n        raise ValueError(\n            f\"{self.product} product is not available at 10 (10m) resolution\"\n        )\n\n    if \"GHS_BUILT_H\" in self.product:\n        if self.year != 2018:\n            self.logger.warning(\n                \"Building height product is only available for 2018, year is set as 2018\"\n            )\n            self.year = 2018\n\n    if self.product == \"GHS_BUILT_S\":\n        if self.year == 2018 and self.resolution != 10:\n            self.logger.warning(\n                \"Built-up surface product for 2018 is only available at 10m resolution, resolution is set as 10m\"\n            )\n            self.resolution = 10\n\n        if self.resolution == 10 and self.year != 2018:\n            self.logger.warning(\n                \"Built-up surface product at resolution 10 is only available for 2018, year is set as 2018\"\n            )\n            self.year = 2018\n\n        if self.resolution == 10 and self.coord_system != CoordSystem.Mollweide:\n            self.logger.warning(\n                f\"Built-up surface product at resolution 10 is only available with Mollweide ({CoordSystem.Mollweide}) projection, coordinate system is set as Mollweide\"\n            )\n            self.coord_system = CoordSystem.Mollweide\n\n    if self.product == \"GHS_SMOD\":\n        if self.resolution != 1000:\n            self.logger.warning(\n                f\"Settlement model (SMOD) product is only available at 1000 (1km) resolution, resolution is set as 1000\"\n            )\n            self.resolution = 1000\n\n        if self.coord_system != CoordSystem.Mollweide:\n            self.logger.warning(\n                f\"Settlement model (SMOD) product is only available with Mollweide ({CoordSystem.Mollweide}) projection, coordinate system is set as Mollweide\"\n            )\n            self.coord_system = CoordSystem.Mollweide\n\n    self.TILES_URL = self.TILES_URL.format(self.coord_system)\n\n    self._load_tiles()\n\n    return self\n</code></pre>"},{"location":"api/handlers/#gigaspatial.handlers.ghsl.GHSLDataDownloader","title":"<code>GHSLDataDownloader</code>","text":"<p>A class to handle downloads of WorldPop datasets.</p> Source code in <code>gigaspatial/handlers/ghsl.py</code> <pre><code>class GHSLDataDownloader:\n    \"\"\"A class to handle downloads of WorldPop datasets.\"\"\"\n\n    def __init__(\n        self,\n        config: Union[GHSLDataConfig, dict[str, Union[str, int]]],\n        data_store: Optional[DataStore] = None,\n        logger: Optional[logging.Logger] = None,\n    ):\n        \"\"\"\n        Initialize the downloader.\n\n        Args:\n            config: Configuration for the GHSL dataset, either as a GHSLDataConfig object or a dictionary of parameters\n            data_store: Optional data storage interface. If not provided, uses LocalDataStore.\n            logger: Optional custom logger. If not provided, uses default logger.\n        \"\"\"\n        self.logger = logger or global_config.get_logger(__name__)\n        self.data_store = data_store or LocalDataStore()\n        self.config = (\n            config if isinstance(config, GHSLDataConfig) else GHSLDataConfig(**config)\n        )\n        self.config.logger = self.logger\n\n    def _download_tile(self, tile_id: str) -&gt; str:\n        \"\"\"\n        Download the configured dataset to the provided output path.\n\n        Args:\n            tile_id: tile ID to download\n\n        Returns:\n            path to extracted files\n        \"\"\"\n\n        try:\n            response = requests.get(\n                self.config.compute_dataset_url(tile_id=tile_id), stream=True\n            )\n            response.raise_for_status()\n\n            output_path = str(self.config.get_tile_path(tile_id=tile_id))\n\n            total_size = int(response.headers.get(\"content-length\", 0))\n\n            with self.data_store.open(output_path, \"wb\") as file:\n                with tqdm(\n                    total=total_size,\n                    unit=\"B\",\n                    unit_scale=True,\n                    desc=f\"Downloading {os.path.basename(output_path)}\",\n                ) as pbar:\n                    for chunk in response.iter_content(chunk_size=8192):\n                        if chunk:\n                            file.write(chunk)\n                            pbar.update(len(chunk))\n\n            self.logger.debug(f\"Successfully downloaded dataset: {self.config}\")\n\n            return output_path\n\n        except requests.exceptions.RequestException as e:\n            self.logger.error(f\"Failed to download dataset {self.config}: {str(e)}\")\n            return None\n        except Exception as e:\n            self.logger.error(f\"Unexpected error downloading dataset: {str(e)}\")\n            return None\n\n    def download_and_extract_tile(self, tile_id, file_pattern=None):\n        \"\"\"\n        Download and extract specific files from GHSL dataset tile zip archives.\n\n        Args:\n            tile_id: tile ID to download and extract\n            file_pattern: Optional regex pattern to filter which files to extract\n                        (e.g., '.*\\\\.tif$' for only TIF files)\n\n        Returns:\n            path to extracted files\n        \"\"\"\n        output_path = self.config.get_tile_path(tile_id=tile_id).parents[0]\n\n        extracted_files = []\n\n        url = self.config.compute_dataset_url(tile_id=tile_id)\n        self.logger.info(f\"Downloading zip from {url}\")\n\n        try:\n            # download zip to temporary file\n            with tempfile.NamedTemporaryFile(delete=False, suffix=\".zip\") as temp_file:\n                with requests.get(url, stream=True) as response:\n                    response.raise_for_status()\n                    shutil.copyfileobj(response.raw, temp_file)\n\n            with zipfile.ZipFile(temp_file.name, \"r\") as zip_ref:\n                # get list of files in the zip (filter if pattern provided)\n                if file_pattern:\n                    import re\n\n                    pattern = re.compile(file_pattern)\n                    files_to_extract = [\n                        f for f in zip_ref.namelist() if pattern.match(f)\n                    ]\n                else:\n                    files_to_extract = zip_ref.namelist()\n\n                for file in files_to_extract:\n                    extracted_path = output_path / Path(file).name\n                    with zip_ref.open(file) as source, open(\n                        extracted_path, \"wb\"\n                    ) as target:\n                        shutil.copyfileobj(source, target)\n                    extracted_files.append(extracted_path)\n                    self.logger.info(f\"Extracted {file} to {extracted_path}\")\n\n            Path(temp_file.name).unlink()\n\n        except Exception as e:\n            self.logger.error(f\"Error downloading/extracting tile {tile_id}: {e}\")\n            raise\n\n        return extracted_files\n\n    def download_by_country(\n        self,\n        country: str,\n        data_store: Optional[DataStore] = None,\n        country_geom_path: Optional[Union[str, Path]] = None,\n        extract: bool = False,\n    ) -&gt; List[str]:\n        \"\"\"\n        Download GHSL data for a specific country.\n\n        Args:\n            country: ISO 3166-1 alpha-3 country code, ISO alpha-2 country code or country name\n\n        Returns:\n            List of paths to downloaded files\n        \"\"\"\n\n        # Get intersecting tiles\n        country_tiles = self.config.get_country_tiles(\n            country=country, data_store=data_store, country_geom_path=country_geom_path\n        )\n\n        if not country_tiles:\n            self.logger.warning(f\"There is no matching data for {country}\")\n            return []\n\n        # Download tiles in parallel\n        with multiprocessing.Pool(self.config.n_workers) as pool:\n            download_func = functools.partial(\n                self._download_tile if not extract else self.download_and_extract_tile\n            )\n            file_paths = list(\n                tqdm(\n                    pool.imap(download_func, country_tiles),\n                    total=len(country_tiles),\n                    desc=f\"Downloading for {country}\",\n                )\n            )\n\n        # Filter out None values (failed downloads)\n        return [path for path in file_paths if path is not None]\n\n    def download_by_points(\n        self, points_gdf: gpd.GeoDataFrame, extract: bool = False\n    ) -&gt; List[str]:\n        \"\"\"\n        Download GHSL data for areas containing specific points.\n\n        Args:\n            points_gdf: GeoDataFrame containing points of interest\n\n        Returns:\n            List of paths to downloaded files\n        \"\"\"\n        # Get intersecting tiles\n        int_tiles = self.config.get_intersecting_tiles(points_gdf, points_gdf.crs)\n\n        if not int_tiles:\n            self.logger.warning(f\"There is no matching data for the points\")\n            return []\n\n        # Download tiles in parallel\n        with multiprocessing.Pool(self.config.n_workers) as pool:\n            download_func = functools.partial(\n                self._download_tile if not extract else self.download_and_extract_tile\n            )\n            file_paths = list(\n                tqdm(\n                    pool.imap(download_func, int_tiles),\n                    total=len(int_tiles),\n                    desc=f\"Downloading for points dataset\",\n                )\n            )\n\n        # Filter out None values (failed downloads)\n        return [path for path in file_paths if path is not None]\n</code></pre>"},{"location":"api/handlers/#gigaspatial.handlers.ghsl.GHSLDataDownloader.__init__","title":"<code>__init__(config, data_store=None, logger=None)</code>","text":"<p>Initialize the downloader.</p> <p>Parameters:</p> Name Type Description Default <code>config</code> <code>Union[GHSLDataConfig, dict[str, Union[str, int]]]</code> <p>Configuration for the GHSL dataset, either as a GHSLDataConfig object or a dictionary of parameters</p> required <code>data_store</code> <code>Optional[DataStore]</code> <p>Optional data storage interface. If not provided, uses LocalDataStore.</p> <code>None</code> <code>logger</code> <code>Optional[Logger]</code> <p>Optional custom logger. If not provided, uses default logger.</p> <code>None</code> Source code in <code>gigaspatial/handlers/ghsl.py</code> <pre><code>def __init__(\n    self,\n    config: Union[GHSLDataConfig, dict[str, Union[str, int]]],\n    data_store: Optional[DataStore] = None,\n    logger: Optional[logging.Logger] = None,\n):\n    \"\"\"\n    Initialize the downloader.\n\n    Args:\n        config: Configuration for the GHSL dataset, either as a GHSLDataConfig object or a dictionary of parameters\n        data_store: Optional data storage interface. If not provided, uses LocalDataStore.\n        logger: Optional custom logger. If not provided, uses default logger.\n    \"\"\"\n    self.logger = logger or global_config.get_logger(__name__)\n    self.data_store = data_store or LocalDataStore()\n    self.config = (\n        config if isinstance(config, GHSLDataConfig) else GHSLDataConfig(**config)\n    )\n    self.config.logger = self.logger\n</code></pre>"},{"location":"api/handlers/#gigaspatial.handlers.ghsl.GHSLDataDownloader.download_and_extract_tile","title":"<code>download_and_extract_tile(tile_id, file_pattern=None)</code>","text":"<p>Download and extract specific files from GHSL dataset tile zip archives.</p> <p>Parameters:</p> Name Type Description Default <code>tile_id</code> <p>tile ID to download and extract</p> required <code>file_pattern</code> <p>Optional regex pattern to filter which files to extract         (e.g., '.*.tif$' for only TIF files)</p> <code>None</code> <p>Returns:</p> Type Description <p>path to extracted files</p> Source code in <code>gigaspatial/handlers/ghsl.py</code> <pre><code>def download_and_extract_tile(self, tile_id, file_pattern=None):\n    \"\"\"\n    Download and extract specific files from GHSL dataset tile zip archives.\n\n    Args:\n        tile_id: tile ID to download and extract\n        file_pattern: Optional regex pattern to filter which files to extract\n                    (e.g., '.*\\\\.tif$' for only TIF files)\n\n    Returns:\n        path to extracted files\n    \"\"\"\n    output_path = self.config.get_tile_path(tile_id=tile_id).parents[0]\n\n    extracted_files = []\n\n    url = self.config.compute_dataset_url(tile_id=tile_id)\n    self.logger.info(f\"Downloading zip from {url}\")\n\n    try:\n        # download zip to temporary file\n        with tempfile.NamedTemporaryFile(delete=False, suffix=\".zip\") as temp_file:\n            with requests.get(url, stream=True) as response:\n                response.raise_for_status()\n                shutil.copyfileobj(response.raw, temp_file)\n\n        with zipfile.ZipFile(temp_file.name, \"r\") as zip_ref:\n            # get list of files in the zip (filter if pattern provided)\n            if file_pattern:\n                import re\n\n                pattern = re.compile(file_pattern)\n                files_to_extract = [\n                    f for f in zip_ref.namelist() if pattern.match(f)\n                ]\n            else:\n                files_to_extract = zip_ref.namelist()\n\n            for file in files_to_extract:\n                extracted_path = output_path / Path(file).name\n                with zip_ref.open(file) as source, open(\n                    extracted_path, \"wb\"\n                ) as target:\n                    shutil.copyfileobj(source, target)\n                extracted_files.append(extracted_path)\n                self.logger.info(f\"Extracted {file} to {extracted_path}\")\n\n        Path(temp_file.name).unlink()\n\n    except Exception as e:\n        self.logger.error(f\"Error downloading/extracting tile {tile_id}: {e}\")\n        raise\n\n    return extracted_files\n</code></pre>"},{"location":"api/handlers/#gigaspatial.handlers.ghsl.GHSLDataDownloader.download_by_country","title":"<code>download_by_country(country, data_store=None, country_geom_path=None, extract=False)</code>","text":"<p>Download GHSL data for a specific country.</p> <p>Parameters:</p> Name Type Description Default <code>country</code> <code>str</code> <p>ISO 3166-1 alpha-3 country code, ISO alpha-2 country code or country name</p> required <p>Returns:</p> Type Description <code>List[str]</code> <p>List of paths to downloaded files</p> Source code in <code>gigaspatial/handlers/ghsl.py</code> <pre><code>def download_by_country(\n    self,\n    country: str,\n    data_store: Optional[DataStore] = None,\n    country_geom_path: Optional[Union[str, Path]] = None,\n    extract: bool = False,\n) -&gt; List[str]:\n    \"\"\"\n    Download GHSL data for a specific country.\n\n    Args:\n        country: ISO 3166-1 alpha-3 country code, ISO alpha-2 country code or country name\n\n    Returns:\n        List of paths to downloaded files\n    \"\"\"\n\n    # Get intersecting tiles\n    country_tiles = self.config.get_country_tiles(\n        country=country, data_store=data_store, country_geom_path=country_geom_path\n    )\n\n    if not country_tiles:\n        self.logger.warning(f\"There is no matching data for {country}\")\n        return []\n\n    # Download tiles in parallel\n    with multiprocessing.Pool(self.config.n_workers) as pool:\n        download_func = functools.partial(\n            self._download_tile if not extract else self.download_and_extract_tile\n        )\n        file_paths = list(\n            tqdm(\n                pool.imap(download_func, country_tiles),\n                total=len(country_tiles),\n                desc=f\"Downloading for {country}\",\n            )\n        )\n\n    # Filter out None values (failed downloads)\n    return [path for path in file_paths if path is not None]\n</code></pre>"},{"location":"api/handlers/#gigaspatial.handlers.ghsl.GHSLDataDownloader.download_by_points","title":"<code>download_by_points(points_gdf, extract=False)</code>","text":"<p>Download GHSL data for areas containing specific points.</p> <p>Parameters:</p> Name Type Description Default <code>points_gdf</code> <code>GeoDataFrame</code> <p>GeoDataFrame containing points of interest</p> required <p>Returns:</p> Type Description <code>List[str]</code> <p>List of paths to downloaded files</p> Source code in <code>gigaspatial/handlers/ghsl.py</code> <pre><code>def download_by_points(\n    self, points_gdf: gpd.GeoDataFrame, extract: bool = False\n) -&gt; List[str]:\n    \"\"\"\n    Download GHSL data for areas containing specific points.\n\n    Args:\n        points_gdf: GeoDataFrame containing points of interest\n\n    Returns:\n        List of paths to downloaded files\n    \"\"\"\n    # Get intersecting tiles\n    int_tiles = self.config.get_intersecting_tiles(points_gdf, points_gdf.crs)\n\n    if not int_tiles:\n        self.logger.warning(f\"There is no matching data for the points\")\n        return []\n\n    # Download tiles in parallel\n    with multiprocessing.Pool(self.config.n_workers) as pool:\n        download_func = functools.partial(\n            self._download_tile if not extract else self.download_and_extract_tile\n        )\n        file_paths = list(\n            tqdm(\n                pool.imap(download_func, int_tiles),\n                total=len(int_tiles),\n                desc=f\"Downloading for points dataset\",\n            )\n        )\n\n    # Filter out None values (failed downloads)\n    return [path for path in file_paths if path is not None]\n</code></pre>"},{"location":"api/handlers/#gigaspatial.handlers.google_open_buildings","title":"<code>google_open_buildings</code>","text":""},{"location":"api/handlers/#gigaspatial.handlers.google_open_buildings.GoogleOpenBuildingsConfig","title":"<code>GoogleOpenBuildingsConfig</code>  <code>dataclass</code>","text":"<p>Configuration for Google Open Buildings dataset files.</p> Source code in <code>gigaspatial/handlers/google_open_buildings.py</code> <pre><code>@dataclass\nclass GoogleOpenBuildingsConfig:\n    \"\"\"Configuration for Google Open Buildings dataset files.\"\"\"\n\n    base_path: Path = global_config.get_path(\"google_open_buildings\", \"bronze\")\n    data_types: tuple = (\"polygons\", \"points\")\n    n_workers: int = 4  # number of workers for parallel processing\n\n    def get_tile_path(\n        self, tile_id: str, data_type: Literal[\"polygons\", \"points\"]\n    ) -&gt; Path:\n        \"\"\"\n        Construct the full path for a tile file.\n\n        Args:\n            tile_id: S2 tile identifier\n            data_type: Type of building data ('polygons' or 'points')\n\n        Returns:\n            Full path to the tile file\n        \"\"\"\n        if data_type not in self.data_types:\n            raise ValueError(f\"data_type must be one of {self.data_types}\")\n\n        return self.base_path / f\"{data_type}_s2_level_4_{tile_id}_buildings.csv.gz\"\n</code></pre>"},{"location":"api/handlers/#gigaspatial.handlers.google_open_buildings.GoogleOpenBuildingsConfig.get_tile_path","title":"<code>get_tile_path(tile_id, data_type)</code>","text":"<p>Construct the full path for a tile file.</p> <p>Parameters:</p> Name Type Description Default <code>tile_id</code> <code>str</code> <p>S2 tile identifier</p> required <code>data_type</code> <code>Literal['polygons', 'points']</code> <p>Type of building data ('polygons' or 'points')</p> required <p>Returns:</p> Type Description <code>Path</code> <p>Full path to the tile file</p> Source code in <code>gigaspatial/handlers/google_open_buildings.py</code> <pre><code>def get_tile_path(\n    self, tile_id: str, data_type: Literal[\"polygons\", \"points\"]\n) -&gt; Path:\n    \"\"\"\n    Construct the full path for a tile file.\n\n    Args:\n        tile_id: S2 tile identifier\n        data_type: Type of building data ('polygons' or 'points')\n\n    Returns:\n        Full path to the tile file\n    \"\"\"\n    if data_type not in self.data_types:\n        raise ValueError(f\"data_type must be one of {self.data_types}\")\n\n    return self.base_path / f\"{data_type}_s2_level_4_{tile_id}_buildings.csv.gz\"\n</code></pre>"},{"location":"api/handlers/#gigaspatial.handlers.google_open_buildings.GoogleOpenBuildingsDownloader","title":"<code>GoogleOpenBuildingsDownloader</code>","text":"<p>A class to handle downloads of Google's Open Buildings dataset.</p> Source code in <code>gigaspatial/handlers/google_open_buildings.py</code> <pre><code>class GoogleOpenBuildingsDownloader:\n    \"\"\"A class to handle downloads of Google's Open Buildings dataset.\"\"\"\n\n    TILES_URL = \"https://openbuildings-public-dot-gweb-research.uw.r.appspot.com/public/tiles.geojson\"\n\n    def __init__(\n        self,\n        config: Optional[GoogleOpenBuildingsConfig] = None,\n        data_store: Optional[DataStore] = None,\n        logger: Optional[logging.Logger] = None,\n    ):\n        \"\"\"\n        Initialize the downloader.\n\n        Args:\n            config: Optional configuration for file paths\n            data_store: Instance of DataStore for accessing data storage\n            logger: Optional custom logger. If not provided, uses default logger.\n        \"\"\"\n        self.data_store = data_store or LocalDataStore()\n        self.config = config or GoogleOpenBuildingsConfig()\n        self.logger = logger or global_config.get_logger(__name__)\n\n        # Load and cache S2 tiles\n        self._load_s2_tiles()\n\n    def _load_s2_tiles(self):\n        \"\"\"Load S2 tiles from GeoJSON file.\"\"\"\n        response = requests.get(self.TILES_URL)\n        response.raise_for_status()\n\n        # Convert to GeoDataFrame\n        self.tiles_gdf = gpd.GeoDataFrame.from_features(\n            response.json()[\"features\"], crs=\"EPSG:4326\"\n        )\n\n    def _get_intersecting_tiles(\n        self, geometry: Union[Polygon, MultiPolygon, gpd.GeoDataFrame]\n    ) -&gt; pd.DataFrame:\n        \"\"\"Get tiles that intersect with the given geometry.\"\"\"\n\n        if isinstance(geometry, gpd.GeoDataFrame):\n            if geometry.crs != \"EPSG:4326\":\n                geometry = geometry.to_crs(\"EPSG:4326\")\n            search_geom = geometry.geometry.unary_union\n        elif isinstance(geometry, (Polygon, MultiPolygon)):\n            search_geom = geometry\n        else:\n            raise ValueError(\n                f\"Expected Polygon, Multipolygon or GeoDataFrame got {geometry.__class__}\"\n            )\n\n        # Find intersecting tiles\n        mask = (\n            tile_geom.intersects(search_geom) for tile_geom in self.tiles_gdf.geometry\n        )\n\n        return self.tiles_gdf.loc[mask, [\"tile_id\", \"tile_url\", \"size_mb\"]]\n\n    def _download_tile(\n        self,\n        tile_info: Union[pd.Series, dict],\n        data_type: Literal[\"polygons\", \"points\"],\n    ) -&gt; Optional[str]:\n        \"\"\"Download data file for a single tile.\"\"\"\n\n        tile_url = tile_info[\"tile_url\"]\n        if data_type == \"points\":\n            tile_url = tile_url.replace(\"polygons\", \"points\")\n\n        try:\n            response = requests.get(tile_url, stream=True)\n            response.raise_for_status()\n\n            file_path = str(self.config.get_tile_path(tile_info[\"tile_id\"], data_type))\n\n            with self.data_store.open(file_path, \"wb\") as file:\n                for chunk in response.iter_content(chunk_size=8192):\n                    file.write(chunk)\n\n                self.logger.debug(\n                    f\"Successfully downloaded tile: {tile_info['tile_id']}\"\n                )\n                return file_path\n\n        except requests.exceptions.RequestException as e:\n            self.logger.error(\n                f\"Failed to download tile {tile_info['tile_id']}: {str(e)}\"\n            )\n            return None\n        except Exception as e:\n            self.logger.error(f\"Unexpected error downloading dataset: {str(e)}\")\n            return None\n\n    def get_download_size_estimate(\n        self, geometry: Union[Polygon, MultiPolygon, gpd.GeoDataFrame]\n    ) -&gt; float:\n        \"\"\"\n        Estimate the download size in MB for a given geometry or GeoDataFrame.\n\n        Args:\n            geometry: Shapely Polygon/MultiPolygon or GeoDataFrame with geometries\n\n        Returns:\n            Estimated size in megabytes\n        \"\"\"\n        gdf_tiles = self._get_intersecting_tiles(geometry)\n\n        return gdf_tiles[\"size_mb\"].sum()\n\n    def download_by_country(\n        self,\n        country_code: str,\n        data_type: Literal[\"polygons\", \"points\"] = \"polygons\",\n        data_store: Optional[DataStore] = None,\n        country_geom_path: Optional[Union[str, Path]] = None,\n    ) -&gt; List[str]:\n        \"\"\"\n        Download Google Open Buildings data for a specific country.\n\n        Args:\n            country_code: ISO 3166-1 alpha-3 country code\n            data_type: Type of data to download ('polygons' or 'points')\n\n        Returns:\n            List of paths to downloaded files\n        \"\"\"\n\n        gdf_admin0 = AdminBoundaries.create(\n            country_code=country_code,\n            admin_level=0,\n            data_store=data_store,\n            path=country_geom_path,\n        ).to_geodataframe()\n\n        # Get intersecting tiles\n        gdf_tiles = self._get_intersecting_tiles(gdf_admin0)\n\n        if gdf_tiles.empty:\n            self.logger.warning(f\"There is no matching data for {country_code}\")\n            return []\n\n        # Download tiles in parallel\n        with multiprocessing.Pool(self.config.n_workers) as pool:\n            download_func = functools.partial(self._download_tile, data_type=data_type)\n            file_paths = list(\n                tqdm(\n                    pool.imap(download_func, [row for _, row in gdf_tiles.iterrows()]),\n                    total=len(gdf_tiles),\n                    desc=f\"Downloading {data_type} for {country_code}\",\n                )\n            )\n\n        # Filter out None values (failed downloads)\n        return [path for path in file_paths if path is not None]\n\n    def download_by_points(\n        self,\n        points_gdf: gpd.GeoDataFrame,\n        data_type: Literal[\"polygons\", \"points\"] = \"polygons\",\n    ) -&gt; List[str]:\n        \"\"\"\n        Download Google Open Buildings data for areas containing specific points.\n\n        Args:\n            points_gdf: GeoDataFrame containing points of interest\n            data_type: Type of data to download ('polygons' or 'points')\n\n        Returns:\n            List of paths to downloaded files\n        \"\"\"\n        # Get intersecting tiles\n        gdf_tiles = self._get_intersecting_tiles(points_gdf)\n\n        if gdf_tiles.empty:\n            self.logger.warning(f\"There is no matching data for the points\")\n            return []\n\n        # Download tiles in parallel\n        with multiprocessing.Pool(self.config.n_workers) as pool:\n            download_func = functools.partial(self._download_tile, data_type=data_type)\n            file_paths = list(\n                tqdm(\n                    pool.imap(download_func, [row for _, row in gdf_tiles.iterrows()]),\n                    total=len(gdf_tiles),\n                    desc=f\"Downloading {data_type} for points dataset\",\n                )\n            )\n\n        # Filter out None values (failed downloads)\n        return [path for path in file_paths if path is not None]\n</code></pre>"},{"location":"api/handlers/#gigaspatial.handlers.google_open_buildings.GoogleOpenBuildingsDownloader.__init__","title":"<code>__init__(config=None, data_store=None, logger=None)</code>","text":"<p>Initialize the downloader.</p> <p>Parameters:</p> Name Type Description Default <code>config</code> <code>Optional[GoogleOpenBuildingsConfig]</code> <p>Optional configuration for file paths</p> <code>None</code> <code>data_store</code> <code>Optional[DataStore]</code> <p>Instance of DataStore for accessing data storage</p> <code>None</code> <code>logger</code> <code>Optional[Logger]</code> <p>Optional custom logger. If not provided, uses default logger.</p> <code>None</code> Source code in <code>gigaspatial/handlers/google_open_buildings.py</code> <pre><code>def __init__(\n    self,\n    config: Optional[GoogleOpenBuildingsConfig] = None,\n    data_store: Optional[DataStore] = None,\n    logger: Optional[logging.Logger] = None,\n):\n    \"\"\"\n    Initialize the downloader.\n\n    Args:\n        config: Optional configuration for file paths\n        data_store: Instance of DataStore for accessing data storage\n        logger: Optional custom logger. If not provided, uses default logger.\n    \"\"\"\n    self.data_store = data_store or LocalDataStore()\n    self.config = config or GoogleOpenBuildingsConfig()\n    self.logger = logger or global_config.get_logger(__name__)\n\n    # Load and cache S2 tiles\n    self._load_s2_tiles()\n</code></pre>"},{"location":"api/handlers/#gigaspatial.handlers.google_open_buildings.GoogleOpenBuildingsDownloader.download_by_country","title":"<code>download_by_country(country_code, data_type='polygons', data_store=None, country_geom_path=None)</code>","text":"<p>Download Google Open Buildings data for a specific country.</p> <p>Parameters:</p> Name Type Description Default <code>country_code</code> <code>str</code> <p>ISO 3166-1 alpha-3 country code</p> required <code>data_type</code> <code>Literal['polygons', 'points']</code> <p>Type of data to download ('polygons' or 'points')</p> <code>'polygons'</code> <p>Returns:</p> Type Description <code>List[str]</code> <p>List of paths to downloaded files</p> Source code in <code>gigaspatial/handlers/google_open_buildings.py</code> <pre><code>def download_by_country(\n    self,\n    country_code: str,\n    data_type: Literal[\"polygons\", \"points\"] = \"polygons\",\n    data_store: Optional[DataStore] = None,\n    country_geom_path: Optional[Union[str, Path]] = None,\n) -&gt; List[str]:\n    \"\"\"\n    Download Google Open Buildings data for a specific country.\n\n    Args:\n        country_code: ISO 3166-1 alpha-3 country code\n        data_type: Type of data to download ('polygons' or 'points')\n\n    Returns:\n        List of paths to downloaded files\n    \"\"\"\n\n    gdf_admin0 = AdminBoundaries.create(\n        country_code=country_code,\n        admin_level=0,\n        data_store=data_store,\n        path=country_geom_path,\n    ).to_geodataframe()\n\n    # Get intersecting tiles\n    gdf_tiles = self._get_intersecting_tiles(gdf_admin0)\n\n    if gdf_tiles.empty:\n        self.logger.warning(f\"There is no matching data for {country_code}\")\n        return []\n\n    # Download tiles in parallel\n    with multiprocessing.Pool(self.config.n_workers) as pool:\n        download_func = functools.partial(self._download_tile, data_type=data_type)\n        file_paths = list(\n            tqdm(\n                pool.imap(download_func, [row for _, row in gdf_tiles.iterrows()]),\n                total=len(gdf_tiles),\n                desc=f\"Downloading {data_type} for {country_code}\",\n            )\n        )\n\n    # Filter out None values (failed downloads)\n    return [path for path in file_paths if path is not None]\n</code></pre>"},{"location":"api/handlers/#gigaspatial.handlers.google_open_buildings.GoogleOpenBuildingsDownloader.download_by_points","title":"<code>download_by_points(points_gdf, data_type='polygons')</code>","text":"<p>Download Google Open Buildings data for areas containing specific points.</p> <p>Parameters:</p> Name Type Description Default <code>points_gdf</code> <code>GeoDataFrame</code> <p>GeoDataFrame containing points of interest</p> required <code>data_type</code> <code>Literal['polygons', 'points']</code> <p>Type of data to download ('polygons' or 'points')</p> <code>'polygons'</code> <p>Returns:</p> Type Description <code>List[str]</code> <p>List of paths to downloaded files</p> Source code in <code>gigaspatial/handlers/google_open_buildings.py</code> <pre><code>def download_by_points(\n    self,\n    points_gdf: gpd.GeoDataFrame,\n    data_type: Literal[\"polygons\", \"points\"] = \"polygons\",\n) -&gt; List[str]:\n    \"\"\"\n    Download Google Open Buildings data for areas containing specific points.\n\n    Args:\n        points_gdf: GeoDataFrame containing points of interest\n        data_type: Type of data to download ('polygons' or 'points')\n\n    Returns:\n        List of paths to downloaded files\n    \"\"\"\n    # Get intersecting tiles\n    gdf_tiles = self._get_intersecting_tiles(points_gdf)\n\n    if gdf_tiles.empty:\n        self.logger.warning(f\"There is no matching data for the points\")\n        return []\n\n    # Download tiles in parallel\n    with multiprocessing.Pool(self.config.n_workers) as pool:\n        download_func = functools.partial(self._download_tile, data_type=data_type)\n        file_paths = list(\n            tqdm(\n                pool.imap(download_func, [row for _, row in gdf_tiles.iterrows()]),\n                total=len(gdf_tiles),\n                desc=f\"Downloading {data_type} for points dataset\",\n            )\n        )\n\n    # Filter out None values (failed downloads)\n    return [path for path in file_paths if path is not None]\n</code></pre>"},{"location":"api/handlers/#gigaspatial.handlers.google_open_buildings.GoogleOpenBuildingsDownloader.get_download_size_estimate","title":"<code>get_download_size_estimate(geometry)</code>","text":"<p>Estimate the download size in MB for a given geometry or GeoDataFrame.</p> <p>Parameters:</p> Name Type Description Default <code>geometry</code> <code>Union[Polygon, MultiPolygon, GeoDataFrame]</code> <p>Shapely Polygon/MultiPolygon or GeoDataFrame with geometries</p> required <p>Returns:</p> Type Description <code>float</code> <p>Estimated size in megabytes</p> Source code in <code>gigaspatial/handlers/google_open_buildings.py</code> <pre><code>def get_download_size_estimate(\n    self, geometry: Union[Polygon, MultiPolygon, gpd.GeoDataFrame]\n) -&gt; float:\n    \"\"\"\n    Estimate the download size in MB for a given geometry or GeoDataFrame.\n\n    Args:\n        geometry: Shapely Polygon/MultiPolygon or GeoDataFrame with geometries\n\n    Returns:\n        Estimated size in megabytes\n    \"\"\"\n    gdf_tiles = self._get_intersecting_tiles(geometry)\n\n    return gdf_tiles[\"size_mb\"].sum()\n</code></pre>"},{"location":"api/handlers/#gigaspatial.handlers.mapbox_image","title":"<code>mapbox_image</code>","text":""},{"location":"api/handlers/#gigaspatial.handlers.mapbox_image.MapboxImageDownloader","title":"<code>MapboxImageDownloader</code>","text":"<p>Class to download images from Mapbox Static Images API using a specific style</p> Source code in <code>gigaspatial/handlers/mapbox_image.py</code> <pre><code>class MapboxImageDownloader:\n    \"\"\"Class to download images from Mapbox Static Images API using a specific style\"\"\"\n\n    BASE_URL = \"https://api.mapbox.com/styles/v1\"\n\n    def __init__(\n        self,\n        access_token: str = config.MAPBOX_ACCESS_TOKEN,\n        style_id: Optional[str] = None,\n        data_store: Optional[DataStore] = None,\n    ):\n        \"\"\"\n        Initialize the downloader with Mapbox credentials\n\n        Args:\n            access_token: Mapbox access token\n            style_id: Mapbox style ID to use for image download\n            data_store: Instance of DataStore for accessing data storage\n        \"\"\"\n        self.access_token = access_token\n        self.style_id = style_id if style_id else \"mapbox/satellite-v9\"\n        self.data_store = data_store or LocalDataStore()\n        self.logger = config.get_logger(__name__)\n\n    def _construct_url(self, bounds: Iterable[float], image_size: str) -&gt; str:\n        \"\"\"Construct the Mapbox Static Images API URL\"\"\"\n        bounds_str = f\"[{','.join(map(str, bounds))}]\"\n\n        return (\n            f\"{self.BASE_URL}/{self.style_id}/static/{bounds_str}/{image_size}\"\n            f\"?access_token={self.access_token}&amp;attribution=false&amp;logo=false\"\n        )\n\n    def _download_single_image(self, url: str, output_path: Path) -&gt; bool:\n        \"\"\"Download a single image from URL\"\"\"\n        try:\n            response = requests.get(url)\n            response.raise_for_status()\n\n            with self.data_store.open(str(output_path), \"wb\") as f:\n                f.write(response.content)\n            return True\n        except Exception as e:\n            self.logger.warning(f\"Error downloading {output_path.name}: {str(e)}\")\n            return False\n\n    def download_images_by_tiles(\n        self,\n        mercator_tiles: \"MercatorTiles\",\n        output_dir: Union[str, Path],\n        image_size: Tuple[int, int] = (512, 512),\n        max_workers: int = 4,\n        image_prefix: str = \"image_\",\n    ) -&gt; None:\n        \"\"\"\n        Download images for given mercator tiles using the specified style\n\n        Args:\n            mercator_tiles: MercatorTiles instance containing quadkeys\n            output_dir: Directory to save images\n            image_size: Tuple of (width, height) for output images\n            max_workers: Maximum number of concurrent downloads\n            image_prefix: Prefix for output image names\n        \"\"\"\n        output_dir = Path(output_dir)\n        # self.data_store.makedirs(str(output_dir), exist_ok=True)\n\n        image_size_str = f\"{image_size[0]}x{image_size[1]}\"\n        total_tiles = len(mercator_tiles.quadkeys)\n\n        self.logger.info(\n            f\"Downloading {total_tiles} tiles with size {image_size_str}...\"\n        )\n\n        def _get_tile_bounds(quadkey: str) -&gt; List[float]:\n            \"\"\"Get tile bounds from quadkey\"\"\"\n            tile = mercantile.quadkey_to_tile(quadkey)\n            bounds = mercantile.bounds(tile)\n            return [bounds.west, bounds.south, bounds.east, bounds.north]\n\n        def download_image(quadkey: str) -&gt; bool:\n            bounds = _get_tile_bounds(quadkey)\n            file_name = f\"{image_prefix}{quadkey}.png\"\n\n            url = self._construct_url(bounds, image_size_str)\n            success = self._download_single_image(url, output_dir / file_name)\n\n            return success\n\n        with ThreadPoolExecutor(max_workers=max_workers) as executor:\n            futures = [\n                executor.submit(download_image, quadkey)\n                for quadkey in mercator_tiles.quadkeys\n            ]\n\n            successful_downloads = 0\n            with tqdm(total=total_tiles) as pbar:\n                for future in as_completed(futures):\n                    if future.result():\n                        successful_downloads += 1\n                    pbar.update(1)\n\n        self.logger.info(\n            f\"Successfully downloaded {successful_downloads}/{total_tiles} images!\"\n        )\n\n    def download_images_by_bounds(\n        self,\n        gdf: gpd.GeoDataFrame,\n        output_dir: Union[str, Path],\n        image_size: Tuple[int, int] = (512, 512),\n        max_workers: int = 4,\n        image_prefix: str = \"image_\",\n    ) -&gt; None:\n        \"\"\"\n        Download images for given points using the specified style\n\n        Args:\n            gdf_points: GeoDataFrame containing bounding box polygons\n            output_dir: Directory to save images\n            image_size: Tuple of (width, height) for output images\n            max_workers: Maximum number of concurrent downloads\n            image_prefix: Prefix for output image names\n        \"\"\"\n        output_dir = Path(output_dir)\n        # self.data_store.makedirs(str(output_dir), exist_ok=True)\n\n        image_size_str = f\"{image_size[0]}x{image_size[1]}\"\n        total_images = len(gdf)\n\n        self.logger.info(\n            f\"Downloading {total_images} images with size {image_size_str}...\"\n        )\n\n        def download_image(idx: Any, bounds: Tuple[float, float, float, float]) -&gt; bool:\n            file_name = f\"{image_prefix}{idx}.png\"\n            url = self._construct_url(bounds, image_size_str)\n            success = self._download_single_image(url, output_dir / file_name)\n            return success\n\n        with ThreadPoolExecutor(max_workers=max_workers) as executor:\n            futures = [\n                executor.submit(download_image, row.Index, row.geometry.bounds)\n                for row in gdf.itertuples()\n            ]\n\n            successful_downloads = 0\n            with tqdm(total=total_images) as pbar:\n                for future in as_completed(futures):\n                    if future.result():\n                        successful_downloads += 1\n                    pbar.update(1)\n\n        self.logger.info(\n            f\"Successfully downloaded {successful_downloads}/{total_images} images!\"\n        )\n\n    def download_images_by_coordinates(\n        self,\n        data: Union[pd.DataFrame, List[Tuple[float, float]]],\n        res_meters_pixel: float,\n        output_dir: Union[str, Path],\n        image_size: Tuple[int, int] = (512, 512),\n        max_workers: int = 4,\n        image_prefix: str = \"image_\",\n    ) -&gt; None:\n        \"\"\"\n        Download images for given coordinates by creating bounded boxes around points\n\n        Args:\n            data: Either a DataFrame with either latitude/longitude columns or a geometry column or a list of (lat, lon) tuples\n            res_meters_pixel: Size of the bounding box in meters (creates a square)\n            output_dir: Directory to save images\n            image_size: Tuple of (width, height) for output images\n            max_workers: Maximum number of concurrent downloads\n            image_prefix: Prefix for output image names\n        \"\"\"\n\n        if isinstance(data, pd.DataFrame):\n            coordinates_df = data\n        else:\n            coordinates_df = pd.DataFrame(data, columns=[\"latitude\", \"longitude\"])\n\n        gdf = convert_to_geodataframe(coordinates_df)\n\n        buffered_gdf = buffer_geodataframe(\n            gdf, res_meters_pixel / 2, cap_style=\"square\"\n        )\n\n        self.download_images_by_bounds(\n            buffered_gdf, output_dir, image_size, max_workers, image_prefix\n        )\n</code></pre>"},{"location":"api/handlers/#gigaspatial.handlers.mapbox_image.MapboxImageDownloader.__init__","title":"<code>__init__(access_token=config.MAPBOX_ACCESS_TOKEN, style_id=None, data_store=None)</code>","text":"<p>Initialize the downloader with Mapbox credentials</p> <p>Parameters:</p> Name Type Description Default <code>access_token</code> <code>str</code> <p>Mapbox access token</p> <code>MAPBOX_ACCESS_TOKEN</code> <code>style_id</code> <code>Optional[str]</code> <p>Mapbox style ID to use for image download</p> <code>None</code> <code>data_store</code> <code>Optional[DataStore]</code> <p>Instance of DataStore for accessing data storage</p> <code>None</code> Source code in <code>gigaspatial/handlers/mapbox_image.py</code> <pre><code>def __init__(\n    self,\n    access_token: str = config.MAPBOX_ACCESS_TOKEN,\n    style_id: Optional[str] = None,\n    data_store: Optional[DataStore] = None,\n):\n    \"\"\"\n    Initialize the downloader with Mapbox credentials\n\n    Args:\n        access_token: Mapbox access token\n        style_id: Mapbox style ID to use for image download\n        data_store: Instance of DataStore for accessing data storage\n    \"\"\"\n    self.access_token = access_token\n    self.style_id = style_id if style_id else \"mapbox/satellite-v9\"\n    self.data_store = data_store or LocalDataStore()\n    self.logger = config.get_logger(__name__)\n</code></pre>"},{"location":"api/handlers/#gigaspatial.handlers.mapbox_image.MapboxImageDownloader.download_images_by_bounds","title":"<code>download_images_by_bounds(gdf, output_dir, image_size=(512, 512), max_workers=4, image_prefix='image_')</code>","text":"<p>Download images for given points using the specified style</p> <p>Parameters:</p> Name Type Description Default <code>gdf_points</code> <p>GeoDataFrame containing bounding box polygons</p> required <code>output_dir</code> <code>Union[str, Path]</code> <p>Directory to save images</p> required <code>image_size</code> <code>Tuple[int, int]</code> <p>Tuple of (width, height) for output images</p> <code>(512, 512)</code> <code>max_workers</code> <code>int</code> <p>Maximum number of concurrent downloads</p> <code>4</code> <code>image_prefix</code> <code>str</code> <p>Prefix for output image names</p> <code>'image_'</code> Source code in <code>gigaspatial/handlers/mapbox_image.py</code> <pre><code>def download_images_by_bounds(\n    self,\n    gdf: gpd.GeoDataFrame,\n    output_dir: Union[str, Path],\n    image_size: Tuple[int, int] = (512, 512),\n    max_workers: int = 4,\n    image_prefix: str = \"image_\",\n) -&gt; None:\n    \"\"\"\n    Download images for given points using the specified style\n\n    Args:\n        gdf_points: GeoDataFrame containing bounding box polygons\n        output_dir: Directory to save images\n        image_size: Tuple of (width, height) for output images\n        max_workers: Maximum number of concurrent downloads\n        image_prefix: Prefix for output image names\n    \"\"\"\n    output_dir = Path(output_dir)\n    # self.data_store.makedirs(str(output_dir), exist_ok=True)\n\n    image_size_str = f\"{image_size[0]}x{image_size[1]}\"\n    total_images = len(gdf)\n\n    self.logger.info(\n        f\"Downloading {total_images} images with size {image_size_str}...\"\n    )\n\n    def download_image(idx: Any, bounds: Tuple[float, float, float, float]) -&gt; bool:\n        file_name = f\"{image_prefix}{idx}.png\"\n        url = self._construct_url(bounds, image_size_str)\n        success = self._download_single_image(url, output_dir / file_name)\n        return success\n\n    with ThreadPoolExecutor(max_workers=max_workers) as executor:\n        futures = [\n            executor.submit(download_image, row.Index, row.geometry.bounds)\n            for row in gdf.itertuples()\n        ]\n\n        successful_downloads = 0\n        with tqdm(total=total_images) as pbar:\n            for future in as_completed(futures):\n                if future.result():\n                    successful_downloads += 1\n                pbar.update(1)\n\n    self.logger.info(\n        f\"Successfully downloaded {successful_downloads}/{total_images} images!\"\n    )\n</code></pre>"},{"location":"api/handlers/#gigaspatial.handlers.mapbox_image.MapboxImageDownloader.download_images_by_coordinates","title":"<code>download_images_by_coordinates(data, res_meters_pixel, output_dir, image_size=(512, 512), max_workers=4, image_prefix='image_')</code>","text":"<p>Download images for given coordinates by creating bounded boxes around points</p> <p>Parameters:</p> Name Type Description Default <code>data</code> <code>Union[DataFrame, List[Tuple[float, float]]]</code> <p>Either a DataFrame with either latitude/longitude columns or a geometry column or a list of (lat, lon) tuples</p> required <code>res_meters_pixel</code> <code>float</code> <p>Size of the bounding box in meters (creates a square)</p> required <code>output_dir</code> <code>Union[str, Path]</code> <p>Directory to save images</p> required <code>image_size</code> <code>Tuple[int, int]</code> <p>Tuple of (width, height) for output images</p> <code>(512, 512)</code> <code>max_workers</code> <code>int</code> <p>Maximum number of concurrent downloads</p> <code>4</code> <code>image_prefix</code> <code>str</code> <p>Prefix for output image names</p> <code>'image_'</code> Source code in <code>gigaspatial/handlers/mapbox_image.py</code> <pre><code>def download_images_by_coordinates(\n    self,\n    data: Union[pd.DataFrame, List[Tuple[float, float]]],\n    res_meters_pixel: float,\n    output_dir: Union[str, Path],\n    image_size: Tuple[int, int] = (512, 512),\n    max_workers: int = 4,\n    image_prefix: str = \"image_\",\n) -&gt; None:\n    \"\"\"\n    Download images for given coordinates by creating bounded boxes around points\n\n    Args:\n        data: Either a DataFrame with either latitude/longitude columns or a geometry column or a list of (lat, lon) tuples\n        res_meters_pixel: Size of the bounding box in meters (creates a square)\n        output_dir: Directory to save images\n        image_size: Tuple of (width, height) for output images\n        max_workers: Maximum number of concurrent downloads\n        image_prefix: Prefix for output image names\n    \"\"\"\n\n    if isinstance(data, pd.DataFrame):\n        coordinates_df = data\n    else:\n        coordinates_df = pd.DataFrame(data, columns=[\"latitude\", \"longitude\"])\n\n    gdf = convert_to_geodataframe(coordinates_df)\n\n    buffered_gdf = buffer_geodataframe(\n        gdf, res_meters_pixel / 2, cap_style=\"square\"\n    )\n\n    self.download_images_by_bounds(\n        buffered_gdf, output_dir, image_size, max_workers, image_prefix\n    )\n</code></pre>"},{"location":"api/handlers/#gigaspatial.handlers.mapbox_image.MapboxImageDownloader.download_images_by_tiles","title":"<code>download_images_by_tiles(mercator_tiles, output_dir, image_size=(512, 512), max_workers=4, image_prefix='image_')</code>","text":"<p>Download images for given mercator tiles using the specified style</p> <p>Parameters:</p> Name Type Description Default <code>mercator_tiles</code> <code>MercatorTiles</code> <p>MercatorTiles instance containing quadkeys</p> required <code>output_dir</code> <code>Union[str, Path]</code> <p>Directory to save images</p> required <code>image_size</code> <code>Tuple[int, int]</code> <p>Tuple of (width, height) for output images</p> <code>(512, 512)</code> <code>max_workers</code> <code>int</code> <p>Maximum number of concurrent downloads</p> <code>4</code> <code>image_prefix</code> <code>str</code> <p>Prefix for output image names</p> <code>'image_'</code> Source code in <code>gigaspatial/handlers/mapbox_image.py</code> <pre><code>def download_images_by_tiles(\n    self,\n    mercator_tiles: \"MercatorTiles\",\n    output_dir: Union[str, Path],\n    image_size: Tuple[int, int] = (512, 512),\n    max_workers: int = 4,\n    image_prefix: str = \"image_\",\n) -&gt; None:\n    \"\"\"\n    Download images for given mercator tiles using the specified style\n\n    Args:\n        mercator_tiles: MercatorTiles instance containing quadkeys\n        output_dir: Directory to save images\n        image_size: Tuple of (width, height) for output images\n        max_workers: Maximum number of concurrent downloads\n        image_prefix: Prefix for output image names\n    \"\"\"\n    output_dir = Path(output_dir)\n    # self.data_store.makedirs(str(output_dir), exist_ok=True)\n\n    image_size_str = f\"{image_size[0]}x{image_size[1]}\"\n    total_tiles = len(mercator_tiles.quadkeys)\n\n    self.logger.info(\n        f\"Downloading {total_tiles} tiles with size {image_size_str}...\"\n    )\n\n    def _get_tile_bounds(quadkey: str) -&gt; List[float]:\n        \"\"\"Get tile bounds from quadkey\"\"\"\n        tile = mercantile.quadkey_to_tile(quadkey)\n        bounds = mercantile.bounds(tile)\n        return [bounds.west, bounds.south, bounds.east, bounds.north]\n\n    def download_image(quadkey: str) -&gt; bool:\n        bounds = _get_tile_bounds(quadkey)\n        file_name = f\"{image_prefix}{quadkey}.png\"\n\n        url = self._construct_url(bounds, image_size_str)\n        success = self._download_single_image(url, output_dir / file_name)\n\n        return success\n\n    with ThreadPoolExecutor(max_workers=max_workers) as executor:\n        futures = [\n            executor.submit(download_image, quadkey)\n            for quadkey in mercator_tiles.quadkeys\n        ]\n\n        successful_downloads = 0\n        with tqdm(total=total_tiles) as pbar:\n            for future in as_completed(futures):\n                if future.result():\n                    successful_downloads += 1\n                pbar.update(1)\n\n    self.logger.info(\n        f\"Successfully downloaded {successful_downloads}/{total_tiles} images!\"\n    )\n</code></pre>"},{"location":"api/handlers/#gigaspatial.handlers.maxar_image","title":"<code>maxar_image</code>","text":""},{"location":"api/handlers/#gigaspatial.handlers.maxar_image.MaxarConfig","title":"<code>MaxarConfig</code>","text":"<p>               Bases: <code>BaseModel</code></p> <p>Configuration for Maxar Image Downloader using Pydantic</p> Source code in <code>gigaspatial/handlers/maxar_image.py</code> <pre><code>class MaxarConfig(BaseModel):\n    \"\"\"Configuration for Maxar Image Downloader using Pydantic\"\"\"\n\n    username: str = Field(\n        default=global_config.MAXAR_USERNAME, description=\"Maxar API username\"\n    )\n    password: str = Field(\n        default=global_config.MAXAR_PASSWORD, description=\"Maxar API password\"\n    )\n    connection_string: str = Field(\n        default=global_config.MAXAR_CONNECTION_STRING,\n        description=\"Maxar WMS connection string\",\n    )\n\n    base_url: HttpUrl = Field(\n        default=\"https://evwhs.digitalglobe.com/mapservice/wmsaccess?\",\n        description=\"Base URL for Maxar WMS service\",\n    )\n\n    layers: List[Literal[\"DigitalGlobe:ImageryFootprint\", \"DigitalGlobe:Imagery\"]] = (\n        Field(\n            default=[\"DigitalGlobe:Imagery\"],\n            description=\"List of layers to request from WMS\",\n        )\n    )\n\n    feature_profile: str = Field(\n        default=\"Most_Aesthetic_Mosaic_Profile\",\n        description=\"Feature profile to use for WMS requests\",\n    )\n\n    coverage_cql_filter: str = Field(\n        default=\"\", description=\"CQL filter for coverage selection\"\n    )\n\n    exceptions: str = Field(\n        default=\"application/vnd.ogc.se_xml\",\n        description=\"Exception handling format for WMS\",\n    )\n\n    transparent: bool = Field(\n        default=True,\n        description=\"Whether the requested images should have transparency\",\n    )\n\n    image_format: Literal[\"image/png\", \"image/jpeg\", \"image/geotiff\"] = Field(\n        default=\"image/png\",\n    )\n\n    data_crs: Literal[\"EPSG:4326\", \"EPSG:3395\", \"EPSG:3857\", \"CAR:42004\"] = Field(\n        default=\"EPSG:4326\"\n    )\n\n    max_retries: int = Field(\n        default=3, description=\"Number of retries for failed image downloads\"\n    )\n\n    retry_delay: int = Field(default=5, description=\"Delay in seconds between retries\")\n\n    @field_validator(\"username\", \"password\", \"connection_string\")\n    @classmethod\n    def validate_non_empty(cls, value: str, field) -&gt; str:\n        \"\"\"Ensure required credentials are provided\"\"\"\n        if not value or value.strip() == \"\":\n            raise ValueError(\n                f\"{field.name} cannot be empty. Please provide a valid {field.name}.\"\n            )\n        return value\n\n    @property\n    def wms_url(self) -&gt; str:\n        \"\"\"Generate the full WMS URL with connection string\"\"\"\n        return f\"{self.base_url}connectid={self.connection_string}\"\n\n    @property\n    def suffix(self) -&gt; str:\n        return f\".{self.image_format.split('/')[1]}\"\n</code></pre>"},{"location":"api/handlers/#gigaspatial.handlers.maxar_image.MaxarConfig.wms_url","title":"<code>wms_url: str</code>  <code>property</code>","text":"<p>Generate the full WMS URL with connection string</p>"},{"location":"api/handlers/#gigaspatial.handlers.maxar_image.MaxarConfig.validate_non_empty","title":"<code>validate_non_empty(value, field)</code>  <code>classmethod</code>","text":"<p>Ensure required credentials are provided</p> Source code in <code>gigaspatial/handlers/maxar_image.py</code> <pre><code>@field_validator(\"username\", \"password\", \"connection_string\")\n@classmethod\ndef validate_non_empty(cls, value: str, field) -&gt; str:\n    \"\"\"Ensure required credentials are provided\"\"\"\n    if not value or value.strip() == \"\":\n        raise ValueError(\n            f\"{field.name} cannot be empty. Please provide a valid {field.name}.\"\n        )\n    return value\n</code></pre>"},{"location":"api/handlers/#gigaspatial.handlers.maxar_image.MaxarImageDownloader","title":"<code>MaxarImageDownloader</code>","text":"<p>Class to download images from Maxar</p> Source code in <code>gigaspatial/handlers/maxar_image.py</code> <pre><code>class MaxarImageDownloader:\n    \"\"\"Class to download images from Maxar\"\"\"\n\n    def __init__(\n        self,\n        config: Optional[MaxarConfig] = None,\n        data_store: Optional[DataStore] = None,\n    ):\n        \"\"\"\n        Initialize the downloader with Maxar config.\n\n        Args:\n            config: MaxarConfig instance containing credentials and settings\n            data_store: Instance of DataStore for accessing data storage\n        \"\"\"\n        self.config = config or MaxarConfig()\n        self.wms = WebMapService(\n            self.config.wms_url,\n            username=self.config.username,\n            password=self.config.password,\n        )\n        self.data_store = data_store or LocalDataStore()\n        self.logger = global_config.get_logger(__name__)\n\n    def _download_single_image(self, bbox, output_path: Union[Path, str], size) -&gt; bool:\n        \"\"\"Download a single image from bbox and pixel size\"\"\"\n        for attempt in range(self.config.max_retries):\n            try:\n                img_data = self.wms.getmap(\n                    bbox=bbox,\n                    layers=self.config.layers,\n                    srs=self.config.data_crs,\n                    size=size,\n                    featureProfile=self.config.feature_profile,\n                    coverage_cql_filter=self.config.coverage_cql_filter,\n                    exceptions=self.config.exceptions,\n                    transparent=self.config.transparent,\n                    format=self.config.image_format,\n                )\n                self.data_store.write_file(str(output_path), img_data.read())\n                return True\n            except Exception as e:\n                self.logger.warning(\n                    f\"Attempt {attempt + 1} of downloading {output_path.name} failed: {str(e)}\"\n                )\n                if attempt &lt; self.max_retries - 1:\n                    sleep(self.config.retry_delay)\n                else:\n                    self.logger.warning(\n                        f\"Failed to download {output_path.name} after {self.config.max_retries} attemps: {str(e)}\"\n                    )\n                    return False\n\n    def download_images_by_tiles(\n        self,\n        mercator_tiles: \"MercatorTiles\",\n        output_dir: Union[str, Path],\n        image_size: Tuple[int, int] = (512, 512),\n        image_prefix: str = \"maxar_image_\",\n    ) -&gt; None:\n        \"\"\"\n        Download images for given mercator tiles using the specified style\n\n        Args:\n            mercator_tiles: MercatorTiles instance containing quadkeys\n            output_dir: Directory to save images\n            image_size: Tuple of (width, height) for output images\n            image_prefix: Prefix for output image names\n        \"\"\"\n        output_dir = Path(output_dir)\n\n        image_size_str = f\"{image_size[0]}x{image_size[1]}\"\n        total_tiles = len(mercator_tiles.quadkeys)\n\n        self.logger.info(\n            f\"Downloading {total_tiles} tiles with size {image_size_str}...\"\n        )\n\n        def _get_tile_bounds(quadkey: str) -&gt; Tuple[float]:\n            \"\"\"Get tile bounds from quadkey\"\"\"\n            tile = mercantile.quadkey_to_tile(quadkey)\n            bounds = mercantile.bounds(tile)\n            return (bounds.west, bounds.south, bounds.east, bounds.north)\n\n        def download_image(\n            quadkey: str, image_size: Tuple[int, int], suffix: str = self.config.suffix\n        ) -&gt; bool:\n            bounds = _get_tile_bounds(quadkey)\n            file_name = f\"{image_prefix}{quadkey}{suffix}\"\n\n            success = self._download_single_image(\n                bounds, output_dir / file_name, image_size\n            )\n\n            return success\n\n        successful_downloads = 0\n        with tqdm(total=total_tiles) as pbar:\n            for quadkey in mercator_tiles.quadkeys:\n                if download_image(quadkey, image_size):\n                    successful_downloads += 1\n                pbar.update(1)\n\n        self.logger.info(\n            f\"Successfully downloaded {successful_downloads}/{total_tiles} images!\"\n        )\n\n    def download_images_by_bounds(\n        self,\n        gdf: gpd.GeoDataFrame,\n        output_dir: Union[str, Path],\n        image_size: Tuple[int, int] = (512, 512),\n        image_prefix: str = \"maxar_image_\",\n    ) -&gt; None:\n        \"\"\"\n        Download images for given points using the specified style\n\n        Args:\n            gdf_points: GeoDataFrame containing bounding box polygons\n            output_dir: Directory to save images\n            image_size: Tuple of (width, height) for output images\n            image_prefix: Prefix for output image names\n        \"\"\"\n        output_dir = Path(output_dir)\n\n        image_size_str = f\"{image_size[0]}x{image_size[1]}\"\n        total_images = len(gdf)\n\n        self.logger.info(\n            f\"Downloading {total_images} images with size {image_size_str}...\"\n        )\n\n        def download_image(\n            idx: Any,\n            bounds: Tuple[float, float, float, float],\n            image_size,\n            suffix: str = self.config.suffix,\n        ) -&gt; bool:\n            file_name = f\"{image_prefix}{idx}{suffix}\"\n            success = self._download_single_image(\n                bounds, output_dir / file_name, image_size\n            )\n            return success\n\n        gdf = gdf.to_crs(self.config.data_crs)\n\n        successful_downloads = 0\n        with tqdm(total=total_images) as pbar:\n            for row in gdf.itertuples():\n                if download_image(row.Index, tuple(row.geometry.bounds), image_size):\n                    successful_downloads += 1\n                pbar.update(1)\n\n        self.logger.info(\n            f\"Successfully downloaded {successful_downloads}/{total_images} images!\"\n        )\n\n    def download_images_by_coordinates(\n        self,\n        data: Union[pd.DataFrame, List[Tuple[float, float]]],\n        res_meters_pixel: float,\n        output_dir: Union[str, Path],\n        image_size: Tuple[int, int] = (512, 512),\n        image_prefix: str = \"maxar_image_\",\n    ) -&gt; None:\n        \"\"\"\n        Download images for given coordinates by creating bounded boxes around points\n\n        Args:\n            data: Either a DataFrame with either latitude/longitude columns or a geometry column or a list of (lat, lon) tuples\n            res_meters_pixel: resolution in meters per pixel\n            output_dir: Directory to save images\n            image_size: Tuple of (width, height) for output images\n            image_prefix: Prefix for output image names\n        \"\"\"\n\n        if isinstance(data, pd.DataFrame):\n            coordinates_df = data\n        else:\n            coordinates_df = pd.DataFrame(data, columns=[\"latitude\", \"longitude\"])\n\n        gdf = convert_to_geodataframe(coordinates_df)\n\n        buffered_gdf = buffer_geodataframe(\n            gdf, res_meters_pixel / 2, cap_style=\"square\"\n        )\n\n        buffered_gdf = buffered_gdf.to_crs(self.config.data_crs)\n\n        self.download_images_by_bounds(\n            buffered_gdf, output_dir, image_size, image_prefix\n        )\n</code></pre>"},{"location":"api/handlers/#gigaspatial.handlers.maxar_image.MaxarImageDownloader.__init__","title":"<code>__init__(config=None, data_store=None)</code>","text":"<p>Initialize the downloader with Maxar config.</p> <p>Parameters:</p> Name Type Description Default <code>config</code> <code>Optional[MaxarConfig]</code> <p>MaxarConfig instance containing credentials and settings</p> <code>None</code> <code>data_store</code> <code>Optional[DataStore]</code> <p>Instance of DataStore for accessing data storage</p> <code>None</code> Source code in <code>gigaspatial/handlers/maxar_image.py</code> <pre><code>def __init__(\n    self,\n    config: Optional[MaxarConfig] = None,\n    data_store: Optional[DataStore] = None,\n):\n    \"\"\"\n    Initialize the downloader with Maxar config.\n\n    Args:\n        config: MaxarConfig instance containing credentials and settings\n        data_store: Instance of DataStore for accessing data storage\n    \"\"\"\n    self.config = config or MaxarConfig()\n    self.wms = WebMapService(\n        self.config.wms_url,\n        username=self.config.username,\n        password=self.config.password,\n    )\n    self.data_store = data_store or LocalDataStore()\n    self.logger = global_config.get_logger(__name__)\n</code></pre>"},{"location":"api/handlers/#gigaspatial.handlers.maxar_image.MaxarImageDownloader.download_images_by_bounds","title":"<code>download_images_by_bounds(gdf, output_dir, image_size=(512, 512), image_prefix='maxar_image_')</code>","text":"<p>Download images for given points using the specified style</p> <p>Parameters:</p> Name Type Description Default <code>gdf_points</code> <p>GeoDataFrame containing bounding box polygons</p> required <code>output_dir</code> <code>Union[str, Path]</code> <p>Directory to save images</p> required <code>image_size</code> <code>Tuple[int, int]</code> <p>Tuple of (width, height) for output images</p> <code>(512, 512)</code> <code>image_prefix</code> <code>str</code> <p>Prefix for output image names</p> <code>'maxar_image_'</code> Source code in <code>gigaspatial/handlers/maxar_image.py</code> <pre><code>def download_images_by_bounds(\n    self,\n    gdf: gpd.GeoDataFrame,\n    output_dir: Union[str, Path],\n    image_size: Tuple[int, int] = (512, 512),\n    image_prefix: str = \"maxar_image_\",\n) -&gt; None:\n    \"\"\"\n    Download images for given points using the specified style\n\n    Args:\n        gdf_points: GeoDataFrame containing bounding box polygons\n        output_dir: Directory to save images\n        image_size: Tuple of (width, height) for output images\n        image_prefix: Prefix for output image names\n    \"\"\"\n    output_dir = Path(output_dir)\n\n    image_size_str = f\"{image_size[0]}x{image_size[1]}\"\n    total_images = len(gdf)\n\n    self.logger.info(\n        f\"Downloading {total_images} images with size {image_size_str}...\"\n    )\n\n    def download_image(\n        idx: Any,\n        bounds: Tuple[float, float, float, float],\n        image_size,\n        suffix: str = self.config.suffix,\n    ) -&gt; bool:\n        file_name = f\"{image_prefix}{idx}{suffix}\"\n        success = self._download_single_image(\n            bounds, output_dir / file_name, image_size\n        )\n        return success\n\n    gdf = gdf.to_crs(self.config.data_crs)\n\n    successful_downloads = 0\n    with tqdm(total=total_images) as pbar:\n        for row in gdf.itertuples():\n            if download_image(row.Index, tuple(row.geometry.bounds), image_size):\n                successful_downloads += 1\n            pbar.update(1)\n\n    self.logger.info(\n        f\"Successfully downloaded {successful_downloads}/{total_images} images!\"\n    )\n</code></pre>"},{"location":"api/handlers/#gigaspatial.handlers.maxar_image.MaxarImageDownloader.download_images_by_coordinates","title":"<code>download_images_by_coordinates(data, res_meters_pixel, output_dir, image_size=(512, 512), image_prefix='maxar_image_')</code>","text":"<p>Download images for given coordinates by creating bounded boxes around points</p> <p>Parameters:</p> Name Type Description Default <code>data</code> <code>Union[DataFrame, List[Tuple[float, float]]]</code> <p>Either a DataFrame with either latitude/longitude columns or a geometry column or a list of (lat, lon) tuples</p> required <code>res_meters_pixel</code> <code>float</code> <p>resolution in meters per pixel</p> required <code>output_dir</code> <code>Union[str, Path]</code> <p>Directory to save images</p> required <code>image_size</code> <code>Tuple[int, int]</code> <p>Tuple of (width, height) for output images</p> <code>(512, 512)</code> <code>image_prefix</code> <code>str</code> <p>Prefix for output image names</p> <code>'maxar_image_'</code> Source code in <code>gigaspatial/handlers/maxar_image.py</code> <pre><code>def download_images_by_coordinates(\n    self,\n    data: Union[pd.DataFrame, List[Tuple[float, float]]],\n    res_meters_pixel: float,\n    output_dir: Union[str, Path],\n    image_size: Tuple[int, int] = (512, 512),\n    image_prefix: str = \"maxar_image_\",\n) -&gt; None:\n    \"\"\"\n    Download images for given coordinates by creating bounded boxes around points\n\n    Args:\n        data: Either a DataFrame with either latitude/longitude columns or a geometry column or a list of (lat, lon) tuples\n        res_meters_pixel: resolution in meters per pixel\n        output_dir: Directory to save images\n        image_size: Tuple of (width, height) for output images\n        image_prefix: Prefix for output image names\n    \"\"\"\n\n    if isinstance(data, pd.DataFrame):\n        coordinates_df = data\n    else:\n        coordinates_df = pd.DataFrame(data, columns=[\"latitude\", \"longitude\"])\n\n    gdf = convert_to_geodataframe(coordinates_df)\n\n    buffered_gdf = buffer_geodataframe(\n        gdf, res_meters_pixel / 2, cap_style=\"square\"\n    )\n\n    buffered_gdf = buffered_gdf.to_crs(self.config.data_crs)\n\n    self.download_images_by_bounds(\n        buffered_gdf, output_dir, image_size, image_prefix\n    )\n</code></pre>"},{"location":"api/handlers/#gigaspatial.handlers.maxar_image.MaxarImageDownloader.download_images_by_tiles","title":"<code>download_images_by_tiles(mercator_tiles, output_dir, image_size=(512, 512), image_prefix='maxar_image_')</code>","text":"<p>Download images for given mercator tiles using the specified style</p> <p>Parameters:</p> Name Type Description Default <code>mercator_tiles</code> <code>MercatorTiles</code> <p>MercatorTiles instance containing quadkeys</p> required <code>output_dir</code> <code>Union[str, Path]</code> <p>Directory to save images</p> required <code>image_size</code> <code>Tuple[int, int]</code> <p>Tuple of (width, height) for output images</p> <code>(512, 512)</code> <code>image_prefix</code> <code>str</code> <p>Prefix for output image names</p> <code>'maxar_image_'</code> Source code in <code>gigaspatial/handlers/maxar_image.py</code> <pre><code>def download_images_by_tiles(\n    self,\n    mercator_tiles: \"MercatorTiles\",\n    output_dir: Union[str, Path],\n    image_size: Tuple[int, int] = (512, 512),\n    image_prefix: str = \"maxar_image_\",\n) -&gt; None:\n    \"\"\"\n    Download images for given mercator tiles using the specified style\n\n    Args:\n        mercator_tiles: MercatorTiles instance containing quadkeys\n        output_dir: Directory to save images\n        image_size: Tuple of (width, height) for output images\n        image_prefix: Prefix for output image names\n    \"\"\"\n    output_dir = Path(output_dir)\n\n    image_size_str = f\"{image_size[0]}x{image_size[1]}\"\n    total_tiles = len(mercator_tiles.quadkeys)\n\n    self.logger.info(\n        f\"Downloading {total_tiles} tiles with size {image_size_str}...\"\n    )\n\n    def _get_tile_bounds(quadkey: str) -&gt; Tuple[float]:\n        \"\"\"Get tile bounds from quadkey\"\"\"\n        tile = mercantile.quadkey_to_tile(quadkey)\n        bounds = mercantile.bounds(tile)\n        return (bounds.west, bounds.south, bounds.east, bounds.north)\n\n    def download_image(\n        quadkey: str, image_size: Tuple[int, int], suffix: str = self.config.suffix\n    ) -&gt; bool:\n        bounds = _get_tile_bounds(quadkey)\n        file_name = f\"{image_prefix}{quadkey}{suffix}\"\n\n        success = self._download_single_image(\n            bounds, output_dir / file_name, image_size\n        )\n\n        return success\n\n    successful_downloads = 0\n    with tqdm(total=total_tiles) as pbar:\n        for quadkey in mercator_tiles.quadkeys:\n            if download_image(quadkey, image_size):\n                successful_downloads += 1\n            pbar.update(1)\n\n    self.logger.info(\n        f\"Successfully downloaded {successful_downloads}/{total_tiles} images!\"\n    )\n</code></pre>"},{"location":"api/handlers/#gigaspatial.handlers.microsoft_global_buildings","title":"<code>microsoft_global_buildings</code>","text":""},{"location":"api/handlers/#gigaspatial.handlers.microsoft_global_buildings.MSBuildingsConfig","title":"<code>MSBuildingsConfig</code>","text":"<p>Configuration for Microsoft Global Buildings dataset files.</p> Source code in <code>gigaspatial/handlers/microsoft_global_buildings.py</code> <pre><code>@dataclass(config=ConfigDict(arbitrary_types_allowed=True))\nclass MSBuildingsConfig:\n    \"\"\"Configuration for Microsoft Global Buildings dataset files.\"\"\"\n\n    data_store: DataStore = field(\n        default_factory=LocalDataStore\n    )  # instance of DataStore for accessing data storage\n    BASE_PATH: Path = global_config.get_path(\"microsoft_global_buildings\", \"bronze\")\n\n    TILE_URLS: str = (\n        \"https://minedbuildings.z5.web.core.windows.net/global-buildings/dataset-links.csv\"\n    )\n    MERCATOR_ZOOM_LEVEL: int = 9\n\n    LOCATION_MAPPING_FILE: Path = BASE_PATH / \"location_mapping.json\"\n    SIMILARITY_SCORE: float = 0.8\n    DEFAULT_MAPPING: Dict[str, str] = field(\n        default_factory=lambda: {\n            \"Bonaire\": \"BES\",\n            \"Brunei\": \"BRN\",\n            \"IvoryCoast\": \"CIV\",\n            \"CongoDRC\": \"COD\",\n            \"DemocraticRepublicoftheCongo\": \"COD\",\n            \"RepublicoftheCongo\": \"COG\",\n            \"TheGambia\": \"GMB\",\n            \"FYROMakedonija\": \"MKD\",\n            \"SultanateofOman\": \"OMN\",\n            \"StateofQatar\": \"QAT\",\n            \"Russia\": \"RUS\",\n            \"KingdomofSaudiArabia\": \"SAU\",\n            \"Svalbard\": \"SJM\",\n            \"Swaziland\": \"SWZ\",\n            \"StMartin\": \"SXM\",\n            \"leSaint-Martin\": \"MAF\",\n            \"Turkey\": \"TUR\",\n            \"VaticanCity\": \"VAT\",\n            \"BritishVirginIslands\": \"VGB\",\n            \"USVirginIslands\": \"VIR\",\n            \"RepublicofYemen\": \"YEM\",\n            \"CzechRepublic\": \"CZE\",\n            \"French-Martinique\": \"MTQ\",\n            \"French-Guadeloupe\": \"GLP\",\n            \"UnitedStates\": \"USA\",\n        }\n    )\n    CUSTOM_MAPPING: Optional[Dict[str, str]] = None\n\n    n_workers: int = 4  # number of workers for parallel processing\n    logger: logging.Logger = global_config.get_logger(__name__)\n\n    def __post_init__(self):\n        \"\"\"Validate inputs and set location mapping\"\"\"\n\n        self._load_tile_urls()\n\n        self.upload_date = self.df_tiles.upload_date[0]\n\n        if self.data_store.file_exists(str(self.LOCATION_MAPPING_FILE)):\n            self.location_mapping = read_json(\n                self.data_store, str(self.LOCATION_MAPPING_FILE)\n            )\n        else:\n            self.location_mapping = self.create_location_mapping(\n                similarity_score_threshold=self.SIMILARITY_SCORE\n            )\n            self.location_mapping.update(self.DEFAULT_MAPPING)\n            write_json(\n                self.location_mapping, self.data_store, str(self.LOCATION_MAPPING_FILE)\n            )\n\n        self.location_mapping.update(self.CUSTOM_MAPPING or {})\n\n        self._map_locations()\n\n        self.df_tiles.loc[self.df_tiles.country.isnull(), \"country\"] = None\n\n    def _load_tile_urls(self):\n        \"\"\"Load dataset links from csv file.\"\"\"\n        self.df_tiles = pd.read_csv(\n            self.TILE_URLS,\n            names=[\"location\", \"quadkey\", \"url\", \"size\", \"upload_date\"],\n            dtype={\"quadkey\": str},\n            header=0,\n        )\n\n    def _map_locations(self):\n        self.df_tiles[\"country\"] = self.df_tiles.location.map(self.location_mapping)\n\n    def create_location_mapping(self, similarity_score_threshold: float = 0.8):\n\n        def similar(a, b):\n            return SequenceMatcher(None, a, b).ratio()\n\n        location_mapping = dict()\n\n        for country in pycountry.countries:\n            if country.name not in self.df_tiles.location.unique():\n                try:\n                    country_quadkey = CountryMercatorTiles.create(\n                        country.alpha_3, self.MERCATOR_ZOOM_LEVEL\n                    )\n                except:\n                    self.logger.warning(f\"{country.name} is not mapped.\")\n                    continue\n                country_datasets = country_quadkey.filter_quadkeys(\n                    self.df_tiles.quadkey\n                )\n                matching_locations = self.df_tiles[\n                    self.df_tiles.quadkey.isin(country_datasets.quadkeys)\n                ].location.unique()\n                scores = np.array(\n                    [\n                        (\n                            similar(c, country.common_name)\n                            if hasattr(country, \"common_name\")\n                            else similar(c, country.name)\n                        )\n                        for c in matching_locations\n                    ]\n                )\n                if any(scores &gt; similarity_score_threshold):\n                    matched = matching_locations[scores &gt; similarity_score_threshold]\n                    if len(matched) &gt; 2:\n                        self.logger.warning(\n                            f\"Multiple matches exist for {country.name}. {country.name} is not mapped.\"\n                        )\n                    location_mapping[matched[0]] = country.alpha_3\n                    self.logger.debug(f\"{country.name} matched with {matched[0]}!\")\n                else:\n                    self.logger.warning(\n                        f\"No direct matches for {country.name}. {country.name} is not mapped.\"\n                    )\n                    self.logger.debug(\"Possible matches are: \")\n                    for c, score in zip(matching_locations, scores):\n                        self.logger.debug(c, score)\n            else:\n                location_mapping[country.name] = country.alpha_3\n\n        return location_mapping\n\n    def get_tiles_for_country(self, country_code: str) -&gt; pd.DataFrame:\n        country_tiles = self.df_tiles[self.df_tiles[\"country\"] == country_code]\n\n        if not country_tiles.empty:\n            return country_tiles\n\n        self.logger.warning(\n            f\"The country is not in location mapping. Manually checking if there are overlapping locations with country boundary.\"\n        )\n\n        country_mercator = CountryMercatorTiles.create(\n            country_code, self.MERCATOR_ZOOM_LEVEL\n        )\n        country_tiles = country_mercator.filter_quadkeys(self.df_tiles.quadkey)\n\n        if country_tiles:\n            filtered_tiles = self.df_tiles[\n                self.df_tiles.country.isnull()\n                &amp; self.df_tiles.quadkey.isin(country_tiles.quadkeys)\n            ]\n            return filtered_tiles\n\n        return pd.DataFrame(columns=self.df_tiles.columns)\n\n    def get_tiles_for_geometry(\n        self, geometry: Union[Polygon, MultiPolygon]\n    ) -&gt; pd.DataFrame:\n        if isinstance(geometry, MultiPolygon):\n            geom_mercator = MercatorTiles.from_multipolygon(\n                geometry, self.MERCATOR_ZOOM_LEVEL\n            )\n        elif isinstance(geometry, Polygon):\n            geom_mercator = MercatorTiles.from_polygon(\n                geometry, self.MERCATOR_ZOOM_LEVEL\n            )\n\n        geom_tiles = geom_mercator.filter_quadkeys(self.df_tiles.quadkey)\n\n        if geom_tiles:\n            return self.df_tiles[self.df_tiles.quadkey.isin(geom_tiles.quadkeys)]\n\n        return pd.DataFrame(columns=self.df_tiles.columns)\n\n    def get_tiles_for_points(\n        self, points: List[Union[Point, Tuple[float, float]]]\n    ) -&gt; pd.DataFrame:\n\n        points_mercator = MercatorTiles.from_points(points, self.MERCATOR_ZOOM_LEVEL)\n\n        points_tiles = points_mercator.filter_quadkeys(self.df_tiles.quadkey)\n\n        if points_tiles:\n            return self.df_tiles[self.df_tiles.quadkey.isin(points_tiles.quadkeys)]\n\n        return pd.DataFrame(columns=self.df_tiles.columns)\n\n    def get_tile_path(self, quadkey: str, location: str) -&gt; Path:\n        return self.BASE_PATH / location / self.upload_date / f\"{quadkey}.csv.gz\"\n</code></pre>"},{"location":"api/handlers/#gigaspatial.handlers.microsoft_global_buildings.MSBuildingsConfig.__post_init__","title":"<code>__post_init__()</code>","text":"<p>Validate inputs and set location mapping</p> Source code in <code>gigaspatial/handlers/microsoft_global_buildings.py</code> <pre><code>def __post_init__(self):\n    \"\"\"Validate inputs and set location mapping\"\"\"\n\n    self._load_tile_urls()\n\n    self.upload_date = self.df_tiles.upload_date[0]\n\n    if self.data_store.file_exists(str(self.LOCATION_MAPPING_FILE)):\n        self.location_mapping = read_json(\n            self.data_store, str(self.LOCATION_MAPPING_FILE)\n        )\n    else:\n        self.location_mapping = self.create_location_mapping(\n            similarity_score_threshold=self.SIMILARITY_SCORE\n        )\n        self.location_mapping.update(self.DEFAULT_MAPPING)\n        write_json(\n            self.location_mapping, self.data_store, str(self.LOCATION_MAPPING_FILE)\n        )\n\n    self.location_mapping.update(self.CUSTOM_MAPPING or {})\n\n    self._map_locations()\n\n    self.df_tiles.loc[self.df_tiles.country.isnull(), \"country\"] = None\n</code></pre>"},{"location":"api/handlers/#gigaspatial.handlers.microsoft_global_buildings.MSBuildingsDownloader","title":"<code>MSBuildingsDownloader</code>","text":"<p>A class to handle downloads of Microsoft's Global ML Building Footprints dataset.</p> Source code in <code>gigaspatial/handlers/microsoft_global_buildings.py</code> <pre><code>class MSBuildingsDownloader:\n    \"\"\"A class to handle downloads of Microsoft's Global ML Building Footprints dataset.\"\"\"\n\n    def __init__(\n        self,\n        config: Optional[MSBuildingsConfig] = None,\n        data_store: Optional[DataStore] = None,\n        logger: Optional[logging.Logger] = None,\n    ):\n        \"\"\"\n        Initialize the downloader.\n\n        Args:\n            config: Optional configuration for customizing file paths.\n            logger: Optional custom logger. If not provided, uses default logger.\n        \"\"\"\n        self.logger = logger or global_config.get_logger(__name__)\n        self.data_store = data_store or LocalDataStore()\n        self.config = config or MSBuildingsConfig(\n            data_store=self.data_store, logger=self.logger\n        )\n\n    def _download_tile(\n        self,\n        tile_info: Union[pd.Series, dict],\n    ) -&gt; Optional[str]:\n        \"\"\"Download data file for a single tile.\"\"\"\n\n        # Modify URL based on data type if needed\n        tile_url = tile_info[\"url\"]\n\n        try:\n            response = requests.get(tile_url, stream=True)\n            response.raise_for_status()\n\n            file_path = str(\n                self.config.get_tile_path(\n                    quadkey=tile_info[\"quadkey\"],\n                    location=(\n                        tile_info[\"country\"]\n                        if tile_info[\"country\"]\n                        else tile_info[\"location\"]\n                    ),\n                )\n            )\n\n            with self.config.data_store.open(file_path, \"wb\") as file:\n                for chunk in response.iter_content(chunk_size=8192):\n                    file.write(chunk)\n\n                self.logger.debug(\n                    f\"Successfully downloaded tile: {tile_info['quadkey']}\"\n                )\n                return file_path\n\n        except requests.exceptions.RequestException as e:\n            self.logger.error(\n                f\"Failed to download tile {tile_info['quadkey']}: {str(e)}\"\n            )\n            return None\n\n    def download_by_country(self, country_code: str) -&gt; List[str]:\n        \"\"\"\n        Download Microsoft Global ML Building Footprints data for a specific country.\n\n        Args:\n            country_code: ISO 3166-1 alpha-3 country code\n\n        Returns:\n            List of paths to downloaded files\n        \"\"\"\n\n        country_tiles = self.config.get_tiles_for_country(country_code)\n\n        if not country_tiles.empty:\n\n            with multiprocessing.Pool(self.config.n_workers) as pool:\n                download_func = functools.partial(self._download_tile)\n                file_paths = list(\n                    tqdm(\n                        pool.imap(\n                            download_func, [row for _, row in country_tiles.iterrows()]\n                        ),\n                        total=len(country_tiles),\n                        desc=f\"Downloading tiles for {country_code}\",\n                    )\n                )\n\n            return [path for path in file_paths if path is not None]\n\n        return []\n\n    def download_by_geometry(self, geometry: Union[Polygon, MultiPolygon]) -&gt; List[str]:\n        \"\"\"\n        Download Microsoft Global ML Building Footprints data for a specific geometry.\n\n        Args:\n            geometry: Polygon or MultiPolygon geometry\n\n        Returns:\n            List of paths to downloaded files\n        \"\"\"\n\n        geom_tiles = self.config.get_tiles_for_geometry(geometry)\n\n        if not geom_tiles.empty:\n\n            with multiprocessing.Pool(self.config.n_workers) as pool:\n                download_func = functools.partial(self._download_tile)\n                file_paths = list(\n                    tqdm(\n                        pool.imap(\n                            download_func, [row for _, row in geom_tiles.iterrows()]\n                        ),\n                        total=len(geom_tiles),\n                        desc=\"Downloading tiles for the geometry\",\n                    )\n                )\n\n            return [path for path in file_paths if path is not None]\n\n        return []\n\n    def download_by_points(\n        self,\n        points: List[Union[Point, Tuple[float, float]]],\n    ) -&gt; List[str]:\n        \"\"\"\n        Download  Microsoft Global ML Building Footprints data for areas containing specific points.\n\n        Args:\n            points_gdf: GeoDataFrame containing points of interest\n\n        Returns:\n            List of paths to downloaded files\n        \"\"\"\n\n        points_tiles = self.config.get_tiles_for_points(points)\n\n        if not points_tiles.empty:\n\n            with multiprocessing.Pool(self.config.n_workers) as pool:\n                download_func = functools.partial(self._download_tile)\n                file_paths = list(\n                    tqdm(\n                        pool.imap(\n                            download_func, [row for _, row in points_tiles.iterrows()]\n                        ),\n                        total=len(points_tiles),\n                        desc=f\"Downloading tiles for the points\",\n                    )\n                )\n\n            return [path for path in file_paths if path is not None]\n\n        return []\n</code></pre>"},{"location":"api/handlers/#gigaspatial.handlers.microsoft_global_buildings.MSBuildingsDownloader.__init__","title":"<code>__init__(config=None, data_store=None, logger=None)</code>","text":"<p>Initialize the downloader.</p> <p>Parameters:</p> Name Type Description Default <code>config</code> <code>Optional[MSBuildingsConfig]</code> <p>Optional configuration for customizing file paths.</p> <code>None</code> <code>logger</code> <code>Optional[Logger]</code> <p>Optional custom logger. If not provided, uses default logger.</p> <code>None</code> Source code in <code>gigaspatial/handlers/microsoft_global_buildings.py</code> <pre><code>def __init__(\n    self,\n    config: Optional[MSBuildingsConfig] = None,\n    data_store: Optional[DataStore] = None,\n    logger: Optional[logging.Logger] = None,\n):\n    \"\"\"\n    Initialize the downloader.\n\n    Args:\n        config: Optional configuration for customizing file paths.\n        logger: Optional custom logger. If not provided, uses default logger.\n    \"\"\"\n    self.logger = logger or global_config.get_logger(__name__)\n    self.data_store = data_store or LocalDataStore()\n    self.config = config or MSBuildingsConfig(\n        data_store=self.data_store, logger=self.logger\n    )\n</code></pre>"},{"location":"api/handlers/#gigaspatial.handlers.microsoft_global_buildings.MSBuildingsDownloader.download_by_country","title":"<code>download_by_country(country_code)</code>","text":"<p>Download Microsoft Global ML Building Footprints data for a specific country.</p> <p>Parameters:</p> Name Type Description Default <code>country_code</code> <code>str</code> <p>ISO 3166-1 alpha-3 country code</p> required <p>Returns:</p> Type Description <code>List[str]</code> <p>List of paths to downloaded files</p> Source code in <code>gigaspatial/handlers/microsoft_global_buildings.py</code> <pre><code>def download_by_country(self, country_code: str) -&gt; List[str]:\n    \"\"\"\n    Download Microsoft Global ML Building Footprints data for a specific country.\n\n    Args:\n        country_code: ISO 3166-1 alpha-3 country code\n\n    Returns:\n        List of paths to downloaded files\n    \"\"\"\n\n    country_tiles = self.config.get_tiles_for_country(country_code)\n\n    if not country_tiles.empty:\n\n        with multiprocessing.Pool(self.config.n_workers) as pool:\n            download_func = functools.partial(self._download_tile)\n            file_paths = list(\n                tqdm(\n                    pool.imap(\n                        download_func, [row for _, row in country_tiles.iterrows()]\n                    ),\n                    total=len(country_tiles),\n                    desc=f\"Downloading tiles for {country_code}\",\n                )\n            )\n\n        return [path for path in file_paths if path is not None]\n\n    return []\n</code></pre>"},{"location":"api/handlers/#gigaspatial.handlers.microsoft_global_buildings.MSBuildingsDownloader.download_by_geometry","title":"<code>download_by_geometry(geometry)</code>","text":"<p>Download Microsoft Global ML Building Footprints data for a specific geometry.</p> <p>Parameters:</p> Name Type Description Default <code>geometry</code> <code>Union[Polygon, MultiPolygon]</code> <p>Polygon or MultiPolygon geometry</p> required <p>Returns:</p> Type Description <code>List[str]</code> <p>List of paths to downloaded files</p> Source code in <code>gigaspatial/handlers/microsoft_global_buildings.py</code> <pre><code>def download_by_geometry(self, geometry: Union[Polygon, MultiPolygon]) -&gt; List[str]:\n    \"\"\"\n    Download Microsoft Global ML Building Footprints data for a specific geometry.\n\n    Args:\n        geometry: Polygon or MultiPolygon geometry\n\n    Returns:\n        List of paths to downloaded files\n    \"\"\"\n\n    geom_tiles = self.config.get_tiles_for_geometry(geometry)\n\n    if not geom_tiles.empty:\n\n        with multiprocessing.Pool(self.config.n_workers) as pool:\n            download_func = functools.partial(self._download_tile)\n            file_paths = list(\n                tqdm(\n                    pool.imap(\n                        download_func, [row for _, row in geom_tiles.iterrows()]\n                    ),\n                    total=len(geom_tiles),\n                    desc=\"Downloading tiles for the geometry\",\n                )\n            )\n\n        return [path for path in file_paths if path is not None]\n\n    return []\n</code></pre>"},{"location":"api/handlers/#gigaspatial.handlers.microsoft_global_buildings.MSBuildingsDownloader.download_by_points","title":"<code>download_by_points(points)</code>","text":"<p>Download  Microsoft Global ML Building Footprints data for areas containing specific points.</p> <p>Parameters:</p> Name Type Description Default <code>points_gdf</code> <p>GeoDataFrame containing points of interest</p> required <p>Returns:</p> Type Description <code>List[str]</code> <p>List of paths to downloaded files</p> Source code in <code>gigaspatial/handlers/microsoft_global_buildings.py</code> <pre><code>def download_by_points(\n    self,\n    points: List[Union[Point, Tuple[float, float]]],\n) -&gt; List[str]:\n    \"\"\"\n    Download  Microsoft Global ML Building Footprints data for areas containing specific points.\n\n    Args:\n        points_gdf: GeoDataFrame containing points of interest\n\n    Returns:\n        List of paths to downloaded files\n    \"\"\"\n\n    points_tiles = self.config.get_tiles_for_points(points)\n\n    if not points_tiles.empty:\n\n        with multiprocessing.Pool(self.config.n_workers) as pool:\n            download_func = functools.partial(self._download_tile)\n            file_paths = list(\n                tqdm(\n                    pool.imap(\n                        download_func, [row for _, row in points_tiles.iterrows()]\n                    ),\n                    total=len(points_tiles),\n                    desc=f\"Downloading tiles for the points\",\n                )\n            )\n\n        return [path for path in file_paths if path is not None]\n\n    return []\n</code></pre>"},{"location":"api/handlers/#gigaspatial.handlers.microsoft_global_buildings.read_dataset","title":"<code>read_dataset(data_store, path, compression=None, **kwargs)</code>","text":"<p>Read data from various file formats stored in both local and cloud-based storage.</p>"},{"location":"api/handlers/#gigaspatial.handlers.microsoft_global_buildings.read_dataset--parameters","title":"Parameters:","text":"<p>data_store : DataStore     Instance of DataStore for accessing data storage. path : str, Path     Path to the file in data storage. **kwargs : dict     Additional arguments passed to the specific reader function.</p>"},{"location":"api/handlers/#gigaspatial.handlers.microsoft_global_buildings.read_dataset--returns","title":"Returns:","text":"<p>pandas.DataFrame or geopandas.GeoDataFrame     The data read from the file.</p>"},{"location":"api/handlers/#gigaspatial.handlers.microsoft_global_buildings.read_dataset--raises","title":"Raises:","text":"<p>FileNotFoundError     If the file doesn't exist in blob storage. ValueError     If the file type is unsupported or if there's an error reading the file.</p> Source code in <code>gigaspatial/core/io/readers.py</code> <pre><code>def read_dataset(data_store: DataStore, path: str, compression: str = None, **kwargs):\n    \"\"\"\n    Read data from various file formats stored in both local and cloud-based storage.\n\n    Parameters:\n    ----------\n    data_store : DataStore\n        Instance of DataStore for accessing data storage.\n    path : str, Path\n        Path to the file in data storage.\n    **kwargs : dict\n        Additional arguments passed to the specific reader function.\n\n    Returns:\n    -------\n    pandas.DataFrame or geopandas.GeoDataFrame\n        The data read from the file.\n\n    Raises:\n    ------\n    FileNotFoundError\n        If the file doesn't exist in blob storage.\n    ValueError\n        If the file type is unsupported or if there's an error reading the file.\n    \"\"\"\n\n    # Define supported file formats and their readers\n    BINARY_FORMATS = {\n        \".shp\",\n        \".zip\",\n        \".parquet\",\n        \".gpkg\",\n        \".xlsx\",\n        \".xls\",\n        \".kmz\",\n        \".gz\",\n    }\n\n    PANDAS_READERS = {\n        \".csv\": pd.read_csv,\n        \".xlsx\": lambda f, **kw: pd.read_excel(f, engine=\"openpyxl\", **kw),\n        \".xls\": lambda f, **kw: pd.read_excel(f, engine=\"xlrd\", **kw),\n        \".json\": pd.read_json,\n        # \".gz\": lambda f, **kw: pd.read_csv(f, compression=\"gzip\", **kw),\n    }\n\n    GEO_READERS = {\n        \".shp\": gpd.read_file,\n        \".zip\": gpd.read_file,\n        \".geojson\": gpd.read_file,\n        \".gpkg\": gpd.read_file,\n        \".parquet\": gpd.read_parquet,\n        \".kmz\": read_kmz,\n    }\n\n    COMPRESSION_FORMATS = {\n        \".gz\": \"gzip\",\n        \".bz2\": \"bz2\",\n        \".zip\": \"zip\",\n        \".xz\": \"xz\",\n    }\n\n    try:\n        # Check if file exists\n        if not data_store.file_exists(path):\n            raise FileNotFoundError(f\"File '{path}' not found in blob storage\")\n\n        path_obj = Path(path)\n        suffixes = path_obj.suffixes\n        file_extension = suffixes[-1].lower() if suffixes else \"\"\n\n        if compression is None and file_extension in COMPRESSION_FORMATS:\n            compression_format = COMPRESSION_FORMATS[file_extension]\n\n            # if file has multiple extensions (e.g., .csv.gz), get the inner format\n            if len(suffixes) &gt; 1:\n                inner_extension = suffixes[-2].lower()\n\n                if inner_extension == \".tar\":\n                    raise ValueError(\n                        \"Tar archives (.tar.gz) are not directly supported\"\n                    )\n\n                if inner_extension in PANDAS_READERS:\n                    try:\n                        with data_store.open(path, \"rb\") as f:\n                            return PANDAS_READERS[inner_extension](\n                                f, compression=compression_format, **kwargs\n                            )\n                    except Exception as e:\n                        raise ValueError(f\"Error reading compressed file: {str(e)}\")\n                elif inner_extension in GEO_READERS:\n                    try:\n                        with data_store.open(path, \"rb\") as f:\n                            if compression_format == \"gzip\":\n                                import gzip\n\n                                decompressed_data = gzip.decompress(f.read())\n                                import io\n\n                                return GEO_READERS[inner_extension](\n                                    io.BytesIO(decompressed_data), **kwargs\n                                )\n                            else:\n                                raise ValueError(\n                                    f\"Compression format {compression_format} not supported for geo data\"\n                                )\n                    except Exception as e:\n                        raise ValueError(f\"Error reading compressed geo file: {str(e)}\")\n            else:\n                # if just .gz without clear inner type, assume csv\n                try:\n                    with data_store.open(path, \"rb\") as f:\n                        return pd.read_csv(f, compression=compression_format, **kwargs)\n                except Exception as e:\n                    raise ValueError(\n                        f\"Error reading compressed file as CSV: {str(e)}. \"\n                        f\"If not a CSV, specify the format in the filename (e.g., .json.gz)\"\n                    )\n\n        # Special handling for compressed files\n        if file_extension == \".zip\":\n            # For zip files, we need to use binary mode\n            with data_store.open(path, \"rb\") as f:\n                return gpd.read_file(f)\n\n        # Determine if we need binary mode based on file type\n        mode = \"rb\" if file_extension in BINARY_FORMATS else \"r\"\n\n        # Try reading with appropriate reader\n        if file_extension in PANDAS_READERS:\n            try:\n                with data_store.open(path, mode) as f:\n                    return PANDAS_READERS[file_extension](f, **kwargs)\n            except Exception as e:\n                raise ValueError(f\"Error reading file with pandas: {str(e)}\")\n\n        if file_extension in GEO_READERS:\n            try:\n                with data_store.open(path, \"rb\") as f:\n                    return GEO_READERS[file_extension](f, **kwargs)\n            except Exception as e:\n                # For parquet files, try pandas reader if geopandas fails\n                if file_extension == \".parquet\":\n                    try:\n                        with data_store.open(path, \"rb\") as f:\n                            return pd.read_parquet(f, **kwargs)\n                    except Exception as e2:\n                        raise ValueError(\n                            f\"Failed to read parquet with both geopandas ({str(e)}) \"\n                            f\"and pandas ({str(e2)})\"\n                        )\n                raise ValueError(f\"Error reading file with geopandas: {str(e)}\")\n\n        # If we get here, the file type is unsupported\n        supported_formats = sorted(set(PANDAS_READERS.keys()) | set(GEO_READERS.keys()))\n        supported_compressions = sorted(COMPRESSION_FORMATS.keys())\n        raise ValueError(\n            f\"Unsupported file type: {file_extension}\\n\"\n            f\"Supported formats: {', '.join(supported_formats)}\"\n            f\"Supported compressions: {', '.join(supported_compressions)}\"\n        )\n\n    except Exception as e:\n        if isinstance(e, (FileNotFoundError, ValueError)):\n            raise\n        raise RuntimeError(f\"Unexpected error reading dataset: {str(e)}\")\n</code></pre>"},{"location":"api/handlers/#gigaspatial.handlers.microsoft_global_buildings.read_datasets","title":"<code>read_datasets(data_store, paths, **kwargs)</code>","text":"<p>Read multiple datasets from data storage at once.</p>"},{"location":"api/handlers/#gigaspatial.handlers.microsoft_global_buildings.read_datasets--parameters","title":"Parameters:","text":"<p>data_store : DataStore     Instance of DataStore for accessing data storage. paths : list of str     Paths to files in data storage. **kwargs : dict     Additional arguments passed to read_dataset.</p>"},{"location":"api/handlers/#gigaspatial.handlers.microsoft_global_buildings.read_datasets--returns","title":"Returns:","text":"<p>dict     Dictionary mapping paths to their corresponding DataFrames/GeoDataFrames.</p> Source code in <code>gigaspatial/core/io/readers.py</code> <pre><code>def read_datasets(data_store: DataStore, paths, **kwargs):\n    \"\"\"\n    Read multiple datasets from data storage at once.\n\n    Parameters:\n    ----------\n    data_store : DataStore\n        Instance of DataStore for accessing data storage.\n    paths : list of str\n        Paths to files in data storage.\n    **kwargs : dict\n        Additional arguments passed to read_dataset.\n\n    Returns:\n    -------\n    dict\n        Dictionary mapping paths to their corresponding DataFrames/GeoDataFrames.\n    \"\"\"\n    results = {}\n    errors = {}\n\n    for path in paths:\n        try:\n            results[path] = read_dataset(data_store, path, **kwargs)\n        except Exception as e:\n            errors[path] = str(e)\n\n    if errors:\n        error_msg = \"\\n\".join(f\"- {path}: {error}\" for path, error in errors.items())\n        raise ValueError(f\"Errors reading datasets:\\n{error_msg}\")\n\n    return results\n</code></pre>"},{"location":"api/handlers/#gigaspatial.handlers.microsoft_global_buildings.read_gzipped_json_or_csv","title":"<code>read_gzipped_json_or_csv(file_path, data_store)</code>","text":"<p>Reads a gzipped file, attempting to parse it as JSON (lines=True) or CSV.</p> Source code in <code>gigaspatial/core/io/readers.py</code> <pre><code>def read_gzipped_json_or_csv(file_path, data_store):\n    \"\"\"Reads a gzipped file, attempting to parse it as JSON (lines=True) or CSV.\"\"\"\n\n    with data_store.open(file_path, \"rb\") as f:\n        g = gzip.GzipFile(fileobj=f)\n        text = g.read().decode(\"utf-8\")\n        try:\n            df = pd.read_json(io.StringIO(text), lines=True)\n            return df\n        except json.JSONDecodeError:\n            try:\n                df = pd.read_csv(io.StringIO(text))\n                return df\n            except pd.errors.ParserError:\n                print(f\"Error: Could not parse {file_path} as JSON or CSV.\")\n                return None\n</code></pre>"},{"location":"api/handlers/#gigaspatial.handlers.microsoft_global_buildings.read_kmz","title":"<code>read_kmz(file_obj, **kwargs)</code>","text":"<p>Helper function to read KMZ files and return a GeoDataFrame.</p> Source code in <code>gigaspatial/core/io/readers.py</code> <pre><code>def read_kmz(file_obj, **kwargs):\n    \"\"\"Helper function to read KMZ files and return a GeoDataFrame.\"\"\"\n    try:\n        with zipfile.ZipFile(file_obj) as kmz:\n            # Find the KML file in the archive (usually doc.kml)\n            kml_filename = next(\n                name for name in kmz.namelist() if name.endswith(\".kml\")\n            )\n\n            # Read the KML content\n            kml_content = io.BytesIO(kmz.read(kml_filename))\n\n            gdf = gpd.read_file(kml_content)\n\n            # Validate the GeoDataFrame\n            if gdf.empty:\n                raise ValueError(\n                    \"The KML file is empty or does not contain valid geospatial data.\"\n                )\n\n        return gdf\n\n    except zipfile.BadZipFile:\n        raise ValueError(\"The provided file is not a valid KMZ file.\")\n    except StopIteration:\n        raise ValueError(\"No KML file found in the KMZ archive.\")\n    except Exception as e:\n        raise RuntimeError(f\"An error occurred: {e}\")\n</code></pre>"},{"location":"api/handlers/#gigaspatial.handlers.microsoft_global_buildings.write_dataset","title":"<code>write_dataset(data, data_store, path, **kwargs)</code>","text":"<p>Write DataFrame or GeoDataFrame to various file formats in Azure Blob Storage.</p>"},{"location":"api/handlers/#gigaspatial.handlers.microsoft_global_buildings.write_dataset--parameters","title":"Parameters:","text":"<p>data : pandas.DataFrame or geopandas.GeoDataFrame     The data to write to blob storage. data_store : DataStore     Instance of DataStore for accessing data storage. path : str     Path where the file will be written in data storage. **kwargs : dict     Additional arguments passed to the specific writer function.</p>"},{"location":"api/handlers/#gigaspatial.handlers.microsoft_global_buildings.write_dataset--raises","title":"Raises:","text":"<p>ValueError     If the file type is unsupported or if there's an error writing the file. TypeError     If input data is not a DataFrame or GeoDataFrame.</p> Source code in <code>gigaspatial/core/io/writers.py</code> <pre><code>def write_dataset(data, data_store: DataStore, path, **kwargs):\n    \"\"\"\n    Write DataFrame or GeoDataFrame to various file formats in Azure Blob Storage.\n\n    Parameters:\n    ----------\n    data : pandas.DataFrame or geopandas.GeoDataFrame\n        The data to write to blob storage.\n    data_store : DataStore\n        Instance of DataStore for accessing data storage.\n    path : str\n        Path where the file will be written in data storage.\n    **kwargs : dict\n        Additional arguments passed to the specific writer function.\n\n    Raises:\n    ------\n    ValueError\n        If the file type is unsupported or if there's an error writing the file.\n    TypeError\n        If input data is not a DataFrame or GeoDataFrame.\n    \"\"\"\n\n    # Define supported file formats and their writers\n    BINARY_FORMATS = {\".shp\", \".zip\", \".parquet\", \".gpkg\", \".xlsx\", \".xls\"}\n\n    PANDAS_WRITERS = {\n        \".csv\": lambda df, buf, **kw: df.to_csv(buf, **kw),\n        \".xlsx\": lambda df, buf, **kw: df.to_excel(buf, engine=\"openpyxl\", **kw),\n        \".json\": lambda df, buf, **kw: df.to_json(buf, **kw),\n        \".parquet\": lambda df, buf, **kw: df.to_parquet(buf, **kw),\n    }\n\n    GEO_WRITERS = {\n        \".geojson\": lambda gdf, buf, **kw: gdf.to_file(buf, driver=\"GeoJSON\", **kw),\n        \".gpkg\": lambda gdf, buf, **kw: gdf.to_file(buf, driver=\"GPKG\", **kw),\n        \".parquet\": lambda gdf, buf, **kw: gdf.to_parquet(buf, **kw),\n    }\n\n    try:\n        # Input validation\n        if not isinstance(data, (pd.DataFrame, gpd.GeoDataFrame)):\n            raise TypeError(\"Input data must be a pandas DataFrame or GeoDataFrame\")\n\n        # Get file suffix and ensure it's lowercase\n        suffix = Path(path).suffix.lower()\n\n        # Determine if we need binary mode based on file type\n        mode = \"wb\" if suffix in BINARY_FORMATS else \"w\"\n\n        # Handle different data types and formats\n        if isinstance(data, gpd.GeoDataFrame):\n            if suffix not in GEO_WRITERS:\n                supported_formats = sorted(GEO_WRITERS.keys())\n                raise ValueError(\n                    f\"Unsupported file type for GeoDataFrame: {suffix}\\n\"\n                    f\"Supported formats: {', '.join(supported_formats)}\"\n                )\n\n            try:\n                with data_store.open(path, \"wb\") as f:\n                    GEO_WRITERS[suffix](data, f, **kwargs)\n            except Exception as e:\n                raise ValueError(f\"Error writing GeoDataFrame: {str(e)}\")\n\n        else:  # pandas DataFrame\n            if suffix not in PANDAS_WRITERS:\n                supported_formats = sorted(PANDAS_WRITERS.keys())\n                raise ValueError(\n                    f\"Unsupported file type for DataFrame: {suffix}\\n\"\n                    f\"Supported formats: {', '.join(supported_formats)}\"\n                )\n\n            try:\n                with data_store.open(path, mode) as f:\n                    PANDAS_WRITERS[suffix](data, f, **kwargs)\n            except Exception as e:\n                raise ValueError(f\"Error writing DataFrame: {str(e)}\")\n\n    except Exception as e:\n        if isinstance(e, (TypeError, ValueError)):\n            raise\n        raise RuntimeError(f\"Unexpected error writing dataset: {str(e)}\")\n</code></pre>"},{"location":"api/handlers/#gigaspatial.handlers.microsoft_global_buildings.write_datasets","title":"<code>write_datasets(data_dict, data_store, **kwargs)</code>","text":"<p>Write multiple datasets to data storage at once.</p>"},{"location":"api/handlers/#gigaspatial.handlers.microsoft_global_buildings.write_datasets--parameters","title":"Parameters:","text":"<p>data_dict : dict     Dictionary mapping paths to DataFrames/GeoDataFrames. data_store : DataStore     Instance of DataStore for accessing data storage. **kwargs : dict     Additional arguments passed to write_dataset.</p>"},{"location":"api/handlers/#gigaspatial.handlers.microsoft_global_buildings.write_datasets--raises","title":"Raises:","text":"<p>ValueError     If there are any errors writing the datasets.</p> Source code in <code>gigaspatial/core/io/writers.py</code> <pre><code>def write_datasets(data_dict, data_store: DataStore, **kwargs):\n    \"\"\"\n    Write multiple datasets to data storage at once.\n\n    Parameters:\n    ----------\n    data_dict : dict\n        Dictionary mapping paths to DataFrames/GeoDataFrames.\n    data_store : DataStore\n        Instance of DataStore for accessing data storage.\n    **kwargs : dict\n        Additional arguments passed to write_dataset.\n\n    Raises:\n    ------\n    ValueError\n        If there are any errors writing the datasets.\n    \"\"\"\n    errors = {}\n\n    for path, data in data_dict.items():\n        try:\n            write_dataset(data, data_store, path, **kwargs)\n        except Exception as e:\n            errors[path] = str(e)\n\n    if errors:\n        error_msg = \"\\n\".join(f\"- {path}: {error}\" for path, error in errors.items())\n        raise ValueError(f\"Errors writing datasets:\\n{error_msg}\")\n</code></pre>"},{"location":"api/handlers/#gigaspatial.handlers.osm","title":"<code>osm</code>","text":""},{"location":"api/handlers/#gigaspatial.handlers.osm.OSMLocationFetcher","title":"<code>OSMLocationFetcher</code>  <code>dataclass</code>","text":"<p>A class to fetch and process location data from OpenStreetMap using the Overpass API.</p> <p>This class supports fetching various OSM location types including amenities, buildings, shops, and other POI categories.</p> Source code in <code>gigaspatial/handlers/osm.py</code> <pre><code>@dataclass\nclass OSMLocationFetcher:\n    \"\"\"\n    A class to fetch and process location data from OpenStreetMap using the Overpass API.\n\n    This class supports fetching various OSM location types including amenities, buildings,\n    shops, and other POI categories.\n    \"\"\"\n\n    country: str\n    location_types: Union[List[str], Dict[str, List[str]]]\n    base_url: str = \"http://overpass-api.de/api/interpreter\"\n    timeout: int = 600\n    max_retries: int = 3\n    retry_delay: int = 5\n\n    def __post_init__(self):\n        \"\"\"Validate inputs, normalize location_types, and set up logging.\"\"\"\n        try:\n            self.country = pycountry.countries.lookup(self.country).alpha_2\n        except LookupError:\n            raise ValueError(f\"Invalid country code provided: {self.country}\")\n\n        # Normalize location_types to always be a dictionary\n        if isinstance(self.location_types, list):\n            self.location_types = {\"amenity\": self.location_types}\n        elif not isinstance(self.location_types, dict):\n            raise TypeError(\n                \"location_types must be a list of strings or a dictionary mapping categories to type lists\"\n            )\n\n        self.logger = config.get_logger(__name__)\n\n    def _build_queries(self, since_year: Optional[int] = None) -&gt; List[str]:\n        \"\"\"\n        Construct separate Overpass QL queries for different element types and categories.\n        Returns list of [nodes_relations_query, ways_query]\n        \"\"\"\n        if since_year:\n            date_filter = f'(newer:\"{since_year}-01-01T00:00:00Z\")'\n        else:\n            date_filter = \"\"\n\n        # Query for nodes and relations (with center output)\n        nodes_relations_queries = []\n        for category, types in self.location_types.items():\n            nodes_relations_queries.extend(\n                [\n                    f\"\"\"node[\"{category}\"~\"^({\"|\".join(types)})\"]{date_filter}(area.searchArea);\"\"\",\n                    f\"\"\"relation[\"{category}\"~\"^({\"|\".join(types)})\"]{date_filter}(area.searchArea);\"\"\",\n                ]\n            )\n\n        nodes_relations_queries = \"\\n\".join(nodes_relations_queries)\n\n        nodes_relations_query = f\"\"\"\n        [out:json][timeout:{self.timeout}];\n        area[\"ISO3166-1\"={self.country}]-&gt;.searchArea;\n        (\n            {nodes_relations_queries}\n        );\n        out center;\n        \"\"\"\n\n        # Query for ways (with geometry output)\n        ways_queries = []\n        for category, types in self.location_types.items():\n            ways_queries.append(\n                f\"\"\"way[\"{category}\"~\"^({\"|\".join(types)})\"]{date_filter}(area.searchArea);\"\"\"\n            )\n\n        ways_queries = \"\\n\".join(ways_queries)\n\n        ways_query = f\"\"\"\n        [out:json][timeout:{self.timeout}];\n        area[\"ISO3166-1\"={self.country}]-&gt;.searchArea;\n        (\n            {ways_queries}\n        );\n        out geom;\n        \"\"\"\n\n        return [nodes_relations_query, ways_query]\n\n    def _make_request(self, query: str) -&gt; Dict:\n        \"\"\"Make HTTP request to Overpass API with retry mechanism.\"\"\"\n        for attempt in range(self.max_retries):\n            try:\n                self.logger.debug(f\"Executing query:\\n{query}\")\n                response = requests.get(\n                    self.base_url, params={\"data\": query}, timeout=self.timeout\n                )\n                response.raise_for_status()\n                return response.json()\n            except RequestException as e:\n                self.logger.warning(f\"Attempt {attempt + 1} failed: {str(e)}\")\n                if attempt &lt; self.max_retries - 1:\n                    sleep(self.retry_delay)\n                else:\n                    raise RuntimeError(\n                        f\"Failed to fetch data after {self.max_retries} attempts\"\n                    ) from e\n\n    def _extract_matching_categories(self, tags: Dict[str, str]) -&gt; Dict[str, str]:\n        \"\"\"\n        Extract all matching categories and their values from the tags.\n        Returns:\n            Dict mapping each matching category to its value\n        \"\"\"\n        matches = {}\n        for category, types in self.location_types.items():\n            if category in tags and tags[category] in types:\n                matches[category] = tags[category]\n        return matches\n\n    def _process_node_relation(self, element: Dict) -&gt; List[Dict[str, any]]:\n        \"\"\"\n        Process a node or relation element.\n        May return multiple processed elements if the element matches multiple categories.\n        \"\"\"\n        try:\n            tags = element.get(\"tags\", {})\n            matching_categories = self._extract_matching_categories(tags)\n\n            if not matching_categories:\n                self.logger.warning(\n                    f\"Element {element['id']} missing or not matching specified category tags\"\n                )\n                return []\n\n            _lat = element.get(\"lat\") or element[\"center\"][\"lat\"]\n            _lon = element.get(\"lon\") or element[\"center\"][\"lon\"]\n            point_geom = Point(_lon, _lat)\n\n            # for each matching category, create a separate element\n            results = []\n            for category, value in matching_categories.items():\n                results.append(\n                    {\n                        \"source_id\": element[\"id\"],\n                        \"category\": category,\n                        \"category_value\": value,\n                        \"name\": tags.get(\"name\", \"\"),\n                        \"name_en\": tags.get(\"name:en\", \"\"),\n                        \"type\": element[\"type\"],\n                        \"geometry\": point_geom,\n                        \"latitude\": _lat,\n                        \"longitude\": _lon,\n                        \"matching_categories\": list(matching_categories.keys()),\n                    }\n                )\n\n            return results\n\n        except KeyError as e:\n            self.logger.error(f\"Corrupt data received for node/relation: {str(e)}\")\n            return []\n\n    def _process_way(self, element: Dict) -&gt; List[Dict[str, any]]:\n        \"\"\"\n        Process a way element with geometry.\n        May return multiple processed elements if the element matches multiple categories.\n        \"\"\"\n        try:\n            tags = element.get(\"tags\", {})\n            matching_categories = self._extract_matching_categories(tags)\n\n            if not matching_categories:\n                self.logger.warning(\n                    f\"Element {element['id']} missing or not matching specified category tags\"\n                )\n                return []\n\n            # Create polygon from geometry points\n            polygon = Polygon([(p[\"lon\"], p[\"lat\"]) for p in element[\"geometry\"]])\n            centroid = polygon.centroid\n\n            # For each matching category, create a separate element\n            results = []\n            for category, value in matching_categories.items():\n                results.append(\n                    {\n                        \"source_id\": element[\"id\"],\n                        \"category\": category,\n                        \"category_value\": value,\n                        \"name\": tags.get(\"name\", \"\"),\n                        \"name_en\": tags.get(\"name:en\", \"\"),\n                        \"type\": element[\"type\"],\n                        \"geometry\": polygon,\n                        \"latitude\": centroid.y,\n                        \"longitude\": centroid.x,\n                        \"matching_categories\": list(matching_categories.keys()),\n                    }\n                )\n\n            return results\n        except (KeyError, ValueError) as e:\n            self.logger.error(f\"Error processing way geometry: {str(e)}\")\n            return []\n\n    def fetch_locations(\n        self,\n        since_year: Optional[int] = None,\n        handle_duplicates: Literal[\"separate\", \"combine\", \"primary\"] = \"separate\",\n    ) -&gt; pd.DataFrame:\n        \"\"\"\n        Fetch and process OSM locations.\n\n        Args:\n            since_year (int, optional): Filter for locations added/modified since this year.\n            handle_duplicates (str): How to handle objects matching multiple categories:\n                - 'separate': Create separate entries for each category (default)\n                - 'combine': Use a single entry with a list of matching categories\n                - 'primary': Keep only the first matching category\n\n        Returns:\n            pd.DataFrame: Processed OSM locations\n        \"\"\"\n        if handle_duplicates not in (\"separate\", \"combine\", \"primary\"):\n            raise ValueError(\n                \"handle_duplicates must be one of: 'separate', 'combine', 'primary'\"\n            )\n\n        self.logger.info(\n            f\"Fetching OSM locations from Overpass API for country: {self.country}\"\n        )\n        self.logger.info(f\"Location types: {self.location_types}\")\n        self.logger.info(f\"Handling duplicate category matches as: {handle_duplicates}\")\n\n        # Get queries for different element types\n        nodes_relations_query, ways_query = self._build_queries(since_year)\n\n        # Fetch nodes and relations\n        nodes_relations_response = self._make_request(nodes_relations_query)\n        nodes_relations = nodes_relations_response.get(\"elements\", [])\n\n        # Fetch ways\n        ways_response = self._make_request(ways_query)\n        ways = ways_response.get(\"elements\", [])\n\n        if not nodes_relations and not ways:\n            self.logger.warning(\"No locations found for the specified criteria\")\n            return pd.DataFrame()\n\n        self.logger.info(\n            f\"Processing {len(nodes_relations)} nodes/relations and {len(ways)} ways...\"\n        )\n\n        # Process nodes and relations\n        with ThreadPoolExecutor() as executor:\n            processed_nodes_relations = [\n                item\n                for sublist in executor.map(\n                    self._process_node_relation, nodes_relations\n                )\n                for item in sublist\n            ]\n\n        # Process ways\n        with ThreadPoolExecutor() as executor:\n            processed_ways = [\n                item\n                for sublist in executor.map(self._process_way, ways)\n                for item in sublist\n            ]\n\n        # Combine all processed elements\n        all_elements = processed_nodes_relations + processed_ways\n\n        if not all_elements:\n            self.logger.warning(\"No matching elements found after processing\")\n            return pd.DataFrame()\n\n        # Handle duplicates based on the specified strategy\n        if handle_duplicates != \"separate\":\n            # Group by source_id\n            grouped_elements = {}\n            for elem in all_elements:\n                source_id = elem[\"source_id\"]\n                if source_id not in grouped_elements:\n                    grouped_elements[source_id] = elem\n                elif handle_duplicates == \"combine\":\n                    # Combine matching categories\n                    if grouped_elements[source_id][\"category\"] != elem[\"category\"]:\n                        if isinstance(grouped_elements[source_id][\"category\"], str):\n                            grouped_elements[source_id][\"category\"] = [\n                                grouped_elements[source_id][\"category\"]\n                            ]\n                            grouped_elements[source_id][\"category_value\"] = [\n                                grouped_elements[source_id][\"category_value\"]\n                            ]\n\n                        if (\n                            elem[\"category\"]\n                            not in grouped_elements[source_id][\"category\"]\n                        ):\n                            grouped_elements[source_id][\"category\"].append(\n                                elem[\"category\"]\n                            )\n                            grouped_elements[source_id][\"category_value\"].append(\n                                elem[\"category_value\"]\n                            )\n                # For 'primary', just keep the first one we encountered\n\n            all_elements = list(grouped_elements.values())\n\n        locations = pd.DataFrame(all_elements)\n\n        # Log element type distribution\n        type_counts = locations[\"type\"].value_counts()\n        self.logger.info(\"\\nElement type distribution:\")\n        for element_type, count in type_counts.items():\n            self.logger.info(f\"{element_type}: {count}\")\n\n        # Log category distribution\n        if handle_duplicates == \"combine\":\n            # Count each category separately when they're in lists\n            category_counts = {}\n            for cats in locations[\"category\"]:\n                if isinstance(cats, list):\n                    for cat in cats:\n                        category_counts[cat] = category_counts.get(cat, 0) + 1\n                else:\n                    category_counts[cats] = category_counts.get(cats, 0) + 1\n\n            self.logger.info(\"\\nCategory distribution:\")\n            for category, count in category_counts.items():\n                self.logger.info(f\"{category}: {count}\")\n        else:\n            category_counts = locations[\"category\"].value_counts()\n            self.logger.info(\"\\nCategory distribution:\")\n            for category, count in category_counts.items():\n                self.logger.info(f\"{category}: {count}\")\n\n        # Log elements with multiple matching categories\n        multi_category = [e for e in all_elements if len(e[\"matching_categories\"]) &gt; 1]\n        if multi_category:\n            self.logger.info(\n                f\"\\n{len(multi_category)} elements matched multiple categories\"\n            )\n\n        self.logger.info(f\"Successfully processed {len(locations)} locations\")\n        return locations\n</code></pre>"},{"location":"api/handlers/#gigaspatial.handlers.osm.OSMLocationFetcher.__post_init__","title":"<code>__post_init__()</code>","text":"<p>Validate inputs, normalize location_types, and set up logging.</p> Source code in <code>gigaspatial/handlers/osm.py</code> <pre><code>def __post_init__(self):\n    \"\"\"Validate inputs, normalize location_types, and set up logging.\"\"\"\n    try:\n        self.country = pycountry.countries.lookup(self.country).alpha_2\n    except LookupError:\n        raise ValueError(f\"Invalid country code provided: {self.country}\")\n\n    # Normalize location_types to always be a dictionary\n    if isinstance(self.location_types, list):\n        self.location_types = {\"amenity\": self.location_types}\n    elif not isinstance(self.location_types, dict):\n        raise TypeError(\n            \"location_types must be a list of strings or a dictionary mapping categories to type lists\"\n        )\n\n    self.logger = config.get_logger(__name__)\n</code></pre>"},{"location":"api/handlers/#gigaspatial.handlers.osm.OSMLocationFetcher.fetch_locations","title":"<code>fetch_locations(since_year=None, handle_duplicates='separate')</code>","text":"<p>Fetch and process OSM locations.</p> <p>Parameters:</p> Name Type Description Default <code>since_year</code> <code>int</code> <p>Filter for locations added/modified since this year.</p> <code>None</code> <code>handle_duplicates</code> <code>str</code> <p>How to handle objects matching multiple categories: - 'separate': Create separate entries for each category (default) - 'combine': Use a single entry with a list of matching categories - 'primary': Keep only the first matching category</p> <code>'separate'</code> <p>Returns:</p> Type Description <code>DataFrame</code> <p>pd.DataFrame: Processed OSM locations</p> Source code in <code>gigaspatial/handlers/osm.py</code> <pre><code>def fetch_locations(\n    self,\n    since_year: Optional[int] = None,\n    handle_duplicates: Literal[\"separate\", \"combine\", \"primary\"] = \"separate\",\n) -&gt; pd.DataFrame:\n    \"\"\"\n    Fetch and process OSM locations.\n\n    Args:\n        since_year (int, optional): Filter for locations added/modified since this year.\n        handle_duplicates (str): How to handle objects matching multiple categories:\n            - 'separate': Create separate entries for each category (default)\n            - 'combine': Use a single entry with a list of matching categories\n            - 'primary': Keep only the first matching category\n\n    Returns:\n        pd.DataFrame: Processed OSM locations\n    \"\"\"\n    if handle_duplicates not in (\"separate\", \"combine\", \"primary\"):\n        raise ValueError(\n            \"handle_duplicates must be one of: 'separate', 'combine', 'primary'\"\n        )\n\n    self.logger.info(\n        f\"Fetching OSM locations from Overpass API for country: {self.country}\"\n    )\n    self.logger.info(f\"Location types: {self.location_types}\")\n    self.logger.info(f\"Handling duplicate category matches as: {handle_duplicates}\")\n\n    # Get queries for different element types\n    nodes_relations_query, ways_query = self._build_queries(since_year)\n\n    # Fetch nodes and relations\n    nodes_relations_response = self._make_request(nodes_relations_query)\n    nodes_relations = nodes_relations_response.get(\"elements\", [])\n\n    # Fetch ways\n    ways_response = self._make_request(ways_query)\n    ways = ways_response.get(\"elements\", [])\n\n    if not nodes_relations and not ways:\n        self.logger.warning(\"No locations found for the specified criteria\")\n        return pd.DataFrame()\n\n    self.logger.info(\n        f\"Processing {len(nodes_relations)} nodes/relations and {len(ways)} ways...\"\n    )\n\n    # Process nodes and relations\n    with ThreadPoolExecutor() as executor:\n        processed_nodes_relations = [\n            item\n            for sublist in executor.map(\n                self._process_node_relation, nodes_relations\n            )\n            for item in sublist\n        ]\n\n    # Process ways\n    with ThreadPoolExecutor() as executor:\n        processed_ways = [\n            item\n            for sublist in executor.map(self._process_way, ways)\n            for item in sublist\n        ]\n\n    # Combine all processed elements\n    all_elements = processed_nodes_relations + processed_ways\n\n    if not all_elements:\n        self.logger.warning(\"No matching elements found after processing\")\n        return pd.DataFrame()\n\n    # Handle duplicates based on the specified strategy\n    if handle_duplicates != \"separate\":\n        # Group by source_id\n        grouped_elements = {}\n        for elem in all_elements:\n            source_id = elem[\"source_id\"]\n            if source_id not in grouped_elements:\n                grouped_elements[source_id] = elem\n            elif handle_duplicates == \"combine\":\n                # Combine matching categories\n                if grouped_elements[source_id][\"category\"] != elem[\"category\"]:\n                    if isinstance(grouped_elements[source_id][\"category\"], str):\n                        grouped_elements[source_id][\"category\"] = [\n                            grouped_elements[source_id][\"category\"]\n                        ]\n                        grouped_elements[source_id][\"category_value\"] = [\n                            grouped_elements[source_id][\"category_value\"]\n                        ]\n\n                    if (\n                        elem[\"category\"]\n                        not in grouped_elements[source_id][\"category\"]\n                    ):\n                        grouped_elements[source_id][\"category\"].append(\n                            elem[\"category\"]\n                        )\n                        grouped_elements[source_id][\"category_value\"].append(\n                            elem[\"category_value\"]\n                        )\n            # For 'primary', just keep the first one we encountered\n\n        all_elements = list(grouped_elements.values())\n\n    locations = pd.DataFrame(all_elements)\n\n    # Log element type distribution\n    type_counts = locations[\"type\"].value_counts()\n    self.logger.info(\"\\nElement type distribution:\")\n    for element_type, count in type_counts.items():\n        self.logger.info(f\"{element_type}: {count}\")\n\n    # Log category distribution\n    if handle_duplicates == \"combine\":\n        # Count each category separately when they're in lists\n        category_counts = {}\n        for cats in locations[\"category\"]:\n            if isinstance(cats, list):\n                for cat in cats:\n                    category_counts[cat] = category_counts.get(cat, 0) + 1\n            else:\n                category_counts[cats] = category_counts.get(cats, 0) + 1\n\n        self.logger.info(\"\\nCategory distribution:\")\n        for category, count in category_counts.items():\n            self.logger.info(f\"{category}: {count}\")\n    else:\n        category_counts = locations[\"category\"].value_counts()\n        self.logger.info(\"\\nCategory distribution:\")\n        for category, count in category_counts.items():\n            self.logger.info(f\"{category}: {count}\")\n\n    # Log elements with multiple matching categories\n    multi_category = [e for e in all_elements if len(e[\"matching_categories\"]) &gt; 1]\n    if multi_category:\n        self.logger.info(\n            f\"\\n{len(multi_category)} elements matched multiple categories\"\n        )\n\n    self.logger.info(f\"Successfully processed {len(locations)} locations\")\n    return locations\n</code></pre>"},{"location":"api/handlers/#gigaspatial.handlers.overture","title":"<code>overture</code>","text":""},{"location":"api/handlers/#gigaspatial.handlers.overture.OvertureAmenityFetcher","title":"<code>OvertureAmenityFetcher</code>","text":"<p>A class to fetch and process amenity locations from OpenStreetMap using the Overpass API.</p> Source code in <code>gigaspatial/handlers/overture.py</code> <pre><code>@dataclass(config=ConfigDict(arbitrary_types_allowed=True))\nclass OvertureAmenityFetcher:\n    \"\"\"\n    A class to fetch and process amenity locations from OpenStreetMap using the Overpass API.\n    \"\"\"\n\n    # constants\n    release: Optional[str] = \"2024-12-18.0\"\n    base_url: Optional[str] = (\n        \"s3://overturemaps-us-west-2/release/{release}/theme=places/*/*\"\n    )\n\n    # user config\n    country: str = Field(...)\n    amenity_types: List[str] = Field(..., description=\"List of amenity types to fetch\")\n    geom: Union[Polygon, MultiPolygon] = None\n\n    # config for country boundary access from data storage\n    # if None GADM boundaries will be used\n    data_store: DataStore = None\n    country_geom_path: Optional[Union[str, Path]] = None\n\n    def __post_init__(self):\n        \"\"\"Validate inputs and set up logging.\"\"\"\n        try:\n            self.country = pycountry.countries.lookup(self.country).alpha_2\n        except LookupError:\n            raise ValueError(f\"Invalid country code provided: {self.country}\")\n\n        self.base_url = self.base_url.format(release=self.release)\n        self.logger = config.get_logger(__name__)\n\n        self.connection = self._set_connection()\n\n    def _set_connection(self):\n        \"\"\"Set the connection to the DB\"\"\"\n        db = duckdb.connect()\n        db.install_extension(\"spatial\")\n        db.load_extension(\"spatial\")\n        return db\n\n    def _load_country_geometry(\n        self,\n    ) -&gt; Union[Polygon, MultiPolygon]:\n        \"\"\"Load country boundary geometry from DataStore or GADM.\"\"\"\n\n        gdf_admin0 = AdminBoundaries.create(\n            country_code=pycountry.countries.lookup(self.country).alpha_3,\n            admin_level=0,\n            data_store=self.data_store,\n            path=self.country_geom_path,\n        ).to_geodataframe()\n\n        return gdf_admin0.geometry.iloc[0]\n\n    def _build_query(self, match_pattern: bool = False, **kwargs) -&gt; str:\n        \"\"\"Constructs and returns the query\"\"\"\n\n        if match_pattern:\n            amenity_query = \" OR \".join(\n                [f\"category ilike '%{amenity}%'\" for amenity in self.amenity_types]\n            )\n        else:\n            amenity_query = \" OR \".join(\n                [f\"category == '{amenity}'\" for amenity in self.amenity_types]\n            )\n\n        query = \"\"\"\n        SELECT id,\n            names.primary AS name,\n            ROUND(confidence,2) as confidence,\n            categories.primary AS category,\n            ST_AsText(geometry) as geometry,\n        FROM read_parquet('s3://overturemaps-us-west-2/release/2024-12-18.0/theme=places/type=place/*',\n            hive_partitioning=1)\n        WHERE bbox.xmin &gt; {}\n            AND bbox.ymin &gt; {} \n            AND bbox.xmax &lt;  {}\n            AND bbox.ymax &lt; {}\n            AND ({})\n        \"\"\"\n\n        if not self.geom:\n            self.geom = self._load_country_geometry()\n\n        return query.format(*self.geom.bounds, amenity_query)\n\n    def fetch_locations(\n        self, match_pattern: bool = False, **kwargs\n    ) -&gt; gpd.GeoDataFrame:\n        \"\"\"Fetch and process amenity locations.\"\"\"\n        self.logger.info(\"Fetching amenity locations from Overture DB...\")\n\n        query = self._build_query(match_pattern=match_pattern, **kwargs)\n\n        df = self.connection.execute(query).df()\n\n        self.logger.info(\"Processing geometries\")\n        gdf = gpd.GeoDataFrame(\n            df, geometry=gpd.GeoSeries.from_wkt(df[\"geometry\"]), crs=\"EPSG:4326\"\n        )\n\n        # filter by geometry boundary\n        s = STRtree(gdf.geometry)\n        result = s.query(self.geom, predicate=\"intersects\")\n\n        locations = gdf.iloc[result].reset_index(drop=True)\n\n        self.logger.info(f\"Successfully processed {len(locations)} amenity locations\")\n        return locations\n</code></pre>"},{"location":"api/handlers/#gigaspatial.handlers.overture.OvertureAmenityFetcher.__post_init__","title":"<code>__post_init__()</code>","text":"<p>Validate inputs and set up logging.</p> Source code in <code>gigaspatial/handlers/overture.py</code> <pre><code>def __post_init__(self):\n    \"\"\"Validate inputs and set up logging.\"\"\"\n    try:\n        self.country = pycountry.countries.lookup(self.country).alpha_2\n    except LookupError:\n        raise ValueError(f\"Invalid country code provided: {self.country}\")\n\n    self.base_url = self.base_url.format(release=self.release)\n    self.logger = config.get_logger(__name__)\n\n    self.connection = self._set_connection()\n</code></pre>"},{"location":"api/handlers/#gigaspatial.handlers.overture.OvertureAmenityFetcher.fetch_locations","title":"<code>fetch_locations(match_pattern=False, **kwargs)</code>","text":"<p>Fetch and process amenity locations.</p> Source code in <code>gigaspatial/handlers/overture.py</code> <pre><code>def fetch_locations(\n    self, match_pattern: bool = False, **kwargs\n) -&gt; gpd.GeoDataFrame:\n    \"\"\"Fetch and process amenity locations.\"\"\"\n    self.logger.info(\"Fetching amenity locations from Overture DB...\")\n\n    query = self._build_query(match_pattern=match_pattern, **kwargs)\n\n    df = self.connection.execute(query).df()\n\n    self.logger.info(\"Processing geometries\")\n    gdf = gpd.GeoDataFrame(\n        df, geometry=gpd.GeoSeries.from_wkt(df[\"geometry\"]), crs=\"EPSG:4326\"\n    )\n\n    # filter by geometry boundary\n    s = STRtree(gdf.geometry)\n    result = s.query(self.geom, predicate=\"intersects\")\n\n    locations = gdf.iloc[result].reset_index(drop=True)\n\n    self.logger.info(f\"Successfully processed {len(locations)} amenity locations\")\n    return locations\n</code></pre>"},{"location":"api/handlers/#gigaspatial.handlers.worldpop","title":"<code>worldpop</code>","text":""},{"location":"api/handlers/#gigaspatial.handlers.worldpop.WorldPopConfig","title":"<code>WorldPopConfig</code>","text":"<p>               Bases: <code>BaseModel</code></p> Source code in <code>gigaspatial/handlers/worldpop.py</code> <pre><code>class WorldPopConfig(BaseModel):\n    # class variables\n    _metadata_cache: ClassVar[Optional[pd.DataFrame]] = None\n\n    # constants\n    CURRENT_MAX_YEAR: int = 2022\n    EARLIEST_YEAR: int = 2000\n    SCHOOL_AGE_YEAR: int = 2020\n\n    # base config\n    WORLDPOP_DB_BASE_URL: HttpUrl = Field(default=\"https://data.worldpop.org/\")\n    SCHOOL_AGE_POPULATION_PATH: str = Field(\n        default=\"GIS/AgeSex_structures/school_age_population/v1/2020/\"\n    )\n    PPP_2021_2022_PATH: str = Field(\n        default=\"GIS/Population/Global_2021_2022_1km_UNadj/\"\n    )\n    DATASETS_METADATA_PATH: str = Field(default=\"assets/wpgpDatasets.csv\")\n\n    # user config\n    base_path: Path = Field(default=global_config.get_path(\"worldpop\", \"bronze\"))\n    country: str = Field(...)\n    year: int = Field(..., ge=EARLIEST_YEAR, le=CURRENT_MAX_YEAR)\n    resolution: Literal[\"HIGH\", \"LOW\"] = Field(\n        default=\"LOW\",\n        description=\"Spatial resolution of the population grid: HIGH (100m) or LOW (1km)\",\n    )\n    un_adjusted: bool = True\n    constrained: bool = False\n    school_age: Optional[Literal[\"PRIMARY\", \"SECONDARY\"]] = None\n    gender: Literal[\"F\", \"M\", \"F_M\"] = \"F_M\"\n\n    @field_validator(\"country\")\n    def validate_country(cls, value: str) -&gt; str:\n        try:\n            return pycountry.countries.lookup(value).alpha_3\n        except LookupError:\n            raise ValueError(f\"Invalid country code provided: {value}\")\n\n    @model_validator(mode=\"after\")\n    def validate_configuration(self):\n        \"\"\"\n        Validate that the configuration is valid based on dataset availability constraints.\n\n        Specific rules:\n        - Post-2020 data is only available at 1km resolution with UN adjustment\n        - School age population data is only available for 2020 at 1km resolution\n        \"\"\"\n        if self.year &gt; self.SCHOOL_AGE_YEAR:\n            if self.resolution != \"LOW\":\n                raise ValueError(\n                    f\"Data for year {self.year} is only available at LOW (1km) resolution\"\n                )\n\n            if not self.un_adjusted:\n                raise ValueError(\n                    f\"Data for year {self.year} is only available with UN adjustment\"\n                )\n\n        if self.school_age:\n            if self.resolution != \"LOW\":\n                raise ValueError(\n                    f\"School age data is only available at LOW (1km) resolution\"\n                )\n\n            if self.year != self.SCHOOL_AGE_YEAR:\n                self.year = self.SCHOOL_AGE_YEAR\n                raise ValueError(f\"School age data is only available for 2020\")\n\n        return self\n\n    @property\n    def dataset_url(self) -&gt; str:\n        \"\"\"Get the URL for the configured dataset. The URL is computed on first access and then cached for subsequent calls.\"\"\"\n        if not hasattr(self, \"_dataset_url\"):\n            self._dataset_url = self._compute_dataset_url()\n        return self._dataset_url\n\n    @property\n    def dataset_path(self) -&gt; Path:\n        \"\"\"Construct and return the path for the configured dataset.\"\"\"\n        url_parts = self.dataset_url.split(\"/\")\n        file_path = (\n            \"/\".join(\n                [url_parts[4], url_parts[5], url_parts[7], self.country, url_parts[-1]]\n            )\n            if self.school_age\n            else \"/\".join([url_parts[4], url_parts[6], self.country, url_parts[-1]])\n        )\n        return self.base_path / file_path\n\n    def _load_datasets_metadata(self) -&gt; pd.DataFrame:\n        \"\"\"Load and return the WorldPop datasets metadata, using cache if available.\"\"\"\n        if WorldPopConfig._metadata_cache is not None:\n            return WorldPopConfig._metadata_cache\n\n        try:\n            WorldPopConfig._metadata_cache = pd.read_csv(\n                str(self.WORLDPOP_DB_BASE_URL) + self.DATASETS_METADATA_PATH\n            )\n            return WorldPopConfig._metadata_cache\n        except (URLError, pd.errors.EmptyDataError) as e:\n            raise RuntimeError(f\"Failed to load WorldPop datasets metadata: {e}\")\n\n    def _compute_dataset_url(self) -&gt; str:\n        \"\"\"Construct and return the URL for the configured dataset.\"\"\"\n        # handle post-2020 datasets\n        if self.year &gt; self.SCHOOL_AGE_YEAR:\n            return (\n                str(self.WORLDPOP_DB_BASE_URL)\n                + self.PPP_2021_2022_PATH\n                + f\"{'' if self.constrained else 'un'}constrained/{self.year}/{self.country}/{self.country.lower()}_ppp_{self.year}_1km_UNadj{'_constrained' if self.constrained else ''}.tif\"\n            )\n\n        # handle school-age population datasets\n        if self.school_age:\n            return (\n                str(self.WORLDPOP_DB_BASE_URL)\n                + self.SCHOOL_AGE_POPULATION_PATH\n                + f\"{self.country}/{self.country}_SAP_1km_2020/{self.country}_{self.gender}_{self.school_age}_2020_1km.tif\"\n            )\n\n        # handle standard population datasets\n        wp_metadata = self._load_datasets_metadata()\n\n        try:\n            dataset_url = (\n                self.WORLDPOP_DB_BASE_URL\n                + wp_metadata[\n                    (wp_metadata.ISO3 == self.country)\n                    &amp; (\n                        wp_metadata.Covariate\n                        == \"ppp_\"\n                        + str(self.year)\n                        + (\"_UNadj\" if self.un_adjusted else \"\")\n                    )\n                ].PathToRaster.values[0]\n            )\n        except IndexError:\n            raise ValueError(\n                f\"No dataset found for country={self.country}, year={self.year}, un_adjusted={self.un_adjusted}\"\n            )\n\n        # handle resolution conversion if needed\n        if self.resolution == \"HIGH\":\n            return dataset_url\n\n        url_parts = dataset_url.split(\"/\")\n        url_parts[5] = (\n            url_parts[5] + \"_1km\" + (\"_UNadj\" if self.un_adjusted else \"\")\n        )  # get 1km folder with UNadj specification\n        url_parts[8] = url_parts[8].replace(\n            str(self.year), str(self.year) + \"_1km_Aggregated\"\n        )  # get filename with 1km res\n        dataset_url = \"/\".join(url_parts)\n\n        return dataset_url\n\n    def __repr__(self) -&gt; str:\n\n        parts = [\n            f\"WorldpopConfig(\",\n            f\"  country='{self.country}'\",\n            f\"  year={self.year}\",\n            f\"  resolution={self.resolution}\",\n            f\"  un_adjusted={self.un_adjusted}\",\n            f\"  constrained={self.constrained}\",\n        ]\n\n        if self.school_age:\n            parts.append(f\"  school_age='{self.school_age}'\")\n            parts.append(f\"  gender='{self.gender}'\")\n\n        parts.append(\")\")\n\n        return \"\\n\".join(parts)\n</code></pre>"},{"location":"api/handlers/#gigaspatial.handlers.worldpop.WorldPopConfig.dataset_path","title":"<code>dataset_path: Path</code>  <code>property</code>","text":"<p>Construct and return the path for the configured dataset.</p>"},{"location":"api/handlers/#gigaspatial.handlers.worldpop.WorldPopConfig.dataset_url","title":"<code>dataset_url: str</code>  <code>property</code>","text":"<p>Get the URL for the configured dataset. The URL is computed on first access and then cached for subsequent calls.</p>"},{"location":"api/handlers/#gigaspatial.handlers.worldpop.WorldPopConfig.validate_configuration","title":"<code>validate_configuration()</code>","text":"<p>Validate that the configuration is valid based on dataset availability constraints.</p> <p>Specific rules: - Post-2020 data is only available at 1km resolution with UN adjustment - School age population data is only available for 2020 at 1km resolution</p> Source code in <code>gigaspatial/handlers/worldpop.py</code> <pre><code>@model_validator(mode=\"after\")\ndef validate_configuration(self):\n    \"\"\"\n    Validate that the configuration is valid based on dataset availability constraints.\n\n    Specific rules:\n    - Post-2020 data is only available at 1km resolution with UN adjustment\n    - School age population data is only available for 2020 at 1km resolution\n    \"\"\"\n    if self.year &gt; self.SCHOOL_AGE_YEAR:\n        if self.resolution != \"LOW\":\n            raise ValueError(\n                f\"Data for year {self.year} is only available at LOW (1km) resolution\"\n            )\n\n        if not self.un_adjusted:\n            raise ValueError(\n                f\"Data for year {self.year} is only available with UN adjustment\"\n            )\n\n    if self.school_age:\n        if self.resolution != \"LOW\":\n            raise ValueError(\n                f\"School age data is only available at LOW (1km) resolution\"\n            )\n\n        if self.year != self.SCHOOL_AGE_YEAR:\n            self.year = self.SCHOOL_AGE_YEAR\n            raise ValueError(f\"School age data is only available for 2020\")\n\n    return self\n</code></pre>"},{"location":"api/handlers/#gigaspatial.handlers.worldpop.WorldPopDownloader","title":"<code>WorldPopDownloader</code>","text":"<p>A class to handle downloads of WorldPop datasets.</p> Source code in <code>gigaspatial/handlers/worldpop.py</code> <pre><code>class WorldPopDownloader:\n    \"\"\"A class to handle downloads of WorldPop datasets.\"\"\"\n\n    def __init__(\n        self,\n        config: Union[WorldPopConfig, dict[str, Union[str, int]]],\n        data_store: Optional[DataStore] = None,\n        logger: Optional[logging.Logger] = None,\n    ):\n        \"\"\"\n        Initialize the downloader.\n\n        Args:\n            config: Configuration for the WorldPop dataset, either as a WorldPopConfig object or a dictionary of parameters\n            data_store: Optional data storage interface. If not provided, uses LocalDataStore.\n            logger: Optional custom logger. If not provided, uses default logger.\n        \"\"\"\n        self.logger = logger or global_config.get_logger(__name__)\n        self.data_store = data_store or LocalDataStore()\n        self.config = (\n            config if isinstance(config, WorldPopConfig) else WorldPopConfig(**config)\n        )\n\n    @classmethod\n    def from_country_year(cls, country: str, year: int, **kwargs):\n        \"\"\"\n        Create a downloader instance from country and year.\n\n        Args:\n            country: Country code or name\n            year: Year of the dataset\n            **kwargs: Additional parameters for WorldPopConfig or the downloader\n        \"\"\"\n        return cls({\"country\": country, \"year\": year}, **kwargs)\n\n    def download_dataset(self) -&gt; str:\n        \"\"\"\n        Download the configured dataset to the provided output path.\n        \"\"\"\n\n        try:\n            response = requests.get(self.config.dataset_url, stream=True)\n            response.raise_for_status()\n\n            output_path = str(self.config.dataset_path)\n\n            total_size = int(response.headers.get(\"content-length\", 0))\n\n            with self.data_store.open(output_path, \"wb\") as file:\n                with tqdm(\n                    total=total_size,\n                    unit=\"B\",\n                    unit_scale=True,\n                    desc=f\"Downloading {os.path.basename(output_path)}\",\n                ) as pbar:\n                    for chunk in response.iter_content(chunk_size=8192):\n                        if chunk:\n                            file.write(chunk)\n                            pbar.update(len(chunk))\n\n            self.logger.debug(f\"Successfully downloaded dataset: {self.config}\")\n\n            return output_path\n\n        except requests.exceptions.RequestException as e:\n            self.logger.error(f\"Failed to download dataset {self.config}: {str(e)}\")\n            return None\n        except Exception as e:\n            self.logger.error(f\"Unexpected error downloading dataset: {str(e)}\")\n            return None\n</code></pre>"},{"location":"api/handlers/#gigaspatial.handlers.worldpop.WorldPopDownloader.__init__","title":"<code>__init__(config, data_store=None, logger=None)</code>","text":"<p>Initialize the downloader.</p> <p>Parameters:</p> Name Type Description Default <code>config</code> <code>Union[WorldPopConfig, dict[str, Union[str, int]]]</code> <p>Configuration for the WorldPop dataset, either as a WorldPopConfig object or a dictionary of parameters</p> required <code>data_store</code> <code>Optional[DataStore]</code> <p>Optional data storage interface. If not provided, uses LocalDataStore.</p> <code>None</code> <code>logger</code> <code>Optional[Logger]</code> <p>Optional custom logger. If not provided, uses default logger.</p> <code>None</code> Source code in <code>gigaspatial/handlers/worldpop.py</code> <pre><code>def __init__(\n    self,\n    config: Union[WorldPopConfig, dict[str, Union[str, int]]],\n    data_store: Optional[DataStore] = None,\n    logger: Optional[logging.Logger] = None,\n):\n    \"\"\"\n    Initialize the downloader.\n\n    Args:\n        config: Configuration for the WorldPop dataset, either as a WorldPopConfig object or a dictionary of parameters\n        data_store: Optional data storage interface. If not provided, uses LocalDataStore.\n        logger: Optional custom logger. If not provided, uses default logger.\n    \"\"\"\n    self.logger = logger or global_config.get_logger(__name__)\n    self.data_store = data_store or LocalDataStore()\n    self.config = (\n        config if isinstance(config, WorldPopConfig) else WorldPopConfig(**config)\n    )\n</code></pre>"},{"location":"api/handlers/#gigaspatial.handlers.worldpop.WorldPopDownloader.download_dataset","title":"<code>download_dataset()</code>","text":"<p>Download the configured dataset to the provided output path.</p> Source code in <code>gigaspatial/handlers/worldpop.py</code> <pre><code>def download_dataset(self) -&gt; str:\n    \"\"\"\n    Download the configured dataset to the provided output path.\n    \"\"\"\n\n    try:\n        response = requests.get(self.config.dataset_url, stream=True)\n        response.raise_for_status()\n\n        output_path = str(self.config.dataset_path)\n\n        total_size = int(response.headers.get(\"content-length\", 0))\n\n        with self.data_store.open(output_path, \"wb\") as file:\n            with tqdm(\n                total=total_size,\n                unit=\"B\",\n                unit_scale=True,\n                desc=f\"Downloading {os.path.basename(output_path)}\",\n            ) as pbar:\n                for chunk in response.iter_content(chunk_size=8192):\n                    if chunk:\n                        file.write(chunk)\n                        pbar.update(len(chunk))\n\n        self.logger.debug(f\"Successfully downloaded dataset: {self.config}\")\n\n        return output_path\n\n    except requests.exceptions.RequestException as e:\n        self.logger.error(f\"Failed to download dataset {self.config}: {str(e)}\")\n        return None\n    except Exception as e:\n        self.logger.error(f\"Unexpected error downloading dataset: {str(e)}\")\n        return None\n</code></pre>"},{"location":"api/handlers/#gigaspatial.handlers.worldpop.WorldPopDownloader.from_country_year","title":"<code>from_country_year(country, year, **kwargs)</code>  <code>classmethod</code>","text":"<p>Create a downloader instance from country and year.</p> <p>Parameters:</p> Name Type Description Default <code>country</code> <code>str</code> <p>Country code or name</p> required <code>year</code> <code>int</code> <p>Year of the dataset</p> required <code>**kwargs</code> <p>Additional parameters for WorldPopConfig or the downloader</p> <code>{}</code> Source code in <code>gigaspatial/handlers/worldpop.py</code> <pre><code>@classmethod\ndef from_country_year(cls, country: str, year: int, **kwargs):\n    \"\"\"\n    Create a downloader instance from country and year.\n\n    Args:\n        country: Country code or name\n        year: Year of the dataset\n        **kwargs: Additional parameters for WorldPopConfig or the downloader\n    \"\"\"\n    return cls({\"country\": country, \"year\": year}, **kwargs)\n</code></pre>"},{"location":"api/handlers/#gigaspatial.handlers.worldpop.read_dataset","title":"<code>read_dataset(data_store, path, compression=None, **kwargs)</code>","text":"<p>Read data from various file formats stored in both local and cloud-based storage.</p>"},{"location":"api/handlers/#gigaspatial.handlers.worldpop.read_dataset--parameters","title":"Parameters:","text":"<p>data_store : DataStore     Instance of DataStore for accessing data storage. path : str, Path     Path to the file in data storage. **kwargs : dict     Additional arguments passed to the specific reader function.</p>"},{"location":"api/handlers/#gigaspatial.handlers.worldpop.read_dataset--returns","title":"Returns:","text":"<p>pandas.DataFrame or geopandas.GeoDataFrame     The data read from the file.</p>"},{"location":"api/handlers/#gigaspatial.handlers.worldpop.read_dataset--raises","title":"Raises:","text":"<p>FileNotFoundError     If the file doesn't exist in blob storage. ValueError     If the file type is unsupported or if there's an error reading the file.</p> Source code in <code>gigaspatial/core/io/readers.py</code> <pre><code>def read_dataset(data_store: DataStore, path: str, compression: str = None, **kwargs):\n    \"\"\"\n    Read data from various file formats stored in both local and cloud-based storage.\n\n    Parameters:\n    ----------\n    data_store : DataStore\n        Instance of DataStore for accessing data storage.\n    path : str, Path\n        Path to the file in data storage.\n    **kwargs : dict\n        Additional arguments passed to the specific reader function.\n\n    Returns:\n    -------\n    pandas.DataFrame or geopandas.GeoDataFrame\n        The data read from the file.\n\n    Raises:\n    ------\n    FileNotFoundError\n        If the file doesn't exist in blob storage.\n    ValueError\n        If the file type is unsupported or if there's an error reading the file.\n    \"\"\"\n\n    # Define supported file formats and their readers\n    BINARY_FORMATS = {\n        \".shp\",\n        \".zip\",\n        \".parquet\",\n        \".gpkg\",\n        \".xlsx\",\n        \".xls\",\n        \".kmz\",\n        \".gz\",\n    }\n\n    PANDAS_READERS = {\n        \".csv\": pd.read_csv,\n        \".xlsx\": lambda f, **kw: pd.read_excel(f, engine=\"openpyxl\", **kw),\n        \".xls\": lambda f, **kw: pd.read_excel(f, engine=\"xlrd\", **kw),\n        \".json\": pd.read_json,\n        # \".gz\": lambda f, **kw: pd.read_csv(f, compression=\"gzip\", **kw),\n    }\n\n    GEO_READERS = {\n        \".shp\": gpd.read_file,\n        \".zip\": gpd.read_file,\n        \".geojson\": gpd.read_file,\n        \".gpkg\": gpd.read_file,\n        \".parquet\": gpd.read_parquet,\n        \".kmz\": read_kmz,\n    }\n\n    COMPRESSION_FORMATS = {\n        \".gz\": \"gzip\",\n        \".bz2\": \"bz2\",\n        \".zip\": \"zip\",\n        \".xz\": \"xz\",\n    }\n\n    try:\n        # Check if file exists\n        if not data_store.file_exists(path):\n            raise FileNotFoundError(f\"File '{path}' not found in blob storage\")\n\n        path_obj = Path(path)\n        suffixes = path_obj.suffixes\n        file_extension = suffixes[-1].lower() if suffixes else \"\"\n\n        if compression is None and file_extension in COMPRESSION_FORMATS:\n            compression_format = COMPRESSION_FORMATS[file_extension]\n\n            # if file has multiple extensions (e.g., .csv.gz), get the inner format\n            if len(suffixes) &gt; 1:\n                inner_extension = suffixes[-2].lower()\n\n                if inner_extension == \".tar\":\n                    raise ValueError(\n                        \"Tar archives (.tar.gz) are not directly supported\"\n                    )\n\n                if inner_extension in PANDAS_READERS:\n                    try:\n                        with data_store.open(path, \"rb\") as f:\n                            return PANDAS_READERS[inner_extension](\n                                f, compression=compression_format, **kwargs\n                            )\n                    except Exception as e:\n                        raise ValueError(f\"Error reading compressed file: {str(e)}\")\n                elif inner_extension in GEO_READERS:\n                    try:\n                        with data_store.open(path, \"rb\") as f:\n                            if compression_format == \"gzip\":\n                                import gzip\n\n                                decompressed_data = gzip.decompress(f.read())\n                                import io\n\n                                return GEO_READERS[inner_extension](\n                                    io.BytesIO(decompressed_data), **kwargs\n                                )\n                            else:\n                                raise ValueError(\n                                    f\"Compression format {compression_format} not supported for geo data\"\n                                )\n                    except Exception as e:\n                        raise ValueError(f\"Error reading compressed geo file: {str(e)}\")\n            else:\n                # if just .gz without clear inner type, assume csv\n                try:\n                    with data_store.open(path, \"rb\") as f:\n                        return pd.read_csv(f, compression=compression_format, **kwargs)\n                except Exception as e:\n                    raise ValueError(\n                        f\"Error reading compressed file as CSV: {str(e)}. \"\n                        f\"If not a CSV, specify the format in the filename (e.g., .json.gz)\"\n                    )\n\n        # Special handling for compressed files\n        if file_extension == \".zip\":\n            # For zip files, we need to use binary mode\n            with data_store.open(path, \"rb\") as f:\n                return gpd.read_file(f)\n\n        # Determine if we need binary mode based on file type\n        mode = \"rb\" if file_extension in BINARY_FORMATS else \"r\"\n\n        # Try reading with appropriate reader\n        if file_extension in PANDAS_READERS:\n            try:\n                with data_store.open(path, mode) as f:\n                    return PANDAS_READERS[file_extension](f, **kwargs)\n            except Exception as e:\n                raise ValueError(f\"Error reading file with pandas: {str(e)}\")\n\n        if file_extension in GEO_READERS:\n            try:\n                with data_store.open(path, \"rb\") as f:\n                    return GEO_READERS[file_extension](f, **kwargs)\n            except Exception as e:\n                # For parquet files, try pandas reader if geopandas fails\n                if file_extension == \".parquet\":\n                    try:\n                        with data_store.open(path, \"rb\") as f:\n                            return pd.read_parquet(f, **kwargs)\n                    except Exception as e2:\n                        raise ValueError(\n                            f\"Failed to read parquet with both geopandas ({str(e)}) \"\n                            f\"and pandas ({str(e2)})\"\n                        )\n                raise ValueError(f\"Error reading file with geopandas: {str(e)}\")\n\n        # If we get here, the file type is unsupported\n        supported_formats = sorted(set(PANDAS_READERS.keys()) | set(GEO_READERS.keys()))\n        supported_compressions = sorted(COMPRESSION_FORMATS.keys())\n        raise ValueError(\n            f\"Unsupported file type: {file_extension}\\n\"\n            f\"Supported formats: {', '.join(supported_formats)}\"\n            f\"Supported compressions: {', '.join(supported_compressions)}\"\n        )\n\n    except Exception as e:\n        if isinstance(e, (FileNotFoundError, ValueError)):\n            raise\n        raise RuntimeError(f\"Unexpected error reading dataset: {str(e)}\")\n</code></pre>"},{"location":"api/handlers/#gigaspatial.handlers.worldpop.read_datasets","title":"<code>read_datasets(data_store, paths, **kwargs)</code>","text":"<p>Read multiple datasets from data storage at once.</p>"},{"location":"api/handlers/#gigaspatial.handlers.worldpop.read_datasets--parameters","title":"Parameters:","text":"<p>data_store : DataStore     Instance of DataStore for accessing data storage. paths : list of str     Paths to files in data storage. **kwargs : dict     Additional arguments passed to read_dataset.</p>"},{"location":"api/handlers/#gigaspatial.handlers.worldpop.read_datasets--returns","title":"Returns:","text":"<p>dict     Dictionary mapping paths to their corresponding DataFrames/GeoDataFrames.</p> Source code in <code>gigaspatial/core/io/readers.py</code> <pre><code>def read_datasets(data_store: DataStore, paths, **kwargs):\n    \"\"\"\n    Read multiple datasets from data storage at once.\n\n    Parameters:\n    ----------\n    data_store : DataStore\n        Instance of DataStore for accessing data storage.\n    paths : list of str\n        Paths to files in data storage.\n    **kwargs : dict\n        Additional arguments passed to read_dataset.\n\n    Returns:\n    -------\n    dict\n        Dictionary mapping paths to their corresponding DataFrames/GeoDataFrames.\n    \"\"\"\n    results = {}\n    errors = {}\n\n    for path in paths:\n        try:\n            results[path] = read_dataset(data_store, path, **kwargs)\n        except Exception as e:\n            errors[path] = str(e)\n\n    if errors:\n        error_msg = \"\\n\".join(f\"- {path}: {error}\" for path, error in errors.items())\n        raise ValueError(f\"Errors reading datasets:\\n{error_msg}\")\n\n    return results\n</code></pre>"},{"location":"api/handlers/#gigaspatial.handlers.worldpop.read_gzipped_json_or_csv","title":"<code>read_gzipped_json_or_csv(file_path, data_store)</code>","text":"<p>Reads a gzipped file, attempting to parse it as JSON (lines=True) or CSV.</p> Source code in <code>gigaspatial/core/io/readers.py</code> <pre><code>def read_gzipped_json_or_csv(file_path, data_store):\n    \"\"\"Reads a gzipped file, attempting to parse it as JSON (lines=True) or CSV.\"\"\"\n\n    with data_store.open(file_path, \"rb\") as f:\n        g = gzip.GzipFile(fileobj=f)\n        text = g.read().decode(\"utf-8\")\n        try:\n            df = pd.read_json(io.StringIO(text), lines=True)\n            return df\n        except json.JSONDecodeError:\n            try:\n                df = pd.read_csv(io.StringIO(text))\n                return df\n            except pd.errors.ParserError:\n                print(f\"Error: Could not parse {file_path} as JSON or CSV.\")\n                return None\n</code></pre>"},{"location":"api/handlers/#gigaspatial.handlers.worldpop.read_kmz","title":"<code>read_kmz(file_obj, **kwargs)</code>","text":"<p>Helper function to read KMZ files and return a GeoDataFrame.</p> Source code in <code>gigaspatial/core/io/readers.py</code> <pre><code>def read_kmz(file_obj, **kwargs):\n    \"\"\"Helper function to read KMZ files and return a GeoDataFrame.\"\"\"\n    try:\n        with zipfile.ZipFile(file_obj) as kmz:\n            # Find the KML file in the archive (usually doc.kml)\n            kml_filename = next(\n                name for name in kmz.namelist() if name.endswith(\".kml\")\n            )\n\n            # Read the KML content\n            kml_content = io.BytesIO(kmz.read(kml_filename))\n\n            gdf = gpd.read_file(kml_content)\n\n            # Validate the GeoDataFrame\n            if gdf.empty:\n                raise ValueError(\n                    \"The KML file is empty or does not contain valid geospatial data.\"\n                )\n\n        return gdf\n\n    except zipfile.BadZipFile:\n        raise ValueError(\"The provided file is not a valid KMZ file.\")\n    except StopIteration:\n        raise ValueError(\"No KML file found in the KMZ archive.\")\n    except Exception as e:\n        raise RuntimeError(f\"An error occurred: {e}\")\n</code></pre>"},{"location":"api/handlers/#gigaspatial.handlers.worldpop.write_dataset","title":"<code>write_dataset(data, data_store, path, **kwargs)</code>","text":"<p>Write DataFrame or GeoDataFrame to various file formats in Azure Blob Storage.</p>"},{"location":"api/handlers/#gigaspatial.handlers.worldpop.write_dataset--parameters","title":"Parameters:","text":"<p>data : pandas.DataFrame or geopandas.GeoDataFrame     The data to write to blob storage. data_store : DataStore     Instance of DataStore for accessing data storage. path : str     Path where the file will be written in data storage. **kwargs : dict     Additional arguments passed to the specific writer function.</p>"},{"location":"api/handlers/#gigaspatial.handlers.worldpop.write_dataset--raises","title":"Raises:","text":"<p>ValueError     If the file type is unsupported or if there's an error writing the file. TypeError     If input data is not a DataFrame or GeoDataFrame.</p> Source code in <code>gigaspatial/core/io/writers.py</code> <pre><code>def write_dataset(data, data_store: DataStore, path, **kwargs):\n    \"\"\"\n    Write DataFrame or GeoDataFrame to various file formats in Azure Blob Storage.\n\n    Parameters:\n    ----------\n    data : pandas.DataFrame or geopandas.GeoDataFrame\n        The data to write to blob storage.\n    data_store : DataStore\n        Instance of DataStore for accessing data storage.\n    path : str\n        Path where the file will be written in data storage.\n    **kwargs : dict\n        Additional arguments passed to the specific writer function.\n\n    Raises:\n    ------\n    ValueError\n        If the file type is unsupported or if there's an error writing the file.\n    TypeError\n        If input data is not a DataFrame or GeoDataFrame.\n    \"\"\"\n\n    # Define supported file formats and their writers\n    BINARY_FORMATS = {\".shp\", \".zip\", \".parquet\", \".gpkg\", \".xlsx\", \".xls\"}\n\n    PANDAS_WRITERS = {\n        \".csv\": lambda df, buf, **kw: df.to_csv(buf, **kw),\n        \".xlsx\": lambda df, buf, **kw: df.to_excel(buf, engine=\"openpyxl\", **kw),\n        \".json\": lambda df, buf, **kw: df.to_json(buf, **kw),\n        \".parquet\": lambda df, buf, **kw: df.to_parquet(buf, **kw),\n    }\n\n    GEO_WRITERS = {\n        \".geojson\": lambda gdf, buf, **kw: gdf.to_file(buf, driver=\"GeoJSON\", **kw),\n        \".gpkg\": lambda gdf, buf, **kw: gdf.to_file(buf, driver=\"GPKG\", **kw),\n        \".parquet\": lambda gdf, buf, **kw: gdf.to_parquet(buf, **kw),\n    }\n\n    try:\n        # Input validation\n        if not isinstance(data, (pd.DataFrame, gpd.GeoDataFrame)):\n            raise TypeError(\"Input data must be a pandas DataFrame or GeoDataFrame\")\n\n        # Get file suffix and ensure it's lowercase\n        suffix = Path(path).suffix.lower()\n\n        # Determine if we need binary mode based on file type\n        mode = \"wb\" if suffix in BINARY_FORMATS else \"w\"\n\n        # Handle different data types and formats\n        if isinstance(data, gpd.GeoDataFrame):\n            if suffix not in GEO_WRITERS:\n                supported_formats = sorted(GEO_WRITERS.keys())\n                raise ValueError(\n                    f\"Unsupported file type for GeoDataFrame: {suffix}\\n\"\n                    f\"Supported formats: {', '.join(supported_formats)}\"\n                )\n\n            try:\n                with data_store.open(path, \"wb\") as f:\n                    GEO_WRITERS[suffix](data, f, **kwargs)\n            except Exception as e:\n                raise ValueError(f\"Error writing GeoDataFrame: {str(e)}\")\n\n        else:  # pandas DataFrame\n            if suffix not in PANDAS_WRITERS:\n                supported_formats = sorted(PANDAS_WRITERS.keys())\n                raise ValueError(\n                    f\"Unsupported file type for DataFrame: {suffix}\\n\"\n                    f\"Supported formats: {', '.join(supported_formats)}\"\n                )\n\n            try:\n                with data_store.open(path, mode) as f:\n                    PANDAS_WRITERS[suffix](data, f, **kwargs)\n            except Exception as e:\n                raise ValueError(f\"Error writing DataFrame: {str(e)}\")\n\n    except Exception as e:\n        if isinstance(e, (TypeError, ValueError)):\n            raise\n        raise RuntimeError(f\"Unexpected error writing dataset: {str(e)}\")\n</code></pre>"},{"location":"api/handlers/#gigaspatial.handlers.worldpop.write_datasets","title":"<code>write_datasets(data_dict, data_store, **kwargs)</code>","text":"<p>Write multiple datasets to data storage at once.</p>"},{"location":"api/handlers/#gigaspatial.handlers.worldpop.write_datasets--parameters","title":"Parameters:","text":"<p>data_dict : dict     Dictionary mapping paths to DataFrames/GeoDataFrames. data_store : DataStore     Instance of DataStore for accessing data storage. **kwargs : dict     Additional arguments passed to write_dataset.</p>"},{"location":"api/handlers/#gigaspatial.handlers.worldpop.write_datasets--raises","title":"Raises:","text":"<p>ValueError     If there are any errors writing the datasets.</p> Source code in <code>gigaspatial/core/io/writers.py</code> <pre><code>def write_datasets(data_dict, data_store: DataStore, **kwargs):\n    \"\"\"\n    Write multiple datasets to data storage at once.\n\n    Parameters:\n    ----------\n    data_dict : dict\n        Dictionary mapping paths to DataFrames/GeoDataFrames.\n    data_store : DataStore\n        Instance of DataStore for accessing data storage.\n    **kwargs : dict\n        Additional arguments passed to write_dataset.\n\n    Raises:\n    ------\n    ValueError\n        If there are any errors writing the datasets.\n    \"\"\"\n    errors = {}\n\n    for path, data in data_dict.items():\n        try:\n            write_dataset(data, data_store, path, **kwargs)\n        except Exception as e:\n            errors[path] = str(e)\n\n    if errors:\n        error_msg = \"\\n\".join(f\"- {path}: {error}\" for path, error in errors.items())\n        raise ValueError(f\"Errors writing datasets:\\n{error_msg}\")\n</code></pre>"},{"location":"api/processing/","title":"Processing Module","text":""},{"location":"api/processing/#gigaspatial.processing","title":"<code>gigaspatial.processing</code>","text":""},{"location":"api/processing/#gigaspatial.processing.geo","title":"<code>geo</code>","text":""},{"location":"api/processing/#gigaspatial.processing.geo.add_spatial_jitter","title":"<code>add_spatial_jitter(df, columns=['latitude', 'longitude'], amount=0.0001, seed=None, copy=True)</code>","text":"<p>Add random jitter to duplicated geographic coordinates to create slight separation between overlapping points.</p>"},{"location":"api/processing/#gigaspatial.processing.geo.add_spatial_jitter--parameters","title":"Parameters:","text":"<p>df : pandas.DataFrame     DataFrame containing geographic coordinates. columns : list of str, optional     Column names containing coordinates to jitter. Default is ['latitude', 'longitude']. amount : float or dict, optional     Amount of jitter to add. If float, same amount used for all columns.     If dict, specify amount per column, e.g., {'lat': 0.0001, 'lon': 0.0002}.     Default is 0.0001 (approximately 11 meters at the equator). seed : int, optional     Random seed for reproducibility. Default is None. copy : bool, optional     Whether to create a copy of the input DataFrame. Default is True.</p>"},{"location":"api/processing/#gigaspatial.processing.geo.add_spatial_jitter--returns","title":"Returns:","text":"<p>pandas.DataFrame     DataFrame with jittered coordinates for previously duplicated points.</p>"},{"location":"api/processing/#gigaspatial.processing.geo.add_spatial_jitter--raises","title":"Raises:","text":"<p>ValueError     If columns don't exist or jitter amount is invalid. TypeError     If input types are incorrect.</p> Source code in <code>gigaspatial/processing/geo.py</code> <pre><code>def add_spatial_jitter(\n    df: pd.DataFrame,\n    columns: List[str] = [\"latitude\", \"longitude\"],\n    amount: float = 0.0001,\n    seed=None,\n    copy=True,\n) -&gt; pd.DataFrame:\n    \"\"\"\n    Add random jitter to duplicated geographic coordinates to create slight separation\n    between overlapping points.\n\n    Parameters:\n    ----------\n    df : pandas.DataFrame\n        DataFrame containing geographic coordinates.\n    columns : list of str, optional\n        Column names containing coordinates to jitter. Default is ['latitude', 'longitude'].\n    amount : float or dict, optional\n        Amount of jitter to add. If float, same amount used for all columns.\n        If dict, specify amount per column, e.g., {'lat': 0.0001, 'lon': 0.0002}.\n        Default is 0.0001 (approximately 11 meters at the equator).\n    seed : int, optional\n        Random seed for reproducibility. Default is None.\n    copy : bool, optional\n        Whether to create a copy of the input DataFrame. Default is True.\n\n    Returns:\n    -------\n    pandas.DataFrame\n        DataFrame with jittered coordinates for previously duplicated points.\n\n    Raises:\n    ------\n    ValueError\n        If columns don't exist or jitter amount is invalid.\n    TypeError\n        If input types are incorrect.\n    \"\"\"\n\n    # Input validation\n    if not isinstance(df, pd.DataFrame):\n        raise TypeError(\"Input must be a pandas DataFrame\")\n\n    if not all(col in df.columns for col in columns):\n        raise ValueError(f\"Not all columns {columns} found in DataFrame\")\n\n    # Handle jitter amounts\n    if isinstance(amount, (int, float)):\n        if amount &lt;= 0:\n            raise ValueError(\"Jitter amount must be positive\")\n        jitter_amounts = {col: amount for col in columns}\n    elif isinstance(amount, dict):\n        if not all(col in amount for col in columns):\n            raise ValueError(\"Must specify jitter amount for each column\")\n        if not all(amt &gt; 0 for amt in amount.values()):\n            raise ValueError(\"All jitter amounts must be positive\")\n        jitter_amounts = amount\n    else:\n        raise TypeError(\"amount must be a number or dictionary\")\n\n    # Create copy if requested\n    df_work = df.copy() if copy else df\n\n    # Set random seed if provided\n    if seed is not None:\n        np.random.seed(seed)\n\n    try:\n        # Find duplicated coordinates\n        duplicate_mask = df_work.duplicated(subset=columns, keep=False)\n        n_duplicates = duplicate_mask.sum()\n\n        if n_duplicates &gt; 0:\n            # Add jitter to each column separately\n            for col in columns:\n                jitter = np.random.uniform(\n                    low=-jitter_amounts[col],\n                    high=jitter_amounts[col],\n                    size=n_duplicates,\n                )\n                df_work.loc[duplicate_mask, col] += jitter\n\n            # Validate results (ensure no remaining duplicates)\n            if df_work.duplicated(subset=columns, keep=False).any():\n                # If duplicates remain, recursively add more jitter\n                df_work = add_spatial_jitter(\n                    df_work,\n                    columns=columns,\n                    amount={col: amt * 2 for col, amt in jitter_amounts.items()},\n                    seed=seed,\n                    copy=False,\n                )\n\n        return df_work\n\n    except Exception as e:\n        raise RuntimeError(f\"Error during jittering operation: {str(e)}\")\n</code></pre>"},{"location":"api/processing/#gigaspatial.processing.geo.annotate_with_admin_regions","title":"<code>annotate_with_admin_regions(gdf, country_code, data_store=None, admin_id_column_suffix='_giga')</code>","text":"<p>Annotate a GeoDataFrame with administrative region information.</p> <p>Performs a spatial join between the input points and administrative boundaries at levels 1 and 2, resolving conflicts when points intersect multiple admin regions.</p> <p>Parameters:</p> Name Type Description Default <code>gdf</code> <code>GeoDataFrame</code> <p>GeoDataFrame containing points to annotate</p> required <code>country_code</code> <code>str</code> <p>Country code for administrative boundaries</p> required <code>data_store</code> <code>Optional[DataStore]</code> <p>Optional DataStore for loading admin boundary data</p> <code>None</code> <p>Returns:</p> Type Description <code>GeoDataFrame</code> <p>GeoDataFrame with added administrative region columns</p> Source code in <code>gigaspatial/processing/geo.py</code> <pre><code>def annotate_with_admin_regions(\n    gdf: gpd.GeoDataFrame,\n    country_code: str,\n    data_store: Optional[DataStore] = None,\n    admin_id_column_suffix=\"_giga\",\n) -&gt; gpd.GeoDataFrame:\n    \"\"\"\n    Annotate a GeoDataFrame with administrative region information.\n\n    Performs a spatial join between the input points and administrative boundaries\n    at levels 1 and 2, resolving conflicts when points intersect multiple admin regions.\n\n    Args:\n        gdf: GeoDataFrame containing points to annotate\n        country_code: Country code for administrative boundaries\n        data_store: Optional DataStore for loading admin boundary data\n\n    Returns:\n        GeoDataFrame with added administrative region columns\n    \"\"\"\n\n    if not isinstance(gdf, gpd.GeoDataFrame):\n        raise TypeError(\"gdf must be a GeoDataFrame\")\n\n    if gdf.empty:\n        LOGGER.warning(\"Empty GeoDataFrame provided, returning as-is\")\n        return gdf\n\n    # read country admin data\n    admin1_data = AdminBoundaries.create(\n        country_code=country_code, admin_level=1, data_store=data_store\n    ).to_geodataframe()\n\n    admin1_data.rename(\n        columns={\"id\": f\"admin1_id{admin_id_column_suffix}\", \"name\": \"admin1\"},\n        inplace=True,\n    )\n    admin1_data.drop(columns=[\"name_en\", \"parent_id\", \"country_code\"], inplace=True)\n\n    admin2_data = AdminBoundaries.create(\n        country_code=country_code, admin_level=2, data_store=data_store\n    ).to_geodataframe()\n\n    admin2_data.rename(\n        columns={\n            \"id\": f\"admin2_id{admin_id_column_suffix}\",\n            \"parent_id\": f\"admin1_id{admin_id_column_suffix}\",\n            \"name\": \"admin2\",\n        },\n        inplace=True,\n    )\n    admin2_data.drop(columns=[\"name_en\", \"country_code\"], inplace=True)\n\n    # Join dataframes based on 'admin1_id_giga'\n    admin_data = admin2_data.merge(\n        admin1_data[[f\"admin1_id{admin_id_column_suffix}\", \"admin1\", \"geometry\"]],\n        left_on=f\"admin1_id{admin_id_column_suffix}\",\n        right_on=f\"admin1_id{admin_id_column_suffix}\",\n        how=\"outer\",\n    )\n\n    admin_data[\"geometry\"] = admin_data.apply(\n        lambda x: x.geometry_x if x.geometry_x else x.geometry_y, axis=1\n    )\n\n    admin_data = gpd.GeoDataFrame(\n        admin_data.drop(columns=[\"geometry_x\", \"geometry_y\"]),\n        geometry=\"geometry\",\n        crs=4326,\n    )\n\n    admin_data[\"admin2\"].fillna(\"Unknown\", inplace=True)\n    admin_data[f\"admin2_id{admin_id_column_suffix}\"] = admin_data[\n        f\"admin2_id{admin_id_column_suffix}\"\n    ].replace({np.nan: None})\n\n    if gdf.crs is None:\n        LOGGER.warning(\"Input GeoDataFrame has no CRS, assuming EPSG:4326\")\n        gdf.set_crs(epsg=4326, inplace=True)\n    elif gdf.crs != \"EPSG:4326\":\n        LOGGER.info(f\"Reprojecting from {gdf.crs} to EPSG:4326\")\n        gdf = gdf.to_crs(epsg=4326)\n\n    # spatial join gdf to admins\n    gdf_w_admins = gdf.copy().sjoin(\n        admin_data,\n        how=\"left\",\n        predicate=\"intersects\",\n    )\n\n    # Check for duplicates caused by points intersecting multiple polygons\n    if len(gdf_w_admins) != len(gdf):\n        LOGGER.warning(\n            \"Some points intersect multiple administrative boundaries. Resolving conflicts...\"\n        )\n\n        # Group by original index and select the closest admin area for ties\n        gdf_w_admins[\"distance\"] = gdf_w_admins.apply(\n            lambda row: row.geometry.distance(\n                admin_data.loc[row.index_right, \"geometry\"].centroid\n            ),\n            axis=1,\n        )\n\n        # For points with multiple matches, keep the closest polygon\n        gdf_w_admins = gdf_w_admins.loc[\n            gdf_w_admins.groupby(gdf.index)[\"distance\"].idxmin()\n        ].drop(columns=\"distance\")\n\n    # Drop unnecessary columns and reset the index\n    gdf_w_admins = gdf_w_admins.drop(columns=\"index_right\").reset_index(drop=True)\n\n    return gdf_w_admins\n</code></pre>"},{"location":"api/processing/#gigaspatial.processing.geo.buffer_geodataframe","title":"<code>buffer_geodataframe(gdf, buffer_distance_meters, cap_style='round', copy=True)</code>","text":"<p>Buffers a GeoDataFrame with a given buffer distance in meters.</p> <ul> <li>gdf : geopandas.GeoDataFrame     The GeoDataFrame to be buffered.</li> <li>buffer_distance_meters : float     The buffer distance in meters.</li> <li>cap_style : str, optional     The style of caps. round, flat, square. Default is round.</li> </ul> <ul> <li>geopandas.GeoDataFrame     The buffered GeoDataFrame.</li> </ul> Source code in <code>gigaspatial/processing/geo.py</code> <pre><code>def buffer_geodataframe(\n    gdf: gpd.GeoDataFrame,\n    buffer_distance_meters: float,\n    cap_style: Literal[\"round\", \"square\", \"flat\"] = \"round\",\n    copy=True,\n) -&gt; gpd.GeoDataFrame:\n    \"\"\"\n    Buffers a GeoDataFrame with a given buffer distance in meters.\n\n    Parameters:\n    - gdf : geopandas.GeoDataFrame\n        The GeoDataFrame to be buffered.\n    - buffer_distance_meters : float\n        The buffer distance in meters.\n    - cap_style : str, optional\n        The style of caps. round, flat, square. Default is round.\n\n    Returns:\n    - geopandas.GeoDataFrame\n        The buffered GeoDataFrame.\n    \"\"\"\n\n    # Input validation\n    if not isinstance(gdf, gpd.GeoDataFrame):\n        raise TypeError(\"Input must be a GeoDataFrame\")\n\n    if not isinstance(buffer_distance_meters, (float, int)):\n        raise TypeError(\"Buffer distance must be a number\")\n\n    if cap_style not in [\"round\", \"square\", \"flat\"]:\n        raise ValueError(\"cap_style must be round, flat or square.\")\n\n    if gdf.crs is None:\n        raise ValueError(\"Input GeoDataFrame must have a defined CRS\")\n\n    # Create a copy if requested\n    gdf_work = gdf.copy() if copy else gdf\n\n    # Store input CRS\n    input_crs = gdf_work.crs\n\n    try:\n        # Create a custom UTM CRS based on the calculated UTM zone\n        utm_crs = gdf_work.estimate_utm_crs()\n\n        # Transform to UTM, create buffer, and transform back\n        gdf_work = gdf_work.to_crs(utm_crs)\n        gdf_work[\"geometry\"] = gdf_work[\"geometry\"].buffer(\n            buffer_distance_meters, cap_style=cap_style\n        )\n        gdf_work = gdf_work.to_crs(input_crs)\n\n        return gdf_work\n\n    except Exception as e:\n        raise RuntimeError(f\"Error during buffering operation: {str(e)}\")\n</code></pre>"},{"location":"api/processing/#gigaspatial.processing.geo.convert_to_geodataframe","title":"<code>convert_to_geodataframe(data, lat_col=None, lon_col=None, crs='EPSG:4326')</code>","text":"<p>Convert a pandas DataFrame to a GeoDataFrame, either from latitude/longitude columns or from a WKT geometry column.</p>"},{"location":"api/processing/#gigaspatial.processing.geo.convert_to_geodataframe--parameters","title":"Parameters:","text":"<p>data : pandas.DataFrame     Input DataFrame containing either lat/lon columns or a geometry column. lat_col : str, optional     Name of the latitude column. Default is 'lat'. lon_col : str, optional     Name of the longitude column. Default is 'lon'. crs : str or pyproj.CRS, optional     Coordinate Reference System of the geometry data. Default is 'EPSG:4326'.</p>"},{"location":"api/processing/#gigaspatial.processing.geo.convert_to_geodataframe--returns","title":"Returns:","text":"<p>geopandas.GeoDataFrame     A GeoDataFrame containing the input data with a geometry column.</p>"},{"location":"api/processing/#gigaspatial.processing.geo.convert_to_geodataframe--raises","title":"Raises:","text":"<p>TypeError     If input is not a pandas DataFrame. ValueError     If required columns are missing or contain invalid data.</p> Source code in <code>gigaspatial/processing/geo.py</code> <pre><code>def convert_to_geodataframe(\n    data: pd.DataFrame, lat_col: str = None, lon_col: str = None, crs=\"EPSG:4326\"\n) -&gt; gpd.GeoDataFrame:\n    \"\"\"\n    Convert a pandas DataFrame to a GeoDataFrame, either from latitude/longitude columns\n    or from a WKT geometry column.\n\n    Parameters:\n    ----------\n    data : pandas.DataFrame\n        Input DataFrame containing either lat/lon columns or a geometry column.\n    lat_col : str, optional\n        Name of the latitude column. Default is 'lat'.\n    lon_col : str, optional\n        Name of the longitude column. Default is 'lon'.\n    crs : str or pyproj.CRS, optional\n        Coordinate Reference System of the geometry data. Default is 'EPSG:4326'.\n\n    Returns:\n    -------\n    geopandas.GeoDataFrame\n        A GeoDataFrame containing the input data with a geometry column.\n\n    Raises:\n    ------\n    TypeError\n        If input is not a pandas DataFrame.\n    ValueError\n        If required columns are missing or contain invalid data.\n    \"\"\"\n\n    # Input validation\n    if not isinstance(data, pd.DataFrame):\n        raise TypeError(\"Input 'data' must be a pandas DataFrame\")\n\n    # Create a copy to avoid modifying the input\n    df = data.copy()\n\n    try:\n        if \"geometry\" not in df.columns:\n            # If column names not provided, try to detect them\n            if lat_col is None or lon_col is None:\n                try:\n                    detected_lat, detected_lon = detect_coordinate_columns(df)\n                    lat_col = lat_col or detected_lat\n                    lon_col = lon_col or detected_lon\n                except ValueError as e:\n                    raise ValueError(\n                        f\"Could not automatically detect coordinate columns and no \"\n                        f\"'geometry' column found. Error: {str(e)}\"\n                    )\n\n            # Validate latitude/longitude columns exist\n            if lat_col not in df.columns or lon_col not in df.columns:\n                raise ValueError(\n                    f\"Could not find columns: {lat_col} and/or {lon_col} in the DataFrame\"\n                )\n\n            # Check for missing values\n            if df[lat_col].isna().any() or df[lon_col].isna().any():\n                raise ValueError(\n                    f\"Missing values found in {lat_col} and/or {lon_col} columns\"\n                )\n\n            # Create geometry from lat/lon\n            geometry = gpd.points_from_xy(x=df[lon_col], y=df[lat_col])\n\n        else:\n            # Check if geometry column already contains valid geometries\n            if df[\"geometry\"].apply(lambda x: isinstance(x, base.BaseGeometry)).all():\n                geometry = df[\"geometry\"]\n            elif df[\"geometry\"].apply(lambda x: isinstance(x, str)).all():\n                # Convert WKT strings to geometry objects\n                geometry = df[\"geometry\"].apply(wkt.loads)\n            else:\n                raise ValueError(\n                    \"Invalid geometry format: contains mixed or unsupported types\"\n                )\n\n        # drop the WKT column if conversion was done\n        if (\n            \"geometry\" in df.columns\n            and not df[\"geometry\"]\n            .apply(lambda x: isinstance(x, base.BaseGeometry))\n            .all()\n        ):\n            df = df.drop(columns=[\"geometry\"])\n\n        return gpd.GeoDataFrame(df, geometry=geometry, crs=crs)\n\n    except Exception as e:\n        raise RuntimeError(f\"Error converting to GeoDataFrame: {str(e)}\")\n</code></pre>"},{"location":"api/processing/#gigaspatial.processing.geo.count_points_within_polygons","title":"<code>count_points_within_polygons(base_gdf, count_gdf, base_gdf_key)</code>","text":"<p>Counts the number of points from <code>count_gdf</code> that fall within each polygon in <code>base_gdf</code>.</p>"},{"location":"api/processing/#gigaspatial.processing.geo.count_points_within_polygons--parameters","title":"Parameters:","text":"<p>base_gdf : geopandas.GeoDataFrame     GeoDataFrame containing polygon geometries to count points within. count_gdf : geopandas.GeoDataFrame     GeoDataFrame containing point geometries to be counted. base_gdf_key : str     Column name in <code>base_gdf</code> to use as the key for grouping and merging.</p>"},{"location":"api/processing/#gigaspatial.processing.geo.count_points_within_polygons--returns","title":"Returns:","text":"<p>geopandas.GeoDataFrame     The <code>base_gdf</code> with an additional column containing the count of points within each polygon.</p>"},{"location":"api/processing/#gigaspatial.processing.geo.count_points_within_polygons--raises","title":"Raises:","text":"<p>ValueError     If <code>base_gdf_key</code> is missing in <code>base_gdf</code>.</p> Source code in <code>gigaspatial/processing/geo.py</code> <pre><code>def count_points_within_polygons(base_gdf, count_gdf, base_gdf_key):\n    \"\"\"\n    Counts the number of points from `count_gdf` that fall within each polygon in `base_gdf`.\n\n    Parameters:\n    ----------\n    base_gdf : geopandas.GeoDataFrame\n        GeoDataFrame containing polygon geometries to count points within.\n    count_gdf : geopandas.GeoDataFrame\n        GeoDataFrame containing point geometries to be counted.\n    base_gdf_key : str\n        Column name in `base_gdf` to use as the key for grouping and merging.\n\n    Returns:\n    -------\n    geopandas.GeoDataFrame\n        The `base_gdf` with an additional column containing the count of points within each polygon.\n\n    Raises:\n    ------\n    ValueError\n        If `base_gdf_key` is missing in `base_gdf`.\n    \"\"\"\n    # Validate inputs\n    if base_gdf_key not in base_gdf.columns:\n        raise ValueError(f\"Key column '{base_gdf_key}' not found in `base_gdf`.\")\n\n    # Ensure clean index for the join\n    count_gdf = count_gdf.reset_index()\n\n    # Spatial join: Find points intersecting polygons\n    joined_gdf = gpd.sjoin(\n        base_gdf, count_gdf[[\"geometry\"]], how=\"left\", predicate=\"intersects\"\n    )\n\n    # Count points grouped by the base_gdf_key\n    point_counts = joined_gdf.groupby(base_gdf_key)[\"index_right\"].count()\n    point_counts.name = \"point_count\"\n\n    # Merge point counts back to the base GeoDataFrame\n    result_gdf = base_gdf.merge(point_counts, on=base_gdf_key, how=\"left\")\n\n    # Fill NaN counts with 0 for polygons with no points\n    result_gdf[\"point_count\"] = result_gdf[\"point_count\"].fillna(0).astype(int)\n\n    return result_gdf\n</code></pre>"},{"location":"api/processing/#gigaspatial.processing.geo.detect_coordinate_columns","title":"<code>detect_coordinate_columns(data, lat_keywords=None, lon_keywords=None, case_sensitive=False)</code>","text":"<p>Detect latitude and longitude columns in a DataFrame using keyword matching.</p>"},{"location":"api/processing/#gigaspatial.processing.geo.detect_coordinate_columns--parameters","title":"Parameters:","text":"<p>data : pandas.DataFrame     DataFrame to search for coordinate columns. lat_keywords : list of str, optional     Keywords for identifying latitude columns. If None, uses default keywords. lon_keywords : list of str, optional     Keywords for identifying longitude columns. If None, uses default keywords. case_sensitive : bool, optional     Whether to perform case-sensitive matching. Default is False.</p>"},{"location":"api/processing/#gigaspatial.processing.geo.detect_coordinate_columns--returns","title":"Returns:","text":"<p>tuple[str, str]     Names of detected (latitude, longitude) columns.</p>"},{"location":"api/processing/#gigaspatial.processing.geo.detect_coordinate_columns--raises","title":"Raises:","text":"<p>ValueError     If no unique pair of latitude/longitude columns can be found. TypeError     If input data is not a pandas DataFrame.</p> Source code in <code>gigaspatial/processing/geo.py</code> <pre><code>def detect_coordinate_columns(\n    data, lat_keywords=None, lon_keywords=None, case_sensitive=False\n) -&gt; Tuple[str, str]:\n    \"\"\"\n    Detect latitude and longitude columns in a DataFrame using keyword matching.\n\n    Parameters:\n    ----------\n    data : pandas.DataFrame\n        DataFrame to search for coordinate columns.\n    lat_keywords : list of str, optional\n        Keywords for identifying latitude columns. If None, uses default keywords.\n    lon_keywords : list of str, optional\n        Keywords for identifying longitude columns. If None, uses default keywords.\n    case_sensitive : bool, optional\n        Whether to perform case-sensitive matching. Default is False.\n\n    Returns:\n    -------\n    tuple[str, str]\n        Names of detected (latitude, longitude) columns.\n\n    Raises:\n    ------\n    ValueError\n        If no unique pair of latitude/longitude columns can be found.\n    TypeError\n        If input data is not a pandas DataFrame.\n    \"\"\"\n\n    # Default keywords if none provided\n    default_lat = [\n        \"latitude\",\n        \"lat\",\n        \"y\",\n        \"lat_\",\n        \"lat(s)\",\n        \"_lat\",\n        \"ylat\",\n        \"latitude_y\",\n    ]\n    default_lon = [\n        \"longitude\",\n        \"lon\",\n        \"long\",\n        \"x\",\n        \"lon_\",\n        \"lon(e)\",\n        \"long(e)\",\n        \"_lon\",\n        \"xlon\",\n        \"longitude_x\",\n    ]\n\n    lat_keywords = lat_keywords or default_lat\n    lon_keywords = lon_keywords or default_lon\n\n    # Input validation\n    if not isinstance(data, pd.DataFrame):\n        raise TypeError(\"Input must be a pandas DataFrame\")\n\n    if not data.columns.is_unique:\n        raise ValueError(\"DataFrame contains duplicate column names\")\n\n    def create_pattern(keywords):\n        \"\"\"Create regex pattern from keywords.\"\"\"\n        return \"|\".join(rf\"\\b{re.escape(keyword)}\\b\" for keyword in keywords)\n\n    def find_matching_columns(columns, pattern, case_sensitive) -&gt; List:\n        \"\"\"Find columns matching the pattern.\"\"\"\n        flags = 0 if case_sensitive else re.IGNORECASE\n        return [col for col in columns if re.search(pattern, col, flags=flags)]\n\n    try:\n        # Create patterns\n        lat_pattern = create_pattern(lat_keywords)\n        lon_pattern = create_pattern(lon_keywords)\n\n        # Find matching columns\n        lat_cols = find_matching_columns(data.columns, lat_pattern, case_sensitive)\n        lon_cols = find_matching_columns(data.columns, lon_pattern, case_sensitive)\n\n        # Remove any longitude matches from latitude columns and vice versa\n        lat_cols = [col for col in lat_cols if col not in lon_cols]\n        lon_cols = [col for col in lon_cols if col not in lat_cols]\n\n        # Detailed error messages based on what was found\n        if not lat_cols and not lon_cols:\n            columns_list = \"\\n\".join(f\"- {col}\" for col in data.columns)\n            raise ValueError(\n                f\"No latitude or longitude columns found. Available columns are:\\n{columns_list}\\n\"\n                f\"Consider adding more keywords or checking column names.\"\n            )\n\n        if not lat_cols:\n            found_lons = \", \".join(lon_cols)\n            raise ValueError(\n                f\"Found longitude columns ({found_lons}) but no latitude columns. \"\n                \"Check latitude keywords or column names.\"\n            )\n\n        if not lon_cols:\n            found_lats = \", \".join(lat_cols)\n            raise ValueError(\n                f\"Found latitude columns ({found_lats}) but no longitude columns. \"\n                \"Check longitude keywords or column names.\"\n            )\n\n        if len(lat_cols) &gt; 1 or len(lon_cols) &gt; 1:\n            raise ValueError(\n                f\"Multiple possible coordinate columns found:\\n\"\n                f\"Latitude candidates: {lat_cols}\\n\"\n                f\"Longitude candidates: {lon_cols}\\n\"\n                \"Please specify more precise keywords.\"\n            )\n\n        return lat_cols[0], lon_cols[0]\n\n    except Exception as e:\n        if isinstance(e, ValueError):\n            raise\n        raise RuntimeError(f\"Error detecting coordinate columns: {str(e)}\")\n</code></pre>"},{"location":"api/processing/#gigaspatial.processing.geo.map_points_within_polygons","title":"<code>map_points_within_polygons(base_points_gdf, polygon_gdf)</code>","text":"<p>Maps whether each point in <code>base_points_gdf</code> is within any polygon in <code>polygon_gdf</code>.</p>"},{"location":"api/processing/#gigaspatial.processing.geo.map_points_within_polygons--parameters","title":"Parameters:","text":"<p>base_points_gdf : geopandas.GeoDataFrame     GeoDataFrame containing point geometries to check. polygon_gdf : geopandas.GeoDataFrame     GeoDataFrame containing polygon geometries.</p>"},{"location":"api/processing/#gigaspatial.processing.geo.map_points_within_polygons--returns","title":"Returns:","text":"<p>geopandas.GeoDataFrame     The <code>base_points_gdf</code> with an additional column <code>is_within</code> (True/False).</p>"},{"location":"api/processing/#gigaspatial.processing.geo.map_points_within_polygons--raises","title":"Raises:","text":"<p>ValueError     If the geometries in either GeoDataFrame are invalid or not of the expected type.</p> Source code in <code>gigaspatial/processing/geo.py</code> <pre><code>def map_points_within_polygons(base_points_gdf, polygon_gdf):\n    \"\"\"\n    Maps whether each point in `base_points_gdf` is within any polygon in `polygon_gdf`.\n\n    Parameters:\n    ----------\n    base_points_gdf : geopandas.GeoDataFrame\n        GeoDataFrame containing point geometries to check.\n    polygon_gdf : geopandas.GeoDataFrame\n        GeoDataFrame containing polygon geometries.\n\n    Returns:\n    -------\n    geopandas.GeoDataFrame\n        The `base_points_gdf` with an additional column `is_within` (True/False).\n\n    Raises:\n    ------\n    ValueError\n        If the geometries in either GeoDataFrame are invalid or not of the expected type.\n    \"\"\"\n    # Validate input GeoDataFrames\n    if not all(base_points_gdf.geometry.geom_type == \"Point\"):\n        raise ValueError(\"`base_points_gdf` must contain only Point geometries.\")\n    if not all(polygon_gdf.geometry.geom_type.isin([\"Polygon\", \"MultiPolygon\"])):\n        raise ValueError(\n            \"`polygon_gdf` must contain only Polygon or MultiPolygon geometries.\"\n        )\n\n    if not base_points_gdf.crs == polygon_gdf.crs:\n        raise ValueError(\"CRS of `base_points_gdf` and `polygon_gdf` must match.\")\n\n    # Perform spatial join to check if points fall within any polygon\n    joined_gdf = gpd.sjoin(\n        base_points_gdf, polygon_gdf[[\"geometry\"]], how=\"left\", predicate=\"within\"\n    )\n\n    # Add `is_within` column to base_points_gdf\n    base_points_gdf[\"is_within\"] = base_points_gdf.index.isin(\n        set(joined_gdf.index[~joined_gdf.index_right.isna()])\n    )\n\n    return base_points_gdf\n</code></pre>"},{"location":"api/processing/#gigaspatial.processing.geo.overlay_aggregate_polygon_data","title":"<code>overlay_aggregate_polygon_data(base_gdf, overlay_gdf, overlay_columns, base_gdf_key, overlay_gdf_key=None, agg_func='sum')</code>","text":"<p>Overlay polygon data and aggregate values over the base GeoDataFrame.</p>"},{"location":"api/processing/#gigaspatial.processing.geo.overlay_aggregate_polygon_data--parameters","title":"Parameters:","text":"<p>base_gdf : geopandas.GeoDataFrame     GeoDataFrame representing the base geometries. overlay_gdf : geopandas.GeoDataFrame     GeoDataFrame with polygon geometries to overlay and aggregate. overlay_columns : list of str     Columns in <code>overlay_gdf</code> to aggregate based on overlapping areas. base_gdf_key : str     Column in <code>base_gdf</code> to use as the key for aggregation. overlay_gdf_key : str, optional     Column in <code>overlay_gdf</code> to use as the index for merging. Defaults to the overlay GeoDataFrame's index. agg_func : str, callable, or dict, default=\"sum\"     Aggregation function or dictionary of column-specific aggregation functions.     Examples: \"sum\", \"mean\", \"max\", or {\"column1\": \"mean\", \"column2\": \"sum\"}.</p>"},{"location":"api/processing/#gigaspatial.processing.geo.overlay_aggregate_polygon_data--returns","title":"Returns:","text":"<p>geopandas.GeoDataFrame     Base GeoDataFrame with aggregated values from the overlay.</p>"},{"location":"api/processing/#gigaspatial.processing.geo.overlay_aggregate_polygon_data--raises","title":"Raises:","text":"<p>ValueError     If the overlay GeoDataFrame has duplicate index values or missing columns. RuntimeError     If any geometry operations fail.</p> Source code in <code>gigaspatial/processing/geo.py</code> <pre><code>def overlay_aggregate_polygon_data(\n    base_gdf: gpd.GeoDataFrame,\n    overlay_gdf: gpd.GeoDataFrame,\n    overlay_columns: List[str],\n    base_gdf_key: str,\n    overlay_gdf_key: str = None,\n    agg_func: str = \"sum\",\n) -&gt; gpd.GeoDataFrame:\n    \"\"\"\n    Overlay polygon data and aggregate values over the base GeoDataFrame.\n\n    Parameters:\n    ----------\n    base_gdf : geopandas.GeoDataFrame\n        GeoDataFrame representing the base geometries.\n    overlay_gdf : geopandas.GeoDataFrame\n        GeoDataFrame with polygon geometries to overlay and aggregate.\n    overlay_columns : list of str\n        Columns in `overlay_gdf` to aggregate based on overlapping areas.\n    base_gdf_key : str\n        Column in `base_gdf` to use as the key for aggregation.\n    overlay_gdf_key : str, optional\n        Column in `overlay_gdf` to use as the index for merging. Defaults to the overlay GeoDataFrame's index.\n    agg_func : str, callable, or dict, default=\"sum\"\n        Aggregation function or dictionary of column-specific aggregation functions.\n        Examples: \"sum\", \"mean\", \"max\", or {\"column1\": \"mean\", \"column2\": \"sum\"}.\n\n    Returns:\n    -------\n    geopandas.GeoDataFrame\n        Base GeoDataFrame with aggregated values from the overlay.\n\n    Raises:\n    ------\n    ValueError\n        If the overlay GeoDataFrame has duplicate index values or missing columns.\n    RuntimeError\n        If any geometry operations fail.\n    \"\"\"\n\n    # Validate inputs\n    if not isinstance(base_gdf, gpd.GeoDataFrame) or not isinstance(\n        overlay_gdf, gpd.GeoDataFrame\n    ):\n        raise TypeError(\"Both base_gdf and overlay_gdf must be GeoDataFrames.\")\n    if not set(overlay_columns).issubset(overlay_gdf.columns):\n        missing_cols = set(overlay_columns) - set(overlay_gdf.columns)\n        raise ValueError(f\"Missing columns in overlay_gdf: {missing_cols}\")\n    if overlay_gdf.index.duplicated().any():\n        raise ValueError(\"Overlay GeoDataFrame contains duplicate index values.\")\n    if base_gdf_key not in base_gdf.columns:\n        raise ValueError(f\"base_gdf_key '{base_gdf_key}' not found in base_gdf.\")\n\n    # Set index name for overlay_gdf\n    index_name = (\n        overlay_gdf_key if overlay_gdf_key else overlay_gdf.index.name or \"index\"\n    )\n\n    # Ensure geometries are valid\n    overlay_gdf = overlay_gdf.copy()\n\n    # Perform the spatial overlay\n    try:\n        gdf_overlayed = (\n            overlay_gdf[overlay_columns + [\"geometry\"]]\n            .reset_index()\n            .overlay(base_gdf, how=\"intersection\")\n            .set_index(index_name)\n        )\n    except Exception as e:\n        raise RuntimeError(f\"Error during spatial overlay: {e}\")\n\n    # Compute UTM CRS for accurate area calculations\n    overlay_utm_crs = overlay_gdf.estimate_utm_crs()\n\n    # Calculate pixel areas in overlay_gdf\n    overlay_gdf_utm = overlay_gdf.to_crs(overlay_utm_crs)\n    pixel_area = overlay_gdf_utm.area\n    pixel_area.name = \"pixel_area\"\n\n    # Add pixel area to the overlayed data\n    gdf_overlayed = gdf_overlayed.join(pixel_area, how=\"left\")\n\n    # Compute overlap fraction\n    gdf_overlayed_utm = gdf_overlayed.to_crs(overlay_utm_crs)\n    gdf_overlayed[\"overlap_fraction\"] = (\n        gdf_overlayed_utm.area / gdf_overlayed[\"pixel_area\"]\n    )\n\n    # Adjust overlay column values by overlap fraction\n    gdf_overlayed[overlay_columns] = gdf_overlayed[overlay_columns].mul(\n        gdf_overlayed[\"overlap_fraction\"], axis=0\n    )\n\n    # Validate and apply aggregation\n    if isinstance(agg_func, str) or callable(agg_func):\n        aggregation_dict = {col: agg_func for col in overlay_columns}\n    elif isinstance(agg_func, dict):\n        aggregation_dict = agg_func\n        for col in overlay_columns:\n            if col not in aggregation_dict:\n                raise ValueError(f\"Missing aggregation function for column: {col}\")\n    else:\n        raise ValueError(\n            \"Invalid aggregation function. Must be a string, callable, or dictionary.\"\n        )\n\n    # Aggregate data\n    aggregated = (\n        gdf_overlayed[overlay_columns + [base_gdf_key]]\n        .groupby(base_gdf_key)\n        .agg(aggregation_dict)\n        .reset_index()\n    )\n\n    # Aggregate overlay columns by the base GeoDataFrame key\n    # aggregated = (\n    #    gdf_overlayed.groupby(base_gdf_key)[overlay_columns].sum().reset_index()\n    # )\n\n    # Merge aggregated values back to the base GeoDataFrame\n    result = base_gdf.merge(aggregated, on=base_gdf_key, how=\"left\")\n\n    return result\n</code></pre>"},{"location":"api/processing/#gigaspatial.processing.geo.simplify_geometries","title":"<code>simplify_geometries(gdf, tolerance=0.01, preserve_topology=True, geometry_column='geometry')</code>","text":"<p>Simplify geometries in a GeoDataFrame to reduce file size and improve visualization performance.</p>"},{"location":"api/processing/#gigaspatial.processing.geo.simplify_geometries--parameters","title":"Parameters","text":"<p>gdf : geopandas.GeoDataFrame     GeoDataFrame containing geometries to simplify. tolerance : float, optional     Tolerance for simplification. Larger values simplify more but reduce detail (default is 0.01). preserve_topology : bool, optional     Whether to preserve topology while simplifying. Preserving topology prevents invalid geometries (default is True). geometry_column : str, optional     Name of the column containing geometries (default is \"geometry\").</p>"},{"location":"api/processing/#gigaspatial.processing.geo.simplify_geometries--returns","title":"Returns","text":"<p>geopandas.GeoDataFrame     A new GeoDataFrame with simplified geometries.</p>"},{"location":"api/processing/#gigaspatial.processing.geo.simplify_geometries--raises","title":"Raises","text":"<p>ValueError     If the specified geometry column does not exist or contains invalid geometries. TypeError     If the geometry column does not contain valid geometries.</p>"},{"location":"api/processing/#gigaspatial.processing.geo.simplify_geometries--examples","title":"Examples","text":"<p>Simplify geometries in a GeoDataFrame:</p> <p>simplified_gdf = simplify_geometries(gdf, tolerance=0.05)</p> Source code in <code>gigaspatial/processing/geo.py</code> <pre><code>def simplify_geometries(\n    gdf: gpd.GeoDataFrame,\n    tolerance: float = 0.01,\n    preserve_topology: bool = True,\n    geometry_column: str = \"geometry\",\n) -&gt; gpd.GeoDataFrame:\n    \"\"\"\n    Simplify geometries in a GeoDataFrame to reduce file size and improve visualization performance.\n\n    Parameters\n    ----------\n    gdf : geopandas.GeoDataFrame\n        GeoDataFrame containing geometries to simplify.\n    tolerance : float, optional\n        Tolerance for simplification. Larger values simplify more but reduce detail (default is 0.01).\n    preserve_topology : bool, optional\n        Whether to preserve topology while simplifying. Preserving topology prevents invalid geometries (default is True).\n    geometry_column : str, optional\n        Name of the column containing geometries (default is \"geometry\").\n\n    Returns\n    -------\n    geopandas.GeoDataFrame\n        A new GeoDataFrame with simplified geometries.\n\n    Raises\n    ------\n    ValueError\n        If the specified geometry column does not exist or contains invalid geometries.\n    TypeError\n        If the geometry column does not contain valid geometries.\n\n    Examples\n    --------\n    Simplify geometries in a GeoDataFrame:\n    &gt;&gt;&gt; simplified_gdf = simplify_geometries(gdf, tolerance=0.05)\n    \"\"\"\n\n    # Check if the specified geometry column exists\n    if geometry_column not in gdf.columns:\n        raise ValueError(\n            f\"Geometry column '{geometry_column}' not found in the GeoDataFrame.\"\n        )\n\n    # Check if the specified column contains geometries\n    if not gpd.GeoSeries(gdf[geometry_column]).is_valid.all():\n        raise TypeError(\n            f\"Geometry column '{geometry_column}' contains invalid geometries.\"\n        )\n\n    # Simplify geometries (non-destructive)\n    gdf_simplified = gdf.copy()\n    gdf_simplified[geometry_column] = gdf_simplified[geometry_column].simplify(\n        tolerance=tolerance, preserve_topology=preserve_topology\n    )\n\n    return gdf_simplified\n</code></pre>"},{"location":"api/processing/#gigaspatial.processing.sat_images","title":"<code>sat_images</code>","text":""},{"location":"api/processing/#gigaspatial.processing.sat_images.calculate_pixels_at_location","title":"<code>calculate_pixels_at_location(gdf, resolution, bbox_size=300, crs='EPSG:3857')</code>","text":"<p>Calculates the number of pixels required to cover a given bounding box around a geographic coordinate, given a resolution in meters per pixel.</p> <p>Parameters:</p> Name Type Description Default <code>gdf</code> <p>a geodataframe with Point geometries that are geographic coordinates</p> required <code>resolution</code> <code>float</code> <p>Desired resolution (meters per pixel).</p> required <code>bbox_size</code> <code>float</code> <p>Bounding box size in meters (default 300m x 300m).</p> <code>300</code> <code>crs</code> <code>str</code> <p>Target projection (default is EPSG:3857).</p> <code>'EPSG:3857'</code> <p>Returns:</p> Name Type Description <code>int</code> <p>Number of pixels per side (width and height).</p> Source code in <code>gigaspatial/processing/sat_images.py</code> <pre><code>def calculate_pixels_at_location(gdf, resolution, bbox_size=300, crs=\"EPSG:3857\"):\n    \"\"\"\n    Calculates the number of pixels required to cover a given bounding box\n    around a geographic coordinate, given a resolution in meters per pixel.\n\n    Parameters:\n        gdf: a geodataframe with Point geometries that are geographic coordinates\n        resolution (float): Desired resolution (meters per pixel).\n        bbox_size (float): Bounding box size in meters (default 300m x 300m).\n        crs (str): Target projection (default is EPSG:3857).\n\n    Returns:\n        int: Number of pixels per side (width and height).\n    \"\"\"\n\n    # Calculate avg lat and lon\n    lon = gdf.geometry.x.mean()\n    lat = gdf.geometry.y.mean()\n\n    # Define projections\n    wgs84 = pyproj.CRS(\"EPSG:4326\")  # Geographic coordinate system\n    mercator = pyproj.CRS(crs)  # Target CRS (EPSG:3857)\n\n    # Transform the center coordinate to EPSG:3857\n    transformer = pyproj.Transformer.from_crs(wgs84, mercator, always_xy=True)\n    x, y = transformer.transform(lon, lat)\n\n    # Calculate scale factor (distortion) at given latitude\n    scale_factor = np.cos(np.radians(lat))  # Mercator scale correction\n\n    # Adjust the effective resolution\n    effective_resolution = resolution * scale_factor\n\n    # Compute number of pixels per side\n    pixels = bbox_size / effective_resolution\n    return int(round(pixels))\n</code></pre>"},{"location":"api/processing/#gigaspatial.processing.tif_processor","title":"<code>tif_processor</code>","text":""},{"location":"api/processing/#gigaspatial.processing.tif_processor.TifProcessor","title":"<code>TifProcessor</code>","text":"<p>A class to handle tif data processing, supporting single-band, RGB, and RGBA data.</p> Source code in <code>gigaspatial/processing/tif_processor.py</code> <pre><code>@dataclass(config=ConfigDict(arbitrary_types_allowed=True))\nclass TifProcessor:\n    \"\"\"\n    A class to handle tif data processing, supporting single-band, RGB, and RGBA data.\n    \"\"\"\n\n    dataset_path: Union[Path, str]\n    data_store: Optional[DataStore] = None\n    mode: Literal[\"single\", \"rgb\", \"rgba\"] = \"single\"\n\n    def __post_init__(self):\n        \"\"\"Validate inputs and set up logging.\"\"\"\n        self.data_store = self.data_store or LocalDataStore()\n        self.logger = config.get_logger(__name__)\n        self._cache = {}\n\n        if not self.data_store.file_exists(self.dataset_path):\n            raise FileNotFoundError(f\"Dataset not found at {self.dataset_path}\")\n\n        self._load_metadata()\n\n        if self.mode == \"rgba\" and self.count != 4:\n            raise ValueError(\"RGBA mode requires a 4-band TIF file\")\n        if self.mode == \"rgb\" and self.count != 3:\n            raise ValueError(\"RGB mode requires a 3-band TIF file\")\n\n    @contextmanager\n    def open_dataset(self):\n        \"\"\"Context manager for accessing the dataset\"\"\"\n        with self.data_store.open(self.dataset_path, \"rb\") as f:\n            with rasterio.MemoryFile(f.read()) as memfile:\n                with memfile.open() as src:\n                    yield src\n\n    def _load_metadata(self):\n        \"\"\"Load metadata from the TIF file if not already cached\"\"\"\n        if not self._cache:\n            with self.open_dataset() as src:\n                self._cache[\"transform\"] = src.transform\n                self._cache[\"crs\"] = src.crs.to_string()\n                self._cache[\"bounds\"] = src.bounds\n                self._cache[\"width\"] = src.width\n                self._cache[\"height\"] = src.height\n                self._cache[\"resolution\"] = (abs(src.transform.a), abs(src.transform.e))\n                self._cache[\"x_transform\"] = src.transform.a\n                self._cache[\"y_transform\"] = src.transform.e\n                self._cache[\"nodata\"] = src.nodata\n                self._cache[\"count\"] = src.count\n                self._cache[\"dtype\"] = src.dtypes[0]\n\n    @property\n    def transform(self):\n        \"\"\"Get the transform from the TIF file\"\"\"\n        return self._cache[\"transform\"]\n\n    @property\n    def crs(self):\n        \"\"\"Get the coordinate reference system from the TIF file\"\"\"\n        return self._cache[\"crs\"]\n\n    @property\n    def bounds(self):\n        \"\"\"Get the bounds of the TIF file\"\"\"\n        return self._cache[\"bounds\"]\n\n    @property\n    def resolution(self) -&gt; Tuple[float, float]:\n        \"\"\"Get the x and y resolution (pixel width and height or pixel size) from the TIF file\"\"\"\n        return self._cache[\"resolution\"]\n\n    @property\n    def x_transform(self) -&gt; float:\n        \"\"\"Get the x transform from the TIF file\"\"\"\n        return self._cache[\"x_transform\"]\n\n    @property\n    def y_transform(self) -&gt; float:\n        \"\"\"Get the y transform from the TIF file\"\"\"\n        return self._cache[\"y_transform\"]\n\n    @property\n    def count(self) -&gt; int:\n        \"\"\"Get the band count from the TIF file\"\"\"\n        return self._cache[\"count\"]\n\n    @property\n    def nodata(self) -&gt; int:\n        \"\"\"Get the value representing no data in the rasters\"\"\"\n        return self._cache[\"nodata\"]\n\n    @property\n    def tabular(self) -&gt; pd.DataFrame:\n        \"\"\"Get the data from the TIF file\"\"\"\n        if not hasattr(self, \"_tabular\"):\n            try:\n                if self.mode == \"single\":\n                    self._tabular = self._to_band_dataframe(\n                        drop_nodata=True, drop_values=[]\n                    )\n                elif self.mode == \"rgb\":\n                    self._tabular = self._to_rgb_dataframe(drop_nodata=True)\n                elif self.mode == \"rgba\":\n                    self._tabular = self._to_rgba_dataframe(drop_transparent=True)\n            except Exception as e:\n                raise ValueError(\n                    f\"Failed to process TIF file in mode '{self.mode}'. \"\n                    f\"Please ensure the file is valid and matches the selected mode. \"\n                    f\"Original error: {str(e)}\"\n                )\n\n        return self._tabular\n\n    def to_dataframe(self) -&gt; pd.DataFrame:\n        return self.tabular\n\n    def get_zoned_geodataframe(self) -&gt; gpd.GeoDataFrame:\n        \"\"\"\n        Convert the processed TIF data into a GeoDataFrame, where each row represents a pixel zone.\n        Each zone is defined by its bounding box, based on pixel resolution and coordinates.\n        \"\"\"\n        self.logger.info(\"Converting data to GeoDataFrame with zones...\")\n\n        df = self.tabular\n\n        x_res, y_res = self.resolution\n\n        # create bounding box for each pixel\n        geometries = [\n            box(lon - x_res / 2, lat - y_res / 2, lon + x_res / 2, lat + y_res / 2)\n            for lon, lat in zip(df[\"lon\"], df[\"lat\"])\n        ]\n\n        gdf = gpd.GeoDataFrame(df, geometry=geometries, crs=self.crs)\n\n        self.logger.info(\"Conversion to GeoDataFrame complete!\")\n        return gdf\n\n    def sample_by_coordinates(\n        self, coordinate_list: List[Tuple[float, float]]\n    ) -&gt; Union[np.ndarray, dict]:\n        self.logger.info(\"Sampling raster values at the coordinates...\")\n\n        with self.open_dataset() as src:\n            if self.mode == \"rgba\":\n                if self.count != 4:\n                    raise ValueError(\"RGBA mode requires a 4-band TIF file\")\n\n                rgba_values = {\"red\": [], \"green\": [], \"blue\": [], \"alpha\": []}\n\n                for band_idx, color in enumerate([\"red\", \"green\", \"blue\", \"alpha\"], 1):\n                    rgba_values[color] = [\n                        vals[0]\n                        for vals in src.sample(coordinate_list, indexes=band_idx)\n                    ]\n\n                return rgba_values\n\n            elif self.mode == \"rgb\":\n                if self.count != 3:\n                    raise ValueError(\"RGB mode requires a 3-band TIF file\")\n\n                rgb_values = {\"red\": [], \"green\": [], \"blue\": []}\n\n                for band_idx, color in enumerate([\"red\", \"green\", \"blue\"], 1):\n                    rgb_values[color] = [\n                        vals[0]\n                        for vals in src.sample(coordinate_list, indexes=band_idx)\n                    ]\n\n                return rgb_values\n            else:\n                if src.count != 1:\n                    raise ValueError(\"Single band mode requires a 1-band TIF file\")\n                return np.array([vals[0] for vals in src.sample(coordinate_list)])\n\n    def sample_by_polygons(\n        self, polygon_list: List[Union[Polygon, MultiPolygon]], stat: str = \"mean\"\n    ) -&gt; gpd.GeoDataFrame:\n        \"\"\"\n        Sample raster values within each polygon of a GeoDataFrame.\n\n        Parameters:\n            polygon_list: List of polygon geometries (can include MultiPolygons).\n            stat (str): Aggregation statistic to compute within each polygon.\n                        Options: \"mean\", \"median\", \"sum\", \"min\", \"max\".\n\n        Returns:\n            GeoDataFrame: A copy of the input GeoDataFrame with an added column\n                        containing sampled raster values.\n        \"\"\"\n        self.logger.info(\"Sampling raster values within polygons...\")\n\n        with self.open_dataset() as src:\n            results = []\n\n            for geom in polygon_list:\n                if geom.is_empty:\n                    results.append(np.nan)\n                    continue\n\n                try:\n                    # Mask the raster with the polygon\n                    out_image, _ = mask(src, [geom], crop=True)\n\n                    # Flatten the raster values and remove NoData values\n                    values = out_image[out_image != src.nodata].flatten()\n\n                    # Compute the desired statistic\n                    if len(values) == 0:\n                        results.append(np.nan)\n                    else:\n                        if stat == \"mean\":\n                            results.append(np.mean(values))\n                        elif stat == \"median\":\n                            results.append(np.median(values))\n                        elif stat == \"sum\":\n                            results.append(np.sum(values))\n                        elif stat == \"min\":\n                            results.append(np.min(values))\n                        elif stat == \"max\":\n                            results.append(np.max(values))\n                        else:\n                            raise ValueError(f\"Unknown statistic: {stat}\")\n\n                except Exception as e:\n                    self.logger.error(f\"Error processing polygon: {e}\")\n                    results.append(np.nan)\n\n        return np.array(results)\n\n    def _to_rgba_dataframe(self, drop_transparent: bool = False) -&gt; pd.DataFrame:\n        \"\"\"\n        Convert RGBA TIF to DataFrame with separate columns for R, G, B, A values.\n        \"\"\"\n        self.logger.info(\"Processing RGBA dataset...\")\n\n        with self.open_dataset() as src:\n            if self.count != 4:\n                raise ValueError(\"RGBA mode requires a 4-band TIF file\")\n\n            # Read all four bands\n            red, green, blue, alpha = src.read()\n\n            x_coords, y_coords = self._get_pixel_coordinates()\n\n            if drop_transparent:\n                mask = alpha &gt; 0\n                red = np.extract(mask, red)\n                green = np.extract(mask, green)\n                blue = np.extract(mask, blue)\n                alpha = np.extract(mask, alpha)\n                lons = np.extract(mask, x_coords)\n                lats = np.extract(mask, y_coords)\n            else:\n                lons = x_coords.flatten()\n                lats = y_coords.flatten()\n                red = red.flatten()\n                green = green.flatten()\n                blue = blue.flatten()\n                alpha = alpha.flatten()\n\n            # Create DataFrame with RGBA values\n            data = pd.DataFrame(\n                {\n                    \"lon\": lons,\n                    \"lat\": lats,\n                    \"red\": red,\n                    \"green\": green,\n                    \"blue\": blue,\n                    \"alpha\": alpha,\n                }\n            )\n\n            # Normalize alpha values if they're not in [0, 1] range\n            if data[\"alpha\"].max() &gt; 1:\n                data[\"alpha\"] = data[\"alpha\"] / data[\"alpha\"].max()\n\n        self.logger.info(\"RGBA dataset is processed!\")\n        return data\n\n    def _to_rgb_dataframe(self, drop_nodata: bool = True) -&gt; pd.DataFrame:\n        \"\"\"Convert RGB TIF to DataFrame with separate columns for R, G, B values.\"\"\"\n        if self.mode != \"rgb\":\n            raise ValueError(\"Use appropriate method for current mode\")\n\n        self.logger.info(\"Processing RGB dataset...\")\n\n        with self.open_dataset() as src:\n            if self.count != 3:\n                raise ValueError(\"RGB mode requires a 3-band TIF file\")\n\n            # Read all three bands\n            red, green, blue = src.read()\n\n            x_coords, y_coords = self._get_pixel_coordinates()\n\n            if drop_nodata:\n                nodata_value = src.nodata\n                if nodata_value is not None:\n                    mask = ~(\n                        (red == nodata_value)\n                        | (green == nodata_value)\n                        | (blue == nodata_value)\n                    )\n                    red = np.extract(mask, red)\n                    green = np.extract(mask, green)\n                    blue = np.extract(mask, blue)\n                    lons = np.extract(mask, x_coords)\n                    lats = np.extract(mask, y_coords)\n                else:\n                    lons = x_coords.flatten()\n                    lats = y_coords.flatten()\n                    red = red.flatten()\n                    green = green.flatten()\n                    blue = blue.flatten()\n            else:\n                lons = x_coords.flatten()\n                lats = y_coords.flatten()\n                red = red.flatten()\n                green = green.flatten()\n                blue = blue.flatten()\n\n            data = pd.DataFrame(\n                {\n                    \"lon\": lons,\n                    \"lat\": lats,\n                    \"red\": red,\n                    \"green\": green,\n                    \"blue\": blue,\n                }\n            )\n\n        self.logger.info(\"RGB dataset is processed!\")\n        return data\n\n    def _to_band_dataframe(\n        self, band_number: int = 1, drop_nodata: bool = True, drop_values: list = []\n    ) -&gt; pd.DataFrame:\n        \"\"\"Process single-band TIF to DataFrame.\"\"\"\n        if self.mode != \"single\":\n            raise ValueError(\"Use appropriate method for current mode\")\n\n        self.logger.info(\"Processing single-band dataset...\")\n\n        if band_number &lt;= 0 or band_number &gt; self.count:\n            self.logger.error(\n                f\"Error: Band number {band_number} is out of range. The file has {self.count} bands.\"\n            )\n            return None\n\n        with self.open_dataset() as src:\n\n            band = src.read(band_number)\n\n            x_coords, y_coords = self._get_pixel_coordinates()\n\n            values_to_mask = []\n            if drop_nodata:\n                nodata_value = src.nodata\n                if nodata_value is not None:\n                    values_to_mask.append(nodata_value)\n\n            if drop_values:\n                values_to_mask.extend(drop_values)\n\n            if values_to_mask:\n                data_mask = ~np.isin(band, values_to_mask)\n                pixel_values = np.extract(data_mask, band)\n                lons = np.extract(data_mask, x_coords)\n                lats = np.extract(data_mask, y_coords)\n            else:\n                pixel_values = band.flatten()\n                lons = x_coords.flatten()\n                lats = y_coords.flatten()\n\n            data = pd.DataFrame({\"lon\": lons, \"lat\": lats, \"pixel_value\": pixel_values})\n\n        self.logger.info(\"Dataset is processed!\")\n        return data\n\n    def _get_pixel_coordinates(self):\n        \"\"\"Helper method to generate coordinate arrays for all pixels\"\"\"\n        if \"pixel_coords\" not in self._cache:\n            # use cached values\n            bounds = self._cache[\"bounds\"]\n            width = self._cache[\"width\"]\n            height = self._cache[\"height\"]\n            pixel_size_x = self._cache[\"x_transform\"]\n            pixel_size_y = self._cache[\"y_transform\"]\n\n            self._cache[\"pixel_coords\"] = np.meshgrid(\n                np.linspace(\n                    bounds.left + pixel_size_x / 2,\n                    bounds.right - pixel_size_x / 2,\n                    width,\n                ),\n                np.linspace(\n                    bounds.top + pixel_size_y / 2,\n                    bounds.bottom - pixel_size_y / 2,\n                    height,\n                ),\n            )\n\n        return self._cache[\"pixel_coords\"]\n</code></pre>"},{"location":"api/processing/#gigaspatial.processing.tif_processor.TifProcessor.bounds","title":"<code>bounds</code>  <code>property</code>","text":"<p>Get the bounds of the TIF file</p>"},{"location":"api/processing/#gigaspatial.processing.tif_processor.TifProcessor.count","title":"<code>count: int</code>  <code>property</code>","text":"<p>Get the band count from the TIF file</p>"},{"location":"api/processing/#gigaspatial.processing.tif_processor.TifProcessor.crs","title":"<code>crs</code>  <code>property</code>","text":"<p>Get the coordinate reference system from the TIF file</p>"},{"location":"api/processing/#gigaspatial.processing.tif_processor.TifProcessor.nodata","title":"<code>nodata: int</code>  <code>property</code>","text":"<p>Get the value representing no data in the rasters</p>"},{"location":"api/processing/#gigaspatial.processing.tif_processor.TifProcessor.resolution","title":"<code>resolution: Tuple[float, float]</code>  <code>property</code>","text":"<p>Get the x and y resolution (pixel width and height or pixel size) from the TIF file</p>"},{"location":"api/processing/#gigaspatial.processing.tif_processor.TifProcessor.tabular","title":"<code>tabular: pd.DataFrame</code>  <code>property</code>","text":"<p>Get the data from the TIF file</p>"},{"location":"api/processing/#gigaspatial.processing.tif_processor.TifProcessor.transform","title":"<code>transform</code>  <code>property</code>","text":"<p>Get the transform from the TIF file</p>"},{"location":"api/processing/#gigaspatial.processing.tif_processor.TifProcessor.x_transform","title":"<code>x_transform: float</code>  <code>property</code>","text":"<p>Get the x transform from the TIF file</p>"},{"location":"api/processing/#gigaspatial.processing.tif_processor.TifProcessor.y_transform","title":"<code>y_transform: float</code>  <code>property</code>","text":"<p>Get the y transform from the TIF file</p>"},{"location":"api/processing/#gigaspatial.processing.tif_processor.TifProcessor.__post_init__","title":"<code>__post_init__()</code>","text":"<p>Validate inputs and set up logging.</p> Source code in <code>gigaspatial/processing/tif_processor.py</code> <pre><code>def __post_init__(self):\n    \"\"\"Validate inputs and set up logging.\"\"\"\n    self.data_store = self.data_store or LocalDataStore()\n    self.logger = config.get_logger(__name__)\n    self._cache = {}\n\n    if not self.data_store.file_exists(self.dataset_path):\n        raise FileNotFoundError(f\"Dataset not found at {self.dataset_path}\")\n\n    self._load_metadata()\n\n    if self.mode == \"rgba\" and self.count != 4:\n        raise ValueError(\"RGBA mode requires a 4-band TIF file\")\n    if self.mode == \"rgb\" and self.count != 3:\n        raise ValueError(\"RGB mode requires a 3-band TIF file\")\n</code></pre>"},{"location":"api/processing/#gigaspatial.processing.tif_processor.TifProcessor.get_zoned_geodataframe","title":"<code>get_zoned_geodataframe()</code>","text":"<p>Convert the processed TIF data into a GeoDataFrame, where each row represents a pixel zone. Each zone is defined by its bounding box, based on pixel resolution and coordinates.</p> Source code in <code>gigaspatial/processing/tif_processor.py</code> <pre><code>def get_zoned_geodataframe(self) -&gt; gpd.GeoDataFrame:\n    \"\"\"\n    Convert the processed TIF data into a GeoDataFrame, where each row represents a pixel zone.\n    Each zone is defined by its bounding box, based on pixel resolution and coordinates.\n    \"\"\"\n    self.logger.info(\"Converting data to GeoDataFrame with zones...\")\n\n    df = self.tabular\n\n    x_res, y_res = self.resolution\n\n    # create bounding box for each pixel\n    geometries = [\n        box(lon - x_res / 2, lat - y_res / 2, lon + x_res / 2, lat + y_res / 2)\n        for lon, lat in zip(df[\"lon\"], df[\"lat\"])\n    ]\n\n    gdf = gpd.GeoDataFrame(df, geometry=geometries, crs=self.crs)\n\n    self.logger.info(\"Conversion to GeoDataFrame complete!\")\n    return gdf\n</code></pre>"},{"location":"api/processing/#gigaspatial.processing.tif_processor.TifProcessor.open_dataset","title":"<code>open_dataset()</code>","text":"<p>Context manager for accessing the dataset</p> Source code in <code>gigaspatial/processing/tif_processor.py</code> <pre><code>@contextmanager\ndef open_dataset(self):\n    \"\"\"Context manager for accessing the dataset\"\"\"\n    with self.data_store.open(self.dataset_path, \"rb\") as f:\n        with rasterio.MemoryFile(f.read()) as memfile:\n            with memfile.open() as src:\n                yield src\n</code></pre>"},{"location":"api/processing/#gigaspatial.processing.tif_processor.TifProcessor.sample_by_polygons","title":"<code>sample_by_polygons(polygon_list, stat='mean')</code>","text":"<p>Sample raster values within each polygon of a GeoDataFrame.</p> <p>Parameters:</p> Name Type Description Default <code>polygon_list</code> <code>List[Union[Polygon, MultiPolygon]]</code> <p>List of polygon geometries (can include MultiPolygons).</p> required <code>stat</code> <code>str</code> <p>Aggregation statistic to compute within each polygon.         Options: \"mean\", \"median\", \"sum\", \"min\", \"max\".</p> <code>'mean'</code> <p>Returns:</p> Name Type Description <code>GeoDataFrame</code> <code>GeoDataFrame</code> <p>A copy of the input GeoDataFrame with an added column         containing sampled raster values.</p> Source code in <code>gigaspatial/processing/tif_processor.py</code> <pre><code>def sample_by_polygons(\n    self, polygon_list: List[Union[Polygon, MultiPolygon]], stat: str = \"mean\"\n) -&gt; gpd.GeoDataFrame:\n    \"\"\"\n    Sample raster values within each polygon of a GeoDataFrame.\n\n    Parameters:\n        polygon_list: List of polygon geometries (can include MultiPolygons).\n        stat (str): Aggregation statistic to compute within each polygon.\n                    Options: \"mean\", \"median\", \"sum\", \"min\", \"max\".\n\n    Returns:\n        GeoDataFrame: A copy of the input GeoDataFrame with an added column\n                    containing sampled raster values.\n    \"\"\"\n    self.logger.info(\"Sampling raster values within polygons...\")\n\n    with self.open_dataset() as src:\n        results = []\n\n        for geom in polygon_list:\n            if geom.is_empty:\n                results.append(np.nan)\n                continue\n\n            try:\n                # Mask the raster with the polygon\n                out_image, _ = mask(src, [geom], crop=True)\n\n                # Flatten the raster values and remove NoData values\n                values = out_image[out_image != src.nodata].flatten()\n\n                # Compute the desired statistic\n                if len(values) == 0:\n                    results.append(np.nan)\n                else:\n                    if stat == \"mean\":\n                        results.append(np.mean(values))\n                    elif stat == \"median\":\n                        results.append(np.median(values))\n                    elif stat == \"sum\":\n                        results.append(np.sum(values))\n                    elif stat == \"min\":\n                        results.append(np.min(values))\n                    elif stat == \"max\":\n                        results.append(np.max(values))\n                    else:\n                        raise ValueError(f\"Unknown statistic: {stat}\")\n\n            except Exception as e:\n                self.logger.error(f\"Error processing polygon: {e}\")\n                results.append(np.nan)\n\n    return np.array(results)\n</code></pre>"},{"location":"api/processing/#gigaspatial.processing.tif_processor.sample_multiple_tifs_by_coordinates","title":"<code>sample_multiple_tifs_by_coordinates(tif_processors, coordinate_list)</code>","text":"<p>Sample raster values from multiple TIFF files for given coordinates.</p> <p>Parameters: - tif_processors: List of TifProcessor instances. - coordinate_list: List of (x, y) coordinates.</p> <p>Returns: - A NumPy array of sampled values, taking the first non-nodata value encountered.</p> Source code in <code>gigaspatial/processing/tif_processor.py</code> <pre><code>def sample_multiple_tifs_by_coordinates(\n    tif_processors: List[TifProcessor], coordinate_list: List[Tuple[float, float]]\n):\n    \"\"\"\n    Sample raster values from multiple TIFF files for given coordinates.\n\n    Parameters:\n    - tif_processors: List of TifProcessor instances.\n    - coordinate_list: List of (x, y) coordinates.\n\n    Returns:\n    - A NumPy array of sampled values, taking the first non-nodata value encountered.\n    \"\"\"\n    sampled_values = np.full(len(coordinate_list), np.nan, dtype=np.float32)\n\n    for tp in tif_processors:\n        values = tp.sample_by_coordinates(coordinate_list=coordinate_list)\n\n        if tp.nodata is not None:\n            mask = (np.isnan(sampled_values)) &amp; (\n                values != tp.nodata\n            )  # Replace only NaNs\n        else:\n            mask = np.isnan(sampled_values)  # No explicit nodata, replace all NaNs\n\n        sampled_values[mask] = values[mask]  # Update only missing values\n\n    return sampled_values\n</code></pre>"},{"location":"api/processing/#gigaspatial.processing.tif_processor.sample_multiple_tifs_by_polygons","title":"<code>sample_multiple_tifs_by_polygons(tif_processors, polygon_list, stat='mean')</code>","text":"<p>Sample raster values from multiple TIFF files for polygons in a list and join the results.</p> <p>Parameters: - tif_processors: List of TifProcessor instances. - polygon_list: List of polygon geometries (can include MultiPolygons). - stat: Aggregation statistic to compute within each polygon (mean, median, sum, min, max).</p> <p>Returns: - A NumPy array of sampled values, taking the first non-nodata value encountered.</p> Source code in <code>gigaspatial/processing/tif_processor.py</code> <pre><code>def sample_multiple_tifs_by_polygons(\n    tif_processors: List[TifProcessor],\n    polygon_list: List[Union[Polygon, MultiPolygon]],\n    stat: str = \"mean\",\n):\n    \"\"\"\n    Sample raster values from multiple TIFF files for polygons in a list and join the results.\n\n    Parameters:\n    - tif_processors: List of TifProcessor instances.\n    - polygon_list: List of polygon geometries (can include MultiPolygons).\n    - stat: Aggregation statistic to compute within each polygon (mean, median, sum, min, max).\n\n    Returns:\n    - A NumPy array of sampled values, taking the first non-nodata value encountered.\n    \"\"\"\n    sampled_values = np.full(len(polygon_list), np.nan, dtype=np.float32)\n\n    for tp in tif_processors:\n        values = tp.sample_by_polygons(polygon_list=polygon_list, stat=stat)\n\n        mask = np.isnan(sampled_values)  # replace all NaNs\n\n        sampled_values[mask] = values[mask]  # Update only values with samapled value\n\n    return sampled_values\n</code></pre>"},{"location":"api/processing/#gigaspatial.processing.utils","title":"<code>utils</code>","text":""},{"location":"api/processing/#gigaspatial.processing.utils.assign_id","title":"<code>assign_id(df, required_columns, id_column='id')</code>","text":"<p>Generate IDs for any entity type in a pandas DataFrame.</p> <p>Parameters:</p> Name Type Description Default <code>df</code> <code>DataFrame</code> <p>Input DataFrame containing entity data</p> required <code>required_columns</code> <code>List[str]</code> <p>List of column names required for ID generation</p> required <code>id_column</code> <code>str</code> <p>Name for the id column that will be generated</p> <code>'id'</code> <p>Returns:</p> Type Description <code>DataFrame</code> <p>pd.DataFrame: DataFrame with generated id column</p> Source code in <code>gigaspatial/processing/utils.py</code> <pre><code>def assign_id(\n    df: pd.DataFrame, required_columns: List[str], id_column: str = \"id\"\n) -&gt; pd.DataFrame:\n    \"\"\"\n    Generate IDs for any entity type in a pandas DataFrame.\n\n    Args:\n        df (pd.DataFrame): Input DataFrame containing entity data\n        required_columns (List[str]): List of column names required for ID generation\n        id_column (str): Name for the id column that will be generated\n\n    Returns:\n        pd.DataFrame: DataFrame with generated id column\n    \"\"\"\n    # Create a copy to avoid modifying the original DataFrame\n    df = df.copy()\n\n    # Check if ID column exists, if not create it with None values\n    if id_column not in df.columns:\n        df[id_column] = None\n\n    # Check required columns exist\n    if not all(col in df.columns for col in required_columns):\n        return df\n\n    # Create identifier concat for UUID generation\n    df[\"identifier_concat\"] = (\n        df[required_columns].astype(str).fillna(\"\").agg(\"\".join, axis=1)\n    )\n\n    # Generate UUIDs only where all required fields are present and no existing ID\n    mask = df[id_column].isna()\n    for col in required_columns:\n        mask &amp;= df[col].notna()\n\n    # Apply UUID generation only where mask is True\n    df.loc[mask, id_column] = df.loc[mask, \"identifier_concat\"].apply(\n        lambda x: str(uuid.uuid3(uuid.NAMESPACE_DNS, x))\n    )\n\n    # Drop temporary column\n    df = df.drop(columns=[\"identifier_concat\"])\n\n    return df\n</code></pre>"},{"location":"examples/","title":"Examples","text":"<p>Welcome to the examples section of the <code>gigaspatial</code> package. Here, you\u2019ll find practical examples that demonstrate how to use the package for various geospatial data tasks.</p>"},{"location":"examples/#getting-started","title":"Getting Started","text":"<p>If you\u2019re new to <code>gigaspatial</code>, start with the Quick Start Guide to learn the basics.</p>"},{"location":"examples/#example-categories","title":"Example Categories","text":""},{"location":"examples/#1-data-downloading","title":"1. Data Downloading","text":"<p>Learn how to download geospatial data from various sources.</p> <ul> <li>Downloading GHSL Data</li> <li>Fetching OSM Data</li> </ul>"},{"location":"examples/#2-data-processing","title":"2. Data Processing","text":"<p>Explore examples of processing geospatial data, such as GeoTIFF files.</p> <ul> <li>Processing GeoTIFF Files</li> </ul>"},{"location":"examples/#3-data-storage-in-progress","title":"3. Data Storage (In Progress)","text":"<p>Discover how to store and retrieve geospatial data in different formats.</p> <ul> <li>Saving Data to GeoJSON (Coming Soon)</li> <li>Storing Data in a Database (Coming Soon)</li> </ul>"},{"location":"examples/#4-data-visualization-in-progress","title":"4. Data Visualization (In Progress)","text":"<p>Learn how to visualize geospatial data using popular libraries.</p> <ul> <li>Plotting Data with Matplotlib (Coming Soon)</li> <li>Creating Interactive Maps with Folium (Coming Soon)</li> </ul>"},{"location":"examples/#5-advanced-use-cases-in-progress","title":"5. Advanced Use Cases (In Progress)","text":"<p>Explore advanced examples that combine multiple functionalities.</p> <ul> <li>Building a Geospatial Pipeline (Coming Soon)</li> <li>Integrating with External APIs (Coming Soon)</li> </ul>"},{"location":"examples/#contributing-examples","title":"Contributing Examples","text":"<p>If you\u2019d like to contribute your own examples, please follow the Contributing Guidelines.</p>"},{"location":"examples/#feedback","title":"Feedback","text":"<p>If you have suggestions for additional examples or encounter any issues, feel free to open an issue or join our Discord community.</p>"},{"location":"examples/advanced/","title":"Advanced Examples","text":"<p>It's in development.</p>"},{"location":"examples/basic/","title":"Data Handler Examples","text":"<p>This guide provides examples of how to use various data handlers in GigaSpatial to access and process different types of spatial data.</p>"},{"location":"examples/basic/#population-data-worldpop","title":"Population Data (WorldPop)","text":"<pre><code>from gigaspatial.handlers.worldpop import WorldPopHandler\n\n# Get population data for a specific country and year\nconfig = {\n    \"country_code\": \"KEN\",\n    \"year\": 2020,\n}\n\n# Initialize the WorldPop handler\nworldpop = WorldPopDownloader(config = config)\npath_to_data = worldpop.download_dataset()\n</code></pre>"},{"location":"examples/basic/#building-footprints","title":"Building Footprints","text":""},{"location":"examples/basic/#google-open-buildings","title":"Google Open Buildings","text":"<pre><code>from gigaspatial.handlers.google_open_buildings import GoogleOpenBuildingsDownloader, GoogleOpenBuildingsMapper\nfrom gigaspatial.handlers.boundaries import AdminBoundaries\nfrom gigaspatial.core.io.local_data_store import LocalDataStore\nimport geopandas as gpd\n\n# Initialize downloader\ndata_store = LocalDataStore()\ndownloader = GoogleOpenBuildingsDownloader(data_store=data_store)\n\n# Example 1: Estimate download size for a given country\ncountry_code = \"KEN\"  # Kenya\ngdf_admin0 = AdminBoundaries.create(country_code=country_code, admin_level=0).to_geodataframe()\nestimated_size = downloader.get_download_size_estimate(gdf_admin0)\nprint(f\"Estimated download size for {country_code}: {estimated_size:.2f} MB\")\n\n# Example 2: Download buildings data for a country\nfile_paths = downloader.download_by_country(country_code, data_type=\"polygons\")\nprint(f\"Downloaded files: {file_paths}\")\n\n# Example 3: Download buildings data for specific points\npoints_gdf = gpd.GeoDataFrame(\n    {\"geometry\": [gpd.points_from_xy([36.8219], [-1.2921])]}, crs=\"EPSG:4326\"\n)  # Nairobi, Kenya\nfile_paths = downloader.download_by_points(points_gdf, data_type=\"points\")\nprint(f\"Downloaded files: {file_paths}\")\n\n# Example 4: Load downloaded data and map nearest buildings\nmapper = GoogleOpenBuildingsMapper()\n\ntiles_gdf = downloader._get_intersecting_tiles(gdf_admin0)\npolygon_data = mapper.load_data(tiles_gdf, data_type=\"polygons\")\nprint(f\"Loaded {len(polygon_data)} building polygons\")\n\n# Example 5: Find the nearest building for a given point\nnearest_buildings = mapper.map_nearest_building(points_gdf)\nprint(nearest_buildings)\n</code></pre>"},{"location":"examples/basic/#microsoft-global-buildings","title":"Microsoft Global Buildings","text":"<pre><code>from gigaspatial.handlers.microsoft_global_buildings import MSBuildingsDownloader\n\n# Initialize the handler\nmgb = MSBuildingsDownloader()\n\npoints = [(1.25214, 5.5124), (3.45234, 12.51232)]\n\n# Get building footprints\nlist_of_paths = mgb.download_by_points(\n    points=points\n)\n</code></pre>"},{"location":"examples/basic/#satellite-imagery","title":"Satellite Imagery","text":""},{"location":"examples/basic/#maxar-imagery","title":"Maxar Imagery","text":"<pre><code>from gigaspatial.handlers.maxar_image import MaxarImageHandler\n\n# Initialize woith default config which reads credentials config from your environment\nmaxar = MaxarImageDownloader()\n\n# Download imagery\nmaxar.download_images_by_coordinates(\n    data=coordinates,\n    res_meters_pixel=0.6,\n    output_dir=\"bronze/maxar\",\n    bbox_size = 300.0,\n    image_prefix = \"maxar_\"\n)\n</code></pre>"},{"location":"examples/basic/#mapbox-imagery","title":"Mapbox Imagery","text":"<pre><code>from gigaspatial.handlers.mapbox_image import MapboxImageDownloader\n\n# Initialize with your access token or config will be read from your environment\nmapbox = MapboxImageDownloader(access_token=\"your_access_token\", style_id=\"mapbox/satellite-v9\")\n\n# Get satellite imagery\nmapbox.download_images_by_coordinates(\n    data=coordinates,\n    res_meters_pixel=300.0,\n    output_dir=\"bronze/mapbox\",\n    image_size=(256,256),\n    image_prefix=\"mapbox_\"\n)\n</code></pre>"},{"location":"examples/basic/#internet-speed-data-ookla","title":"Internet Speed Data (Ookla)","text":"<pre><code>from gigaspatial.core.io.local_data_store import LocalDataStore\nfrom gigaspatial.handlers.ookla_speedtest import (\n    OoklaSpeedtestTileConfig, CountryOoklaTiles\n)\n\n# Initialize OoklaSpeedtestTileConfig for a specific quarter and year\nookla_config = OoklaSpeedtestTileConfig(\n    service_type=\"fixed\", year=2023, quarter=3, data_store=LocalDataStore())\n\n# Download and read the Ookla tile data\ndf = ookla_config.read_tile()\nprint(df.head())  # Display the first few rows of the dataset\n\n# Generate country-specific Ookla tiles\ncountry_ookla_tiles = CountryOoklaTiles.from_country(\"KEN\", ookla_config)\n\n# Convert to DataFrame and display\ncountry_df = country_ookla_tiles.to_dataframe()\nprint(country_df.head())\n\n# Convert to GeoDataFrame and display\ncountry_gdf = country_ookla_tiles.to_geodataframe()\nprint(country_gdf.head())\n</code></pre>"},{"location":"examples/basic/#administrative-boundaries","title":"Administrative Boundaries","text":"<pre><code>from gigaspatial.handlers.boundaries import AdminBoundaries\n\n# Load level-1 administrative boundaries for Kenya\nadmin_boundaries = AdminBoundaries.create(country_code=\"KEN\", admin_level=1)\n\n# Convert to a GeoDataFrame\ngdf = admin_boundaries.to_geodataframe()\n</code></pre>"},{"location":"examples/basic/#openstreetmap-data","title":"OpenStreetMap Data","text":"<pre><code>from gigaspatial.handlers.osm import OSMAmenityFetcher\n\n# Example 1: Fetching school amenities in Kenya\nfetcher = OSMAmenityFetcher(country_iso2=\"KE\", amenity_types=[\"school\"])\nschools_df = fetcher.get_locations()\nprint(schools_df.head())\n\n# Example 2: Fetching hospital and clinic amenities in Tanzania\nfetcher = OSMAmenityFetcher(country_iso2=\"TZ\", amenity_types=[\"hospital\", \"clinic\"])\nhealthcare_df = fetcher.get_locations()\nprint(healthcare_df.head())\n\n# Example 3: Fetching restaurant amenities in Ghana since 2020\nfetcher = OSMAmenityFetcher(country_iso2=\"GH\", amenity_types=[\"restaurant\"])\nrestaurants_df = fetcher.get_locations(since_year=2020)\nprint(restaurants_df.head())\n</code></pre>"},{"location":"examples/basic/#overture-places","title":"Overture Places","text":"<pre><code>from gigaspatial.handlers.overture import OvertureAmenityFetcher\nfrom shapely.geometry import Polygon\nimport geopandas as gpd\n\n# Initialize the fetcher with country ISO3 code and amenity types\nfetcher = OvertureAmenityFetcher(country_iso3='KEN', amenity_types=['school', 'hospital'])\n\n# Example 1: Fetching all amenities for a given release\ngdf_all = fetcher.get_locations(release='2024-01-01')\nprint(gdf_all.head())\n\n# Example 2: Fetching amenities within a specific bounding polygon\npolygon = Polygon([(36.8, -1.3), (36.9, -1.3), (36.9, -1.2), (36.8, -1.2), (36.8, -1.3)])\ngdf_filtered = fetcher.get_locations(release='2024-01-01', geometry=polygon)\nprint(gdf_filtered.head())\n</code></pre>"},{"location":"examples/basic/#combining-multiple-data-sources","title":"Combining Multiple Data Sources","text":"<p>Here's an example of how to combine multiple data sources for analysis:</p> <pre><code># Initialize handlers\nbuildings = GoogleOpenBuildingsHandler()\npopulation = WorldPopHandler()\nboundaries = BoundariesHandler()\n\n# Get administrative boundaries\nadmin_areas = boundaries.get_admin_boundaries(\n    country_code=\"KEN\",\n    admin_level=2\n)\n\n# Get building footprints and population data\nfor area in admin_areas.itertuples():\n    # Get buildings\n    area_buildings = buildings.get_buildings(\n        geometry=area.geometry\n    )\n\n    # Get population\n    area_population = population.get_population_stats(\n        boundaries=area.geometry,\n        year=2020\n    )\n\n    # Calculate metrics\n    building_density = len(area_buildings) / area.geometry.area\n    population_density = area_population['total_population'] / area.geometry.area\n\n    # Your analysis here...\n</code></pre>"},{"location":"examples/basic/#best-practices","title":"Best Practices","text":"<ol> <li>Cache downloaded data when possible</li> <li>Use appropriate spatial indices for large datasets</li> <li>Handle errors and edge cases gracefully</li> <li>Consider memory usage when working with large areas</li> </ol>"},{"location":"examples/basic/#additional-resources","title":"Additional Resources","text":"<ul> <li>Check the API Reference for detailed method documentation</li> <li>See Advanced Features for more complex usage</li> <li>Review the Contributing Guide if you want to add new handlers</li> </ul>"},{"location":"examples/use-cases/","title":"Use Cases","text":"<p>It's in development.</p>"},{"location":"examples/downloading/ghsl/","title":"Downloading GHSL Data","text":"<p>This example demonstrates how to download data from the Global Human Settlement Layer (GHSL) using the <code>GHSLDataDownloader</code> class.</p>"},{"location":"examples/downloading/ghsl/#prerequisites","title":"Prerequisites","text":"<p>Ensure you have installed the <code>gigaspatial</code> package and set up the necessary configuration. Follow the Installation Guide if you haven\u2019t already.</p>"},{"location":"examples/downloading/ghsl/#example-code","title":"Example Code","text":"<pre><code>from gigaspatial.handlers.ghsl import GHSLDataDownloader\n\n# Initialize the downloader\ndownloader = GHSLDataDownloader({\n    \"product\": \"GHS_BUILT_S\",\n    \"year\": 2020,\n    \"resolution\": 100,\n    \"coord_system\": \"WGS84\"\n})\n\n# Download data for a specific country\ncountry_code = \"TUR\"\ndownloader.download_by_country(country_code)\n</code></pre>"},{"location":"examples/downloading/ghsl/#explanation","title":"Explanation","text":"<ul> <li>GHSLDataDownloader: This class handles the downloading of GHSL data.</li> <li>download_by_country: This method downloads data for a specific country.</li> </ul>"},{"location":"examples/downloading/ghsl/#next-steps","title":"Next Steps","text":"<p>Once the data is downloaded, you can process it using the Processing Examples.</p> <p>Back to Examples</p>"},{"location":"examples/downloading/osm/","title":"Downloading OSM Data","text":"<p>This example demonstrates how to fetch and process OpenStreetMap (OSM) data using the <code>OSMLocationFetcher</code> class.</p>"},{"location":"examples/downloading/osm/#prerequisites","title":"Prerequisites","text":"<p>Ensure you have installed the <code>gigaspatial</code> package and set up the necessary configuration. Follow the Installation Guide if you haven\u2019t already.</p>"},{"location":"examples/downloading/osm/#example-code","title":"Example Code","text":"<pre><code>from gigaspatial.handlers.osm import OSMLocationFetcher\n\n# Initialize the fetcher\nfetcher = OSMLocationFetcher(\n    country=\"Spain\",\n    location_types=[\"amenity\", \"building\", \"shop\"]\n)\n\n# Fetch and process OSM locations\nlocations = fetcher.fetch_locations(since_year=2020, handle_duplicates=\"combine\")\nprint(locations.head())\n</code></pre>"},{"location":"examples/downloading/osm/#explanation","title":"Explanation","text":"<ul> <li>OSMLocationFetcher: This class fetches and processes location data from OpenStreetMap.</li> <li>fetch_locations: This method fetches and processes OSM data based on the specified criteria.</li> </ul>"},{"location":"examples/downloading/osm/#next-steps","title":"Next Steps","text":"<p>Once the data is fetched, you can process it using the Processing Examples.</p> <p>Back to Examples</p>"},{"location":"examples/processing/tif/","title":"Processing GeoTIFF Files","text":"<p>This example demonstrates how to process GeoTIFF files using the <code>TifProcessor</code> class.</p>"},{"location":"examples/processing/tif/#prerequisites","title":"Prerequisites","text":"<p>Ensure you have installed the <code>gigaspatial</code> package and set up the necessary configuration. Follow the Installation Guide if you haven\u2019t already.</p>"},{"location":"examples/processing/tif/#example-code","title":"Example Code","text":"<pre><code>from gigaspatial.processing.tif_processor import TifProcessor\n\n# Initialize the processor\nprocessor = TifProcessor(\"/path/to/ghsl/data/ghsl_data.tif\")\n\n# Process the GeoTIFF file\nprocessed_data = processor.to_dataframe()\nprint(processed_data.head())\n</code></pre>"},{"location":"examples/processing/tif/#explanation","title":"Explanation","text":"<ul> <li>TifProcessor: This class processes GeoTIFF files and extracts relevant data.</li> <li>process: This method processes the GeoTIFF file and returns the data as a NumPy array.</li> </ul>"},{"location":"examples/processing/tif/#next-steps","title":"Next Steps","text":"<p>Once the data is processed, you can store it using the Storage Examples.</p> <p>Back to Examples</p>"},{"location":"getting-started/installation/","title":"Installation Guide","text":"<p>This guide will walk you through the steps to install the <code>gigaspatial</code> package on your system. The package is compatible with Python 3.7 and above.</p>"},{"location":"getting-started/installation/#prerequisites","title":"Prerequisites","text":"<p>Before installing <code>gigaspatial</code>, ensure you have Python installed on your system. You can check your Python version by running:</p> <pre><code>python --version\n</code></pre> <p>If Python is not installed, you can download it from the official Python website.</p>"},{"location":"getting-started/installation/#installing-via-pip","title":"Installing via pip","text":"<p>You can install the <code>gigaspatial</code> package directly from the source using <code>pip</code>. Follow these steps:</p> <ol> <li> <p>Clone the Repository (if you haven't already):    <pre><code>git clone https://github.com/unicef/giga-spatial.git\ncd giga-spatial\n</code></pre></p> </li> <li> <p>Install the Package:    Run the following command in your terminal to install the package:    <pre><code>pip install .\n</code></pre></p> </li> </ol> <p>This command will install <code>gigaspatial</code> along with its dependencies.</p>"},{"location":"getting-started/installation/#installing-in-development-mode","title":"Installing in Development Mode","text":"<p>If you plan to contribute to the package or modify the source code, you can install it in development mode. This allows you to make changes to the code without reinstalling the package. To install in development mode, run:</p> <pre><code>pip install -e .\n</code></pre>"},{"location":"getting-started/installation/#installing-dependencies","title":"Installing Dependencies","text":"<p>The package dependencies are automatically installed when you install <code>gigaspatial</code>. However, if you need to install them manually, you can use:</p> <pre><code>pip install -r requirements.txt\n</code></pre>"},{"location":"getting-started/installation/#verifying-the-installation","title":"Verifying the Installation","text":"<p>After installation, you can verify that the package is installed correctly by running:</p> <pre><code>python -c \"import gigaspatial; print(gigaspatial.__version__)\"\n</code></pre> <p>This should print the version of the installed package.</p>"},{"location":"getting-started/installation/#troubleshooting","title":"Troubleshooting","text":"<p>If you encounter any issues during installation, consider the following:</p> <ul> <li> <p>Ensure <code>pip</code> is up-to-date:   <pre><code>pip install --upgrade pip\n</code></pre></p> </li> <li> <p>Check for conflicting dependencies: If you have other Python packages installed that might conflict with <code>gigaspatial</code>, consider using a virtual environment.</p> </li> <li> <p>Use a Virtual Environment: To avoid conflicts with other Python packages, you can create a virtual environment:   <pre><code>python -m venv venv\nsource venv/bin/activate  # On Windows, use `venv\\Scripts\\activate`\npip install .\n</code></pre></p> </li> </ul>"},{"location":"getting-started/installation/#next-steps","title":"Next Steps","text":"<p>Once the installation is complete, you can proceed to the Quick Start Guide to begin using the <code>gigaspatial</code> package.</p>"},{"location":"getting-started/quickstart/","title":"Quick Start Guide","text":"<p>This guide will walk you through the basic usage of the <code>gigaspatial</code> package. By the end of this guide, you will be able to download, process, and store geospatial data using the package.</p>"},{"location":"getting-started/quickstart/#prerequisites","title":"Prerequisites","text":"<p>Before you begin, ensure that you have installed the <code>gigaspatial</code> package. If you haven't installed it yet, follow the Installation Guide.</p>"},{"location":"getting-started/quickstart/#importing-the-package","title":"Importing the Package","text":"<p>Start by importing the <code>gigaspatial</code> package:</p> <pre><code>import gigaspatial as gs\n</code></pre>"},{"location":"getting-started/quickstart/#setting-up-configuration","title":"Setting Up Configuration","text":"<p>The <code>gigaspatial</code> package uses a configuration file (<code>config.py</code>) to manage paths, API keys, and other settings. You can customize the configuration as needed.</p>"},{"location":"getting-started/quickstart/#using-environment-variables","title":"Using Environment Variables","text":"<p>The package can read configuration settings from an environment file (e.g., <code>.env</code>). Here's an example of how to set up the <code>.env</code> file based on the <code>env_sample</code>:</p> <pre><code># Paths for different data types\nBRONZE_DIR=/path/to/your/bronze_tier_data\nSILVER_DIR=/path/to/your/silver_tier_data\nGOLD_DIR=/path/to/your/gold_tier_data\nVIEWS_DIR=/path/to/your/views_data\nADMIN_BOUNDARIES_DIR=/path/to/your/admin_boundaries_data\n\n# API keys and tokens\nMAPBOX_ACCESS_TOKEN=your_mapbox_token_here\nMAXAR_USERNAME=your_maxar_username_here\nMAXAR_PASSWORD=your_maxar_password_here\nMAXAR_CONNECTION_STRING=your_maxar_key_here\n</code></pre> <p>The <code>config.py</code> file will automatically read these environment variables and set the paths and keys accordingly.</p>"},{"location":"getting-started/quickstart/#setting-paths-manually","title":"Setting Paths Manually","text":"<p>You can also set paths manually in your code:</p> <pre><code>from gigaspatial.config import config\n\n# Example: Setting custom data storage paths\nconfig.set_path(\"bronze\", \"/path/to/your/bronze_tier_data\")\nconfig.set_path(\"gold\", \"/path/to/your/gold_tier_data\")\nconfig.set_path(\"views\", \"/path/to/your/views_data\")\n</code></pre> <p>API keys and tokens should be set through environment variables.</p>"},{"location":"getting-started/quickstart/#downloading-geospatial-data","title":"Downloading Geospatial Data","text":"<p>To download geospatial data, you can use the <code>GHSLDataDownloader</code> class from the <code>handlers</code> module. Here's an example of downloading data for a specific country:</p> <pre><code>from gigaspatial.handlers.ghsl import GHSLDataDownloader\n\n# Initialize the downloader\ndownloader = GHSLDataDownloader({\n    \"product\": \"GHS_BUILT_S\",\n    \"year\": 2020,\n    \"resolution\": 100,\n    \"coord_system\": \"WGS84\"\n})\n\n# Download data for a specific country\ncountry_code = \"TUR\" \ndownloader.download_by_country(country_code)\n</code></pre>"},{"location":"getting-started/quickstart/#processing-geospatial-data","title":"Processing Geospatial Data","text":"<p>Once the data is downloaded, you can process it using the <code>TifProcessor</code> class from the <code>processing</code> module. Here's an example of processing GHSL data:</p> <pre><code>from gigaspatial.processing.tif_processor import TifProcessor\n\n# Initialize the processor\nprocessor = TifProcessor(dataset_path=\"/path/to/ghsl/data/ghsl_data.tif\", data_store=None, mode=\"single\")\n\n# Process the GHSL data\nprocessed_data = processor.to_dataframe()\nprint(processed_data.head())\n</code></pre>"},{"location":"getting-started/quickstart/#storing-geospatial-data","title":"Storing Geospatial Data","text":"<p>You can store the processed data in various formats using the <code>DataStore</code> class from the <code>core.io</code> module. Here's an example of saving data to a parquet file:</p> <pre><code>from gigaspatial.core.io.local_data_store import LocalDataStore\n\n# Initialize the data store\ndata_store = LocalDataStore()\n\n# Save the processed data to a parquet file\nwith data_store.open(\"/path/to/your/output/processed_data.parquet\", \"rb\") as f:\n    processed_data.to_parquet(f)\n</code></pre> <p>If your dataset is already a <code>pandas.DataFrame</code> or <code>geopandas.GeoDataFrame</code>, <code>write_dataset</code> method from the <code>core.io.writers</code> module can be used to write the dataset in various formats. </p> <pre><code>from gigaspatial.core.io.writers import write_dataset\n\n# Save the processed data to a GeoJSON file\nwrite_dataset(data=processed_data, data_store=data_store, path=\"/path/to/your/output/processed_data.geojson\")\n</code></pre>"},{"location":"getting-started/quickstart/#visualizing-geospatial-data","title":"Visualizing Geospatial Data","text":"<p>To visualize the geospatial data, you can use libraries like <code>geopandas</code> and <code>matplotlib</code>. Here's an example of plotting the processed data on a map:</p> <pre><code>import geopandas as gpd\nimport matplotlib.pyplot as plt\n\n# Load the GeoJSON file\ngdf = gpd.read_file(\"/path/to/your/output/processed_data.geojson\")\n\n# Plot the data\ngdf.plot()\nplt.show()\n</code></pre> <p><code>geopandas.GeoDataFrame.explore</code> can also be used to visualise the data on interactive map based on <code>GeoPandas</code> and <code>folium/leaflet.js</code>: <pre><code># Visualize the data\ngdf.explore(\"population\", cmap=\"Blues\")\n</code></pre></p>"},{"location":"getting-started/quickstart/#next-steps","title":"Next Steps","text":"<p>Now that you have a basic understanding of how to use the <code>gigaspatial</code> package, you can explore more advanced features and configurations. Check out the User Guide for detailed documentation and examples.</p>"},{"location":"getting-started/quickstart/#additional-resources","title":"Additional Resources","text":"<ul> <li>API Documentation: Detailed documentation of all classes and functions.</li> <li>Examples: Real-world examples and use cases.</li> <li>Changelog: Information about the latest updates and changes.</li> </ul>"},{"location":"user-guide/","title":"User Guide","text":"<p>Welcome to the User Guide for the <code>gigaspatial</code> package. This guide provides detailed documentation on how to use the package for various geospatial data tasks.</p>"},{"location":"user-guide/#getting-started","title":"Getting Started","text":"<p>If you\u2019re new to <code>gigaspatial</code>, start with the Quick Start Guide to learn the basics.</p>"},{"location":"user-guide/#core-concepts","title":"Core Concepts","text":""},{"location":"user-guide/#1-configuration","title":"1. Configuration","text":"<p>Learn how to configure the package, including setting paths, API keys, and other settings.</p> <ul> <li>Configuration Overview</li> <li>Using Environment Variables</li> <li>Setting Paths and Keys Manually</li> </ul>"},{"location":"user-guide/#additional-resources","title":"Additional Resources","text":"<ul> <li>Examples: Real-world examples and use cases.</li> <li>API Reference: Detailed documentation of all classes and functions.</li> <li>Changelog: Information about the latest updates and changes.</li> <li>Contributing: Guidelines for contributing to the project.</li> </ul>"},{"location":"user-guide/#support","title":"Support","text":"<p>If you encounter any issues or have questions, feel free to open an issue or join our Discord community.</p>"},{"location":"user-guide/configuration/","title":"Configuration","text":"<p>The <code>gigaspatial</code> package uses a configuration file (<code>config.py</code>) to manage paths, API keys, and other settings. This guide explains how to configure the package to suit your needs.</p>"},{"location":"user-guide/configuration/#using-environment-variables","title":"Using Environment Variables","text":"<p>The package can read configuration settings from an environment file (e.g., <code>.env</code>). Here\u2019s an example of how to set up the <code>.env</code> file based on the <code>env_sample</code>:</p> <pre><code># Paths for different data types\nBRONZE_DIR=/path/to/your/bronze_tier_data\nSILVER_DIR=/path/to/your/silver_tier_data\nGOLD_DIR=/path/to/your/gold_tier_data\nVIEWS_DIR=/path/to/your/views_data\nADMIN_BOUNDARIES_DIR=/path/to/your/admin_boundaries_data\n\n# API keys and tokens\nMAPBOX_ACCESS_TOKEN=your_mapbox_token_here\nMAXAR_USERNAME=your_maxar_username_here\nMAXAR_PASSWORD=your_maxar_password_here\nMAXAR_CONNECTION_STRING=your_maxar_key_here\n</code></pre> <p>The <code>config.py</code> file will automatically read these environment variables and set the paths and keys accordingly.</p>"},{"location":"user-guide/configuration/#setting-paths-manually","title":"Setting Paths Manually","text":"<p>You can also set paths and keys manually in your code. Here\u2019s an example:</p> <pre><code>from gigaspatial.config import config\n\n# Example: Setting custom data storage paths\nconfig.set_path(\"bronze\", \"/path/to/your/bronze_tier_data\")\nconfig.set_path(\"gold\", \"/path/to/your/gold_tier_data\")\nconfig.set_path(\"views\", \"/path/to/your/views_data\")\n</code></pre> <p>API keys and tokens should be set through environment variables.</p>"},{"location":"user-guide/configuration/#verifying-the-configuration","title":"Verifying the Configuration","text":"<p>After setting up the configuration, you can verify it by printing the current settings:</p> <pre><code>from gigaspatial.config import config\n\n# Print all configuration settings\nprint(config)\n</code></pre>"},{"location":"user-guide/configuration/#troubleshooting","title":"Troubleshooting","text":"<p>If you encounter issues with the configuration, consider the following:</p> <ul> <li>Ensure <code>.env</code> File Exists: Make sure the <code>.env</code> file is in the root directory of your project.</li> <li>Check Environment Variables: Verify that the environment variables are correctly set in the <code>.env</code> file.</li> <li>Use Absolute Paths: When setting paths manually, use absolute paths to avoid issues with relative paths.</li> </ul>"},{"location":"user-guide/configuration/#next-steps","title":"Next Steps","text":"<p>Once the configuration is set up, you can proceed to the Data Handling Guide to start using the <code>gigaspatial</code> package.</p> <p>Back to User Guide</p>"}]}