{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Welcome to GigaSpatial","text":"<p>GigaSpatial is a powerful Python package designed for spatial data analysis and processing, providing efficient tools and utilities for handling geographic information systems (GIS) data.</p>"},{"location":"#features","title":"Features","text":"<ul> <li>Efficient Data Processing: Fast and memory-efficient processing of large spatial datasets</li> <li>Multiple Format Support: Support for various spatial data formats</li> <li>Advanced Analysis Tools: Comprehensive set of spatial analysis tools</li> <li>Easy Integration: Seamless integration with popular GIS and data science libraries</li> <li>Scalable Solutions: Designed to handle both small and large-scale spatial data processing tasks</li> </ul>"},{"location":"#quick-links","title":"Quick Links","text":"<ul> <li>Installation Guide</li> <li>Quick Start Tutorial</li> <li>API Reference</li> <li>Example Gallery</li> </ul>"},{"location":"#getting-help","title":"Getting Help","text":"<p>If you need help using GigaSpatial, please check out our:</p> <ul> <li>User Guide for detailed usage instructions</li> <li>API Reference for detailed function and class documentation</li> <li>GitHub Issues for bug reports and feature requests</li> <li>Contributing Guide for guidelines on how to contribute to the project</li> </ul>"},{"location":"#license","title":"License","text":"<p>GigaSpatial is released under the AGPL-3.0 License. See the LICENSE file for more details. </p>"},{"location":"changelog/","title":"Changelog","text":""},{"location":"changelog/#unreleased","title":"[Unreleased]","text":""},{"location":"changelog/#changed","title":"Changed","text":"<ul> <li>Simplified import structure: all handlers now available directly from <code>gigaspatial.handlers</code></li> <li>Updated <code>LocalDataStore</code> import path to <code>gigaspatial.core.io</code></li> <li>Standardized handler method names across the codebase</li> <li>Updated documentation and examples to reflect current API</li> </ul>"},{"location":"changelog/#added","title":"Added","text":"<ul> <li>New <code>PoiViewGenerator</code> for mapping nearest buildings to points</li> <li>Enhanced building data handling with improved point-to-building mapping</li> </ul>"},{"location":"changelog/#fixed","title":"Fixed","text":"<ul> <li>Import path inconsistencies</li> <li>Documentation and installation instructions</li> </ul>"},{"location":"changelog/#deprecated","title":"Deprecated","text":"<ul> <li>Direct imports from individual handler modules</li> <li>Old downloader classes in favor of unified handlers</li> </ul>"},{"location":"contributing/","title":"Contribution Guidelines","text":"<p>Thank you for considering contributing to Giga! We value your input and aim to make the contribution process as accessible and transparent as possible. Whether you're interested in reporting bugs, discussing code, submitting fixes, proposing features, becoming a maintainer, or engaging with the Giga community, we welcome your involvement.</p>"},{"location":"contributing/#how-to-contribute-to-our-giga-project","title":"How to Contribute to our Giga Project?","text":"<ol> <li>Familiarize Yourself: Before contributing, familiarize yourself with the project by reviewing the README, code of conduct, and existing issues or pull requests.</li> <li>Issues and Feature Requests: Check the issue tracker for existing issues or create a new one to report bugs, suggest improvements, or propose new features.</li> <li>Fork and Branch: Fork the repository and create a branch for your contribution. Branch names should be descriptive (e.g., feature/add-new-functionality, bugfix/issue-description).</li> <li>Code Changes: Make changes or additions following our coding conventions and style guide. Ensure to write clear commit messages that explain the purpose of each commit.</li> <li>Testing: If applicable, include tests for the changes made to ensure code reliability. Ensure existing tests pass.</li> <li>Documentation: Update relevant documentation, including README files or any other necessary guides.</li> <li>Pull Request: Open a pull request (PR) against the main branch. Clearly describe the changes introduced, referencing any related issues.</li> </ol>"},{"location":"contributing/#report-a-bug-or-suggestion","title":"Report a Bug or Suggestion","text":"<ul> <li>Bug Reports: Help us understand and address issues by submitting detailed bug reports via GitHub issues. Include as many relevant details as possible in the provided template to expedite resolutions.</li> <li>Suggestions: Share your ideas, feedback, or stay updated on Giga by joining our Discord channel.</li> </ul>"},{"location":"contributing/#making-changes-and-pull-requests","title":"Making Changes and Pull Requests","text":"<p>To contribute code changes:</p> <ol> <li>Fork the repository and create a new branch for your contribution  <p><code>git checkout -b 'my-contribution'</code>.</p> </li> <li>Make your changes on the created branch.</li> <li>Commit with clear messages describing the updates.</li> <li>Submit a pull request in the main repository, ensuring the following:</li> <li>Clear use case or demonstration of bug fix/new feature.</li> <li>Inclusion of relevant tests (unit, functional, and fuzz tests where applicable).</li> <li>Adherence to code style guidelines.</li> <li>No breaking changes to the existing test suite.</li> <li>Bug fixes accompanied by tests to prevent regression.</li> <li>Update of relevant comments and documentation reflecting code behavior changes.</li> </ol>"},{"location":"contributing/#contributing-with-an-issue","title":"Contributing with an Issue","text":"<p>If you encounter a bug but aren't sure how to fix it or submit a pull request, you can create an issue. Issues serve as avenues for bug reports, feedback, and general discussions within the GigaSpatial GitHub repository.</p>"},{"location":"contributing/#other-ways-to-contribute","title":"Other Ways to Contribute","text":"<p>Beyond code contributions:</p> <ul> <li>Feedback and Insights: Share your expertise and experiences related to cash transfer by contacting us at giga@unicef.org.</li> <li>Documentation: Contribute to our journey by sharing reports, case studies, articles, blogs, or surveys. Contact us to contribute and learn more via giga@unicef.org.</li> <li>Designs: If you're passionate about UI/UX, animations, graphics, tutorials, etc., contact us to create visuals for the Giga community via giga@unicef.org.</li> </ul>"},{"location":"contributing/#connect-with-giga-contributors","title":"Connect with Giga Contributors","text":"<p>Connect with fellow contributors via our Discord channel to engage with the Giga community: Click</p>"},{"location":"license/","title":"License","text":"<pre><code>                GNU AFFERO GENERAL PUBLIC LICENSE\n                   Version 3, 19 November 2007\n</code></pre> <p>Copyright (C) 2007 Free Software Foundation, Inc. https://fsf.org/  Everyone is permitted to copy and distribute verbatim copies  of this license document, but changing it is not allowed.</p> <pre><code>                        Preamble\n</code></pre> <p>The GNU Affero General Public License is a free, copyleft license for software and other kinds of works, specifically designed to ensure cooperation with the community in the case of network server software.</p> <p>The licenses for most software and other practical works are designed to take away your freedom to share and change the works.  By contrast, our General Public Licenses are intended to guarantee your freedom to share and change all versions of a program--to make sure it remains free software for all its users.</p> <p>When we speak of free software, we are referring to freedom, not price.  Our General Public Licenses are designed to make sure that you have the freedom to distribute copies of free software (and charge for them if you wish), that you receive source code or can get it if you want it, that you can change the software or use pieces of it in new free programs, and that you know you can do these things.</p> <p>Developers that use our General Public Licenses protect your rights with two steps: (1) assert copyright on the software, and (2) offer you this License which gives you legal permission to copy, distribute and/or modify the software.</p> <p>A secondary benefit of defending all users' freedom is that improvements made in alternate versions of the program, if they receive widespread use, become available for other developers to incorporate.  Many developers of free software are heartened and encouraged by the resulting cooperation.  However, in the case of software used on network servers, this result may fail to come about. The GNU General Public License permits making a modified version and letting the public access it on a server without ever releasing its source code to the public.</p> <p>The GNU Affero General Public License is designed specifically to ensure that, in such cases, the modified source code becomes available to the community.  It requires the operator of a network server to provide the source code of the modified version running there to the users of that server.  Therefore, public use of a modified version, on a publicly accessible server, gives the public access to the source code of the modified version.</p> <p>An older license, called the Affero General Public License and published by Affero, was designed to accomplish similar goals.  This is a different license, not a version of the Affero GPL, but Affero has released a new version of the Affero GPL which permits relicensing under this license.</p> <p>The precise terms and conditions for copying, distribution and modification follow.</p> <pre><code>                   TERMS AND CONDITIONS\n</code></pre> <ol> <li>Definitions.</li> </ol> <p>\"This License\" refers to version 3 of the GNU Affero General Public License.</p> <p>\"Copyright\" also means copyright-like laws that apply to other kinds of works, such as semiconductor masks.</p> <p>\"The Program\" refers to any copyrightable work licensed under this License.  Each licensee is addressed as \"you\".  \"Licensees\" and \"recipients\" may be individuals or organizations.</p> <p>To \"modify\" a work means to copy from or adapt all or part of the work in a fashion requiring copyright permission, other than the making of an exact copy.  The resulting work is called a \"modified version\" of the earlier work or a work \"based on\" the earlier work.</p> <p>A \"covered work\" means either the unmodified Program or a work based on the Program.</p> <p>To \"propagate\" a work means to do anything with it that, without permission, would make you directly or secondarily liable for infringement under applicable copyright law, except executing it on a computer or modifying a private copy.  Propagation includes copying, distribution (with or without modification), making available to the public, and in some countries other activities as well.</p> <p>To \"convey\" a work means any kind of propagation that enables other parties to make or receive copies.  Mere interaction with a user through a computer network, with no transfer of a copy, is not conveying.</p> <p>An interactive user interface displays \"Appropriate Legal Notices\" to the extent that it includes a convenient and prominently visible feature that (1) displays an appropriate copyright notice, and (2) tells the user that there is no warranty for the work (except to the extent that warranties are provided), that licensees may convey the work under this License, and how to view a copy of this License.  If the interface presents a list of user commands or options, such as a menu, a prominent item in the list meets this criterion.</p> <ol> <li>Source Code.</li> </ol> <p>The \"source code\" for a work means the preferred form of the work for making modifications to it.  \"Object code\" means any non-source form of a work.</p> <p>A \"Standard Interface\" means an interface that either is an official standard defined by a recognized standards body, or, in the case of interfaces specified for a particular programming language, one that is widely used among developers working in that language.</p> <p>The \"System Libraries\" of an executable work include anything, other than the work as a whole, that (a) is included in the normal form of packaging a Major Component, but which is not part of that Major Component, and (b) serves only to enable use of the work with that Major Component, or to implement a Standard Interface for which an implementation is available to the public in source code form.  A \"Major Component\", in this context, means a major essential component (kernel, window system, and so on) of the specific operating system (if any) on which the executable work runs, or a compiler used to produce the work, or an object code interpreter used to run it.</p> <p>The \"Corresponding Source\" for a work in object code form means all the source code needed to generate, install, and (for an executable work) run the object code and to modify the work, including scripts to control those activities.  However, it does not include the work's System Libraries, or general-purpose tools or generally available free programs which are used unmodified in performing those activities but which are not part of the work.  For example, Corresponding Source includes interface definition files associated with source files for the work, and the source code for shared libraries and dynamically linked subprograms that the work is specifically designed to require, such as by intimate data communication or control flow between those subprograms and other parts of the work.</p> <p>The Corresponding Source need not include anything that users can regenerate automatically from other parts of the Corresponding Source.</p> <p>The Corresponding Source for a work in source code form is that same work.</p> <ol> <li>Basic Permissions.</li> </ol> <p>All rights granted under this License are granted for the term of copyright on the Program, and are irrevocable provided the stated conditions are met.  This License explicitly affirms your unlimited permission to run the unmodified Program.  The output from running a covered work is covered by this License only if the output, given its content, constitutes a covered work.  This License acknowledges your rights of fair use or other equivalent, as provided by copyright law.</p> <p>You may make, run and propagate covered works that you do not convey, without conditions so long as your license otherwise remains in force.  You may convey covered works to others for the sole purpose of having them make modifications exclusively for you, or provide you with facilities for running those works, provided that you comply with the terms of this License in conveying all material for which you do not control copyright.  Those thus making or running the covered works for you must do so exclusively on your behalf, under your direction and control, on terms that prohibit them from making any copies of your copyrighted material outside their relationship with you.</p> <p>Conveying under any other circumstances is permitted solely under the conditions stated below.  Sublicensing is not allowed; section 10 makes it unnecessary.</p> <ol> <li>Protecting Users' Legal Rights From Anti-Circumvention Law.</li> </ol> <p>No covered work shall be deemed part of an effective technological measure under any applicable law fulfilling obligations under article 11 of the WIPO copyright treaty adopted on 20 December 1996, or similar laws prohibiting or restricting circumvention of such measures.</p> <p>When you convey a covered work, you waive any legal power to forbid circumvention of technological measures to the extent such circumvention is effected by exercising rights under this License with respect to the covered work, and you disclaim any intention to limit operation or modification of the work as a means of enforcing, against the work's users, your or third parties' legal rights to forbid circumvention of technological measures.</p> <ol> <li>Conveying Verbatim Copies.</li> </ol> <p>You may convey verbatim copies of the Program's source code as you receive it, in any medium, provided that you conspicuously and appropriately publish on each copy an appropriate copyright notice; keep intact all notices stating that this License and any non-permissive terms added in accord with section 7 apply to the code; keep intact all notices of the absence of any warranty; and give all recipients a copy of this License along with the Program.</p> <p>You may charge any price or no price for each copy that you convey, and you may offer support or warranty protection for a fee.</p> <ol> <li>Conveying Modified Source Versions.</li> </ol> <p>You may convey a work based on the Program, or the modifications to produce it from the Program, in the form of source code under the terms of section 4, provided that you also meet all of these conditions:</p> <pre><code>a) The work must carry prominent notices stating that you modified\nit, and giving a relevant date.\n\nb) The work must carry prominent notices stating that it is\nreleased under this License and any conditions added under section\n7.  This requirement modifies the requirement in section 4 to\n\"keep intact all notices\".\n\nc) You must license the entire work, as a whole, under this\nLicense to anyone who comes into possession of a copy.  This\nLicense will therefore apply, along with any applicable section 7\nadditional terms, to the whole of the work, and all its parts,\nregardless of how they are packaged.  This License gives no\npermission to license the work in any other way, but it does not\ninvalidate such permission if you have separately received it.\n\nd) If the work has interactive user interfaces, each must display\nAppropriate Legal Notices; however, if the Program has interactive\ninterfaces that do not display Appropriate Legal Notices, your\nwork need not make them do so.\n</code></pre> <p>A compilation of a covered work with other separate and independent works, which are not by their nature extensions of the covered work, and which are not combined with it such as to form a larger program, in or on a volume of a storage or distribution medium, is called an \"aggregate\" if the compilation and its resulting copyright are not used to limit the access or legal rights of the compilation's users beyond what the individual works permit.  Inclusion of a covered work in an aggregate does not cause this License to apply to the other parts of the aggregate.</p> <ol> <li>Conveying Non-Source Forms.</li> </ol> <p>You may convey a covered work in object code form under the terms of sections 4 and 5, provided that you also convey the machine-readable Corresponding Source under the terms of this License, in one of these ways:</p> <pre><code>a) Convey the object code in, or embodied in, a physical product\n(including a physical distribution medium), accompanied by the\nCorresponding Source fixed on a durable physical medium\ncustomarily used for software interchange.\n\nb) Convey the object code in, or embodied in, a physical product\n(including a physical distribution medium), accompanied by a\nwritten offer, valid for at least three years and valid for as\nlong as you offer spare parts or customer support for that product\nmodel, to give anyone who possesses the object code either (1) a\ncopy of the Corresponding Source for all the software in the\nproduct that is covered by this License, on a durable physical\nmedium customarily used for software interchange, for a price no\nmore than your reasonable cost of physically performing this\nconveying of source, or (2) access to copy the\nCorresponding Source from a network server at no charge.\n\nc) Convey individual copies of the object code with a copy of the\nwritten offer to provide the Corresponding Source.  This\nalternative is allowed only occasionally and noncommercially, and\nonly if you received the object code with such an offer, in accord\nwith subsection 6b.\n\nd) Convey the object code by offering access from a designated\nplace (gratis or for a charge), and offer equivalent access to the\nCorresponding Source in the same way through the same place at no\nfurther charge.  You need not require recipients to copy the\nCorresponding Source along with the object code.  If the place to\ncopy the object code is a network server, the Corresponding Source\nmay be on a different server (operated by you or a third party)\nthat supports equivalent copying facilities, provided you maintain\nclear directions next to the object code saying where to find the\nCorresponding Source.  Regardless of what server hosts the\nCorresponding Source, you remain obligated to ensure that it is\navailable for as long as needed to satisfy these requirements.\n\ne) Convey the object code using peer-to-peer transmission, provided\nyou inform other peers where the object code and Corresponding\nSource of the work are being offered to the general public at no\ncharge under subsection 6d.\n</code></pre> <p>A separable portion of the object code, whose source code is excluded from the Corresponding Source as a System Library, need not be included in conveying the object code work.</p> <p>A \"User Product\" is either (1) a \"consumer product\", which means any tangible personal property which is normally used for personal, family, or household purposes, or (2) anything designed or sold for incorporation into a dwelling.  In determining whether a product is a consumer product, doubtful cases shall be resolved in favor of coverage.  For a particular product received by a particular user, \"normally used\" refers to a typical or common use of that class of product, regardless of the status of the particular user or of the way in which the particular user actually uses, or expects or is expected to use, the product.  A product is a consumer product regardless of whether the product has substantial commercial, industrial or non-consumer uses, unless such uses represent the only significant mode of use of the product.</p> <p>\"Installation Information\" for a User Product means any methods, procedures, authorization keys, or other information required to install and execute modified versions of a covered work in that User Product from a modified version of its Corresponding Source.  The information must suffice to ensure that the continued functioning of the modified object code is in no case prevented or interfered with solely because modification has been made.</p> <p>If you convey an object code work under this section in, or with, or specifically for use in, a User Product, and the conveying occurs as part of a transaction in which the right of possession and use of the User Product is transferred to the recipient in perpetuity or for a fixed term (regardless of how the transaction is characterized), the Corresponding Source conveyed under this section must be accompanied by the Installation Information.  But this requirement does not apply if neither you nor any third party retains the ability to install modified object code on the User Product (for example, the work has been installed in ROM).</p> <p>The requirement to provide Installation Information does not include a requirement to continue to provide support service, warranty, or updates for a work that has been modified or installed by the recipient, or for the User Product in which it has been modified or installed.  Access to a network may be denied when the modification itself materially and adversely affects the operation of the network or violates the rules and protocols for communication across the network.</p> <p>Corresponding Source conveyed, and Installation Information provided, in accord with this section must be in a format that is publicly documented (and with an implementation available to the public in source code form), and must require no special password or key for unpacking, reading or copying.</p> <ol> <li>Additional Terms.</li> </ol> <p>\"Additional permissions\" are terms that supplement the terms of this License by making exceptions from one or more of its conditions. Additional permissions that are applicable to the entire Program shall be treated as though they were included in this License, to the extent that they are valid under applicable law.  If additional permissions apply only to part of the Program, that part may be used separately under those permissions, but the entire Program remains governed by this License without regard to the additional permissions.</p> <p>When you convey a copy of a covered work, you may at your option remove any additional permissions from that copy, or from any part of it.  (Additional permissions may be written to require their own removal in certain cases when you modify the work.)  You may place additional permissions on material, added by you to a covered work, for which you have or can give appropriate copyright permission.</p> <p>Notwithstanding any other provision of this License, for material you add to a covered work, you may (if authorized by the copyright holders of that material) supplement the terms of this License with terms:</p> <pre><code>a) Disclaiming warranty or limiting liability differently from the\nterms of sections 15 and 16 of this License; or\n\nb) Requiring preservation of specified reasonable legal notices or\nauthor attributions in that material or in the Appropriate Legal\nNotices displayed by works containing it; or\n\nc) Prohibiting misrepresentation of the origin of that material, or\nrequiring that modified versions of such material be marked in\nreasonable ways as different from the original version; or\n\nd) Limiting the use for publicity purposes of names of licensors or\nauthors of the material; or\n\ne) Declining to grant rights under trademark law for use of some\ntrade names, trademarks, or service marks; or\n\nf) Requiring indemnification of licensors and authors of that\nmaterial by anyone who conveys the material (or modified versions of\nit) with contractual assumptions of liability to the recipient, for\nany liability that these contractual assumptions directly impose on\nthose licensors and authors.\n</code></pre> <p>All other non-permissive additional terms are considered \"further restrictions\" within the meaning of section 10.  If the Program as you received it, or any part of it, contains a notice stating that it is governed by this License along with a term that is a further restriction, you may remove that term.  If a license document contains a further restriction but permits relicensing or conveying under this License, you may add to a covered work material governed by the terms of that license document, provided that the further restriction does not survive such relicensing or conveying.</p> <p>If you add terms to a covered work in accord with this section, you must place, in the relevant source files, a statement of the additional terms that apply to those files, or a notice indicating where to find the applicable terms.</p> <p>Additional terms, permissive or non-permissive, may be stated in the form of a separately written license, or stated as exceptions; the above requirements apply either way.</p> <ol> <li>Termination.</li> </ol> <p>You may not propagate or modify a covered work except as expressly provided under this License.  Any attempt otherwise to propagate or modify it is void, and will automatically terminate your rights under this License (including any patent licenses granted under the third paragraph of section 11).</p> <p>However, if you cease all violation of this License, then your license from a particular copyright holder is reinstated (a) provisionally, unless and until the copyright holder explicitly and finally terminates your license, and (b) permanently, if the copyright holder fails to notify you of the violation by some reasonable means prior to 60 days after the cessation.</p> <p>Moreover, your license from a particular copyright holder is reinstated permanently if the copyright holder notifies you of the violation by some reasonable means, this is the first time you have received notice of violation of this License (for any work) from that copyright holder, and you cure the violation prior to 30 days after your receipt of the notice.</p> <p>Termination of your rights under this section does not terminate the licenses of parties who have received copies or rights from you under this License.  If your rights have been terminated and not permanently reinstated, you do not qualify to receive new licenses for the same material under section 10.</p> <ol> <li>Acceptance Not Required for Having Copies.</li> </ol> <p>You are not required to accept this License in order to receive or run a copy of the Program.  Ancillary propagation of a covered work occurring solely as a consequence of using peer-to-peer transmission to receive a copy likewise does not require acceptance.  However, nothing other than this License grants you permission to propagate or modify any covered work.  These actions infringe copyright if you do not accept this License.  Therefore, by modifying or propagating a covered work, you indicate your acceptance of this License to do so.</p> <ol> <li>Automatic Licensing of Downstream Recipients.</li> </ol> <p>Each time you convey a covered work, the recipient automatically receives a license from the original licensors, to run, modify and propagate that work, subject to this License.  You are not responsible for enforcing compliance by third parties with this License.</p> <p>An \"entity transaction\" is a transaction transferring control of an organization, or substantially all assets of one, or subdividing an organization, or merging organizations.  If propagation of a covered work results from an entity transaction, each party to that transaction who receives a copy of the work also receives whatever licenses to the work the party's predecessor in interest had or could give under the previous paragraph, plus a right to possession of the Corresponding Source of the work from the predecessor in interest, if the predecessor has it or can get it with reasonable efforts.</p> <p>You may not impose any further restrictions on the exercise of the rights granted or affirmed under this License.  For example, you may not impose a license fee, royalty, or other charge for exercise of rights granted under this License, and you may not initiate litigation (including a cross-claim or counterclaim in a lawsuit) alleging that any patent claim is infringed by making, using, selling, offering for sale, or importing the Program or any portion of it.</p> <ol> <li>Patents.</li> </ol> <p>A \"contributor\" is a copyright holder who authorizes use under this License of the Program or a work on which the Program is based.  The work thus licensed is called the contributor's \"contributor version\".</p> <p>A contributor's \"essential patent claims\" are all patent claims owned or controlled by the contributor, whether already acquired or hereafter acquired, that would be infringed by some manner, permitted by this License, of making, using, or selling its contributor version, but do not include claims that would be infringed only as a consequence of further modification of the contributor version.  For purposes of this definition, \"control\" includes the right to grant patent sublicenses in a manner consistent with the requirements of this License.</p> <p>Each contributor grants you a non-exclusive, worldwide, royalty-free patent license under the contributor's essential patent claims, to make, use, sell, offer for sale, import and otherwise run, modify and propagate the contents of its contributor version.</p> <p>In the following three paragraphs, a \"patent license\" is any express agreement or commitment, however denominated, not to enforce a patent (such as an express permission to practice a patent or covenant not to sue for patent infringement).  To \"grant\" such a patent license to a party means to make such an agreement or commitment not to enforce a patent against the party.</p> <p>If you convey a covered work, knowingly relying on a patent license, and the Corresponding Source of the work is not available for anyone to copy, free of charge and under the terms of this License, through a publicly available network server or other readily accessible means, then you must either (1) cause the Corresponding Source to be so available, or (2) arrange to deprive yourself of the benefit of the patent license for this particular work, or (3) arrange, in a manner consistent with the requirements of this License, to extend the patent license to downstream recipients.  \"Knowingly relying\" means you have actual knowledge that, but for the patent license, your conveying the covered work in a country, or your recipient's use of the covered work in a country, would infringe one or more identifiable patents in that country that you have reason to believe are valid.</p> <p>If, pursuant to or in connection with a single transaction or arrangement, you convey, or propagate by procuring conveyance of, a covered work, and grant a patent license to some of the parties receiving the covered work authorizing them to use, propagate, modify or convey a specific copy of the covered work, then the patent license you grant is automatically extended to all recipients of the covered work and works based on it.</p> <p>A patent license is \"discriminatory\" if it does not include within the scope of its coverage, prohibits the exercise of, or is conditioned on the non-exercise of one or more of the rights that are specifically granted under this License.  You may not convey a covered work if you are a party to an arrangement with a third party that is in the business of distributing software, under which you make payment to the third party based on the extent of your activity of conveying the work, and under which the third party grants, to any of the parties who would receive the covered work from you, a discriminatory patent license (a) in connection with copies of the covered work conveyed by you (or copies made from those copies), or (b) primarily for and in connection with specific products or compilations that contain the covered work, unless you entered into that arrangement, or that patent license was granted, prior to 28 March 2007.</p> <p>Nothing in this License shall be construed as excluding or limiting any implied license or other defenses to infringement that may otherwise be available to you under applicable patent law.</p> <ol> <li>No Surrender of Others' Freedom.</li> </ol> <p>If conditions are imposed on you (whether by court order, agreement or otherwise) that contradict the conditions of this License, they do not excuse you from the conditions of this License.  If you cannot convey a covered work so as to satisfy simultaneously your obligations under this License and any other pertinent obligations, then as a consequence you may not convey it at all.  For example, if you agree to terms that obligate you to collect a royalty for further conveying from those to whom you convey the Program, the only way you could satisfy both those terms and this License would be to refrain entirely from conveying the Program.</p> <ol> <li>Remote Network Interaction; Use with the GNU General Public License.</li> </ol> <p>Notwithstanding any other provision of this License, if you modify the Program, your modified version must prominently offer all users interacting with it remotely through a computer network (if your version supports such interaction) an opportunity to receive the Corresponding Source of your version by providing access to the Corresponding Source from a network server at no charge, through some standard or customary means of facilitating copying of software.  This Corresponding Source shall include the Corresponding Source for any work covered by version 3 of the GNU General Public License that is incorporated pursuant to the following paragraph.</p> <p>Notwithstanding any other provision of this License, you have permission to link or combine any covered work with a work licensed under version 3 of the GNU General Public License into a single combined work, and to convey the resulting work.  The terms of this License will continue to apply to the part which is the covered work, but the work with which it is combined will remain governed by version 3 of the GNU General Public License.</p> <ol> <li>Revised Versions of this License.</li> </ol> <p>The Free Software Foundation may publish revised and/or new versions of the GNU Affero General Public License from time to time.  Such new versions will be similar in spirit to the present version, but may differ in detail to address new problems or concerns.</p> <p>Each version is given a distinguishing version number.  If the Program specifies that a certain numbered version of the GNU Affero General Public License \"or any later version\" applies to it, you have the option of following the terms and conditions either of that numbered version or of any later version published by the Free Software Foundation.  If the Program does not specify a version number of the GNU Affero General Public License, you may choose any version ever published by the Free Software Foundation.</p> <p>If the Program specifies that a proxy can decide which future versions of the GNU Affero General Public License can be used, that proxy's public statement of acceptance of a version permanently authorizes you to choose that version for the Program.</p> <p>Later license versions may give you additional or different permissions.  However, no additional obligations are imposed on any author or copyright holder as a result of your choosing to follow a later version.</p> <ol> <li>Disclaimer of Warranty.</li> </ol> <p>THERE IS NO WARRANTY FOR THE PROGRAM, TO THE EXTENT PERMITTED BY APPLICABLE LAW.  EXCEPT WHEN OTHERWISE STATED IN WRITING THE COPYRIGHT HOLDERS AND/OR OTHER PARTIES PROVIDE THE PROGRAM \"AS IS\" WITHOUT WARRANTY OF ANY KIND, EITHER EXPRESSED OR IMPLIED, INCLUDING, BUT NOT LIMITED TO, THE IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE.  THE ENTIRE RISK AS TO THE QUALITY AND PERFORMANCE OF THE PROGRAM IS WITH YOU.  SHOULD THE PROGRAM PROVE DEFECTIVE, YOU ASSUME THE COST OF ALL NECESSARY SERVICING, REPAIR OR CORRECTION.</p> <ol> <li>Limitation of Liability.</li> </ol> <p>IN NO EVENT UNLESS REQUIRED BY APPLICABLE LAW OR AGREED TO IN WRITING WILL ANY COPYRIGHT HOLDER, OR ANY OTHER PARTY WHO MODIFIES AND/OR CONVEYS THE PROGRAM AS PERMITTED ABOVE, BE LIABLE TO YOU FOR DAMAGES, INCLUDING ANY GENERAL, SPECIAL, INCIDENTAL OR CONSEQUENTIAL DAMAGES ARISING OUT OF THE USE OR INABILITY TO USE THE PROGRAM (INCLUDING BUT NOT LIMITED TO LOSS OF DATA OR DATA BEING RENDERED INACCURATE OR LOSSES SUSTAINED BY YOU OR THIRD PARTIES OR A FAILURE OF THE PROGRAM TO OPERATE WITH ANY OTHER PROGRAMS), EVEN IF SUCH HOLDER OR OTHER PARTY HAS BEEN ADVISED OF THE POSSIBILITY OF SUCH DAMAGES.</p> <ol> <li>Interpretation of Sections 15 and 16.</li> </ol> <p>If the disclaimer of warranty and limitation of liability provided above cannot be given local legal effect according to their terms, reviewing courts shall apply local law that most closely approximates an absolute waiver of all civil liability in connection with the Program, unless a warranty or assumption of liability accompanies a copy of the Program in return for a fee.</p> <pre><code>                 END OF TERMS AND CONDITIONS\n\n        How to Apply These Terms to Your New Programs\n</code></pre> <p>If you develop a new program, and you want it to be of the greatest possible use to the public, the best way to achieve this is to make it free software which everyone can redistribute and change under these terms.</p> <p>To do so, attach the following notices to the program.  It is safest to attach them to the start of each source file to most effectively state the exclusion of warranty; and each file should have at least the \"copyright\" line and a pointer to where the full notice is found.</p> <pre><code>&lt;one line to give the program's name and a brief idea of what it does.&gt;\nCopyright (C) &lt;year&gt;  &lt;name of author&gt;\n\nThis program is free software: you can redistribute it and/or modify\nit under the terms of the GNU Affero General Public License as published\nby the Free Software Foundation, either version 3 of the License, or\n(at your option) any later version.\n\nThis program is distributed in the hope that it will be useful,\nbut WITHOUT ANY WARRANTY; without even the implied warranty of\nMERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the\nGNU Affero General Public License for more details.\n\nYou should have received a copy of the GNU Affero General Public License\nalong with this program.  If not, see &lt;https://www.gnu.org/licenses/&gt;.\n</code></pre> <p>Also add information on how to contact you by electronic and paper mail.</p> <p>If your software can interact with users remotely through a computer network, you should also make sure that it provides a way for users to get its source.  For example, if your program is a web application, its interface could display a \"Source\" link that leads users to an archive of the code.  There are many ways you could offer source, and different solutions will be better for different programs; see section 13 for the specific requirements.</p> <p>You should also get your employer (if you work as a programmer) or school, if any, to sign a \"copyright disclaimer\" for the program, if necessary. For more information on this, and how to apply and follow the GNU AGPL, see https://www.gnu.org/licenses/.</p>"},{"location":"api/","title":"API Reference","text":"<p>Welcome to the API reference for the <code>gigaspatial</code> package. This documentation provides detailed information about the modules, classes, and functions available in the package.</p>"},{"location":"api/#modules","title":"Modules","text":"<p>The <code>gigaspatial</code> package is organized into several modules, each serving a specific purpose:</p>"},{"location":"api/#1-handlers","title":"1. Handlers","text":"<p>The <code>handlers</code> module contains classes for downloading and processing geospatial data from various sources, such as OpenStreetMap (OSM) and the Global Human Settlement Layer (GHSL).</p> <ul> <li>OSMLocationFetcher: Fetches and processes location data from OpenStreetMap.</li> <li>GHSLDataDownloader: Downloads and processes data from the Global Human Settlement Layer.</li> </ul> <p>Learn more about the Handlers module</p>"},{"location":"api/#2-processing","title":"2. Processing","text":"<p>The <code>processing</code> module provides tools for processing geospatial data, such as GeoTIFF files.</p> <ul> <li>TifProcessor: Processes GeoTIFF files and extracts relevant data.</li> </ul> <p>Learn more about the Processing module</p>"},{"location":"api/#3-core","title":"3. Core","text":"<p>The <code>core</code> module contains essential utilities and base classes used throughout the package.</p> <ul> <li>DataStore: Handles the storage and retrieval of geospatial data in various formats.</li> <li>Config: Manages configuration settings, such as paths and API keys.</li> </ul> <p>Learn more about the Core module</p>"},{"location":"api/#4-generators","title":"4. Generators","text":"<p>The <code>generators</code> module includes tools for generating geospatial data, such as grids and synthetic datasets.</p> <p>Learn more about the Generators module</p>"},{"location":"api/#5-grid","title":"5. Grid","text":"<p>The <code>grid</code> module provides utilities for working with geospatial grids, such as creating and manipulating grid-based data.</p> <p>Learn more about the Grid module</p>"},{"location":"api/#getting-started","title":"Getting Started","text":"<p>To get started with the <code>gigaspatial</code> package, follow the Quick Start Guide.</p>"},{"location":"api/#additional-resources","title":"Additional Resources","text":"<ul> <li>Examples: Real-world examples and use cases.</li> <li>Changelog: Information about the latest updates and changes.</li> <li>Contributing: Guidelines for contributing to the project.</li> </ul>"},{"location":"api/#support","title":"Support","text":"<p>If you encounter any issues or have questions, feel free to open an issue or join our Discord community.</p>"},{"location":"api/core/","title":"Core Module","text":""},{"location":"api/core/#gigaspatial.core","title":"<code>gigaspatial.core</code>","text":""},{"location":"api/core/#gigaspatial.core.io","title":"<code>io</code>","text":""},{"location":"api/core/#gigaspatial.core.io.DataStore","title":"<code>DataStore</code>","text":"<p>               Bases: <code>ABC</code></p> <p>Abstract base class defining the interface for data store implementations. This class serves as a parent for both local and cloud-based storage solutions.</p> Source code in <code>gigaspatial/core/io/data_store.py</code> <pre><code>class DataStore(ABC):\n    \"\"\"\n    Abstract base class defining the interface for data store implementations.\n    This class serves as a parent for both local and cloud-based storage solutions.\n    \"\"\"\n\n    @abstractmethod\n    def read_file(self, path: str) -&gt; Any:\n        \"\"\"\n        Read contents of a file from the data store.\n\n        Args:\n            path: Path to the file to read\n\n        Returns:\n            Contents of the file\n\n        Raises:\n            IOError: If file cannot be read\n        \"\"\"\n        pass\n\n    @abstractmethod\n    def write_file(self, path: str, data: Any) -&gt; None:\n        \"\"\"\n        Write data to a file in the data store.\n\n        Args:\n            path: Path where to write the file\n            data: Data to write to the file\n\n        Raises:\n            IOError: If file cannot be written\n        \"\"\"\n        pass\n\n    @abstractmethod\n    def file_exists(self, path: str) -&gt; bool:\n        \"\"\"\n        Check if a file exists in the data store.\n\n        Args:\n            path: Path to check\n\n        Returns:\n            True if file exists, False otherwise\n        \"\"\"\n        pass\n\n    @abstractmethod\n    def list_files(self, path: str) -&gt; List[str]:\n        \"\"\"\n        List all files in a directory.\n\n        Args:\n            path: Directory path to list\n\n        Returns:\n            List of file paths in the directory\n        \"\"\"\n        pass\n\n    @abstractmethod\n    def walk(self, top: str) -&gt; Generator:\n        \"\"\"\n        Walk through directory tree, similar to os.walk().\n\n        Args:\n            top: Starting directory for the walk\n\n        Returns:\n            Generator yielding tuples of (dirpath, dirnames, filenames)\n        \"\"\"\n        pass\n\n    @abstractmethod\n    def open(self, file: str, mode: str = \"r\") -&gt; Union[str, bytes]:\n        \"\"\"\n        Context manager for file operations.\n\n        Args:\n            file: Path to the file\n            mode: File mode ('r', 'w', 'rb', 'wb')\n\n        Yields:\n            File-like object\n\n        Raises:\n            IOError: If file cannot be opened\n        \"\"\"\n        pass\n\n    @abstractmethod\n    def is_file(self, path: str) -&gt; bool:\n        \"\"\"\n        Check if path points to a file.\n\n        Args:\n            path: Path to check\n\n        Returns:\n            True if path is a file, False otherwise\n        \"\"\"\n        pass\n\n    @abstractmethod\n    def is_dir(self, path: str) -&gt; bool:\n        \"\"\"\n        Check if path points to a directory.\n\n        Args:\n            path: Path to check\n\n        Returns:\n            True if path is a directory, False otherwise\n        \"\"\"\n        pass\n\n    @abstractmethod\n    def remove(self, path: str) -&gt; None:\n        \"\"\"\n        Remove a file.\n\n        Args:\n            path: Path to the file to remove\n\n        Raises:\n            IOError: If file cannot be removed\n        \"\"\"\n        pass\n\n    @abstractmethod\n    def rmdir(self, dir: str) -&gt; None:\n        \"\"\"\n        Remove a directory and all its contents.\n\n        Args:\n            dir: Path to the directory to remove\n\n        Raises:\n            IOError: If directory cannot be removed\n        \"\"\"\n        pass\n</code></pre>"},{"location":"api/core/#gigaspatial.core.io.DataStore.file_exists","title":"<code>file_exists(path)</code>  <code>abstractmethod</code>","text":"<p>Check if a file exists in the data store.</p> <p>Parameters:</p> Name Type Description Default <code>path</code> <code>str</code> <p>Path to check</p> required <p>Returns:</p> Type Description <code>bool</code> <p>True if file exists, False otherwise</p> Source code in <code>gigaspatial/core/io/data_store.py</code> <pre><code>@abstractmethod\ndef file_exists(self, path: str) -&gt; bool:\n    \"\"\"\n    Check if a file exists in the data store.\n\n    Args:\n        path: Path to check\n\n    Returns:\n        True if file exists, False otherwise\n    \"\"\"\n    pass\n</code></pre>"},{"location":"api/core/#gigaspatial.core.io.DataStore.is_dir","title":"<code>is_dir(path)</code>  <code>abstractmethod</code>","text":"<p>Check if path points to a directory.</p> <p>Parameters:</p> Name Type Description Default <code>path</code> <code>str</code> <p>Path to check</p> required <p>Returns:</p> Type Description <code>bool</code> <p>True if path is a directory, False otherwise</p> Source code in <code>gigaspatial/core/io/data_store.py</code> <pre><code>@abstractmethod\ndef is_dir(self, path: str) -&gt; bool:\n    \"\"\"\n    Check if path points to a directory.\n\n    Args:\n        path: Path to check\n\n    Returns:\n        True if path is a directory, False otherwise\n    \"\"\"\n    pass\n</code></pre>"},{"location":"api/core/#gigaspatial.core.io.DataStore.is_file","title":"<code>is_file(path)</code>  <code>abstractmethod</code>","text":"<p>Check if path points to a file.</p> <p>Parameters:</p> Name Type Description Default <code>path</code> <code>str</code> <p>Path to check</p> required <p>Returns:</p> Type Description <code>bool</code> <p>True if path is a file, False otherwise</p> Source code in <code>gigaspatial/core/io/data_store.py</code> <pre><code>@abstractmethod\ndef is_file(self, path: str) -&gt; bool:\n    \"\"\"\n    Check if path points to a file.\n\n    Args:\n        path: Path to check\n\n    Returns:\n        True if path is a file, False otherwise\n    \"\"\"\n    pass\n</code></pre>"},{"location":"api/core/#gigaspatial.core.io.DataStore.list_files","title":"<code>list_files(path)</code>  <code>abstractmethod</code>","text":"<p>List all files in a directory.</p> <p>Parameters:</p> Name Type Description Default <code>path</code> <code>str</code> <p>Directory path to list</p> required <p>Returns:</p> Type Description <code>List[str]</code> <p>List of file paths in the directory</p> Source code in <code>gigaspatial/core/io/data_store.py</code> <pre><code>@abstractmethod\ndef list_files(self, path: str) -&gt; List[str]:\n    \"\"\"\n    List all files in a directory.\n\n    Args:\n        path: Directory path to list\n\n    Returns:\n        List of file paths in the directory\n    \"\"\"\n    pass\n</code></pre>"},{"location":"api/core/#gigaspatial.core.io.DataStore.open","title":"<code>open(file, mode='r')</code>  <code>abstractmethod</code>","text":"<p>Context manager for file operations.</p> <p>Parameters:</p> Name Type Description Default <code>file</code> <code>str</code> <p>Path to the file</p> required <code>mode</code> <code>str</code> <p>File mode ('r', 'w', 'rb', 'wb')</p> <code>'r'</code> <p>Yields:</p> Type Description <code>Union[str, bytes]</code> <p>File-like object</p> <p>Raises:</p> Type Description <code>IOError</code> <p>If file cannot be opened</p> Source code in <code>gigaspatial/core/io/data_store.py</code> <pre><code>@abstractmethod\ndef open(self, file: str, mode: str = \"r\") -&gt; Union[str, bytes]:\n    \"\"\"\n    Context manager for file operations.\n\n    Args:\n        file: Path to the file\n        mode: File mode ('r', 'w', 'rb', 'wb')\n\n    Yields:\n        File-like object\n\n    Raises:\n        IOError: If file cannot be opened\n    \"\"\"\n    pass\n</code></pre>"},{"location":"api/core/#gigaspatial.core.io.DataStore.read_file","title":"<code>read_file(path)</code>  <code>abstractmethod</code>","text":"<p>Read contents of a file from the data store.</p> <p>Parameters:</p> Name Type Description Default <code>path</code> <code>str</code> <p>Path to the file to read</p> required <p>Returns:</p> Type Description <code>Any</code> <p>Contents of the file</p> <p>Raises:</p> Type Description <code>IOError</code> <p>If file cannot be read</p> Source code in <code>gigaspatial/core/io/data_store.py</code> <pre><code>@abstractmethod\ndef read_file(self, path: str) -&gt; Any:\n    \"\"\"\n    Read contents of a file from the data store.\n\n    Args:\n        path: Path to the file to read\n\n    Returns:\n        Contents of the file\n\n    Raises:\n        IOError: If file cannot be read\n    \"\"\"\n    pass\n</code></pre>"},{"location":"api/core/#gigaspatial.core.io.DataStore.remove","title":"<code>remove(path)</code>  <code>abstractmethod</code>","text":"<p>Remove a file.</p> <p>Parameters:</p> Name Type Description Default <code>path</code> <code>str</code> <p>Path to the file to remove</p> required <p>Raises:</p> Type Description <code>IOError</code> <p>If file cannot be removed</p> Source code in <code>gigaspatial/core/io/data_store.py</code> <pre><code>@abstractmethod\ndef remove(self, path: str) -&gt; None:\n    \"\"\"\n    Remove a file.\n\n    Args:\n        path: Path to the file to remove\n\n    Raises:\n        IOError: If file cannot be removed\n    \"\"\"\n    pass\n</code></pre>"},{"location":"api/core/#gigaspatial.core.io.DataStore.rmdir","title":"<code>rmdir(dir)</code>  <code>abstractmethod</code>","text":"<p>Remove a directory and all its contents.</p> <p>Parameters:</p> Name Type Description Default <code>dir</code> <code>str</code> <p>Path to the directory to remove</p> required <p>Raises:</p> Type Description <code>IOError</code> <p>If directory cannot be removed</p> Source code in <code>gigaspatial/core/io/data_store.py</code> <pre><code>@abstractmethod\ndef rmdir(self, dir: str) -&gt; None:\n    \"\"\"\n    Remove a directory and all its contents.\n\n    Args:\n        dir: Path to the directory to remove\n\n    Raises:\n        IOError: If directory cannot be removed\n    \"\"\"\n    pass\n</code></pre>"},{"location":"api/core/#gigaspatial.core.io.DataStore.walk","title":"<code>walk(top)</code>  <code>abstractmethod</code>","text":"<p>Walk through directory tree, similar to os.walk().</p> <p>Parameters:</p> Name Type Description Default <code>top</code> <code>str</code> <p>Starting directory for the walk</p> required <p>Returns:</p> Type Description <code>Generator</code> <p>Generator yielding tuples of (dirpath, dirnames, filenames)</p> Source code in <code>gigaspatial/core/io/data_store.py</code> <pre><code>@abstractmethod\ndef walk(self, top: str) -&gt; Generator:\n    \"\"\"\n    Walk through directory tree, similar to os.walk().\n\n    Args:\n        top: Starting directory for the walk\n\n    Returns:\n        Generator yielding tuples of (dirpath, dirnames, filenames)\n    \"\"\"\n    pass\n</code></pre>"},{"location":"api/core/#gigaspatial.core.io.DataStore.write_file","title":"<code>write_file(path, data)</code>  <code>abstractmethod</code>","text":"<p>Write data to a file in the data store.</p> <p>Parameters:</p> Name Type Description Default <code>path</code> <code>str</code> <p>Path where to write the file</p> required <code>data</code> <code>Any</code> <p>Data to write to the file</p> required <p>Raises:</p> Type Description <code>IOError</code> <p>If file cannot be written</p> Source code in <code>gigaspatial/core/io/data_store.py</code> <pre><code>@abstractmethod\ndef write_file(self, path: str, data: Any) -&gt; None:\n    \"\"\"\n    Write data to a file in the data store.\n\n    Args:\n        path: Path where to write the file\n        data: Data to write to the file\n\n    Raises:\n        IOError: If file cannot be written\n    \"\"\"\n    pass\n</code></pre>"},{"location":"api/core/#gigaspatial.core.io.read_dataset","title":"<code>read_dataset(data_store, path, compression=None, **kwargs)</code>","text":"<p>Read data from various file formats stored in both local and cloud-based storage.</p>"},{"location":"api/core/#gigaspatial.core.io.read_dataset--parameters","title":"Parameters:","text":"<p>data_store : DataStore     Instance of DataStore for accessing data storage. path : str, Path     Path to the file in data storage. **kwargs : dict     Additional arguments passed to the specific reader function.</p>"},{"location":"api/core/#gigaspatial.core.io.read_dataset--returns","title":"Returns:","text":"<p>pandas.DataFrame or geopandas.GeoDataFrame     The data read from the file.</p>"},{"location":"api/core/#gigaspatial.core.io.read_dataset--raises","title":"Raises:","text":"<p>FileNotFoundError     If the file doesn't exist in blob storage. ValueError     If the file type is unsupported or if there's an error reading the file.</p> Source code in <code>gigaspatial/core/io/readers.py</code> <pre><code>def read_dataset(data_store: DataStore, path: str, compression: str = None, **kwargs):\n    \"\"\"\n    Read data from various file formats stored in both local and cloud-based storage.\n\n    Parameters:\n    ----------\n    data_store : DataStore\n        Instance of DataStore for accessing data storage.\n    path : str, Path\n        Path to the file in data storage.\n    **kwargs : dict\n        Additional arguments passed to the specific reader function.\n\n    Returns:\n    -------\n    pandas.DataFrame or geopandas.GeoDataFrame\n        The data read from the file.\n\n    Raises:\n    ------\n    FileNotFoundError\n        If the file doesn't exist in blob storage.\n    ValueError\n        If the file type is unsupported or if there's an error reading the file.\n    \"\"\"\n\n    # Define supported file formats and their readers\n    BINARY_FORMATS = {\n        \".shp\",\n        \".zip\",\n        \".parquet\",\n        \".gpkg\",\n        \".xlsx\",\n        \".xls\",\n        \".kmz\",\n        \".gz\",\n    }\n\n    PANDAS_READERS = {\n        \".csv\": pd.read_csv,\n        \".xlsx\": lambda f, **kw: pd.read_excel(f, engine=\"openpyxl\", **kw),\n        \".xls\": lambda f, **kw: pd.read_excel(f, engine=\"xlrd\", **kw),\n        \".json\": pd.read_json,\n        # \".gz\": lambda f, **kw: pd.read_csv(f, compression=\"gzip\", **kw),\n    }\n\n    GEO_READERS = {\n        \".shp\": gpd.read_file,\n        \".zip\": gpd.read_file,\n        \".geojson\": gpd.read_file,\n        \".gpkg\": gpd.read_file,\n        \".parquet\": gpd.read_parquet,\n        \".kmz\": read_kmz,\n    }\n\n    COMPRESSION_FORMATS = {\n        \".gz\": \"gzip\",\n        \".bz2\": \"bz2\",\n        \".zip\": \"zip\",\n        \".xz\": \"xz\",\n    }\n\n    try:\n        # Check if file exists\n        if not data_store.file_exists(path):\n            raise FileNotFoundError(f\"File '{path}' not found in blob storage\")\n\n        path_obj = Path(path)\n        suffixes = path_obj.suffixes\n        file_extension = suffixes[-1].lower() if suffixes else \"\"\n\n        if compression is None and file_extension in COMPRESSION_FORMATS:\n            compression_format = COMPRESSION_FORMATS[file_extension]\n\n            # if file has multiple extensions (e.g., .csv.gz), get the inner format\n            if len(suffixes) &gt; 1:\n                inner_extension = suffixes[-2].lower()\n\n                if inner_extension == \".tar\":\n                    raise ValueError(\n                        \"Tar archives (.tar.gz) are not directly supported\"\n                    )\n\n                if inner_extension in PANDAS_READERS:\n                    try:\n                        with data_store.open(path, \"rb\") as f:\n                            return PANDAS_READERS[inner_extension](\n                                f, compression=compression_format, **kwargs\n                            )\n                    except Exception as e:\n                        raise ValueError(f\"Error reading compressed file: {str(e)}\")\n                elif inner_extension in GEO_READERS:\n                    try:\n                        with data_store.open(path, \"rb\") as f:\n                            if compression_format == \"gzip\":\n                                import gzip\n\n                                decompressed_data = gzip.decompress(f.read())\n                                import io\n\n                                return GEO_READERS[inner_extension](\n                                    io.BytesIO(decompressed_data), **kwargs\n                                )\n                            else:\n                                raise ValueError(\n                                    f\"Compression format {compression_format} not supported for geo data\"\n                                )\n                    except Exception as e:\n                        raise ValueError(f\"Error reading compressed geo file: {str(e)}\")\n            else:\n                # if just .gz without clear inner type, assume csv\n                try:\n                    with data_store.open(path, \"rb\") as f:\n                        return pd.read_csv(f, compression=compression_format, **kwargs)\n                except Exception as e:\n                    raise ValueError(\n                        f\"Error reading compressed file as CSV: {str(e)}. \"\n                        f\"If not a CSV, specify the format in the filename (e.g., .json.gz)\"\n                    )\n\n        # Special handling for compressed files\n        if file_extension == \".zip\":\n            # For zip files, we need to use binary mode\n            with data_store.open(path, \"rb\") as f:\n                return gpd.read_file(f)\n\n        # Determine if we need binary mode based on file type\n        mode = \"rb\" if file_extension in BINARY_FORMATS else \"r\"\n\n        # Try reading with appropriate reader\n        if file_extension in PANDAS_READERS:\n            try:\n                with data_store.open(path, mode) as f:\n                    return PANDAS_READERS[file_extension](f, **kwargs)\n            except Exception as e:\n                raise ValueError(f\"Error reading file with pandas: {str(e)}\")\n\n        if file_extension in GEO_READERS:\n            try:\n                with data_store.open(path, \"rb\") as f:\n                    return GEO_READERS[file_extension](f, **kwargs)\n            except Exception as e:\n                # For parquet files, try pandas reader if geopandas fails\n                if file_extension == \".parquet\":\n                    try:\n                        with data_store.open(path, \"rb\") as f:\n                            return pd.read_parquet(f, **kwargs)\n                    except Exception as e2:\n                        raise ValueError(\n                            f\"Failed to read parquet with both geopandas ({str(e)}) \"\n                            f\"and pandas ({str(e2)})\"\n                        )\n                raise ValueError(f\"Error reading file with geopandas: {str(e)}\")\n\n        # If we get here, the file type is unsupported\n        supported_formats = sorted(set(PANDAS_READERS.keys()) | set(GEO_READERS.keys()))\n        supported_compressions = sorted(COMPRESSION_FORMATS.keys())\n        raise ValueError(\n            f\"Unsupported file type: {file_extension}\\n\"\n            f\"Supported formats: {', '.join(supported_formats)}\"\n            f\"Supported compressions: {', '.join(supported_compressions)}\"\n        )\n\n    except Exception as e:\n        if isinstance(e, (FileNotFoundError, ValueError)):\n            raise\n        raise RuntimeError(f\"Unexpected error reading dataset: {str(e)}\")\n</code></pre>"},{"location":"api/core/#gigaspatial.core.io.read_datasets","title":"<code>read_datasets(data_store, paths, **kwargs)</code>","text":"<p>Read multiple datasets from data storage at once.</p>"},{"location":"api/core/#gigaspatial.core.io.read_datasets--parameters","title":"Parameters:","text":"<p>data_store : DataStore     Instance of DataStore for accessing data storage. paths : list of str     Paths to files in data storage. **kwargs : dict     Additional arguments passed to read_dataset.</p>"},{"location":"api/core/#gigaspatial.core.io.read_datasets--returns","title":"Returns:","text":"<p>dict     Dictionary mapping paths to their corresponding DataFrames/GeoDataFrames.</p> Source code in <code>gigaspatial/core/io/readers.py</code> <pre><code>def read_datasets(data_store: DataStore, paths, **kwargs):\n    \"\"\"\n    Read multiple datasets from data storage at once.\n\n    Parameters:\n    ----------\n    data_store : DataStore\n        Instance of DataStore for accessing data storage.\n    paths : list of str\n        Paths to files in data storage.\n    **kwargs : dict\n        Additional arguments passed to read_dataset.\n\n    Returns:\n    -------\n    dict\n        Dictionary mapping paths to their corresponding DataFrames/GeoDataFrames.\n    \"\"\"\n    results = {}\n    errors = {}\n\n    for path in paths:\n        try:\n            results[path] = read_dataset(data_store, path, **kwargs)\n        except Exception as e:\n            errors[path] = str(e)\n\n    if errors:\n        error_msg = \"\\n\".join(f\"- {path}: {error}\" for path, error in errors.items())\n        raise ValueError(f\"Errors reading datasets:\\n{error_msg}\")\n\n    return results\n</code></pre>"},{"location":"api/core/#gigaspatial.core.io.read_gzipped_json_or_csv","title":"<code>read_gzipped_json_or_csv(file_path, data_store)</code>","text":"<p>Reads a gzipped file, attempting to parse it as JSON (lines=True) or CSV.</p> Source code in <code>gigaspatial/core/io/readers.py</code> <pre><code>def read_gzipped_json_or_csv(file_path, data_store):\n    \"\"\"Reads a gzipped file, attempting to parse it as JSON (lines=True) or CSV.\"\"\"\n\n    with data_store.open(file_path, \"rb\") as f:\n        g = gzip.GzipFile(fileobj=f)\n        text = g.read().decode(\"utf-8\")\n        try:\n            df = pd.read_json(io.StringIO(text), lines=True)\n            return df\n        except json.JSONDecodeError:\n            try:\n                df = pd.read_csv(io.StringIO(text))\n                return df\n            except pd.errors.ParserError:\n                print(f\"Error: Could not parse {file_path} as JSON or CSV.\")\n                return None\n</code></pre>"},{"location":"api/core/#gigaspatial.core.io.read_kmz","title":"<code>read_kmz(file_obj, **kwargs)</code>","text":"<p>Helper function to read KMZ files and return a GeoDataFrame.</p> Source code in <code>gigaspatial/core/io/readers.py</code> <pre><code>def read_kmz(file_obj, **kwargs):\n    \"\"\"Helper function to read KMZ files and return a GeoDataFrame.\"\"\"\n    try:\n        with zipfile.ZipFile(file_obj) as kmz:\n            # Find the KML file in the archive (usually doc.kml)\n            kml_filename = next(\n                name for name in kmz.namelist() if name.endswith(\".kml\")\n            )\n\n            # Read the KML content\n            kml_content = io.BytesIO(kmz.read(kml_filename))\n\n            gdf = gpd.read_file(kml_content)\n\n            # Validate the GeoDataFrame\n            if gdf.empty:\n                raise ValueError(\n                    \"The KML file is empty or does not contain valid geospatial data.\"\n                )\n\n        return gdf\n\n    except zipfile.BadZipFile:\n        raise ValueError(\"The provided file is not a valid KMZ file.\")\n    except StopIteration:\n        raise ValueError(\"No KML file found in the KMZ archive.\")\n    except Exception as e:\n        raise RuntimeError(f\"An error occurred: {e}\")\n</code></pre>"},{"location":"api/core/#gigaspatial.core.io.write_dataset","title":"<code>write_dataset(data, data_store, path, **kwargs)</code>","text":"<p>Write DataFrame or GeoDataFrame to various file formats in Azure Blob Storage.</p>"},{"location":"api/core/#gigaspatial.core.io.write_dataset--parameters","title":"Parameters:","text":"<p>data : pandas.DataFrame or geopandas.GeoDataFrame     The data to write to blob storage. data_store : DataStore     Instance of DataStore for accessing data storage. path : str     Path where the file will be written in data storage. **kwargs : dict     Additional arguments passed to the specific writer function.</p>"},{"location":"api/core/#gigaspatial.core.io.write_dataset--raises","title":"Raises:","text":"<p>ValueError     If the file type is unsupported or if there's an error writing the file. TypeError     If input data is not a DataFrame or GeoDataFrame.</p> Source code in <code>gigaspatial/core/io/writers.py</code> <pre><code>def write_dataset(data, data_store: DataStore, path, **kwargs):\n    \"\"\"\n    Write DataFrame or GeoDataFrame to various file formats in Azure Blob Storage.\n\n    Parameters:\n    ----------\n    data : pandas.DataFrame or geopandas.GeoDataFrame\n        The data to write to blob storage.\n    data_store : DataStore\n        Instance of DataStore for accessing data storage.\n    path : str\n        Path where the file will be written in data storage.\n    **kwargs : dict\n        Additional arguments passed to the specific writer function.\n\n    Raises:\n    ------\n    ValueError\n        If the file type is unsupported or if there's an error writing the file.\n    TypeError\n        If input data is not a DataFrame or GeoDataFrame.\n    \"\"\"\n\n    # Define supported file formats and their writers\n    BINARY_FORMATS = {\".shp\", \".zip\", \".parquet\", \".gpkg\", \".xlsx\", \".xls\"}\n\n    PANDAS_WRITERS = {\n        \".csv\": lambda df, buf, **kw: df.to_csv(buf, **kw),\n        \".xlsx\": lambda df, buf, **kw: df.to_excel(buf, engine=\"openpyxl\", **kw),\n        \".json\": lambda df, buf, **kw: df.to_json(buf, **kw),\n        \".parquet\": lambda df, buf, **kw: df.to_parquet(buf, **kw),\n    }\n\n    GEO_WRITERS = {\n        \".geojson\": lambda gdf, buf, **kw: gdf.to_file(buf, driver=\"GeoJSON\", **kw),\n        \".gpkg\": lambda gdf, buf, **kw: gdf.to_file(buf, driver=\"GPKG\", **kw),\n        \".parquet\": lambda gdf, buf, **kw: gdf.to_parquet(buf, **kw),\n    }\n\n    try:\n        # Input validation\n        if not isinstance(data, (pd.DataFrame, gpd.GeoDataFrame)):\n            raise TypeError(\"Input data must be a pandas DataFrame or GeoDataFrame\")\n\n        # Get file suffix and ensure it's lowercase\n        suffix = Path(path).suffix.lower()\n\n        # Determine if we need binary mode based on file type\n        mode = \"wb\" if suffix in BINARY_FORMATS else \"w\"\n\n        # Handle different data types and formats\n        if isinstance(data, gpd.GeoDataFrame):\n            if suffix not in GEO_WRITERS:\n                supported_formats = sorted(GEO_WRITERS.keys())\n                raise ValueError(\n                    f\"Unsupported file type for GeoDataFrame: {suffix}\\n\"\n                    f\"Supported formats: {', '.join(supported_formats)}\"\n                )\n\n            try:\n                with data_store.open(path, \"wb\") as f:\n                    GEO_WRITERS[suffix](data, f, **kwargs)\n            except Exception as e:\n                raise ValueError(f\"Error writing GeoDataFrame: {str(e)}\")\n\n        else:  # pandas DataFrame\n            if suffix not in PANDAS_WRITERS:\n                supported_formats = sorted(PANDAS_WRITERS.keys())\n                raise ValueError(\n                    f\"Unsupported file type for DataFrame: {suffix}\\n\"\n                    f\"Supported formats: {', '.join(supported_formats)}\"\n                )\n\n            try:\n                with data_store.open(path, mode) as f:\n                    PANDAS_WRITERS[suffix](data, f, **kwargs)\n            except Exception as e:\n                raise ValueError(f\"Error writing DataFrame: {str(e)}\")\n\n    except Exception as e:\n        if isinstance(e, (TypeError, ValueError)):\n            raise\n        raise RuntimeError(f\"Unexpected error writing dataset: {str(e)}\")\n</code></pre>"},{"location":"api/core/#gigaspatial.core.io.write_datasets","title":"<code>write_datasets(data_dict, data_store, **kwargs)</code>","text":"<p>Write multiple datasets to data storage at once.</p>"},{"location":"api/core/#gigaspatial.core.io.write_datasets--parameters","title":"Parameters:","text":"<p>data_dict : dict     Dictionary mapping paths to DataFrames/GeoDataFrames. data_store : DataStore     Instance of DataStore for accessing data storage. **kwargs : dict     Additional arguments passed to write_dataset.</p>"},{"location":"api/core/#gigaspatial.core.io.write_datasets--raises","title":"Raises:","text":"<p>ValueError     If there are any errors writing the datasets.</p> Source code in <code>gigaspatial/core/io/writers.py</code> <pre><code>def write_datasets(data_dict, data_store: DataStore, **kwargs):\n    \"\"\"\n    Write multiple datasets to data storage at once.\n\n    Parameters:\n    ----------\n    data_dict : dict\n        Dictionary mapping paths to DataFrames/GeoDataFrames.\n    data_store : DataStore\n        Instance of DataStore for accessing data storage.\n    **kwargs : dict\n        Additional arguments passed to write_dataset.\n\n    Raises:\n    ------\n    ValueError\n        If there are any errors writing the datasets.\n    \"\"\"\n    errors = {}\n\n    for path, data in data_dict.items():\n        try:\n            write_dataset(data, data_store, path, **kwargs)\n        except Exception as e:\n            errors[path] = str(e)\n\n    if errors:\n        error_msg = \"\\n\".join(f\"- {path}: {error}\" for path, error in errors.items())\n        raise ValueError(f\"Errors writing datasets:\\n{error_msg}\")\n</code></pre>"},{"location":"api/core/#gigaspatial.core.io.adls_data_store","title":"<code>adls_data_store</code>","text":""},{"location":"api/core/#gigaspatial.core.io.adls_data_store.ADLSDataStore","title":"<code>ADLSDataStore</code>","text":"<p>               Bases: <code>DataStore</code></p> <p>An implementation of DataStore for Azure Data Lake Storage.</p> Source code in <code>gigaspatial/core/io/adls_data_store.py</code> <pre><code>class ADLSDataStore(DataStore):\n    \"\"\"\n    An implementation of DataStore for Azure Data Lake Storage.\n    \"\"\"\n\n    def __init__(\n        self,\n        container: str = config.ADLS_CONTAINER_NAME,\n        connection_string: str = config.ADLS_CONNECTION_STRING,\n    ):\n        \"\"\"\n        Create a new instance of ADLSDataStore\n        :param container: The name of the container in ADLS to interact with.\n        \"\"\"\n        self.blob_service_client = BlobServiceClient.from_connection_string(\n            connection_string\n        )\n        self.container_client = self.blob_service_client.get_container_client(\n            container=container\n        )\n        self.container = container\n\n    def read_file(self, path: str, encoding: Optional[str] = None) -&gt; Union[str, bytes]:\n        \"\"\"\n        Read file with flexible encoding support.\n\n        :param path: Path to the file in blob storage\n        :param encoding: File encoding (optional)\n        :return: File contents as string or bytes\n        \"\"\"\n        try:\n            blob_client = self.container_client.get_blob_client(path)\n            blob_data = blob_client.download_blob().readall()\n\n            # If no encoding specified, return raw bytes\n            if encoding is None:\n                return blob_data\n\n            # If encoding is specified, decode the bytes\n            return blob_data.decode(encoding)\n\n        except Exception as e:\n            raise IOError(f\"Error reading file {path}: {e}\")\n\n    def write_file(self, path: str, data) -&gt; None:\n        \"\"\"\n        Write file with support for content type and improved type handling.\n\n        :param path: Destination path in blob storage\n        :param data: File contents\n        \"\"\"\n        blob_client = self.blob_service_client.get_blob_client(\n            container=self.container, blob=path, snapshot=None\n        )\n\n        if isinstance(data, str):\n            binary_data = data.encode()\n        elif isinstance(data, bytes):\n            binary_data = data\n        else:\n            raise Exception(f'Unsupported data type. Only \"bytes\" or \"string\" accepted')\n\n        blob_client.upload_blob(binary_data, overwrite=True)\n\n    def upload_file(self, file_path, blob_path):\n        \"\"\"Uploads a single file to Azure Blob Storage.\"\"\"\n        try:\n            blob_client = self.container_client.get_blob_client(blob_path)\n            with open(file_path, \"rb\") as data:\n                blob_client.upload_blob(data, overwrite=True)\n            print(f\"Uploaded {file_path} to {blob_path}\")\n        except Exception as e:\n            print(f\"Failed to upload {file_path}: {e}\")\n\n    def upload_directory(self, dir_path, blob_dir_path):\n        \"\"\"Uploads all files from a directory to Azure Blob Storage.\"\"\"\n        for root, dirs, files in os.walk(dir_path):\n            for file in files:\n                local_file_path = os.path.join(root, file)\n                relative_path = os.path.relpath(local_file_path, dir_path)\n                blob_file_path = os.path.join(blob_dir_path, relative_path).replace(\n                    \"\\\\\", \"/\"\n                )\n\n                self.upload_file(local_file_path, blob_file_path)\n\n    def download_directory(self, blob_dir_path: str, local_dir_path: str):\n        \"\"\"Downloads all files from a directory in Azure Blob Storage to a local directory.\"\"\"\n        try:\n            # Ensure the local directory exists\n            os.makedirs(local_dir_path, exist_ok=True)\n\n            # List all files in the blob directory\n            blob_items = self.container_client.list_blobs(\n                name_starts_with=blob_dir_path\n            )\n\n            for blob_item in blob_items:\n                # Get the relative path of the blob file\n                relative_path = os.path.relpath(blob_item.name, blob_dir_path)\n                # Construct the local file path\n                local_file_path = os.path.join(local_dir_path, relative_path)\n                # Create directories if needed\n                os.makedirs(os.path.dirname(local_file_path), exist_ok=True)\n\n                # Download the blob to the local file\n                blob_client = self.container_client.get_blob_client(blob_item.name)\n                with open(local_file_path, \"wb\") as file:\n                    file.write(blob_client.download_blob().readall())\n\n            print(f\"Downloaded directory {blob_dir_path} to {local_dir_path}\")\n        except Exception as e:\n            print(f\"Failed to download directory {blob_dir_path}: {e}\")\n\n    def copy_directory(self, source_dir: str, destination_dir: str):\n        \"\"\"\n        Copies all files from a source directory to a destination directory within the same container.\n\n        :param source_dir: The source directory path in the blob storage\n        :param destination_dir: The destination directory path in the blob storage\n        \"\"\"\n        try:\n            # Ensure source directory path ends with a trailing slash\n            source_dir = source_dir.rstrip(\"/\") + \"/\"\n            destination_dir = destination_dir.rstrip(\"/\") + \"/\"\n\n            # List all blobs in the source directory\n            source_blobs = self.container_client.list_blobs(name_starts_with=source_dir)\n\n            for blob in source_blobs:\n                # Get the relative path of the blob\n                relative_path = os.path.relpath(blob.name, source_dir)\n\n                # Construct the new blob path\n                new_blob_path = os.path.join(destination_dir, relative_path).replace(\n                    \"\\\\\", \"/\"\n                )\n\n                # Create a source blob client\n                source_blob_client = self.container_client.get_blob_client(blob.name)\n\n                # Create a destination blob client\n                destination_blob_client = self.container_client.get_blob_client(\n                    new_blob_path\n                )\n\n                # Start the copy operation\n                destination_blob_client.start_copy_from_url(source_blob_client.url)\n\n            print(f\"Copied directory from {source_dir} to {destination_dir}\")\n        except Exception as e:\n            print(f\"Failed to copy directory {source_dir}: {e}\")\n\n    def exists(self, path: str) -&gt; bool:\n        blob_client = self.blob_service_client.get_blob_client(\n            container=self.container, blob=path, snapshot=None\n        )\n        return blob_client.exists()\n\n    def file_exists(self, path: str) -&gt; bool:\n        return self.exists(path) and not self.is_dir(path)\n\n    def file_size(self, path: str) -&gt; float:\n        blob_client = self.blob_service_client.get_blob_client(\n            container=self.container, blob=path, snapshot=None\n        )\n        properties = blob_client.get_blob_properties()\n\n        # The size is in bytes, convert it to kilobytes\n        size_in_bytes = properties.size\n        size_in_kb = size_in_bytes / 1024.0\n        return size_in_kb\n\n    def list_files(self, path: str):\n        blob_items = self.container_client.list_blobs(name_starts_with=path)\n        return [item[\"name\"] for item in blob_items]\n\n    def walk(self, top: str):\n        top = top.rstrip(\"/\") + \"/\"\n        blob_items = self.container_client.list_blobs(name_starts_with=top)\n        blobs = [item[\"name\"] for item in blob_items]\n        for blob in blobs:\n            dirpath, filename = os.path.split(blob)\n            yield (dirpath, [], [filename])\n\n    def list_directories(self, path: str) -&gt; list:\n        \"\"\"List only directory names (not files) from a given path in ADLS.\"\"\"\n        search_path = path.rstrip(\"/\") + \"/\" if path else \"\"\n\n        blob_items = self.container_client.list_blobs(name_starts_with=search_path)\n\n        directories = set()\n\n        for blob_item in blob_items:\n            # Get the relative path from the search path\n            relative_path = blob_item.name[len(search_path) :]\n\n            # Skip if it's empty (shouldn't happen but just in case)\n            if not relative_path:\n                continue\n\n            # If there's a \"/\" in the relative path, it means there's a subdirectory\n            if \"/\" in relative_path:\n                # Get the first directory name\n                dir_name = relative_path.split(\"/\")[0]\n                directories.add(dir_name)\n\n        return sorted(list(directories))\n\n    @contextlib.contextmanager\n    def open(self, path: str, mode: str = \"r\"):\n        \"\"\"\n        Context manager for file operations with enhanced mode support.\n\n        :param path: File path in blob storage\n        :param mode: File open mode (r, rb, w, wb)\n        \"\"\"\n        if mode == \"w\":\n            file = io.StringIO()\n            yield file\n            self.write_file(path, file.getvalue())\n\n        elif mode == \"wb\":\n            file = io.BytesIO()\n            yield file\n            self.write_file(path, file.getvalue())\n\n        elif mode == \"r\":\n            data = self.read_file(path, encoding=\"UTF-8\")\n            file = io.StringIO(data)\n            yield file\n\n        elif mode == \"rb\":\n            data = self.read_file(path)\n            file = io.BytesIO(data)\n            yield file\n\n    def get_file_metadata(self, path: str) -&gt; dict:\n        \"\"\"\n        Retrieve comprehensive file metadata.\n\n        :param path: File path in blob storage\n        :return: File metadata dictionary\n        \"\"\"\n        blob_client = self.container_client.get_blob_client(path)\n        properties = blob_client.get_blob_properties()\n\n        return {\n            \"name\": path,\n            \"size_bytes\": properties.size,\n            \"content_type\": properties.content_settings.content_type,\n            \"last_modified\": properties.last_modified,\n            \"etag\": properties.etag,\n        }\n\n    def is_file(self, path: str) -&gt; bool:\n        return self.file_exists(path)\n\n    def is_dir(self, path: str) -&gt; bool:\n        dir_path = path.rstrip(\"/\") + \"/\"\n\n        existing_blobs = self.list_files(dir_path)\n\n        if len(existing_blobs) &gt; 1:\n            return True\n        elif len(existing_blobs) == 1:\n            if existing_blobs[0] != path.rstrip(\"/\"):\n                return True\n\n        return False\n\n    def rmdir(self, dir: str) -&gt; None:\n        blobs = self.list_files(dir)\n        self.container_client.delete_blobs(*blobs)\n\n    def mkdir(self, path: str, exist_ok: bool = False) -&gt; None:\n        \"\"\"\n        Create a directory in Azure Blob Storage.\n\n        In ADLS, directories are conceptual and created by adding a placeholder blob.\n\n        :param path: Path of the directory to create\n        :param exist_ok: If False, raise an error if the directory already exists\n        \"\"\"\n        dir_path = path.rstrip(\"/\") + \"/\"\n\n        existing_blobs = list(self.list_files(dir_path))\n\n        if existing_blobs and not exist_ok:\n            raise FileExistsError(f\"Directory {path} already exists\")\n\n        # Create a placeholder blob to represent the directory\n        placeholder_blob_path = os.path.join(dir_path, \".placeholder\")\n\n        # Only create placeholder if it doesn't already exist\n        if not self.file_exists(placeholder_blob_path):\n            placeholder_content = (\n                b\"This is a placeholder blob to represent a directory.\"\n            )\n            blob_client = self.blob_service_client.get_blob_client(\n                container=self.container, blob=placeholder_blob_path\n            )\n            blob_client.upload_blob(placeholder_content, overwrite=True)\n\n    def remove(self, path: str) -&gt; None:\n        blob_client = self.blob_service_client.get_blob_client(\n            container=self.container, blob=path, snapshot=None\n        )\n        if blob_client.exists():\n            blob_client.delete_blob()\n</code></pre>"},{"location":"api/core/#gigaspatial.core.io.adls_data_store.ADLSDataStore.__init__","title":"<code>__init__(container=config.ADLS_CONTAINER_NAME, connection_string=config.ADLS_CONNECTION_STRING)</code>","text":"<p>Create a new instance of ADLSDataStore :param container: The name of the container in ADLS to interact with.</p> Source code in <code>gigaspatial/core/io/adls_data_store.py</code> <pre><code>def __init__(\n    self,\n    container: str = config.ADLS_CONTAINER_NAME,\n    connection_string: str = config.ADLS_CONNECTION_STRING,\n):\n    \"\"\"\n    Create a new instance of ADLSDataStore\n    :param container: The name of the container in ADLS to interact with.\n    \"\"\"\n    self.blob_service_client = BlobServiceClient.from_connection_string(\n        connection_string\n    )\n    self.container_client = self.blob_service_client.get_container_client(\n        container=container\n    )\n    self.container = container\n</code></pre>"},{"location":"api/core/#gigaspatial.core.io.adls_data_store.ADLSDataStore.copy_directory","title":"<code>copy_directory(source_dir, destination_dir)</code>","text":"<p>Copies all files from a source directory to a destination directory within the same container.</p> <p>:param source_dir: The source directory path in the blob storage :param destination_dir: The destination directory path in the blob storage</p> Source code in <code>gigaspatial/core/io/adls_data_store.py</code> <pre><code>def copy_directory(self, source_dir: str, destination_dir: str):\n    \"\"\"\n    Copies all files from a source directory to a destination directory within the same container.\n\n    :param source_dir: The source directory path in the blob storage\n    :param destination_dir: The destination directory path in the blob storage\n    \"\"\"\n    try:\n        # Ensure source directory path ends with a trailing slash\n        source_dir = source_dir.rstrip(\"/\") + \"/\"\n        destination_dir = destination_dir.rstrip(\"/\") + \"/\"\n\n        # List all blobs in the source directory\n        source_blobs = self.container_client.list_blobs(name_starts_with=source_dir)\n\n        for blob in source_blobs:\n            # Get the relative path of the blob\n            relative_path = os.path.relpath(blob.name, source_dir)\n\n            # Construct the new blob path\n            new_blob_path = os.path.join(destination_dir, relative_path).replace(\n                \"\\\\\", \"/\"\n            )\n\n            # Create a source blob client\n            source_blob_client = self.container_client.get_blob_client(blob.name)\n\n            # Create a destination blob client\n            destination_blob_client = self.container_client.get_blob_client(\n                new_blob_path\n            )\n\n            # Start the copy operation\n            destination_blob_client.start_copy_from_url(source_blob_client.url)\n\n        print(f\"Copied directory from {source_dir} to {destination_dir}\")\n    except Exception as e:\n        print(f\"Failed to copy directory {source_dir}: {e}\")\n</code></pre>"},{"location":"api/core/#gigaspatial.core.io.adls_data_store.ADLSDataStore.download_directory","title":"<code>download_directory(blob_dir_path, local_dir_path)</code>","text":"<p>Downloads all files from a directory in Azure Blob Storage to a local directory.</p> Source code in <code>gigaspatial/core/io/adls_data_store.py</code> <pre><code>def download_directory(self, blob_dir_path: str, local_dir_path: str):\n    \"\"\"Downloads all files from a directory in Azure Blob Storage to a local directory.\"\"\"\n    try:\n        # Ensure the local directory exists\n        os.makedirs(local_dir_path, exist_ok=True)\n\n        # List all files in the blob directory\n        blob_items = self.container_client.list_blobs(\n            name_starts_with=blob_dir_path\n        )\n\n        for blob_item in blob_items:\n            # Get the relative path of the blob file\n            relative_path = os.path.relpath(blob_item.name, blob_dir_path)\n            # Construct the local file path\n            local_file_path = os.path.join(local_dir_path, relative_path)\n            # Create directories if needed\n            os.makedirs(os.path.dirname(local_file_path), exist_ok=True)\n\n            # Download the blob to the local file\n            blob_client = self.container_client.get_blob_client(blob_item.name)\n            with open(local_file_path, \"wb\") as file:\n                file.write(blob_client.download_blob().readall())\n\n        print(f\"Downloaded directory {blob_dir_path} to {local_dir_path}\")\n    except Exception as e:\n        print(f\"Failed to download directory {blob_dir_path}: {e}\")\n</code></pre>"},{"location":"api/core/#gigaspatial.core.io.adls_data_store.ADLSDataStore.get_file_metadata","title":"<code>get_file_metadata(path)</code>","text":"<p>Retrieve comprehensive file metadata.</p> <p>:param path: File path in blob storage :return: File metadata dictionary</p> Source code in <code>gigaspatial/core/io/adls_data_store.py</code> <pre><code>def get_file_metadata(self, path: str) -&gt; dict:\n    \"\"\"\n    Retrieve comprehensive file metadata.\n\n    :param path: File path in blob storage\n    :return: File metadata dictionary\n    \"\"\"\n    blob_client = self.container_client.get_blob_client(path)\n    properties = blob_client.get_blob_properties()\n\n    return {\n        \"name\": path,\n        \"size_bytes\": properties.size,\n        \"content_type\": properties.content_settings.content_type,\n        \"last_modified\": properties.last_modified,\n        \"etag\": properties.etag,\n    }\n</code></pre>"},{"location":"api/core/#gigaspatial.core.io.adls_data_store.ADLSDataStore.list_directories","title":"<code>list_directories(path)</code>","text":"<p>List only directory names (not files) from a given path in ADLS.</p> Source code in <code>gigaspatial/core/io/adls_data_store.py</code> <pre><code>def list_directories(self, path: str) -&gt; list:\n    \"\"\"List only directory names (not files) from a given path in ADLS.\"\"\"\n    search_path = path.rstrip(\"/\") + \"/\" if path else \"\"\n\n    blob_items = self.container_client.list_blobs(name_starts_with=search_path)\n\n    directories = set()\n\n    for blob_item in blob_items:\n        # Get the relative path from the search path\n        relative_path = blob_item.name[len(search_path) :]\n\n        # Skip if it's empty (shouldn't happen but just in case)\n        if not relative_path:\n            continue\n\n        # If there's a \"/\" in the relative path, it means there's a subdirectory\n        if \"/\" in relative_path:\n            # Get the first directory name\n            dir_name = relative_path.split(\"/\")[0]\n            directories.add(dir_name)\n\n    return sorted(list(directories))\n</code></pre>"},{"location":"api/core/#gigaspatial.core.io.adls_data_store.ADLSDataStore.mkdir","title":"<code>mkdir(path, exist_ok=False)</code>","text":"<p>Create a directory in Azure Blob Storage.</p> <p>In ADLS, directories are conceptual and created by adding a placeholder blob.</p> <p>:param path: Path of the directory to create :param exist_ok: If False, raise an error if the directory already exists</p> Source code in <code>gigaspatial/core/io/adls_data_store.py</code> <pre><code>def mkdir(self, path: str, exist_ok: bool = False) -&gt; None:\n    \"\"\"\n    Create a directory in Azure Blob Storage.\n\n    In ADLS, directories are conceptual and created by adding a placeholder blob.\n\n    :param path: Path of the directory to create\n    :param exist_ok: If False, raise an error if the directory already exists\n    \"\"\"\n    dir_path = path.rstrip(\"/\") + \"/\"\n\n    existing_blobs = list(self.list_files(dir_path))\n\n    if existing_blobs and not exist_ok:\n        raise FileExistsError(f\"Directory {path} already exists\")\n\n    # Create a placeholder blob to represent the directory\n    placeholder_blob_path = os.path.join(dir_path, \".placeholder\")\n\n    # Only create placeholder if it doesn't already exist\n    if not self.file_exists(placeholder_blob_path):\n        placeholder_content = (\n            b\"This is a placeholder blob to represent a directory.\"\n        )\n        blob_client = self.blob_service_client.get_blob_client(\n            container=self.container, blob=placeholder_blob_path\n        )\n        blob_client.upload_blob(placeholder_content, overwrite=True)\n</code></pre>"},{"location":"api/core/#gigaspatial.core.io.adls_data_store.ADLSDataStore.open","title":"<code>open(path, mode='r')</code>","text":"<p>Context manager for file operations with enhanced mode support.</p> <p>:param path: File path in blob storage :param mode: File open mode (r, rb, w, wb)</p> Source code in <code>gigaspatial/core/io/adls_data_store.py</code> <pre><code>@contextlib.contextmanager\ndef open(self, path: str, mode: str = \"r\"):\n    \"\"\"\n    Context manager for file operations with enhanced mode support.\n\n    :param path: File path in blob storage\n    :param mode: File open mode (r, rb, w, wb)\n    \"\"\"\n    if mode == \"w\":\n        file = io.StringIO()\n        yield file\n        self.write_file(path, file.getvalue())\n\n    elif mode == \"wb\":\n        file = io.BytesIO()\n        yield file\n        self.write_file(path, file.getvalue())\n\n    elif mode == \"r\":\n        data = self.read_file(path, encoding=\"UTF-8\")\n        file = io.StringIO(data)\n        yield file\n\n    elif mode == \"rb\":\n        data = self.read_file(path)\n        file = io.BytesIO(data)\n        yield file\n</code></pre>"},{"location":"api/core/#gigaspatial.core.io.adls_data_store.ADLSDataStore.read_file","title":"<code>read_file(path, encoding=None)</code>","text":"<p>Read file with flexible encoding support.</p> <p>:param path: Path to the file in blob storage :param encoding: File encoding (optional) :return: File contents as string or bytes</p> Source code in <code>gigaspatial/core/io/adls_data_store.py</code> <pre><code>def read_file(self, path: str, encoding: Optional[str] = None) -&gt; Union[str, bytes]:\n    \"\"\"\n    Read file with flexible encoding support.\n\n    :param path: Path to the file in blob storage\n    :param encoding: File encoding (optional)\n    :return: File contents as string or bytes\n    \"\"\"\n    try:\n        blob_client = self.container_client.get_blob_client(path)\n        blob_data = blob_client.download_blob().readall()\n\n        # If no encoding specified, return raw bytes\n        if encoding is None:\n            return blob_data\n\n        # If encoding is specified, decode the bytes\n        return blob_data.decode(encoding)\n\n    except Exception as e:\n        raise IOError(f\"Error reading file {path}: {e}\")\n</code></pre>"},{"location":"api/core/#gigaspatial.core.io.adls_data_store.ADLSDataStore.upload_directory","title":"<code>upload_directory(dir_path, blob_dir_path)</code>","text":"<p>Uploads all files from a directory to Azure Blob Storage.</p> Source code in <code>gigaspatial/core/io/adls_data_store.py</code> <pre><code>def upload_directory(self, dir_path, blob_dir_path):\n    \"\"\"Uploads all files from a directory to Azure Blob Storage.\"\"\"\n    for root, dirs, files in os.walk(dir_path):\n        for file in files:\n            local_file_path = os.path.join(root, file)\n            relative_path = os.path.relpath(local_file_path, dir_path)\n            blob_file_path = os.path.join(blob_dir_path, relative_path).replace(\n                \"\\\\\", \"/\"\n            )\n\n            self.upload_file(local_file_path, blob_file_path)\n</code></pre>"},{"location":"api/core/#gigaspatial.core.io.adls_data_store.ADLSDataStore.upload_file","title":"<code>upload_file(file_path, blob_path)</code>","text":"<p>Uploads a single file to Azure Blob Storage.</p> Source code in <code>gigaspatial/core/io/adls_data_store.py</code> <pre><code>def upload_file(self, file_path, blob_path):\n    \"\"\"Uploads a single file to Azure Blob Storage.\"\"\"\n    try:\n        blob_client = self.container_client.get_blob_client(blob_path)\n        with open(file_path, \"rb\") as data:\n            blob_client.upload_blob(data, overwrite=True)\n        print(f\"Uploaded {file_path} to {blob_path}\")\n    except Exception as e:\n        print(f\"Failed to upload {file_path}: {e}\")\n</code></pre>"},{"location":"api/core/#gigaspatial.core.io.adls_data_store.ADLSDataStore.write_file","title":"<code>write_file(path, data)</code>","text":"<p>Write file with support for content type and improved type handling.</p> <p>:param path: Destination path in blob storage :param data: File contents</p> Source code in <code>gigaspatial/core/io/adls_data_store.py</code> <pre><code>def write_file(self, path: str, data) -&gt; None:\n    \"\"\"\n    Write file with support for content type and improved type handling.\n\n    :param path: Destination path in blob storage\n    :param data: File contents\n    \"\"\"\n    blob_client = self.blob_service_client.get_blob_client(\n        container=self.container, blob=path, snapshot=None\n    )\n\n    if isinstance(data, str):\n        binary_data = data.encode()\n    elif isinstance(data, bytes):\n        binary_data = data\n    else:\n        raise Exception(f'Unsupported data type. Only \"bytes\" or \"string\" accepted')\n\n    blob_client.upload_blob(binary_data, overwrite=True)\n</code></pre>"},{"location":"api/core/#gigaspatial.core.io.data_api","title":"<code>data_api</code>","text":""},{"location":"api/core/#gigaspatial.core.io.data_api.GigaDataAPI","title":"<code>GigaDataAPI</code>","text":"Source code in <code>gigaspatial/core/io/data_api.py</code> <pre><code>class GigaDataAPI:\n\n    def __init__(\n        self,\n        profile_file: Union[str, Path] = config.API_PROFILE_FILE_PATH,\n        share_name: str = config.API_SHARE_NAME,\n        schema_name: str = config.API_SCHEMA_NAME,\n    ):\n        \"\"\"\n        Initialize the GigaDataAPI class with the profile file, share name, and schema name.\n\n        profile_file: Path to the delta-sharing profile file.\n        share_name: Name of the share (e.g., \"gold\").\n        schema_name: Name of the schema (e.g., \"school-master\").\n        \"\"\"\n        self.profile_file = profile_file\n        self.share_name = share_name\n        self.schema_name = schema_name\n        self.client = delta_sharing.SharingClient(profile_file)\n\n        self._cache = {}\n\n    def get_country_list(self, sort=True):\n        \"\"\"\n        Retrieve a list of available countries in the dataset.\n\n        :param sort: Whether to sort the country list alphabetically (default is True).\n        \"\"\"\n        country_list = [\n            t.name for t in self.client.list_all_tables() if t.share == self.share_name\n        ]\n        if sort:\n            country_list.sort()\n        return country_list\n\n    def load_country_data(self, country, filters=None, use_cache=True):\n        \"\"\"\n        Load the dataset for the specified country with optional filtering and caching.\n\n        country: The country code (e.g., \"MWI\").\n        filters: A dictionary with column names as keys and filter values as values.\n        use_cache: Whether to use cached data if available (default is True).\n        \"\"\"\n        # Check if data is cached\n        if use_cache and country in self._cache:\n            df_country = self._cache[country]\n        else:\n            # Load data from the API\n            table_url = (\n                f\"{self.profile_file}#{self.share_name}.{self.schema_name}.{country}\"\n            )\n            df_country = delta_sharing.load_as_pandas(table_url)\n            self._cache[country] = df_country  # Cache the data\n\n        # Apply filters if provided\n        if filters:\n            for column, value in filters.items():\n                df_country = df_country[df_country[column] == value]\n\n        return df_country\n\n    def load_multiple_countries(self, countries):\n        \"\"\"\n        Load data for multiple countries and combine them into a single DataFrame.\n\n        countries: A list of country codes.\n        \"\"\"\n        df_list = []\n        for country in countries:\n            df_list.append(self.load_country_data(country))\n        return pd.concat(df_list, ignore_index=True)\n\n    def get_country_metadata(self, country):\n        \"\"\"\n        Retrieve metadata (e.g., column names and data types) for a country's dataset.\n\n        country: The country code (e.g., \"MWI\").\n        \"\"\"\n        df_country = self.load_country_data(country)\n        metadata = {\n            \"columns\": df_country.columns.tolist(),\n            \"data_types\": df_country.dtypes.to_dict(),\n            \"num_records\": len(df_country),\n        }\n        return metadata\n\n    def get_all_cached_data_as_dict(self):\n        \"\"\"\n        Retrieve all cached data in a dictionary format, where each key is a country code,\n        and the value is the DataFrame of that country.\n        \"\"\"\n        return self._cache if self._cache else {}\n\n    def get_all_cached_data_as_json(self):\n        \"\"\"\n        Retrieve all cached data in a JSON-like format. Each country is represented as a key,\n        and the value is a list of records (i.e., the DataFrame's `to_dict(orient='records')` format).\n        \"\"\"\n        if not self._cache:\n            return {}\n\n        # Convert each DataFrame in the cache to a JSON-like format (list of records)\n        return {\n            country: df.to_dict(orient=\"records\") for country, df in self._cache.items()\n        }\n</code></pre>"},{"location":"api/core/#gigaspatial.core.io.data_api.GigaDataAPI.__init__","title":"<code>__init__(profile_file=config.API_PROFILE_FILE_PATH, share_name=config.API_SHARE_NAME, schema_name=config.API_SCHEMA_NAME)</code>","text":"<p>Initialize the GigaDataAPI class with the profile file, share name, and schema name.</p> <p>profile_file: Path to the delta-sharing profile file. share_name: Name of the share (e.g., \"gold\"). schema_name: Name of the schema (e.g., \"school-master\").</p> Source code in <code>gigaspatial/core/io/data_api.py</code> <pre><code>def __init__(\n    self,\n    profile_file: Union[str, Path] = config.API_PROFILE_FILE_PATH,\n    share_name: str = config.API_SHARE_NAME,\n    schema_name: str = config.API_SCHEMA_NAME,\n):\n    \"\"\"\n    Initialize the GigaDataAPI class with the profile file, share name, and schema name.\n\n    profile_file: Path to the delta-sharing profile file.\n    share_name: Name of the share (e.g., \"gold\").\n    schema_name: Name of the schema (e.g., \"school-master\").\n    \"\"\"\n    self.profile_file = profile_file\n    self.share_name = share_name\n    self.schema_name = schema_name\n    self.client = delta_sharing.SharingClient(profile_file)\n\n    self._cache = {}\n</code></pre>"},{"location":"api/core/#gigaspatial.core.io.data_api.GigaDataAPI.get_all_cached_data_as_dict","title":"<code>get_all_cached_data_as_dict()</code>","text":"<p>Retrieve all cached data in a dictionary format, where each key is a country code, and the value is the DataFrame of that country.</p> Source code in <code>gigaspatial/core/io/data_api.py</code> <pre><code>def get_all_cached_data_as_dict(self):\n    \"\"\"\n    Retrieve all cached data in a dictionary format, where each key is a country code,\n    and the value is the DataFrame of that country.\n    \"\"\"\n    return self._cache if self._cache else {}\n</code></pre>"},{"location":"api/core/#gigaspatial.core.io.data_api.GigaDataAPI.get_all_cached_data_as_json","title":"<code>get_all_cached_data_as_json()</code>","text":"<p>Retrieve all cached data in a JSON-like format. Each country is represented as a key, and the value is a list of records (i.e., the DataFrame's <code>to_dict(orient='records')</code> format).</p> Source code in <code>gigaspatial/core/io/data_api.py</code> <pre><code>def get_all_cached_data_as_json(self):\n    \"\"\"\n    Retrieve all cached data in a JSON-like format. Each country is represented as a key,\n    and the value is a list of records (i.e., the DataFrame's `to_dict(orient='records')` format).\n    \"\"\"\n    if not self._cache:\n        return {}\n\n    # Convert each DataFrame in the cache to a JSON-like format (list of records)\n    return {\n        country: df.to_dict(orient=\"records\") for country, df in self._cache.items()\n    }\n</code></pre>"},{"location":"api/core/#gigaspatial.core.io.data_api.GigaDataAPI.get_country_list","title":"<code>get_country_list(sort=True)</code>","text":"<p>Retrieve a list of available countries in the dataset.</p> <p>:param sort: Whether to sort the country list alphabetically (default is True).</p> Source code in <code>gigaspatial/core/io/data_api.py</code> <pre><code>def get_country_list(self, sort=True):\n    \"\"\"\n    Retrieve a list of available countries in the dataset.\n\n    :param sort: Whether to sort the country list alphabetically (default is True).\n    \"\"\"\n    country_list = [\n        t.name for t in self.client.list_all_tables() if t.share == self.share_name\n    ]\n    if sort:\n        country_list.sort()\n    return country_list\n</code></pre>"},{"location":"api/core/#gigaspatial.core.io.data_api.GigaDataAPI.get_country_metadata","title":"<code>get_country_metadata(country)</code>","text":"<p>Retrieve metadata (e.g., column names and data types) for a country's dataset.</p> <p>country: The country code (e.g., \"MWI\").</p> Source code in <code>gigaspatial/core/io/data_api.py</code> <pre><code>def get_country_metadata(self, country):\n    \"\"\"\n    Retrieve metadata (e.g., column names and data types) for a country's dataset.\n\n    country: The country code (e.g., \"MWI\").\n    \"\"\"\n    df_country = self.load_country_data(country)\n    metadata = {\n        \"columns\": df_country.columns.tolist(),\n        \"data_types\": df_country.dtypes.to_dict(),\n        \"num_records\": len(df_country),\n    }\n    return metadata\n</code></pre>"},{"location":"api/core/#gigaspatial.core.io.data_api.GigaDataAPI.load_country_data","title":"<code>load_country_data(country, filters=None, use_cache=True)</code>","text":"<p>Load the dataset for the specified country with optional filtering and caching.</p> <p>country: The country code (e.g., \"MWI\"). filters: A dictionary with column names as keys and filter values as values. use_cache: Whether to use cached data if available (default is True).</p> Source code in <code>gigaspatial/core/io/data_api.py</code> <pre><code>def load_country_data(self, country, filters=None, use_cache=True):\n    \"\"\"\n    Load the dataset for the specified country with optional filtering and caching.\n\n    country: The country code (e.g., \"MWI\").\n    filters: A dictionary with column names as keys and filter values as values.\n    use_cache: Whether to use cached data if available (default is True).\n    \"\"\"\n    # Check if data is cached\n    if use_cache and country in self._cache:\n        df_country = self._cache[country]\n    else:\n        # Load data from the API\n        table_url = (\n            f\"{self.profile_file}#{self.share_name}.{self.schema_name}.{country}\"\n        )\n        df_country = delta_sharing.load_as_pandas(table_url)\n        self._cache[country] = df_country  # Cache the data\n\n    # Apply filters if provided\n    if filters:\n        for column, value in filters.items():\n            df_country = df_country[df_country[column] == value]\n\n    return df_country\n</code></pre>"},{"location":"api/core/#gigaspatial.core.io.data_api.GigaDataAPI.load_multiple_countries","title":"<code>load_multiple_countries(countries)</code>","text":"<p>Load data for multiple countries and combine them into a single DataFrame.</p> <p>countries: A list of country codes.</p> Source code in <code>gigaspatial/core/io/data_api.py</code> <pre><code>def load_multiple_countries(self, countries):\n    \"\"\"\n    Load data for multiple countries and combine them into a single DataFrame.\n\n    countries: A list of country codes.\n    \"\"\"\n    df_list = []\n    for country in countries:\n        df_list.append(self.load_country_data(country))\n    return pd.concat(df_list, ignore_index=True)\n</code></pre>"},{"location":"api/core/#gigaspatial.core.io.data_store","title":"<code>data_store</code>","text":""},{"location":"api/core/#gigaspatial.core.io.data_store.DataStore","title":"<code>DataStore</code>","text":"<p>               Bases: <code>ABC</code></p> <p>Abstract base class defining the interface for data store implementations. This class serves as a parent for both local and cloud-based storage solutions.</p> Source code in <code>gigaspatial/core/io/data_store.py</code> <pre><code>class DataStore(ABC):\n    \"\"\"\n    Abstract base class defining the interface for data store implementations.\n    This class serves as a parent for both local and cloud-based storage solutions.\n    \"\"\"\n\n    @abstractmethod\n    def read_file(self, path: str) -&gt; Any:\n        \"\"\"\n        Read contents of a file from the data store.\n\n        Args:\n            path: Path to the file to read\n\n        Returns:\n            Contents of the file\n\n        Raises:\n            IOError: If file cannot be read\n        \"\"\"\n        pass\n\n    @abstractmethod\n    def write_file(self, path: str, data: Any) -&gt; None:\n        \"\"\"\n        Write data to a file in the data store.\n\n        Args:\n            path: Path where to write the file\n            data: Data to write to the file\n\n        Raises:\n            IOError: If file cannot be written\n        \"\"\"\n        pass\n\n    @abstractmethod\n    def file_exists(self, path: str) -&gt; bool:\n        \"\"\"\n        Check if a file exists in the data store.\n\n        Args:\n            path: Path to check\n\n        Returns:\n            True if file exists, False otherwise\n        \"\"\"\n        pass\n\n    @abstractmethod\n    def list_files(self, path: str) -&gt; List[str]:\n        \"\"\"\n        List all files in a directory.\n\n        Args:\n            path: Directory path to list\n\n        Returns:\n            List of file paths in the directory\n        \"\"\"\n        pass\n\n    @abstractmethod\n    def walk(self, top: str) -&gt; Generator:\n        \"\"\"\n        Walk through directory tree, similar to os.walk().\n\n        Args:\n            top: Starting directory for the walk\n\n        Returns:\n            Generator yielding tuples of (dirpath, dirnames, filenames)\n        \"\"\"\n        pass\n\n    @abstractmethod\n    def open(self, file: str, mode: str = \"r\") -&gt; Union[str, bytes]:\n        \"\"\"\n        Context manager for file operations.\n\n        Args:\n            file: Path to the file\n            mode: File mode ('r', 'w', 'rb', 'wb')\n\n        Yields:\n            File-like object\n\n        Raises:\n            IOError: If file cannot be opened\n        \"\"\"\n        pass\n\n    @abstractmethod\n    def is_file(self, path: str) -&gt; bool:\n        \"\"\"\n        Check if path points to a file.\n\n        Args:\n            path: Path to check\n\n        Returns:\n            True if path is a file, False otherwise\n        \"\"\"\n        pass\n\n    @abstractmethod\n    def is_dir(self, path: str) -&gt; bool:\n        \"\"\"\n        Check if path points to a directory.\n\n        Args:\n            path: Path to check\n\n        Returns:\n            True if path is a directory, False otherwise\n        \"\"\"\n        pass\n\n    @abstractmethod\n    def remove(self, path: str) -&gt; None:\n        \"\"\"\n        Remove a file.\n\n        Args:\n            path: Path to the file to remove\n\n        Raises:\n            IOError: If file cannot be removed\n        \"\"\"\n        pass\n\n    @abstractmethod\n    def rmdir(self, dir: str) -&gt; None:\n        \"\"\"\n        Remove a directory and all its contents.\n\n        Args:\n            dir: Path to the directory to remove\n\n        Raises:\n            IOError: If directory cannot be removed\n        \"\"\"\n        pass\n</code></pre>"},{"location":"api/core/#gigaspatial.core.io.data_store.DataStore.file_exists","title":"<code>file_exists(path)</code>  <code>abstractmethod</code>","text":"<p>Check if a file exists in the data store.</p> <p>Parameters:</p> Name Type Description Default <code>path</code> <code>str</code> <p>Path to check</p> required <p>Returns:</p> Type Description <code>bool</code> <p>True if file exists, False otherwise</p> Source code in <code>gigaspatial/core/io/data_store.py</code> <pre><code>@abstractmethod\ndef file_exists(self, path: str) -&gt; bool:\n    \"\"\"\n    Check if a file exists in the data store.\n\n    Args:\n        path: Path to check\n\n    Returns:\n        True if file exists, False otherwise\n    \"\"\"\n    pass\n</code></pre>"},{"location":"api/core/#gigaspatial.core.io.data_store.DataStore.is_dir","title":"<code>is_dir(path)</code>  <code>abstractmethod</code>","text":"<p>Check if path points to a directory.</p> <p>Parameters:</p> Name Type Description Default <code>path</code> <code>str</code> <p>Path to check</p> required <p>Returns:</p> Type Description <code>bool</code> <p>True if path is a directory, False otherwise</p> Source code in <code>gigaspatial/core/io/data_store.py</code> <pre><code>@abstractmethod\ndef is_dir(self, path: str) -&gt; bool:\n    \"\"\"\n    Check if path points to a directory.\n\n    Args:\n        path: Path to check\n\n    Returns:\n        True if path is a directory, False otherwise\n    \"\"\"\n    pass\n</code></pre>"},{"location":"api/core/#gigaspatial.core.io.data_store.DataStore.is_file","title":"<code>is_file(path)</code>  <code>abstractmethod</code>","text":"<p>Check if path points to a file.</p> <p>Parameters:</p> Name Type Description Default <code>path</code> <code>str</code> <p>Path to check</p> required <p>Returns:</p> Type Description <code>bool</code> <p>True if path is a file, False otherwise</p> Source code in <code>gigaspatial/core/io/data_store.py</code> <pre><code>@abstractmethod\ndef is_file(self, path: str) -&gt; bool:\n    \"\"\"\n    Check if path points to a file.\n\n    Args:\n        path: Path to check\n\n    Returns:\n        True if path is a file, False otherwise\n    \"\"\"\n    pass\n</code></pre>"},{"location":"api/core/#gigaspatial.core.io.data_store.DataStore.list_files","title":"<code>list_files(path)</code>  <code>abstractmethod</code>","text":"<p>List all files in a directory.</p> <p>Parameters:</p> Name Type Description Default <code>path</code> <code>str</code> <p>Directory path to list</p> required <p>Returns:</p> Type Description <code>List[str]</code> <p>List of file paths in the directory</p> Source code in <code>gigaspatial/core/io/data_store.py</code> <pre><code>@abstractmethod\ndef list_files(self, path: str) -&gt; List[str]:\n    \"\"\"\n    List all files in a directory.\n\n    Args:\n        path: Directory path to list\n\n    Returns:\n        List of file paths in the directory\n    \"\"\"\n    pass\n</code></pre>"},{"location":"api/core/#gigaspatial.core.io.data_store.DataStore.open","title":"<code>open(file, mode='r')</code>  <code>abstractmethod</code>","text":"<p>Context manager for file operations.</p> <p>Parameters:</p> Name Type Description Default <code>file</code> <code>str</code> <p>Path to the file</p> required <code>mode</code> <code>str</code> <p>File mode ('r', 'w', 'rb', 'wb')</p> <code>'r'</code> <p>Yields:</p> Type Description <code>Union[str, bytes]</code> <p>File-like object</p> <p>Raises:</p> Type Description <code>IOError</code> <p>If file cannot be opened</p> Source code in <code>gigaspatial/core/io/data_store.py</code> <pre><code>@abstractmethod\ndef open(self, file: str, mode: str = \"r\") -&gt; Union[str, bytes]:\n    \"\"\"\n    Context manager for file operations.\n\n    Args:\n        file: Path to the file\n        mode: File mode ('r', 'w', 'rb', 'wb')\n\n    Yields:\n        File-like object\n\n    Raises:\n        IOError: If file cannot be opened\n    \"\"\"\n    pass\n</code></pre>"},{"location":"api/core/#gigaspatial.core.io.data_store.DataStore.read_file","title":"<code>read_file(path)</code>  <code>abstractmethod</code>","text":"<p>Read contents of a file from the data store.</p> <p>Parameters:</p> Name Type Description Default <code>path</code> <code>str</code> <p>Path to the file to read</p> required <p>Returns:</p> Type Description <code>Any</code> <p>Contents of the file</p> <p>Raises:</p> Type Description <code>IOError</code> <p>If file cannot be read</p> Source code in <code>gigaspatial/core/io/data_store.py</code> <pre><code>@abstractmethod\ndef read_file(self, path: str) -&gt; Any:\n    \"\"\"\n    Read contents of a file from the data store.\n\n    Args:\n        path: Path to the file to read\n\n    Returns:\n        Contents of the file\n\n    Raises:\n        IOError: If file cannot be read\n    \"\"\"\n    pass\n</code></pre>"},{"location":"api/core/#gigaspatial.core.io.data_store.DataStore.remove","title":"<code>remove(path)</code>  <code>abstractmethod</code>","text":"<p>Remove a file.</p> <p>Parameters:</p> Name Type Description Default <code>path</code> <code>str</code> <p>Path to the file to remove</p> required <p>Raises:</p> Type Description <code>IOError</code> <p>If file cannot be removed</p> Source code in <code>gigaspatial/core/io/data_store.py</code> <pre><code>@abstractmethod\ndef remove(self, path: str) -&gt; None:\n    \"\"\"\n    Remove a file.\n\n    Args:\n        path: Path to the file to remove\n\n    Raises:\n        IOError: If file cannot be removed\n    \"\"\"\n    pass\n</code></pre>"},{"location":"api/core/#gigaspatial.core.io.data_store.DataStore.rmdir","title":"<code>rmdir(dir)</code>  <code>abstractmethod</code>","text":"<p>Remove a directory and all its contents.</p> <p>Parameters:</p> Name Type Description Default <code>dir</code> <code>str</code> <p>Path to the directory to remove</p> required <p>Raises:</p> Type Description <code>IOError</code> <p>If directory cannot be removed</p> Source code in <code>gigaspatial/core/io/data_store.py</code> <pre><code>@abstractmethod\ndef rmdir(self, dir: str) -&gt; None:\n    \"\"\"\n    Remove a directory and all its contents.\n\n    Args:\n        dir: Path to the directory to remove\n\n    Raises:\n        IOError: If directory cannot be removed\n    \"\"\"\n    pass\n</code></pre>"},{"location":"api/core/#gigaspatial.core.io.data_store.DataStore.walk","title":"<code>walk(top)</code>  <code>abstractmethod</code>","text":"<p>Walk through directory tree, similar to os.walk().</p> <p>Parameters:</p> Name Type Description Default <code>top</code> <code>str</code> <p>Starting directory for the walk</p> required <p>Returns:</p> Type Description <code>Generator</code> <p>Generator yielding tuples of (dirpath, dirnames, filenames)</p> Source code in <code>gigaspatial/core/io/data_store.py</code> <pre><code>@abstractmethod\ndef walk(self, top: str) -&gt; Generator:\n    \"\"\"\n    Walk through directory tree, similar to os.walk().\n\n    Args:\n        top: Starting directory for the walk\n\n    Returns:\n        Generator yielding tuples of (dirpath, dirnames, filenames)\n    \"\"\"\n    pass\n</code></pre>"},{"location":"api/core/#gigaspatial.core.io.data_store.DataStore.write_file","title":"<code>write_file(path, data)</code>  <code>abstractmethod</code>","text":"<p>Write data to a file in the data store.</p> <p>Parameters:</p> Name Type Description Default <code>path</code> <code>str</code> <p>Path where to write the file</p> required <code>data</code> <code>Any</code> <p>Data to write to the file</p> required <p>Raises:</p> Type Description <code>IOError</code> <p>If file cannot be written</p> Source code in <code>gigaspatial/core/io/data_store.py</code> <pre><code>@abstractmethod\ndef write_file(self, path: str, data: Any) -&gt; None:\n    \"\"\"\n    Write data to a file in the data store.\n\n    Args:\n        path: Path where to write the file\n        data: Data to write to the file\n\n    Raises:\n        IOError: If file cannot be written\n    \"\"\"\n    pass\n</code></pre>"},{"location":"api/core/#gigaspatial.core.io.local_data_store","title":"<code>local_data_store</code>","text":""},{"location":"api/core/#gigaspatial.core.io.local_data_store.LocalDataStore","title":"<code>LocalDataStore</code>","text":"<p>               Bases: <code>DataStore</code></p> <p>Implementation for local filesystem storage.</p> Source code in <code>gigaspatial/core/io/local_data_store.py</code> <pre><code>class LocalDataStore(DataStore):\n    \"\"\"Implementation for local filesystem storage.\"\"\"\n\n    def __init__(self, base_path: Union[str, Path] = \"\"):\n        super().__init__()\n        self.base_path = Path(base_path).resolve()\n\n    def _resolve_path(self, path: str) -&gt; Path:\n        \"\"\"Resolve path relative to base directory.\"\"\"\n        return self.base_path / path\n\n    def read_file(self, path: str) -&gt; bytes:\n        full_path = self._resolve_path(path)\n        with open(full_path, \"rb\") as f:\n            return f.read()\n\n    def write_file(self, path: str, data: Union[bytes, str]) -&gt; None:\n        full_path = self._resolve_path(path)\n        self.mkdir(str(full_path.parent), exist_ok=True)\n\n        if isinstance(data, str):\n            mode = \"w\"\n            encoding = \"utf-8\"\n        else:\n            mode = \"wb\"\n            encoding = None\n\n        with open(full_path, mode, encoding=encoding) as f:\n            f.write(data)\n\n    def file_exists(self, path: str) -&gt; bool:\n        return self._resolve_path(path).is_file()\n\n    def list_files(self, path: str) -&gt; List[str]:\n        full_path = self._resolve_path(path)\n        return [\n            str(f.relative_to(self.base_path))\n            for f in full_path.iterdir()\n            if f.is_file()\n        ]\n\n    def walk(self, top: str) -&gt; Generator[Tuple[str, List[str], List[str]], None, None]:\n        full_path = self._resolve_path(top)\n        for root, dirs, files in os.walk(full_path):\n            rel_root = str(Path(root).relative_to(self.base_path))\n            yield rel_root, dirs, files\n\n    def list_directories(self, path: str) -&gt; List[str]:\n        full_path = self._resolve_path(path)\n\n        if not full_path.exists():\n            return []\n\n        if not full_path.is_dir():\n            return []\n\n        return [d.name for d in full_path.iterdir() if d.is_dir()]\n\n    def open(self, path: str, mode: str = \"r\") -&gt; IO:\n        full_path = self._resolve_path(path)\n        self.mkdir(str(full_path.parent), exist_ok=True)\n        return open(full_path, mode)\n\n    def is_file(self, path: str) -&gt; bool:\n        return self._resolve_path(path).is_file()\n\n    def is_dir(self, path: str) -&gt; bool:\n        return self._resolve_path(path).is_dir()\n\n    def remove(self, path: str) -&gt; None:\n        full_path = self._resolve_path(path)\n        if full_path.is_file():\n            os.remove(full_path)\n\n    def rmdir(self, directory: str) -&gt; None:\n        full_path = self._resolve_path(directory)\n        if full_path.is_dir():\n            os.rmdir(full_path)\n\n    def mkdir(self, path: str, exist_ok: bool = False) -&gt; None:\n        full_path = self._resolve_path(path)\n        full_path.mkdir(parents=True, exist_ok=exist_ok)\n\n    def exists(self, path: str) -&gt; bool:\n        return self._resolve_path(path).exists()\n</code></pre>"},{"location":"api/core/#gigaspatial.core.io.readers","title":"<code>readers</code>","text":""},{"location":"api/core/#gigaspatial.core.io.readers.read_dataset","title":"<code>read_dataset(data_store, path, compression=None, **kwargs)</code>","text":"<p>Read data from various file formats stored in both local and cloud-based storage.</p>"},{"location":"api/core/#gigaspatial.core.io.readers.read_dataset--parameters","title":"Parameters:","text":"<p>data_store : DataStore     Instance of DataStore for accessing data storage. path : str, Path     Path to the file in data storage. **kwargs : dict     Additional arguments passed to the specific reader function.</p>"},{"location":"api/core/#gigaspatial.core.io.readers.read_dataset--returns","title":"Returns:","text":"<p>pandas.DataFrame or geopandas.GeoDataFrame     The data read from the file.</p>"},{"location":"api/core/#gigaspatial.core.io.readers.read_dataset--raises","title":"Raises:","text":"<p>FileNotFoundError     If the file doesn't exist in blob storage. ValueError     If the file type is unsupported or if there's an error reading the file.</p> Source code in <code>gigaspatial/core/io/readers.py</code> <pre><code>def read_dataset(data_store: DataStore, path: str, compression: str = None, **kwargs):\n    \"\"\"\n    Read data from various file formats stored in both local and cloud-based storage.\n\n    Parameters:\n    ----------\n    data_store : DataStore\n        Instance of DataStore for accessing data storage.\n    path : str, Path\n        Path to the file in data storage.\n    **kwargs : dict\n        Additional arguments passed to the specific reader function.\n\n    Returns:\n    -------\n    pandas.DataFrame or geopandas.GeoDataFrame\n        The data read from the file.\n\n    Raises:\n    ------\n    FileNotFoundError\n        If the file doesn't exist in blob storage.\n    ValueError\n        If the file type is unsupported or if there's an error reading the file.\n    \"\"\"\n\n    # Define supported file formats and their readers\n    BINARY_FORMATS = {\n        \".shp\",\n        \".zip\",\n        \".parquet\",\n        \".gpkg\",\n        \".xlsx\",\n        \".xls\",\n        \".kmz\",\n        \".gz\",\n    }\n\n    PANDAS_READERS = {\n        \".csv\": pd.read_csv,\n        \".xlsx\": lambda f, **kw: pd.read_excel(f, engine=\"openpyxl\", **kw),\n        \".xls\": lambda f, **kw: pd.read_excel(f, engine=\"xlrd\", **kw),\n        \".json\": pd.read_json,\n        # \".gz\": lambda f, **kw: pd.read_csv(f, compression=\"gzip\", **kw),\n    }\n\n    GEO_READERS = {\n        \".shp\": gpd.read_file,\n        \".zip\": gpd.read_file,\n        \".geojson\": gpd.read_file,\n        \".gpkg\": gpd.read_file,\n        \".parquet\": gpd.read_parquet,\n        \".kmz\": read_kmz,\n    }\n\n    COMPRESSION_FORMATS = {\n        \".gz\": \"gzip\",\n        \".bz2\": \"bz2\",\n        \".zip\": \"zip\",\n        \".xz\": \"xz\",\n    }\n\n    try:\n        # Check if file exists\n        if not data_store.file_exists(path):\n            raise FileNotFoundError(f\"File '{path}' not found in blob storage\")\n\n        path_obj = Path(path)\n        suffixes = path_obj.suffixes\n        file_extension = suffixes[-1].lower() if suffixes else \"\"\n\n        if compression is None and file_extension in COMPRESSION_FORMATS:\n            compression_format = COMPRESSION_FORMATS[file_extension]\n\n            # if file has multiple extensions (e.g., .csv.gz), get the inner format\n            if len(suffixes) &gt; 1:\n                inner_extension = suffixes[-2].lower()\n\n                if inner_extension == \".tar\":\n                    raise ValueError(\n                        \"Tar archives (.tar.gz) are not directly supported\"\n                    )\n\n                if inner_extension in PANDAS_READERS:\n                    try:\n                        with data_store.open(path, \"rb\") as f:\n                            return PANDAS_READERS[inner_extension](\n                                f, compression=compression_format, **kwargs\n                            )\n                    except Exception as e:\n                        raise ValueError(f\"Error reading compressed file: {str(e)}\")\n                elif inner_extension in GEO_READERS:\n                    try:\n                        with data_store.open(path, \"rb\") as f:\n                            if compression_format == \"gzip\":\n                                import gzip\n\n                                decompressed_data = gzip.decompress(f.read())\n                                import io\n\n                                return GEO_READERS[inner_extension](\n                                    io.BytesIO(decompressed_data), **kwargs\n                                )\n                            else:\n                                raise ValueError(\n                                    f\"Compression format {compression_format} not supported for geo data\"\n                                )\n                    except Exception as e:\n                        raise ValueError(f\"Error reading compressed geo file: {str(e)}\")\n            else:\n                # if just .gz without clear inner type, assume csv\n                try:\n                    with data_store.open(path, \"rb\") as f:\n                        return pd.read_csv(f, compression=compression_format, **kwargs)\n                except Exception as e:\n                    raise ValueError(\n                        f\"Error reading compressed file as CSV: {str(e)}. \"\n                        f\"If not a CSV, specify the format in the filename (e.g., .json.gz)\"\n                    )\n\n        # Special handling for compressed files\n        if file_extension == \".zip\":\n            # For zip files, we need to use binary mode\n            with data_store.open(path, \"rb\") as f:\n                return gpd.read_file(f)\n\n        # Determine if we need binary mode based on file type\n        mode = \"rb\" if file_extension in BINARY_FORMATS else \"r\"\n\n        # Try reading with appropriate reader\n        if file_extension in PANDAS_READERS:\n            try:\n                with data_store.open(path, mode) as f:\n                    return PANDAS_READERS[file_extension](f, **kwargs)\n            except Exception as e:\n                raise ValueError(f\"Error reading file with pandas: {str(e)}\")\n\n        if file_extension in GEO_READERS:\n            try:\n                with data_store.open(path, \"rb\") as f:\n                    return GEO_READERS[file_extension](f, **kwargs)\n            except Exception as e:\n                # For parquet files, try pandas reader if geopandas fails\n                if file_extension == \".parquet\":\n                    try:\n                        with data_store.open(path, \"rb\") as f:\n                            return pd.read_parquet(f, **kwargs)\n                    except Exception as e2:\n                        raise ValueError(\n                            f\"Failed to read parquet with both geopandas ({str(e)}) \"\n                            f\"and pandas ({str(e2)})\"\n                        )\n                raise ValueError(f\"Error reading file with geopandas: {str(e)}\")\n\n        # If we get here, the file type is unsupported\n        supported_formats = sorted(set(PANDAS_READERS.keys()) | set(GEO_READERS.keys()))\n        supported_compressions = sorted(COMPRESSION_FORMATS.keys())\n        raise ValueError(\n            f\"Unsupported file type: {file_extension}\\n\"\n            f\"Supported formats: {', '.join(supported_formats)}\"\n            f\"Supported compressions: {', '.join(supported_compressions)}\"\n        )\n\n    except Exception as e:\n        if isinstance(e, (FileNotFoundError, ValueError)):\n            raise\n        raise RuntimeError(f\"Unexpected error reading dataset: {str(e)}\")\n</code></pre>"},{"location":"api/core/#gigaspatial.core.io.readers.read_datasets","title":"<code>read_datasets(data_store, paths, **kwargs)</code>","text":"<p>Read multiple datasets from data storage at once.</p>"},{"location":"api/core/#gigaspatial.core.io.readers.read_datasets--parameters","title":"Parameters:","text":"<p>data_store : DataStore     Instance of DataStore for accessing data storage. paths : list of str     Paths to files in data storage. **kwargs : dict     Additional arguments passed to read_dataset.</p>"},{"location":"api/core/#gigaspatial.core.io.readers.read_datasets--returns","title":"Returns:","text":"<p>dict     Dictionary mapping paths to their corresponding DataFrames/GeoDataFrames.</p> Source code in <code>gigaspatial/core/io/readers.py</code> <pre><code>def read_datasets(data_store: DataStore, paths, **kwargs):\n    \"\"\"\n    Read multiple datasets from data storage at once.\n\n    Parameters:\n    ----------\n    data_store : DataStore\n        Instance of DataStore for accessing data storage.\n    paths : list of str\n        Paths to files in data storage.\n    **kwargs : dict\n        Additional arguments passed to read_dataset.\n\n    Returns:\n    -------\n    dict\n        Dictionary mapping paths to their corresponding DataFrames/GeoDataFrames.\n    \"\"\"\n    results = {}\n    errors = {}\n\n    for path in paths:\n        try:\n            results[path] = read_dataset(data_store, path, **kwargs)\n        except Exception as e:\n            errors[path] = str(e)\n\n    if errors:\n        error_msg = \"\\n\".join(f\"- {path}: {error}\" for path, error in errors.items())\n        raise ValueError(f\"Errors reading datasets:\\n{error_msg}\")\n\n    return results\n</code></pre>"},{"location":"api/core/#gigaspatial.core.io.readers.read_gzipped_json_or_csv","title":"<code>read_gzipped_json_or_csv(file_path, data_store)</code>","text":"<p>Reads a gzipped file, attempting to parse it as JSON (lines=True) or CSV.</p> Source code in <code>gigaspatial/core/io/readers.py</code> <pre><code>def read_gzipped_json_or_csv(file_path, data_store):\n    \"\"\"Reads a gzipped file, attempting to parse it as JSON (lines=True) or CSV.\"\"\"\n\n    with data_store.open(file_path, \"rb\") as f:\n        g = gzip.GzipFile(fileobj=f)\n        text = g.read().decode(\"utf-8\")\n        try:\n            df = pd.read_json(io.StringIO(text), lines=True)\n            return df\n        except json.JSONDecodeError:\n            try:\n                df = pd.read_csv(io.StringIO(text))\n                return df\n            except pd.errors.ParserError:\n                print(f\"Error: Could not parse {file_path} as JSON or CSV.\")\n                return None\n</code></pre>"},{"location":"api/core/#gigaspatial.core.io.readers.read_kmz","title":"<code>read_kmz(file_obj, **kwargs)</code>","text":"<p>Helper function to read KMZ files and return a GeoDataFrame.</p> Source code in <code>gigaspatial/core/io/readers.py</code> <pre><code>def read_kmz(file_obj, **kwargs):\n    \"\"\"Helper function to read KMZ files and return a GeoDataFrame.\"\"\"\n    try:\n        with zipfile.ZipFile(file_obj) as kmz:\n            # Find the KML file in the archive (usually doc.kml)\n            kml_filename = next(\n                name for name in kmz.namelist() if name.endswith(\".kml\")\n            )\n\n            # Read the KML content\n            kml_content = io.BytesIO(kmz.read(kml_filename))\n\n            gdf = gpd.read_file(kml_content)\n\n            # Validate the GeoDataFrame\n            if gdf.empty:\n                raise ValueError(\n                    \"The KML file is empty or does not contain valid geospatial data.\"\n                )\n\n        return gdf\n\n    except zipfile.BadZipFile:\n        raise ValueError(\"The provided file is not a valid KMZ file.\")\n    except StopIteration:\n        raise ValueError(\"No KML file found in the KMZ archive.\")\n    except Exception as e:\n        raise RuntimeError(f\"An error occurred: {e}\")\n</code></pre>"},{"location":"api/core/#gigaspatial.core.io.writers","title":"<code>writers</code>","text":""},{"location":"api/core/#gigaspatial.core.io.writers.write_dataset","title":"<code>write_dataset(data, data_store, path, **kwargs)</code>","text":"<p>Write DataFrame or GeoDataFrame to various file formats in Azure Blob Storage.</p>"},{"location":"api/core/#gigaspatial.core.io.writers.write_dataset--parameters","title":"Parameters:","text":"<p>data : pandas.DataFrame or geopandas.GeoDataFrame     The data to write to blob storage. data_store : DataStore     Instance of DataStore for accessing data storage. path : str     Path where the file will be written in data storage. **kwargs : dict     Additional arguments passed to the specific writer function.</p>"},{"location":"api/core/#gigaspatial.core.io.writers.write_dataset--raises","title":"Raises:","text":"<p>ValueError     If the file type is unsupported or if there's an error writing the file. TypeError     If input data is not a DataFrame or GeoDataFrame.</p> Source code in <code>gigaspatial/core/io/writers.py</code> <pre><code>def write_dataset(data, data_store: DataStore, path, **kwargs):\n    \"\"\"\n    Write DataFrame or GeoDataFrame to various file formats in Azure Blob Storage.\n\n    Parameters:\n    ----------\n    data : pandas.DataFrame or geopandas.GeoDataFrame\n        The data to write to blob storage.\n    data_store : DataStore\n        Instance of DataStore for accessing data storage.\n    path : str\n        Path where the file will be written in data storage.\n    **kwargs : dict\n        Additional arguments passed to the specific writer function.\n\n    Raises:\n    ------\n    ValueError\n        If the file type is unsupported or if there's an error writing the file.\n    TypeError\n        If input data is not a DataFrame or GeoDataFrame.\n    \"\"\"\n\n    # Define supported file formats and their writers\n    BINARY_FORMATS = {\".shp\", \".zip\", \".parquet\", \".gpkg\", \".xlsx\", \".xls\"}\n\n    PANDAS_WRITERS = {\n        \".csv\": lambda df, buf, **kw: df.to_csv(buf, **kw),\n        \".xlsx\": lambda df, buf, **kw: df.to_excel(buf, engine=\"openpyxl\", **kw),\n        \".json\": lambda df, buf, **kw: df.to_json(buf, **kw),\n        \".parquet\": lambda df, buf, **kw: df.to_parquet(buf, **kw),\n    }\n\n    GEO_WRITERS = {\n        \".geojson\": lambda gdf, buf, **kw: gdf.to_file(buf, driver=\"GeoJSON\", **kw),\n        \".gpkg\": lambda gdf, buf, **kw: gdf.to_file(buf, driver=\"GPKG\", **kw),\n        \".parquet\": lambda gdf, buf, **kw: gdf.to_parquet(buf, **kw),\n    }\n\n    try:\n        # Input validation\n        if not isinstance(data, (pd.DataFrame, gpd.GeoDataFrame)):\n            raise TypeError(\"Input data must be a pandas DataFrame or GeoDataFrame\")\n\n        # Get file suffix and ensure it's lowercase\n        suffix = Path(path).suffix.lower()\n\n        # Determine if we need binary mode based on file type\n        mode = \"wb\" if suffix in BINARY_FORMATS else \"w\"\n\n        # Handle different data types and formats\n        if isinstance(data, gpd.GeoDataFrame):\n            if suffix not in GEO_WRITERS:\n                supported_formats = sorted(GEO_WRITERS.keys())\n                raise ValueError(\n                    f\"Unsupported file type for GeoDataFrame: {suffix}\\n\"\n                    f\"Supported formats: {', '.join(supported_formats)}\"\n                )\n\n            try:\n                with data_store.open(path, \"wb\") as f:\n                    GEO_WRITERS[suffix](data, f, **kwargs)\n            except Exception as e:\n                raise ValueError(f\"Error writing GeoDataFrame: {str(e)}\")\n\n        else:  # pandas DataFrame\n            if suffix not in PANDAS_WRITERS:\n                supported_formats = sorted(PANDAS_WRITERS.keys())\n                raise ValueError(\n                    f\"Unsupported file type for DataFrame: {suffix}\\n\"\n                    f\"Supported formats: {', '.join(supported_formats)}\"\n                )\n\n            try:\n                with data_store.open(path, mode) as f:\n                    PANDAS_WRITERS[suffix](data, f, **kwargs)\n            except Exception as e:\n                raise ValueError(f\"Error writing DataFrame: {str(e)}\")\n\n    except Exception as e:\n        if isinstance(e, (TypeError, ValueError)):\n            raise\n        raise RuntimeError(f\"Unexpected error writing dataset: {str(e)}\")\n</code></pre>"},{"location":"api/core/#gigaspatial.core.io.writers.write_datasets","title":"<code>write_datasets(data_dict, data_store, **kwargs)</code>","text":"<p>Write multiple datasets to data storage at once.</p>"},{"location":"api/core/#gigaspatial.core.io.writers.write_datasets--parameters","title":"Parameters:","text":"<p>data_dict : dict     Dictionary mapping paths to DataFrames/GeoDataFrames. data_store : DataStore     Instance of DataStore for accessing data storage. **kwargs : dict     Additional arguments passed to write_dataset.</p>"},{"location":"api/core/#gigaspatial.core.io.writers.write_datasets--raises","title":"Raises:","text":"<p>ValueError     If there are any errors writing the datasets.</p> Source code in <code>gigaspatial/core/io/writers.py</code> <pre><code>def write_datasets(data_dict, data_store: DataStore, **kwargs):\n    \"\"\"\n    Write multiple datasets to data storage at once.\n\n    Parameters:\n    ----------\n    data_dict : dict\n        Dictionary mapping paths to DataFrames/GeoDataFrames.\n    data_store : DataStore\n        Instance of DataStore for accessing data storage.\n    **kwargs : dict\n        Additional arguments passed to write_dataset.\n\n    Raises:\n    ------\n    ValueError\n        If there are any errors writing the datasets.\n    \"\"\"\n    errors = {}\n\n    for path, data in data_dict.items():\n        try:\n            write_dataset(data, data_store, path, **kwargs)\n        except Exception as e:\n            errors[path] = str(e)\n\n    if errors:\n        error_msg = \"\\n\".join(f\"- {path}: {error}\" for path, error in errors.items())\n        raise ValueError(f\"Errors writing datasets:\\n{error_msg}\")\n</code></pre>"},{"location":"api/core/#gigaspatial.core.schemas","title":"<code>schemas</code>","text":""},{"location":"api/core/#gigaspatial.core.schemas.entity","title":"<code>entity</code>","text":""},{"location":"api/core/#gigaspatial.core.schemas.entity.BaseGigaEntity","title":"<code>BaseGigaEntity</code>","text":"<p>               Bases: <code>BaseModel</code></p> <p>Base class for all Giga entities with common fields.</p> Source code in <code>gigaspatial/core/schemas/entity.py</code> <pre><code>class BaseGigaEntity(BaseModel):\n    \"\"\"Base class for all Giga entities with common fields.\"\"\"\n\n    source: Optional[str] = Field(None, max_length=100, description=\"Source reference\")\n    source_detail: Optional[str] = None\n\n    @property\n    def id(self) -&gt; str:\n        \"\"\"Abstract property that must be implemented by subclasses.\"\"\"\n        raise NotImplementedError(\"Subclasses must implement id property\")\n</code></pre>"},{"location":"api/core/#gigaspatial.core.schemas.entity.BaseGigaEntity.id","title":"<code>id: str</code>  <code>property</code>","text":"<p>Abstract property that must be implemented by subclasses.</p>"},{"location":"api/core/#gigaspatial.core.schemas.entity.EntityTable","title":"<code>EntityTable</code>","text":"<p>               Bases: <code>BaseModel</code>, <code>Generic[E]</code></p> Source code in <code>gigaspatial/core/schemas/entity.py</code> <pre><code>class EntityTable(BaseModel, Generic[E]):\n    entities: List[E] = Field(default_factory=list)\n    _cached_kdtree: Optional[cKDTree] = PrivateAttr(\n        default=None\n    )  # Internal cache for the KDTree\n\n    @classmethod\n    def from_file(\n        cls: Type[\"EntityTable\"],\n        file_path: Union[str, Path],\n        entity_class: Type[E],\n        data_store: Optional[DataStore] = None,\n        **kwargs,\n    ) -&gt; \"EntityTable\":\n        \"\"\"\n        Create an EntityTable instance from a file.\n\n        Args:\n            file_path: Path to the dataset file\n            entity_class: The entity class for validation\n\n        Returns:\n            EntityTable instance\n\n        Raises:\n            ValidationError: If any row fails validation\n            FileNotFoundError: If the file doesn't exist\n        \"\"\"\n        data_store = data_store or LocalDataStore()\n        file_path = Path(file_path)\n        if not file_path.exists():\n            raise FileNotFoundError(f\"File not found: {file_path}\")\n\n        df = read_dataset(data_store, file_path, **kwargs)\n        try:\n            entities = [entity_class(**row) for row in df.to_dict(orient=\"records\")]\n            return cls(entities=entities)\n        except ValidationError as e:\n            raise ValueError(f\"Validation error in input data: {e}\")\n        except Exception as e:\n            raise ValueError(f\"Error reading or processing the file: {e}\")\n\n    def _check_has_location(self, method_name: str) -&gt; bool:\n        \"\"\"Helper method to check if entities have location data.\"\"\"\n        if not self.entities:\n            return False\n        if not isinstance(self.entities[0], GigaEntity):\n            raise ValueError(\n                f\"Cannot perform {method_name}: entities of type {type(self.entities[0]).__name__} \"\n                \"do not have location data (latitude/longitude)\"\n            )\n        return True\n\n    def to_dataframe(self) -&gt; pd.DataFrame:\n        \"\"\"Convert the entity table to a pandas DataFrame.\"\"\"\n        return pd.DataFrame([e.model_dump() for e in self.entities])\n\n    def to_geodataframe(self) -&gt; gpd.GeoDataFrame:\n        \"\"\"Convert the entity table to a GeoDataFrame.\"\"\"\n        if not self._check_has_location(\"to_geodataframe\"):\n            raise ValueError(\"Cannot create GeoDataFrame: no entities available\")\n        df = self.to_dataframe()\n        return gpd.GeoDataFrame(\n            df,\n            geometry=gpd.points_from_xy(df[\"longitude\"], df[\"latitude\"]),\n            crs=\"EPSG:4326\",\n        )\n\n    def to_coordinate_vector(self) -&gt; np.ndarray:\n        \"\"\"Transforms the entity table into a numpy vector of coordinates\"\"\"\n        if not self.entities:\n            return np.zeros((0, 2))\n\n        if not self._check_has_location(\"to_coordinate_vector\"):\n            return np.zeros((0, 2))\n\n        return np.array([[e.latitude, e.longitude] for e in self.entities])\n\n    def get_lat_array(self) -&gt; np.ndarray:\n        \"\"\"Get an array of latitude values.\"\"\"\n        if not self._check_has_location(\"get_lat_array\"):\n            return np.array([])\n        return np.array([e.latitude for e in self.entities])\n\n    def get_lon_array(self) -&gt; np.ndarray:\n        \"\"\"Get an array of longitude values.\"\"\"\n        if not self._check_has_location(\"get_lon_array\"):\n            return np.array([])\n        return np.array([e.longitude for e in self.entities])\n\n    def filter_by_admin1(self, admin1_id_giga: str) -&gt; \"EntityTable[E]\":\n        \"\"\"Filter entities by primary administrative division.\"\"\"\n        return self.__class__(\n            entities=[e for e in self.entities if e.admin1_id_giga == admin1_id_giga]\n        )\n\n    def filter_by_admin2(self, admin2_id_giga: str) -&gt; \"EntityTable[E]\":\n        \"\"\"Filter entities by secondary administrative division.\"\"\"\n        return self.__class__(\n            entities=[e for e in self.entities if e.admin2_id_giga == admin2_id_giga]\n        )\n\n    def filter_by_polygon(self, polygon: Polygon) -&gt; \"EntityTable[E]\":\n        \"\"\"Filter entities within a polygon\"\"\"\n        if not self._check_has_location(\"filter_by_polygon\"):\n            return self.__class__(entities=[])\n\n        filtered = [\n            e for e in self.entities if polygon.contains(Point(e.longitude, e.latitude))\n        ]\n        return self.__class__(entities=filtered)\n\n    def filter_by_bounds(\n        self, min_lat: float, max_lat: float, min_lon: float, max_lon: float\n    ) -&gt; \"EntityTable[E]\":\n        \"\"\"Filter entities whose coordinates fall within the given bounds.\"\"\"\n        if not self._check_has_location(\"filter_by_bounds\"):\n            return self.__class__(entities=[])\n\n        filtered = [\n            e\n            for e in self.entities\n            if min_lat &lt;= e.latitude &lt;= max_lat and min_lon &lt;= e.longitude &lt;= max_lon\n        ]\n        return self.__class__(entities=filtered)\n\n    def get_nearest_neighbors(\n        self, lat: float, lon: float, k: int = 5\n    ) -&gt; \"EntityTable[E]\":\n        \"\"\"Find k nearest neighbors to a point using a cached KDTree.\"\"\"\n        if not self._check_has_location(\"get_nearest_neighbors\"):\n            return self.__class__(entities=[])\n\n        if not self._cached_kdtree:\n            self._build_kdtree()  # Build the KDTree if not already cached\n\n        if not self._cached_kdtree:  # If still None after building\n            return self.__class__(entities=[])\n\n        _, indices = self._cached_kdtree.query([[lat, lon]], k=k)\n        return self.__class__(entities=[self.entities[i] for i in indices[0]])\n\n    def _build_kdtree(self):\n        \"\"\"Builds and caches the KDTree.\"\"\"\n        if not self._check_has_location(\"_build_kdtree\"):\n            self._cached_kdtree = None\n            return\n        coords = self.to_coordinate_vector()\n        if coords:\n            self._cached_kdtree = cKDTree(coords)\n\n    def clear_cache(self):\n        \"\"\"Clears the KDTree cache.\"\"\"\n        self._cached_kdtree = None\n\n    def to_file(\n        self,\n        file_path: Union[str, Path],\n        data_store: Optional[DataStore] = None,\n        **kwargs,\n    ) -&gt; None:\n        \"\"\"\n        Save the entity data to a file.\n\n        Args:\n            file_path: Path to save the file\n        \"\"\"\n        if not self.entities:\n            raise ValueError(\"Cannot write to a file: no entities available.\")\n\n        data_store = data_store or LocalDataStore()\n\n        write_dataset(self.to_dataframe(), data_store, file_path, **kwargs)\n\n    def __len__(self) -&gt; int:\n        return len(self.entities)\n\n    def __iter__(self):\n        return iter(self.entities)\n</code></pre>"},{"location":"api/core/#gigaspatial.core.schemas.entity.EntityTable.clear_cache","title":"<code>clear_cache()</code>","text":"<p>Clears the KDTree cache.</p> Source code in <code>gigaspatial/core/schemas/entity.py</code> <pre><code>def clear_cache(self):\n    \"\"\"Clears the KDTree cache.\"\"\"\n    self._cached_kdtree = None\n</code></pre>"},{"location":"api/core/#gigaspatial.core.schemas.entity.EntityTable.filter_by_admin1","title":"<code>filter_by_admin1(admin1_id_giga)</code>","text":"<p>Filter entities by primary administrative division.</p> Source code in <code>gigaspatial/core/schemas/entity.py</code> <pre><code>def filter_by_admin1(self, admin1_id_giga: str) -&gt; \"EntityTable[E]\":\n    \"\"\"Filter entities by primary administrative division.\"\"\"\n    return self.__class__(\n        entities=[e for e in self.entities if e.admin1_id_giga == admin1_id_giga]\n    )\n</code></pre>"},{"location":"api/core/#gigaspatial.core.schemas.entity.EntityTable.filter_by_admin2","title":"<code>filter_by_admin2(admin2_id_giga)</code>","text":"<p>Filter entities by secondary administrative division.</p> Source code in <code>gigaspatial/core/schemas/entity.py</code> <pre><code>def filter_by_admin2(self, admin2_id_giga: str) -&gt; \"EntityTable[E]\":\n    \"\"\"Filter entities by secondary administrative division.\"\"\"\n    return self.__class__(\n        entities=[e for e in self.entities if e.admin2_id_giga == admin2_id_giga]\n    )\n</code></pre>"},{"location":"api/core/#gigaspatial.core.schemas.entity.EntityTable.filter_by_bounds","title":"<code>filter_by_bounds(min_lat, max_lat, min_lon, max_lon)</code>","text":"<p>Filter entities whose coordinates fall within the given bounds.</p> Source code in <code>gigaspatial/core/schemas/entity.py</code> <pre><code>def filter_by_bounds(\n    self, min_lat: float, max_lat: float, min_lon: float, max_lon: float\n) -&gt; \"EntityTable[E]\":\n    \"\"\"Filter entities whose coordinates fall within the given bounds.\"\"\"\n    if not self._check_has_location(\"filter_by_bounds\"):\n        return self.__class__(entities=[])\n\n    filtered = [\n        e\n        for e in self.entities\n        if min_lat &lt;= e.latitude &lt;= max_lat and min_lon &lt;= e.longitude &lt;= max_lon\n    ]\n    return self.__class__(entities=filtered)\n</code></pre>"},{"location":"api/core/#gigaspatial.core.schemas.entity.EntityTable.filter_by_polygon","title":"<code>filter_by_polygon(polygon)</code>","text":"<p>Filter entities within a polygon</p> Source code in <code>gigaspatial/core/schemas/entity.py</code> <pre><code>def filter_by_polygon(self, polygon: Polygon) -&gt; \"EntityTable[E]\":\n    \"\"\"Filter entities within a polygon\"\"\"\n    if not self._check_has_location(\"filter_by_polygon\"):\n        return self.__class__(entities=[])\n\n    filtered = [\n        e for e in self.entities if polygon.contains(Point(e.longitude, e.latitude))\n    ]\n    return self.__class__(entities=filtered)\n</code></pre>"},{"location":"api/core/#gigaspatial.core.schemas.entity.EntityTable.from_file","title":"<code>from_file(file_path, entity_class, data_store=None, **kwargs)</code>  <code>classmethod</code>","text":"<p>Create an EntityTable instance from a file.</p> <p>Parameters:</p> Name Type Description Default <code>file_path</code> <code>Union[str, Path]</code> <p>Path to the dataset file</p> required <code>entity_class</code> <code>Type[E]</code> <p>The entity class for validation</p> required <p>Returns:</p> Type Description <code>EntityTable</code> <p>EntityTable instance</p> <p>Raises:</p> Type Description <code>ValidationError</code> <p>If any row fails validation</p> <code>FileNotFoundError</code> <p>If the file doesn't exist</p> Source code in <code>gigaspatial/core/schemas/entity.py</code> <pre><code>@classmethod\ndef from_file(\n    cls: Type[\"EntityTable\"],\n    file_path: Union[str, Path],\n    entity_class: Type[E],\n    data_store: Optional[DataStore] = None,\n    **kwargs,\n) -&gt; \"EntityTable\":\n    \"\"\"\n    Create an EntityTable instance from a file.\n\n    Args:\n        file_path: Path to the dataset file\n        entity_class: The entity class for validation\n\n    Returns:\n        EntityTable instance\n\n    Raises:\n        ValidationError: If any row fails validation\n        FileNotFoundError: If the file doesn't exist\n    \"\"\"\n    data_store = data_store or LocalDataStore()\n    file_path = Path(file_path)\n    if not file_path.exists():\n        raise FileNotFoundError(f\"File not found: {file_path}\")\n\n    df = read_dataset(data_store, file_path, **kwargs)\n    try:\n        entities = [entity_class(**row) for row in df.to_dict(orient=\"records\")]\n        return cls(entities=entities)\n    except ValidationError as e:\n        raise ValueError(f\"Validation error in input data: {e}\")\n    except Exception as e:\n        raise ValueError(f\"Error reading or processing the file: {e}\")\n</code></pre>"},{"location":"api/core/#gigaspatial.core.schemas.entity.EntityTable.get_lat_array","title":"<code>get_lat_array()</code>","text":"<p>Get an array of latitude values.</p> Source code in <code>gigaspatial/core/schemas/entity.py</code> <pre><code>def get_lat_array(self) -&gt; np.ndarray:\n    \"\"\"Get an array of latitude values.\"\"\"\n    if not self._check_has_location(\"get_lat_array\"):\n        return np.array([])\n    return np.array([e.latitude for e in self.entities])\n</code></pre>"},{"location":"api/core/#gigaspatial.core.schemas.entity.EntityTable.get_lon_array","title":"<code>get_lon_array()</code>","text":"<p>Get an array of longitude values.</p> Source code in <code>gigaspatial/core/schemas/entity.py</code> <pre><code>def get_lon_array(self) -&gt; np.ndarray:\n    \"\"\"Get an array of longitude values.\"\"\"\n    if not self._check_has_location(\"get_lon_array\"):\n        return np.array([])\n    return np.array([e.longitude for e in self.entities])\n</code></pre>"},{"location":"api/core/#gigaspatial.core.schemas.entity.EntityTable.get_nearest_neighbors","title":"<code>get_nearest_neighbors(lat, lon, k=5)</code>","text":"<p>Find k nearest neighbors to a point using a cached KDTree.</p> Source code in <code>gigaspatial/core/schemas/entity.py</code> <pre><code>def get_nearest_neighbors(\n    self, lat: float, lon: float, k: int = 5\n) -&gt; \"EntityTable[E]\":\n    \"\"\"Find k nearest neighbors to a point using a cached KDTree.\"\"\"\n    if not self._check_has_location(\"get_nearest_neighbors\"):\n        return self.__class__(entities=[])\n\n    if not self._cached_kdtree:\n        self._build_kdtree()  # Build the KDTree if not already cached\n\n    if not self._cached_kdtree:  # If still None after building\n        return self.__class__(entities=[])\n\n    _, indices = self._cached_kdtree.query([[lat, lon]], k=k)\n    return self.__class__(entities=[self.entities[i] for i in indices[0]])\n</code></pre>"},{"location":"api/core/#gigaspatial.core.schemas.entity.EntityTable.to_coordinate_vector","title":"<code>to_coordinate_vector()</code>","text":"<p>Transforms the entity table into a numpy vector of coordinates</p> Source code in <code>gigaspatial/core/schemas/entity.py</code> <pre><code>def to_coordinate_vector(self) -&gt; np.ndarray:\n    \"\"\"Transforms the entity table into a numpy vector of coordinates\"\"\"\n    if not self.entities:\n        return np.zeros((0, 2))\n\n    if not self._check_has_location(\"to_coordinate_vector\"):\n        return np.zeros((0, 2))\n\n    return np.array([[e.latitude, e.longitude] for e in self.entities])\n</code></pre>"},{"location":"api/core/#gigaspatial.core.schemas.entity.EntityTable.to_dataframe","title":"<code>to_dataframe()</code>","text":"<p>Convert the entity table to a pandas DataFrame.</p> Source code in <code>gigaspatial/core/schemas/entity.py</code> <pre><code>def to_dataframe(self) -&gt; pd.DataFrame:\n    \"\"\"Convert the entity table to a pandas DataFrame.\"\"\"\n    return pd.DataFrame([e.model_dump() for e in self.entities])\n</code></pre>"},{"location":"api/core/#gigaspatial.core.schemas.entity.EntityTable.to_file","title":"<code>to_file(file_path, data_store=None, **kwargs)</code>","text":"<p>Save the entity data to a file.</p> <p>Parameters:</p> Name Type Description Default <code>file_path</code> <code>Union[str, Path]</code> <p>Path to save the file</p> required Source code in <code>gigaspatial/core/schemas/entity.py</code> <pre><code>def to_file(\n    self,\n    file_path: Union[str, Path],\n    data_store: Optional[DataStore] = None,\n    **kwargs,\n) -&gt; None:\n    \"\"\"\n    Save the entity data to a file.\n\n    Args:\n        file_path: Path to save the file\n    \"\"\"\n    if not self.entities:\n        raise ValueError(\"Cannot write to a file: no entities available.\")\n\n    data_store = data_store or LocalDataStore()\n\n    write_dataset(self.to_dataframe(), data_store, file_path, **kwargs)\n</code></pre>"},{"location":"api/core/#gigaspatial.core.schemas.entity.EntityTable.to_geodataframe","title":"<code>to_geodataframe()</code>","text":"<p>Convert the entity table to a GeoDataFrame.</p> Source code in <code>gigaspatial/core/schemas/entity.py</code> <pre><code>def to_geodataframe(self) -&gt; gpd.GeoDataFrame:\n    \"\"\"Convert the entity table to a GeoDataFrame.\"\"\"\n    if not self._check_has_location(\"to_geodataframe\"):\n        raise ValueError(\"Cannot create GeoDataFrame: no entities available\")\n    df = self.to_dataframe()\n    return gpd.GeoDataFrame(\n        df,\n        geometry=gpd.points_from_xy(df[\"longitude\"], df[\"latitude\"]),\n        crs=\"EPSG:4326\",\n    )\n</code></pre>"},{"location":"api/core/#gigaspatial.core.schemas.entity.GigaEntity","title":"<code>GigaEntity</code>","text":"<p>               Bases: <code>BaseGigaEntity</code></p> <p>Entity with location data.</p> Source code in <code>gigaspatial/core/schemas/entity.py</code> <pre><code>class GigaEntity(BaseGigaEntity):\n    \"\"\"Entity with location data.\"\"\"\n\n    latitude: float = Field(\n        ..., ge=-90, le=90, description=\"Latitude coordinate of the entity\"\n    )\n    longitude: float = Field(\n        ..., ge=-180, le=180, description=\"Longitude coordinate of the entity\"\n    )\n    admin1: Optional[str] = Field(\n        \"Unknown\", max_length=100, description=\"Primary administrative division\"\n    )\n    admin1_id_giga: Optional[str] = Field(\n        None,\n        max_length=50,\n        description=\"Unique identifier for the primary administrative division\",\n    )\n    admin2: Optional[str] = Field(\n        \"Unknown\", max_length=100, description=\"Secondary administrative division\"\n    )\n    admin2_id_giga: Optional[str] = Field(\n        None,\n        max_length=50,\n        description=\"Unique identifier for the secondary administrative division\",\n    )\n</code></pre>"},{"location":"api/core/#gigaspatial.core.schemas.entity.GigaEntityNoLocation","title":"<code>GigaEntityNoLocation</code>","text":"<p>               Bases: <code>BaseGigaEntity</code></p> <p>Entity without location data.</p> Source code in <code>gigaspatial/core/schemas/entity.py</code> <pre><code>class GigaEntityNoLocation(BaseGigaEntity):\n    \"\"\"Entity without location data.\"\"\"\n\n    pass\n</code></pre>"},{"location":"api/generators/","title":"Generators Module","text":""},{"location":"api/generators/#gigaspatial.generators","title":"<code>gigaspatial.generators</code>","text":""},{"location":"api/generators/#gigaspatial.generators.poi","title":"<code>poi</code>","text":""},{"location":"api/generators/#gigaspatial.generators.poi.PoiViewGenerator","title":"<code>PoiViewGenerator</code>","text":"<p>POI View Generator for integrating various geospatial datasets such as Google Open Buildings, Microsoft Global Buildings, GHSL Built Surface, and GHSL Settlement Model (SMOD) data with Points of Interest (POIs).</p> <p>This class provides methods to load, process, and map external geospatial data to a given set of POIs, enriching them with relevant attributes. It leverages handler/reader classes for efficient data access and processing.</p> <p>The POIs can be initialized from a list of (latitude, longitude) tuples, a list of dictionaries, a pandas DataFrame, or a geopandas GeoDataFrame.</p> Source code in <code>gigaspatial/generators/poi.py</code> <pre><code>class PoiViewGenerator:\n    \"\"\"\n    POI View Generator for integrating various geospatial datasets\n    such as Google Open Buildings, Microsoft Global Buildings, GHSL Built Surface,\n    and GHSL Settlement Model (SMOD) data with Points of Interest (POIs).\n\n    This class provides methods to load, process, and map external geospatial\n    data to a given set of POIs, enriching them with relevant attributes.\n    It leverages handler/reader classes for efficient data access and processing.\n\n    The POIs can be initialized from a list of (latitude, longitude) tuples,\n    a list of dictionaries, a pandas DataFrame, or a geopandas GeoDataFrame.\n    \"\"\"\n\n    def __init__(\n        self,\n        points: Union[\n            List[Tuple[float, float]], List[dict], pd.DataFrame, gpd.GeoDataFrame\n        ],\n        config: Optional[PoiViewGeneratorConfig] = None,\n        data_store: Optional[DataStore] = None,\n        logger: logging.Logger = None,\n    ):\n        \"\"\"\n        Initializes the PoiViewGenerator with the input points and configurations.\n\n        The input `points` are converted into an internal GeoDataFrame\n        (`_points_gdf`) for consistent geospatial operations.\n\n        Args:\n            points (Union[List[Tuple[float, float]], List[dict], pd.DataFrame, gpd.GeoDataFrame]):\n                The input points of interest. Can be:\n                - A list of (latitude, longitude) tuples.\n                - A list of dictionaries, where each dict must contain 'latitude' and 'longitude' keys.\n                - A pandas DataFrame with 'latitude' and 'longitude' columns.\n                - A geopandas GeoDataFrame (expected to have a 'geometry' column representing points).\n            generator_config (Optional[PoiViewGeneratorConfig]):\n                Configuration for the POI view generation process. If None, a\n                default `PoiViewGeneratorConfig` will be used.\n            data_store (Optional[DataStore]):\n                An instance of a data store for managing data access (e.g., LocalDataStore).\n                If None, a default `LocalDataStore` will be used.\n        \"\"\"\n        self.config = config or PoiViewGeneratorConfig()\n        self.data_store = data_store or LocalDataStore()\n        self.logger = logger or global_config.get_logger(self.__class__.__name__)\n        self._points_gdf = self._init_points_gdf(points)\n\n    @staticmethod\n    def _init_points_gdf(\n        points: Union[\n            List[Tuple[float, float]], List[dict], pd.DataFrame, gpd.GeoDataFrame\n        ],\n    ) -&gt; gpd.GeoDataFrame:\n        \"\"\"\n        Internal static method to convert various point input formats into a GeoDataFrame.\n\n        This method standardizes coordinate column names to 'latitude' and 'longitude'\n        for consistent internal representation. It also ensures each point has a unique\n        identifier in the 'poi_id' column.\n\n        Args:\n            points: Input points in various formats:\n                - List of (latitude, longitude) tuples\n                - List of dictionaries with coordinate keys\n                - DataFrame with coordinate columns\n                - GeoDataFrame with point geometries\n\n        Returns:\n            gpd.GeoDataFrame: Standardized GeoDataFrame with 'latitude', 'longitude',\n                             and 'poi_id' columns\n\n        Raises:\n            ValueError: If points format is not supported or coordinate columns cannot be detected\n        \"\"\"\n        if isinstance(points, gpd.GeoDataFrame):\n            # Convert geometry to lat/lon if needed\n            if points.geometry.name == \"geometry\":\n                points = points.copy()\n                points[\"latitude\"] = points.geometry.y\n                points[\"longitude\"] = points.geometry.x\n            if \"poi_id\" not in points.columns:\n                points[\"poi_id\"] = [f\"poi_{i}\" for i in range(len(points))]\n            return points\n\n        elif isinstance(points, pd.DataFrame):\n            # Detect and standardize coordinate columns\n            try:\n                lat_col, lon_col = detect_coordinate_columns(points)\n                points = points.copy()\n                points[\"latitude\"] = points[lat_col]\n                points[\"longitude\"] = points[lon_col]\n                if \"poi_id\" not in points.columns:\n                    points[\"poi_id\"] = [f\"poi_{i}\" for i in range(len(points))]\n                return convert_to_geodataframe(points)\n            except ValueError as e:\n                raise ValueError(\n                    f\"Could not detect coordinate columns in DataFrame: {str(e)}\"\n                )\n\n        elif isinstance(points, list):\n            if len(points) == 0:\n                return gpd.GeoDataFrame(\n                    columns=[\"latitude\", \"longitude\", \"poi_id\", \"geometry\"],\n                    geometry=\"geometry\",\n                    crs=\"EPSG:4326\",\n                )\n\n            if isinstance(points[0], tuple) and len(points[0]) == 2:\n                # List of (lat, lon) tuples\n                df = pd.DataFrame(points, columns=[\"latitude\", \"longitude\"])\n                df[\"poi_id\"] = [f\"poi_{i}\" for i in range(len(points))]\n                return convert_to_geodataframe(df)\n\n            elif isinstance(points[0], dict):\n                # List of dictionaries\n                df = pd.DataFrame(points)\n                try:\n                    lat_col, lon_col = detect_coordinate_columns(df)\n                    df[\"latitude\"] = df[lat_col]\n                    df[\"longitude\"] = df[lon_col]\n                    if \"poi_id\" not in df.columns:\n                        df[\"poi_id\"] = [f\"poi_{i}\" for i in range(len(points))]\n                    return convert_to_geodataframe(df)\n                except ValueError as e:\n                    raise ValueError(\n                        f\"Could not detect coordinate columns in dictionary list: {str(e)}\"\n                    )\n\n        raise ValueError(\"Unsupported points input type for PoiViewGenerator.\")\n\n    @property\n    def points_gdf(self) -&gt; gpd.GeoDataFrame:\n        \"\"\"Gets the internal GeoDataFrame of points of interest.\"\"\"\n        return self._points_gdf\n\n    def map_nearest_points(\n        self,\n        points_df: Union[pd.DataFrame, gpd.GeoDataFrame],\n        id_column: str,\n        lat_column: Optional[str] = None,\n        lon_column: Optional[str] = None,\n        output_prefix: str = \"nearest\",\n        **kwargs,\n    ) -&gt; pd.DataFrame:\n        \"\"\"\n        Maps nearest points from a given DataFrame to the POIs.\n\n        Enriches the `points_gdf` with the ID and distance to the nearest point\n        from the input DataFrame for each POI.\n\n        Args:\n            points_df (Union[pd.DataFrame, gpd.GeoDataFrame]):\n                DataFrame containing points to find nearest neighbors from.\n                Must have latitude and longitude columns or point geometries.\n            id_column (str):\n                Name of the column containing unique identifiers for each point.\n            lat_column (str, optional):\n                Name of the latitude column in points_df. If None, will attempt to detect it\n                or extract from geometry if points_df is a GeoDataFrame.\n            lon_column (str, optional):\n                Name of the longitude column in points_df. If None, will attempt to detect it\n                or extract from geometry if points_df is a GeoDataFrame.\n            output_prefix (str, optional):\n                Prefix for the output column names. Defaults to \"nearest\".\n            **kwargs:\n                Additional keyword arguments passed to the data reader (if applicable).\n\n        Returns:\n            pd.DataFrame: The updated GeoDataFrame with new columns:\n                          '{output_prefix}_id' and '{output_prefix}_distance'.\n                          Returns a copy of the current `points_gdf` if no points are found.\n\n        Raises:\n            ValueError: If required columns are missing from points_df or if coordinate\n                       columns cannot be detected or extracted from geometry.\n        \"\"\"\n        self.logger.info(\n            f\"Mapping nearest points from {points_df.__class__.__name__} to POIs\"\n        )\n\n        # Validate input DataFrame\n        if points_df.empty:\n            self.logger.info(\"No points found in the input DataFrame\")\n            return self.points_gdf.copy()\n\n        # Handle GeoDataFrame\n        if isinstance(points_df, gpd.GeoDataFrame):\n            points_df = points_df.copy()\n            if points_df.geometry.name == \"geometry\":\n                points_df[\"latitude\"] = points_df.geometry.y\n                points_df[\"longitude\"] = points_df.geometry.x\n                lat_column = \"latitude\"\n                lon_column = \"longitude\"\n                self.logger.info(\"Extracted coordinates from geometry\")\n\n        # Detect coordinate columns if not provided\n        if lat_column is None or lon_column is None:\n            try:\n                detected_lat, detected_lon = detect_coordinate_columns(points_df)\n                lat_column = lat_column or detected_lat\n                lon_column = lon_column or detected_lon\n                self.logger.info(\n                    f\"Detected coordinate columns: {lat_column}, {lon_column}\"\n                )\n            except ValueError as e:\n                raise ValueError(f\"Could not detect coordinate columns: {str(e)}\")\n\n        # Validate required columns\n        required_columns = [lat_column, lon_column, id_column]\n        missing_columns = [\n            col for col in required_columns if col not in points_df.columns\n        ]\n        if missing_columns:\n            raise ValueError(\n                f\"Missing required columns in points_df: {missing_columns}\"\n            )\n\n        from gigaspatial.processing.geo import calculate_distance\n\n        self.logger.info(\"Calculating nearest points for each POI\")\n        tree = cKDTree(points_df[[lat_column, lon_column]])\n        points_df_poi = self.points_gdf.copy()\n        _, idx = tree.query(points_df_poi[[\"latitude\", \"longitude\"]], k=1)\n        df_nearest = points_df.iloc[idx]\n        dist = calculate_distance(\n            lat1=points_df_poi.latitude,\n            lon1=points_df_poi.longitude,\n            lat2=df_nearest[lat_column],\n            lon2=df_nearest[lon_column],\n        )\n        result = points_df_poi.copy()\n        result[f\"{output_prefix}_id\"] = df_nearest[id_column].to_numpy()\n        result[f\"{output_prefix}_distance\"] = dist\n        self.logger.info(\n            f\"Nearest points mapping complete with prefix '{output_prefix}'\"\n        )\n        self._points_gdf = result\n        return result\n\n    def map_google_buildings(\n        self,\n        handler: Optional[GoogleOpenBuildingsHandler] = None,\n        **kwargs,\n    ) -&gt; pd.DataFrame:\n        \"\"\"\n        Maps Google Open Buildings data to the POIs by finding the nearest building.\n\n        Enriches the `points_gdf` with the ID and distance to the nearest\n        Google Open Building for each POI.\n\n        Args:\n            data_config (Optional[GoogleOpenBuildingsConfig]):\n                Configuration for accessing Google Open Buildings data. If None, a\n                default `GoogleOpenBuildingsConfig` will be used.\n            **kwargs:\n                Additional keyword arguments passed to the data reader (if applicable).\n\n        Returns:\n            pd.DataFrame: The updated GeoDataFrame with new columns:\n                          'nearest_google_building_id' and 'nearest_google_building_distance'.\n                          Returns a copy of the current `points_gdf` if no buildings are found.\n        \"\"\"\n        self.logger.info(\"Mapping Google Open Buildings data to POIs\")\n        handler = handler or GoogleOpenBuildingsHandler(data_store=self.data_store)\n\n        self.logger.info(\"Loading Google Buildings point data\")\n        buildings_df = handler.load_points(\n            self.points_gdf, ensure_available=self.config.ensure_available\n        )\n        if buildings_df is None or len(buildings_df) == 0:\n            self.logger.info(\"No Google buildings data found for the provided POIs\")\n            return self.points_gdf.copy()\n\n        return self.map_nearest_points(\n            points_df=buildings_df,\n            id_column=\"full_plus_code\",\n            output_prefix=\"nearest_google_building\",\n            **kwargs,\n        )\n\n    def map_ms_buildings(\n        self,\n        handler: Optional[MSBuildingsHandler] = None,\n        **kwargs,\n    ) -&gt; pd.DataFrame:\n        \"\"\"\n        Maps Microsoft Global Buildings data to the POIs by finding the nearest building.\n\n        Enriches the `points_gdf` with the ID and distance to the nearest\n        Microsoft Global Building for each POI. If buildings don't have an ID column,\n        creates a unique ID using the building's coordinates.\n\n        Args:\n            data_config (Optional[MSBuildingsConfig]):\n                Configuration for accessing Microsoft Global Buildings data. If None, a\n                default `MSBuildingsConfig` will be used.\n            **kwargs:\n                Additional keyword arguments passed to the data reader (if applicable).\n\n        Returns:\n            pd.DataFrame: The updated GeoDataFrame with new columns:\n                          'nearest_ms_building_id' and 'nearest_ms_building_distance'.\n                          Returns a copy of the current `points_gdf` if no buildings are found.\n        \"\"\"\n        self.logger.info(\"Mapping Microsoft Global Buildings data to POIs\")\n        handler = handler or MSBuildingsHandler(data_store=self.data_store)\n        self.logger.info(\"Loading Microsoft Buildings polygon data\")\n        buildings_gdf = handler.load_data(\n            self.points_gdf, ensure_available=self.config.ensure_available\n        )\n        if buildings_gdf is None or len(buildings_gdf) == 0:\n            self.logger.info(\"No Microsoft buildings data found for the provided POIs\")\n            return self.points_gdf.copy()\n\n        if \"building_id\" not in buildings_gdf:\n            self.logger.info(\"Creating building IDs from coordinates\")\n            buildings_gdf = buildings_gdf.copy()\n            buildings_gdf[\"building_id\"] = buildings_gdf.apply(\n                lambda row: f\"{row.geometry.y:.6f}_{row.geometry.x:.6f}\",\n                axis=1,\n            )\n\n        return self.map_nearest_points(\n            points_df=buildings_gdf,\n            id_column=\"building_id\",\n            output_prefix=\"nearest_ms_building\",\n            **kwargs,\n        )\n\n    def map_zonal_stats(\n        self,\n        data: Union[List[TifProcessor], gpd.GeoDataFrame],\n        stat: str = \"mean\",\n        map_radius_meters: Optional[float] = None,\n        output_column: str = \"zonal_stat\",\n        value_column: Optional[str] = None,\n        area_weighted: bool = False,\n        **kwargs,\n    ) -&gt; pd.DataFrame:\n        \"\"\"\n        Maps zonal statistics from raster or polygon data to POIs.\n\n        Can operate in three modes:\n        1. Raster point sampling: Directly samples raster values at POI locations\n        2. Raster zonal statistics: Creates buffers around POIs and calculates statistics within them\n        3. Polygon aggregation: Aggregates polygon data to POI buffers with optional area weighting\n\n        Args:\n            data (Union[List[TifProcessor], gpd.GeoDataFrame]):\n                Either a list of TifProcessor objects containing raster data to sample,\n                or a GeoDataFrame containing polygon data to aggregate.\n            stat (str, optional):\n                For raster data: Statistic to calculate (\"sum\", \"mean\", \"median\", \"min\", \"max\").\n                For polygon data: Aggregation method to use.\n                Defaults to \"mean\".\n            map_radius_meters (float, optional):\n                If provided, creates circular buffers of this radius around each POI\n                and calculates statistics within the buffers. If None, samples directly\n                at POI locations (only for raster data).\n            output_column (str, optional):\n                Name of the output column to store the results. Defaults to \"zonal_stat\".\n            value_column (str, optional):\n                For polygon data: Name of the column to aggregate. Required for polygon data.\n                Not used for raster data.\n            area_weighted (bool, optional):\n                For polygon data: Whether to weight values by fractional area of\n                intersection. Defaults to False.\n            **kwargs:\n                Additional keyword arguments passed to the sampling/aggregation functions.\n\n        Returns:\n            pd.DataFrame: The updated GeoDataFrame with a new column containing the\n                          calculated statistics. Returns a copy of the current `points_gdf`\n                          if no valid data is found.\n\n        Raises:\n            ValueError: If no valid data is provided, if parameters are incompatible,\n                      or if required parameters (value_column) are missing for polygon data.\n        \"\"\"\n        if isinstance(data, list) and all(isinstance(x, TifProcessor) for x in data):\n            # Handle raster data\n            if not data:\n                self.logger.info(\"No valid raster data found for the provided POIs\")\n                return self.points_gdf.copy()\n\n            if map_radius_meters is not None:\n                self.logger.info(\n                    f\"Calculating {stat} within {map_radius_meters}m buffers around POIs\"\n                )\n                # Create buffers around POIs\n                polygon_list = buffer_geodataframe(\n                    self.points_gdf,\n                    buffer_distance_meters=map_radius_meters,\n                    cap_style=\"round\",\n                ).geometry\n\n                # Calculate zonal statistics\n                sampled_values = sample_multiple_tifs_by_polygons(\n                    tif_processors=data, polygon_list=polygon_list, stat=stat, **kwargs\n                )\n            else:\n                self.logger.info(f\"Sampling {stat} at POI locations\")\n                # Sample directly at POI locations\n                coord_list = self.points_gdf[[\"latitude\", \"longitude\"]].to_numpy()\n                sampled_values = sample_multiple_tifs_by_coordinates(\n                    tif_processors=data, coordinate_list=coord_list, **kwargs\n                )\n\n        elif isinstance(data, gpd.GeoDataFrame):\n            # Handle polygon data\n            if data.empty:\n                self.logger.info(\"No valid polygon data found for the provided POIs\")\n                return self.points_gdf.copy()\n\n            if map_radius_meters is None:\n                raise ValueError(\"map_radius_meters must be provided for polygon data\")\n\n            if value_column is None:\n                raise ValueError(\"value_column must be provided for polygon data\")\n\n            self.logger.info(\n                f\"Aggregating {value_column} within {map_radius_meters}m buffers around POIs\"\n            )\n\n            # Create buffers around POIs\n            buffer_gdf = buffer_geodataframe(\n                self.points_gdf,\n                buffer_distance_meters=map_radius_meters,\n                cap_style=\"round\",\n            )\n\n            # Aggregate polygons to buffers\n            result = aggregate_polygons_to_zones(\n                polygons=data,\n                zones=buffer_gdf,\n                value_columns=value_column,\n                aggregation=stat,\n                area_weighted=area_weighted,\n                zone_id_column=\"poi_id\",\n                **kwargs,\n            )\n\n            # Extract values for each POI\n            sampled_values = result[value_column].values\n\n        else:\n            raise ValueError(\n                \"data must be either a list of TifProcessor objects or a GeoDataFrame\"\n            )\n\n        result = self.points_gdf.copy()\n        result[output_column] = sampled_values\n        self.logger.info(f\"Zonal statistics mapping complete: {output_column}\")\n        self._points_gdf = result\n        return result\n\n    def map_built_s(\n        self,\n        map_radius_meters: float = 150,\n        stat: str = \"sum\",\n        dataset_year=2020,\n        dataset_resolution=100,\n        output_column=\"built_surface_m2\",\n        **kwargs,\n    ) -&gt; pd.DataFrame:\n        \"\"\"\n        Maps GHSL Built Surface (GHS_BUILT_S) data to the POIs.\n\n        Calculates the sum of built surface area within a specified buffer\n        radius around each POI. Enriches `points_gdf` with the 'built_surface_m2' column.\n\n        Args:\n            data_config (Optional[GHSLDataConfig]):\n                Configuration for accessing GHSL Built Surface data. If None, a\n                default `GHSLDataConfig` for 'GHS_BUILT_S' will be used.\n            map_radius_meters (float):\n                The buffer distance in meters around each POI to calculate\n                zonal statistics for built surface. Defaults to 150 meters.\n            **kwargs:\n                Additional keyword arguments passed to the data reader (if applicable).\n\n        Returns:\n            pd.DataFrame: The updated GeoDataFrame with a new column:\n                          'built_surface_m2'. Returns a copy of the current\n                          `points_gdf` if no GHSL Built Surface data is found.\n        \"\"\"\n        self.logger.info(\"Mapping GHSL Built Surface data to POIs\")\n        handler = GHSLDataHandler(\n            product=\"GHS_BUILT_S\",\n            year=dataset_year,\n            resolution=dataset_resolution,\n            data_store=self.data_store,\n            **kwargs,\n        )\n        gdf_points = self.points_gdf.to_crs(handler.config.crs)\n        self.logger.info(\"Loading GHSL Built Surface raster tiles\")\n        tif_processors = handler.load_data(\n            gdf_points, ensure_available=self.config.ensure_available\n        )\n\n        return self.map_zonal_stats(\n            data=tif_processors,\n            stat=stat,\n            map_radius_meters=map_radius_meters,\n            output_column=output_column,\n            **kwargs,\n        )\n\n    def map_smod(\n        self,\n        stat=\"median\",\n        dataset_year=2020,\n        dataset_resolution=100,\n        output_column=\"smod_class\",\n        **kwargs,\n    ) -&gt; pd.DataFrame:\n        \"\"\"\n        Maps GHSL Settlement Model (SMOD) data to the POIs.\n\n        Samples the SMOD class value at each POI's location. Enriches `points_gdf`\n        with the 'smod_class' column.\n\n        Args:\n            data_config (Optional[GHSLDataConfig]):\n                Configuration for accessing GHSL SMOD data. If None, a\n                default `GHSLDataConfig` for 'GHS_SMOD' will be used.\n            **kwargs:\n                Additional keyword arguments passed to the data reader (if applicable).\n\n        Returns:\n            pd.DataFrame: The updated GeoDataFrame with a new column:\n                          'smod_class'. Returns a copy of the current\n                          `points_gdf` if no GHSL SMOD data is found.\n        \"\"\"\n        self.logger.info(\"Mapping GHSL Settlement Model (SMOD) data to POIs\")\n        handler = GHSLDataHandler(\n            product=\"GHS_SMOD\",\n            year=dataset_year,\n            resolution=dataset_resolution,\n            data_store=self.data_store,\n            coord_system=54009,\n            **kwargs,\n        )\n\n        gdf_points = self.points_gdf.to_crs(handler.config.crs)\n        self.logger.info(\"Loading GHSL SMOD raster tiles\")\n        tif_processors = handler.load_data(\n            gdf_points, ensure_available=self.config.ensure_available\n        )\n\n        return self.map_zonal_stats(\n            data=tif_processors,\n            stat=stat,  # Use median for categorical data\n            output_column=output_column,\n            **kwargs,\n        )\n\n    def save_view(\n        self,\n        name: str,\n        output_format: Optional[str] = None,\n    ) -&gt; Path:\n        \"\"\"\n        Saves the current POI view (the enriched GeoDataFrame) to a file.\n\n        The output path and format are determined by the `generator_config`\n        or overridden by the `output_format` parameter.\n\n        Args:\n            name (str): The base name for the output file (without extension).\n            output_format (Optional[str]):\n                The desired output format (e.g., \"csv\", \"geojson\"). If None,\n                the `output_format` from `generator_config` will be used.\n\n        Returns:\n            Path: The full path to the saved output file.\n        \"\"\"\n        format_to_use = output_format or self.generator_config.output_format\n        output_path = self.generator_config.base_path / f\"{name}.{format_to_use}\"\n\n        self.logger.info(f\"Saving POI view to {output_path}\")\n        write_dataset(\n            df=self.points_gdf,\n            path=str(output_path),\n            data_store=self.data_store,\n            format=format_to_use,\n        )\n\n        return output_path\n</code></pre>"},{"location":"api/generators/#gigaspatial.generators.poi.PoiViewGenerator.points_gdf","title":"<code>points_gdf: gpd.GeoDataFrame</code>  <code>property</code>","text":"<p>Gets the internal GeoDataFrame of points of interest.</p>"},{"location":"api/generators/#gigaspatial.generators.poi.PoiViewGenerator.__init__","title":"<code>__init__(points, config=None, data_store=None, logger=None)</code>","text":"<p>Initializes the PoiViewGenerator with the input points and configurations.</p> <p>The input <code>points</code> are converted into an internal GeoDataFrame (<code>_points_gdf</code>) for consistent geospatial operations.</p> <p>Parameters:</p> Name Type Description Default <code>points</code> <code>Union[List[Tuple[float, float]], List[dict], DataFrame, GeoDataFrame]</code> <p>The input points of interest. Can be: - A list of (latitude, longitude) tuples. - A list of dictionaries, where each dict must contain 'latitude' and 'longitude' keys. - A pandas DataFrame with 'latitude' and 'longitude' columns. - A geopandas GeoDataFrame (expected to have a 'geometry' column representing points).</p> required <code>generator_config</code> <code>Optional[PoiViewGeneratorConfig]</code> <p>Configuration for the POI view generation process. If None, a default <code>PoiViewGeneratorConfig</code> will be used.</p> required <code>data_store</code> <code>Optional[DataStore]</code> <p>An instance of a data store for managing data access (e.g., LocalDataStore). If None, a default <code>LocalDataStore</code> will be used.</p> <code>None</code> Source code in <code>gigaspatial/generators/poi.py</code> <pre><code>def __init__(\n    self,\n    points: Union[\n        List[Tuple[float, float]], List[dict], pd.DataFrame, gpd.GeoDataFrame\n    ],\n    config: Optional[PoiViewGeneratorConfig] = None,\n    data_store: Optional[DataStore] = None,\n    logger: logging.Logger = None,\n):\n    \"\"\"\n    Initializes the PoiViewGenerator with the input points and configurations.\n\n    The input `points` are converted into an internal GeoDataFrame\n    (`_points_gdf`) for consistent geospatial operations.\n\n    Args:\n        points (Union[List[Tuple[float, float]], List[dict], pd.DataFrame, gpd.GeoDataFrame]):\n            The input points of interest. Can be:\n            - A list of (latitude, longitude) tuples.\n            - A list of dictionaries, where each dict must contain 'latitude' and 'longitude' keys.\n            - A pandas DataFrame with 'latitude' and 'longitude' columns.\n            - A geopandas GeoDataFrame (expected to have a 'geometry' column representing points).\n        generator_config (Optional[PoiViewGeneratorConfig]):\n            Configuration for the POI view generation process. If None, a\n            default `PoiViewGeneratorConfig` will be used.\n        data_store (Optional[DataStore]):\n            An instance of a data store for managing data access (e.g., LocalDataStore).\n            If None, a default `LocalDataStore` will be used.\n    \"\"\"\n    self.config = config or PoiViewGeneratorConfig()\n    self.data_store = data_store or LocalDataStore()\n    self.logger = logger or global_config.get_logger(self.__class__.__name__)\n    self._points_gdf = self._init_points_gdf(points)\n</code></pre>"},{"location":"api/generators/#gigaspatial.generators.poi.PoiViewGenerator.map_built_s","title":"<code>map_built_s(map_radius_meters=150, stat='sum', dataset_year=2020, dataset_resolution=100, output_column='built_surface_m2', **kwargs)</code>","text":"<p>Maps GHSL Built Surface (GHS_BUILT_S) data to the POIs.</p> <p>Calculates the sum of built surface area within a specified buffer radius around each POI. Enriches <code>points_gdf</code> with the 'built_surface_m2' column.</p> <p>Parameters:</p> Name Type Description Default <code>data_config</code> <code>Optional[GHSLDataConfig]</code> <p>Configuration for accessing GHSL Built Surface data. If None, a default <code>GHSLDataConfig</code> for 'GHS_BUILT_S' will be used.</p> required <code>map_radius_meters</code> <code>float</code> <p>The buffer distance in meters around each POI to calculate zonal statistics for built surface. Defaults to 150 meters.</p> <code>150</code> <code>**kwargs</code> <p>Additional keyword arguments passed to the data reader (if applicable).</p> <code>{}</code> <p>Returns:</p> Type Description <code>DataFrame</code> <p>pd.DataFrame: The updated GeoDataFrame with a new column:           'built_surface_m2'. Returns a copy of the current           <code>points_gdf</code> if no GHSL Built Surface data is found.</p> Source code in <code>gigaspatial/generators/poi.py</code> <pre><code>def map_built_s(\n    self,\n    map_radius_meters: float = 150,\n    stat: str = \"sum\",\n    dataset_year=2020,\n    dataset_resolution=100,\n    output_column=\"built_surface_m2\",\n    **kwargs,\n) -&gt; pd.DataFrame:\n    \"\"\"\n    Maps GHSL Built Surface (GHS_BUILT_S) data to the POIs.\n\n    Calculates the sum of built surface area within a specified buffer\n    radius around each POI. Enriches `points_gdf` with the 'built_surface_m2' column.\n\n    Args:\n        data_config (Optional[GHSLDataConfig]):\n            Configuration for accessing GHSL Built Surface data. If None, a\n            default `GHSLDataConfig` for 'GHS_BUILT_S' will be used.\n        map_radius_meters (float):\n            The buffer distance in meters around each POI to calculate\n            zonal statistics for built surface. Defaults to 150 meters.\n        **kwargs:\n            Additional keyword arguments passed to the data reader (if applicable).\n\n    Returns:\n        pd.DataFrame: The updated GeoDataFrame with a new column:\n                      'built_surface_m2'. Returns a copy of the current\n                      `points_gdf` if no GHSL Built Surface data is found.\n    \"\"\"\n    self.logger.info(\"Mapping GHSL Built Surface data to POIs\")\n    handler = GHSLDataHandler(\n        product=\"GHS_BUILT_S\",\n        year=dataset_year,\n        resolution=dataset_resolution,\n        data_store=self.data_store,\n        **kwargs,\n    )\n    gdf_points = self.points_gdf.to_crs(handler.config.crs)\n    self.logger.info(\"Loading GHSL Built Surface raster tiles\")\n    tif_processors = handler.load_data(\n        gdf_points, ensure_available=self.config.ensure_available\n    )\n\n    return self.map_zonal_stats(\n        data=tif_processors,\n        stat=stat,\n        map_radius_meters=map_radius_meters,\n        output_column=output_column,\n        **kwargs,\n    )\n</code></pre>"},{"location":"api/generators/#gigaspatial.generators.poi.PoiViewGenerator.map_google_buildings","title":"<code>map_google_buildings(handler=None, **kwargs)</code>","text":"<p>Maps Google Open Buildings data to the POIs by finding the nearest building.</p> <p>Enriches the <code>points_gdf</code> with the ID and distance to the nearest Google Open Building for each POI.</p> <p>Parameters:</p> Name Type Description Default <code>data_config</code> <code>Optional[GoogleOpenBuildingsConfig]</code> <p>Configuration for accessing Google Open Buildings data. If None, a default <code>GoogleOpenBuildingsConfig</code> will be used.</p> required <code>**kwargs</code> <p>Additional keyword arguments passed to the data reader (if applicable).</p> <code>{}</code> <p>Returns:</p> Type Description <code>DataFrame</code> <p>pd.DataFrame: The updated GeoDataFrame with new columns:           'nearest_google_building_id' and 'nearest_google_building_distance'.           Returns a copy of the current <code>points_gdf</code> if no buildings are found.</p> Source code in <code>gigaspatial/generators/poi.py</code> <pre><code>def map_google_buildings(\n    self,\n    handler: Optional[GoogleOpenBuildingsHandler] = None,\n    **kwargs,\n) -&gt; pd.DataFrame:\n    \"\"\"\n    Maps Google Open Buildings data to the POIs by finding the nearest building.\n\n    Enriches the `points_gdf` with the ID and distance to the nearest\n    Google Open Building for each POI.\n\n    Args:\n        data_config (Optional[GoogleOpenBuildingsConfig]):\n            Configuration for accessing Google Open Buildings data. If None, a\n            default `GoogleOpenBuildingsConfig` will be used.\n        **kwargs:\n            Additional keyword arguments passed to the data reader (if applicable).\n\n    Returns:\n        pd.DataFrame: The updated GeoDataFrame with new columns:\n                      'nearest_google_building_id' and 'nearest_google_building_distance'.\n                      Returns a copy of the current `points_gdf` if no buildings are found.\n    \"\"\"\n    self.logger.info(\"Mapping Google Open Buildings data to POIs\")\n    handler = handler or GoogleOpenBuildingsHandler(data_store=self.data_store)\n\n    self.logger.info(\"Loading Google Buildings point data\")\n    buildings_df = handler.load_points(\n        self.points_gdf, ensure_available=self.config.ensure_available\n    )\n    if buildings_df is None or len(buildings_df) == 0:\n        self.logger.info(\"No Google buildings data found for the provided POIs\")\n        return self.points_gdf.copy()\n\n    return self.map_nearest_points(\n        points_df=buildings_df,\n        id_column=\"full_plus_code\",\n        output_prefix=\"nearest_google_building\",\n        **kwargs,\n    )\n</code></pre>"},{"location":"api/generators/#gigaspatial.generators.poi.PoiViewGenerator.map_ms_buildings","title":"<code>map_ms_buildings(handler=None, **kwargs)</code>","text":"<p>Maps Microsoft Global Buildings data to the POIs by finding the nearest building.</p> <p>Enriches the <code>points_gdf</code> with the ID and distance to the nearest Microsoft Global Building for each POI. If buildings don't have an ID column, creates a unique ID using the building's coordinates.</p> <p>Parameters:</p> Name Type Description Default <code>data_config</code> <code>Optional[MSBuildingsConfig]</code> <p>Configuration for accessing Microsoft Global Buildings data. If None, a default <code>MSBuildingsConfig</code> will be used.</p> required <code>**kwargs</code> <p>Additional keyword arguments passed to the data reader (if applicable).</p> <code>{}</code> <p>Returns:</p> Type Description <code>DataFrame</code> <p>pd.DataFrame: The updated GeoDataFrame with new columns:           'nearest_ms_building_id' and 'nearest_ms_building_distance'.           Returns a copy of the current <code>points_gdf</code> if no buildings are found.</p> Source code in <code>gigaspatial/generators/poi.py</code> <pre><code>def map_ms_buildings(\n    self,\n    handler: Optional[MSBuildingsHandler] = None,\n    **kwargs,\n) -&gt; pd.DataFrame:\n    \"\"\"\n    Maps Microsoft Global Buildings data to the POIs by finding the nearest building.\n\n    Enriches the `points_gdf` with the ID and distance to the nearest\n    Microsoft Global Building for each POI. If buildings don't have an ID column,\n    creates a unique ID using the building's coordinates.\n\n    Args:\n        data_config (Optional[MSBuildingsConfig]):\n            Configuration for accessing Microsoft Global Buildings data. If None, a\n            default `MSBuildingsConfig` will be used.\n        **kwargs:\n            Additional keyword arguments passed to the data reader (if applicable).\n\n    Returns:\n        pd.DataFrame: The updated GeoDataFrame with new columns:\n                      'nearest_ms_building_id' and 'nearest_ms_building_distance'.\n                      Returns a copy of the current `points_gdf` if no buildings are found.\n    \"\"\"\n    self.logger.info(\"Mapping Microsoft Global Buildings data to POIs\")\n    handler = handler or MSBuildingsHandler(data_store=self.data_store)\n    self.logger.info(\"Loading Microsoft Buildings polygon data\")\n    buildings_gdf = handler.load_data(\n        self.points_gdf, ensure_available=self.config.ensure_available\n    )\n    if buildings_gdf is None or len(buildings_gdf) == 0:\n        self.logger.info(\"No Microsoft buildings data found for the provided POIs\")\n        return self.points_gdf.copy()\n\n    if \"building_id\" not in buildings_gdf:\n        self.logger.info(\"Creating building IDs from coordinates\")\n        buildings_gdf = buildings_gdf.copy()\n        buildings_gdf[\"building_id\"] = buildings_gdf.apply(\n            lambda row: f\"{row.geometry.y:.6f}_{row.geometry.x:.6f}\",\n            axis=1,\n        )\n\n    return self.map_nearest_points(\n        points_df=buildings_gdf,\n        id_column=\"building_id\",\n        output_prefix=\"nearest_ms_building\",\n        **kwargs,\n    )\n</code></pre>"},{"location":"api/generators/#gigaspatial.generators.poi.PoiViewGenerator.map_nearest_points","title":"<code>map_nearest_points(points_df, id_column, lat_column=None, lon_column=None, output_prefix='nearest', **kwargs)</code>","text":"<p>Maps nearest points from a given DataFrame to the POIs.</p> <p>Enriches the <code>points_gdf</code> with the ID and distance to the nearest point from the input DataFrame for each POI.</p> <p>Parameters:</p> Name Type Description Default <code>points_df</code> <code>Union[DataFrame, GeoDataFrame]</code> <p>DataFrame containing points to find nearest neighbors from. Must have latitude and longitude columns or point geometries.</p> required <code>id_column</code> <code>str</code> <p>Name of the column containing unique identifiers for each point.</p> required <code>lat_column</code> <code>str</code> <p>Name of the latitude column in points_df. If None, will attempt to detect it or extract from geometry if points_df is a GeoDataFrame.</p> <code>None</code> <code>lon_column</code> <code>str</code> <p>Name of the longitude column in points_df. If None, will attempt to detect it or extract from geometry if points_df is a GeoDataFrame.</p> <code>None</code> <code>output_prefix</code> <code>str</code> <p>Prefix for the output column names. Defaults to \"nearest\".</p> <code>'nearest'</code> <code>**kwargs</code> <p>Additional keyword arguments passed to the data reader (if applicable).</p> <code>{}</code> <p>Returns:</p> Type Description <code>DataFrame</code> <p>pd.DataFrame: The updated GeoDataFrame with new columns:           '{output_prefix}_id' and '{output_prefix}_distance'.           Returns a copy of the current <code>points_gdf</code> if no points are found.</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If required columns are missing from points_df or if coordinate        columns cannot be detected or extracted from geometry.</p> Source code in <code>gigaspatial/generators/poi.py</code> <pre><code>def map_nearest_points(\n    self,\n    points_df: Union[pd.DataFrame, gpd.GeoDataFrame],\n    id_column: str,\n    lat_column: Optional[str] = None,\n    lon_column: Optional[str] = None,\n    output_prefix: str = \"nearest\",\n    **kwargs,\n) -&gt; pd.DataFrame:\n    \"\"\"\n    Maps nearest points from a given DataFrame to the POIs.\n\n    Enriches the `points_gdf` with the ID and distance to the nearest point\n    from the input DataFrame for each POI.\n\n    Args:\n        points_df (Union[pd.DataFrame, gpd.GeoDataFrame]):\n            DataFrame containing points to find nearest neighbors from.\n            Must have latitude and longitude columns or point geometries.\n        id_column (str):\n            Name of the column containing unique identifiers for each point.\n        lat_column (str, optional):\n            Name of the latitude column in points_df. If None, will attempt to detect it\n            or extract from geometry if points_df is a GeoDataFrame.\n        lon_column (str, optional):\n            Name of the longitude column in points_df. If None, will attempt to detect it\n            or extract from geometry if points_df is a GeoDataFrame.\n        output_prefix (str, optional):\n            Prefix for the output column names. Defaults to \"nearest\".\n        **kwargs:\n            Additional keyword arguments passed to the data reader (if applicable).\n\n    Returns:\n        pd.DataFrame: The updated GeoDataFrame with new columns:\n                      '{output_prefix}_id' and '{output_prefix}_distance'.\n                      Returns a copy of the current `points_gdf` if no points are found.\n\n    Raises:\n        ValueError: If required columns are missing from points_df or if coordinate\n                   columns cannot be detected or extracted from geometry.\n    \"\"\"\n    self.logger.info(\n        f\"Mapping nearest points from {points_df.__class__.__name__} to POIs\"\n    )\n\n    # Validate input DataFrame\n    if points_df.empty:\n        self.logger.info(\"No points found in the input DataFrame\")\n        return self.points_gdf.copy()\n\n    # Handle GeoDataFrame\n    if isinstance(points_df, gpd.GeoDataFrame):\n        points_df = points_df.copy()\n        if points_df.geometry.name == \"geometry\":\n            points_df[\"latitude\"] = points_df.geometry.y\n            points_df[\"longitude\"] = points_df.geometry.x\n            lat_column = \"latitude\"\n            lon_column = \"longitude\"\n            self.logger.info(\"Extracted coordinates from geometry\")\n\n    # Detect coordinate columns if not provided\n    if lat_column is None or lon_column is None:\n        try:\n            detected_lat, detected_lon = detect_coordinate_columns(points_df)\n            lat_column = lat_column or detected_lat\n            lon_column = lon_column or detected_lon\n            self.logger.info(\n                f\"Detected coordinate columns: {lat_column}, {lon_column}\"\n            )\n        except ValueError as e:\n            raise ValueError(f\"Could not detect coordinate columns: {str(e)}\")\n\n    # Validate required columns\n    required_columns = [lat_column, lon_column, id_column]\n    missing_columns = [\n        col for col in required_columns if col not in points_df.columns\n    ]\n    if missing_columns:\n        raise ValueError(\n            f\"Missing required columns in points_df: {missing_columns}\"\n        )\n\n    from gigaspatial.processing.geo import calculate_distance\n\n    self.logger.info(\"Calculating nearest points for each POI\")\n    tree = cKDTree(points_df[[lat_column, lon_column]])\n    points_df_poi = self.points_gdf.copy()\n    _, idx = tree.query(points_df_poi[[\"latitude\", \"longitude\"]], k=1)\n    df_nearest = points_df.iloc[idx]\n    dist = calculate_distance(\n        lat1=points_df_poi.latitude,\n        lon1=points_df_poi.longitude,\n        lat2=df_nearest[lat_column],\n        lon2=df_nearest[lon_column],\n    )\n    result = points_df_poi.copy()\n    result[f\"{output_prefix}_id\"] = df_nearest[id_column].to_numpy()\n    result[f\"{output_prefix}_distance\"] = dist\n    self.logger.info(\n        f\"Nearest points mapping complete with prefix '{output_prefix}'\"\n    )\n    self._points_gdf = result\n    return result\n</code></pre>"},{"location":"api/generators/#gigaspatial.generators.poi.PoiViewGenerator.map_smod","title":"<code>map_smod(stat='median', dataset_year=2020, dataset_resolution=100, output_column='smod_class', **kwargs)</code>","text":"<p>Maps GHSL Settlement Model (SMOD) data to the POIs.</p> <p>Samples the SMOD class value at each POI's location. Enriches <code>points_gdf</code> with the 'smod_class' column.</p> <p>Parameters:</p> Name Type Description Default <code>data_config</code> <code>Optional[GHSLDataConfig]</code> <p>Configuration for accessing GHSL SMOD data. If None, a default <code>GHSLDataConfig</code> for 'GHS_SMOD' will be used.</p> required <code>**kwargs</code> <p>Additional keyword arguments passed to the data reader (if applicable).</p> <code>{}</code> <p>Returns:</p> Type Description <code>DataFrame</code> <p>pd.DataFrame: The updated GeoDataFrame with a new column:           'smod_class'. Returns a copy of the current           <code>points_gdf</code> if no GHSL SMOD data is found.</p> Source code in <code>gigaspatial/generators/poi.py</code> <pre><code>def map_smod(\n    self,\n    stat=\"median\",\n    dataset_year=2020,\n    dataset_resolution=100,\n    output_column=\"smod_class\",\n    **kwargs,\n) -&gt; pd.DataFrame:\n    \"\"\"\n    Maps GHSL Settlement Model (SMOD) data to the POIs.\n\n    Samples the SMOD class value at each POI's location. Enriches `points_gdf`\n    with the 'smod_class' column.\n\n    Args:\n        data_config (Optional[GHSLDataConfig]):\n            Configuration for accessing GHSL SMOD data. If None, a\n            default `GHSLDataConfig` for 'GHS_SMOD' will be used.\n        **kwargs:\n            Additional keyword arguments passed to the data reader (if applicable).\n\n    Returns:\n        pd.DataFrame: The updated GeoDataFrame with a new column:\n                      'smod_class'. Returns a copy of the current\n                      `points_gdf` if no GHSL SMOD data is found.\n    \"\"\"\n    self.logger.info(\"Mapping GHSL Settlement Model (SMOD) data to POIs\")\n    handler = GHSLDataHandler(\n        product=\"GHS_SMOD\",\n        year=dataset_year,\n        resolution=dataset_resolution,\n        data_store=self.data_store,\n        coord_system=54009,\n        **kwargs,\n    )\n\n    gdf_points = self.points_gdf.to_crs(handler.config.crs)\n    self.logger.info(\"Loading GHSL SMOD raster tiles\")\n    tif_processors = handler.load_data(\n        gdf_points, ensure_available=self.config.ensure_available\n    )\n\n    return self.map_zonal_stats(\n        data=tif_processors,\n        stat=stat,  # Use median for categorical data\n        output_column=output_column,\n        **kwargs,\n    )\n</code></pre>"},{"location":"api/generators/#gigaspatial.generators.poi.PoiViewGenerator.map_zonal_stats","title":"<code>map_zonal_stats(data, stat='mean', map_radius_meters=None, output_column='zonal_stat', value_column=None, area_weighted=False, **kwargs)</code>","text":"<p>Maps zonal statistics from raster or polygon data to POIs.</p> <p>Can operate in three modes: 1. Raster point sampling: Directly samples raster values at POI locations 2. Raster zonal statistics: Creates buffers around POIs and calculates statistics within them 3. Polygon aggregation: Aggregates polygon data to POI buffers with optional area weighting</p> <p>Parameters:</p> Name Type Description Default <code>data</code> <code>Union[List[TifProcessor], GeoDataFrame]</code> <p>Either a list of TifProcessor objects containing raster data to sample, or a GeoDataFrame containing polygon data to aggregate.</p> required <code>stat</code> <code>str</code> <p>For raster data: Statistic to calculate (\"sum\", \"mean\", \"median\", \"min\", \"max\"). For polygon data: Aggregation method to use. Defaults to \"mean\".</p> <code>'mean'</code> <code>map_radius_meters</code> <code>float</code> <p>If provided, creates circular buffers of this radius around each POI and calculates statistics within the buffers. If None, samples directly at POI locations (only for raster data).</p> <code>None</code> <code>output_column</code> <code>str</code> <p>Name of the output column to store the results. Defaults to \"zonal_stat\".</p> <code>'zonal_stat'</code> <code>value_column</code> <code>str</code> <p>For polygon data: Name of the column to aggregate. Required for polygon data. Not used for raster data.</p> <code>None</code> <code>area_weighted</code> <code>bool</code> <p>For polygon data: Whether to weight values by fractional area of intersection. Defaults to False.</p> <code>False</code> <code>**kwargs</code> <p>Additional keyword arguments passed to the sampling/aggregation functions.</p> <code>{}</code> <p>Returns:</p> Type Description <code>DataFrame</code> <p>pd.DataFrame: The updated GeoDataFrame with a new column containing the           calculated statistics. Returns a copy of the current <code>points_gdf</code>           if no valid data is found.</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If no valid data is provided, if parameters are incompatible,       or if required parameters (value_column) are missing for polygon data.</p> Source code in <code>gigaspatial/generators/poi.py</code> <pre><code>def map_zonal_stats(\n    self,\n    data: Union[List[TifProcessor], gpd.GeoDataFrame],\n    stat: str = \"mean\",\n    map_radius_meters: Optional[float] = None,\n    output_column: str = \"zonal_stat\",\n    value_column: Optional[str] = None,\n    area_weighted: bool = False,\n    **kwargs,\n) -&gt; pd.DataFrame:\n    \"\"\"\n    Maps zonal statistics from raster or polygon data to POIs.\n\n    Can operate in three modes:\n    1. Raster point sampling: Directly samples raster values at POI locations\n    2. Raster zonal statistics: Creates buffers around POIs and calculates statistics within them\n    3. Polygon aggregation: Aggregates polygon data to POI buffers with optional area weighting\n\n    Args:\n        data (Union[List[TifProcessor], gpd.GeoDataFrame]):\n            Either a list of TifProcessor objects containing raster data to sample,\n            or a GeoDataFrame containing polygon data to aggregate.\n        stat (str, optional):\n            For raster data: Statistic to calculate (\"sum\", \"mean\", \"median\", \"min\", \"max\").\n            For polygon data: Aggregation method to use.\n            Defaults to \"mean\".\n        map_radius_meters (float, optional):\n            If provided, creates circular buffers of this radius around each POI\n            and calculates statistics within the buffers. If None, samples directly\n            at POI locations (only for raster data).\n        output_column (str, optional):\n            Name of the output column to store the results. Defaults to \"zonal_stat\".\n        value_column (str, optional):\n            For polygon data: Name of the column to aggregate. Required for polygon data.\n            Not used for raster data.\n        area_weighted (bool, optional):\n            For polygon data: Whether to weight values by fractional area of\n            intersection. Defaults to False.\n        **kwargs:\n            Additional keyword arguments passed to the sampling/aggregation functions.\n\n    Returns:\n        pd.DataFrame: The updated GeoDataFrame with a new column containing the\n                      calculated statistics. Returns a copy of the current `points_gdf`\n                      if no valid data is found.\n\n    Raises:\n        ValueError: If no valid data is provided, if parameters are incompatible,\n                  or if required parameters (value_column) are missing for polygon data.\n    \"\"\"\n    if isinstance(data, list) and all(isinstance(x, TifProcessor) for x in data):\n        # Handle raster data\n        if not data:\n            self.logger.info(\"No valid raster data found for the provided POIs\")\n            return self.points_gdf.copy()\n\n        if map_radius_meters is not None:\n            self.logger.info(\n                f\"Calculating {stat} within {map_radius_meters}m buffers around POIs\"\n            )\n            # Create buffers around POIs\n            polygon_list = buffer_geodataframe(\n                self.points_gdf,\n                buffer_distance_meters=map_radius_meters,\n                cap_style=\"round\",\n            ).geometry\n\n            # Calculate zonal statistics\n            sampled_values = sample_multiple_tifs_by_polygons(\n                tif_processors=data, polygon_list=polygon_list, stat=stat, **kwargs\n            )\n        else:\n            self.logger.info(f\"Sampling {stat} at POI locations\")\n            # Sample directly at POI locations\n            coord_list = self.points_gdf[[\"latitude\", \"longitude\"]].to_numpy()\n            sampled_values = sample_multiple_tifs_by_coordinates(\n                tif_processors=data, coordinate_list=coord_list, **kwargs\n            )\n\n    elif isinstance(data, gpd.GeoDataFrame):\n        # Handle polygon data\n        if data.empty:\n            self.logger.info(\"No valid polygon data found for the provided POIs\")\n            return self.points_gdf.copy()\n\n        if map_radius_meters is None:\n            raise ValueError(\"map_radius_meters must be provided for polygon data\")\n\n        if value_column is None:\n            raise ValueError(\"value_column must be provided for polygon data\")\n\n        self.logger.info(\n            f\"Aggregating {value_column} within {map_radius_meters}m buffers around POIs\"\n        )\n\n        # Create buffers around POIs\n        buffer_gdf = buffer_geodataframe(\n            self.points_gdf,\n            buffer_distance_meters=map_radius_meters,\n            cap_style=\"round\",\n        )\n\n        # Aggregate polygons to buffers\n        result = aggregate_polygons_to_zones(\n            polygons=data,\n            zones=buffer_gdf,\n            value_columns=value_column,\n            aggregation=stat,\n            area_weighted=area_weighted,\n            zone_id_column=\"poi_id\",\n            **kwargs,\n        )\n\n        # Extract values for each POI\n        sampled_values = result[value_column].values\n\n    else:\n        raise ValueError(\n            \"data must be either a list of TifProcessor objects or a GeoDataFrame\"\n        )\n\n    result = self.points_gdf.copy()\n    result[output_column] = sampled_values\n    self.logger.info(f\"Zonal statistics mapping complete: {output_column}\")\n    self._points_gdf = result\n    return result\n</code></pre>"},{"location":"api/generators/#gigaspatial.generators.poi.PoiViewGenerator.save_view","title":"<code>save_view(name, output_format=None)</code>","text":"<p>Saves the current POI view (the enriched GeoDataFrame) to a file.</p> <p>The output path and format are determined by the <code>generator_config</code> or overridden by the <code>output_format</code> parameter.</p> <p>Parameters:</p> Name Type Description Default <code>name</code> <code>str</code> <p>The base name for the output file (without extension).</p> required <code>output_format</code> <code>Optional[str]</code> <p>The desired output format (e.g., \"csv\", \"geojson\"). If None, the <code>output_format</code> from <code>generator_config</code> will be used.</p> <code>None</code> <p>Returns:</p> Name Type Description <code>Path</code> <code>Path</code> <p>The full path to the saved output file.</p> Source code in <code>gigaspatial/generators/poi.py</code> <pre><code>def save_view(\n    self,\n    name: str,\n    output_format: Optional[str] = None,\n) -&gt; Path:\n    \"\"\"\n    Saves the current POI view (the enriched GeoDataFrame) to a file.\n\n    The output path and format are determined by the `generator_config`\n    or overridden by the `output_format` parameter.\n\n    Args:\n        name (str): The base name for the output file (without extension).\n        output_format (Optional[str]):\n            The desired output format (e.g., \"csv\", \"geojson\"). If None,\n            the `output_format` from `generator_config` will be used.\n\n    Returns:\n        Path: The full path to the saved output file.\n    \"\"\"\n    format_to_use = output_format or self.generator_config.output_format\n    output_path = self.generator_config.base_path / f\"{name}.{format_to_use}\"\n\n    self.logger.info(f\"Saving POI view to {output_path}\")\n    write_dataset(\n        df=self.points_gdf,\n        path=str(output_path),\n        data_store=self.data_store,\n        format=format_to_use,\n    )\n\n    return output_path\n</code></pre>"},{"location":"api/generators/#gigaspatial.generators.poi.PoiViewGeneratorConfig","title":"<code>PoiViewGeneratorConfig</code>","text":"<p>Configuration for POI (Point of Interest) view generation.</p> <p>Attributes:</p> Name Type Description <code>base_path</code> <code>Path</code> <p>The base directory where generated POI views will be saved.               Defaults to a path retrieved from <code>config</code>.</p> <code>output_format</code> <code>str</code> <p>The default format for saving output files (e.g., \"csv\", \"geojson\").                  Defaults to \"csv\".</p> Source code in <code>gigaspatial/generators/poi.py</code> <pre><code>@dataclass\nclass PoiViewGeneratorConfig:\n    \"\"\"\n    Configuration for POI (Point of Interest) view generation.\n\n    Attributes:\n        base_path (Path): The base directory where generated POI views will be saved.\n                          Defaults to a path retrieved from `config`.\n        output_format (str): The default format for saving output files (e.g., \"csv\", \"geojson\").\n                             Defaults to \"csv\".\n    \"\"\"\n\n    base_path: Path = Field(default=global_config.get_path(\"poi\", \"views\"))\n    output_format: str = \"csv\"\n    ensure_available: bool = True\n</code></pre>"},{"location":"api/generators/#gigaspatial.generators.zonal","title":"<code>zonal</code>","text":""},{"location":"api/generators/#gigaspatial.generators.zonal.base","title":"<code>base</code>","text":""},{"location":"api/generators/#gigaspatial.generators.zonal.base.ZonalViewGenerator","title":"<code>ZonalViewGenerator</code>","text":"<p>               Bases: <code>ABC</code>, <code>Generic[T]</code></p> <p>Base class for mapping data to zonal datasets.</p> <p>This class provides the framework for mapping various data sources (points, polygons, rasters) to zonal geometries like grid tiles or catchment areas. It serves as an abstract base class that must be subclassed to implement specific zonal systems.</p> <p>The class supports three main types of data mapping: - Point data aggregation to zones - Polygon data aggregation with optional area weighting - Raster data sampling and statistics</p> <p>Attributes:</p> Name Type Description <code>data_store</code> <code>DataStore</code> <p>The data store for accessing input data.</p> <code>generator_config</code> <code>ZonalViewGeneratorConfig</code> <p>Configuration for the generator.</p> <code>logger</code> <p>Logger instance for this class.</p> Source code in <code>gigaspatial/generators/zonal/base.py</code> <pre><code>class ZonalViewGenerator(ABC, Generic[T]):\n    \"\"\"Base class for mapping data to zonal datasets.\n\n    This class provides the framework for mapping various data sources (points, polygons, rasters)\n    to zonal geometries like grid tiles or catchment areas. It serves as an abstract base class\n    that must be subclassed to implement specific zonal systems.\n\n    The class supports three main types of data mapping:\n    - Point data aggregation to zones\n    - Polygon data aggregation with optional area weighting\n    - Raster data sampling and statistics\n\n    Attributes:\n        data_store (DataStore): The data store for accessing input data.\n        generator_config (ZonalViewGeneratorConfig): Configuration for the generator.\n        logger: Logger instance for this class.\n    \"\"\"\n\n    def __init__(\n        self,\n        config: Optional[ZonalViewGeneratorConfig] = None,\n        data_store: Optional[DataStore] = None,\n        logger: logging.Logger = None,\n    ):\n        \"\"\"Initialize the ZonalViewGenerator.\n\n        Args:\n            generator_config (ZonalViewGeneratorConfig, optional): Configuration for the generator.\n                If None, uses default configuration.\n            data_store (DataStore, optional): The data store for accessing input data.\n                If None, uses LocalDataStore.\n        \"\"\"\n        self.config = config or ZonalViewGeneratorConfig()\n        self.data_store = data_store or LocalDataStore()\n        self.logger = logger or global_config.get_logger(self.__class__.__name__)\n\n    @abstractmethod\n    def get_zonal_geometries(self) -&gt; List[Polygon]:\n        \"\"\"Get the geometries of the zones.\n\n        This method must be implemented by subclasses to return the actual geometric\n        shapes of the zones (e.g., grid tiles, catchment boundaries, administrative areas).\n\n        Returns:\n            List[Polygon]: A list of Shapely Polygon objects representing zone geometries.\n        \"\"\"\n        pass\n\n    @abstractmethod\n    def get_zone_identifiers(self) -&gt; List[T]:\n        \"\"\"Get unique identifiers for each zone.\n\n        This method must be implemented by subclasses to return identifiers that\n        correspond one-to-one with the geometries returned by get_zonal_geometries().\n\n        Returns:\n            List[T]: A list of zone identifiers (e.g., quadkeys, H3 indices, tile IDs).\n                The type T is determined by the specific zonal system implementation.\n        \"\"\"\n        pass\n\n    def to_geodataframe(self) -&gt; gpd.GeoDataFrame:\n        \"\"\"Convert zones to a GeoDataFrame.\n\n        Creates a GeoDataFrame containing zone identifiers and their corresponding\n        geometries in WGS84 (EPSG:4326) coordinate reference system.\n\n        Returns:\n            gpd.GeoDataFrame: A GeoDataFrame with 'zone_id' and 'geometry' columns,\n                where zone_id contains the identifiers and geometry contains the\n                corresponding Polygon objects.\n        \"\"\"\n        return gpd.GeoDataFrame(\n            {\n                \"zone_id\": self.get_zone_identifiers(),\n                \"geometry\": self.get_zonal_geometries(),\n            },\n            crs=\"EPSG:4326\",\n        )\n\n    @property\n    def zone_gdf(self) -&gt; gpd.GeoDataFrame:\n        \"\"\"Cached GeoDataFrame of zones.\n\n        Returns:\n            gpd.GeoDataFrame: Lazily-computed and cached GeoDataFrame of zone geometries\n                and identifiers.\n        \"\"\"\n        if not hasattr(self, \"_zone_gdf\"):\n            self._zone_gdf = self.to_geodataframe()\n        return self._zone_gdf\n\n    def map_points(\n        self,\n        points: Union[pd.DataFrame, gpd.GeoDataFrame],\n        value_columns: Optional[Union[str, List[str]]] = None,\n        aggregation: Union[str, Dict[str, str]] = \"count\",\n        predicate: str = \"within\",\n        output_suffix: str = \"\",\n        mapping_function: Optional[Callable] = None,\n        **mapping_kwargs,\n    ) -&gt; Dict:\n        \"\"\"Map point data to zones with spatial aggregation.\n\n        Aggregates point data to zones using spatial relationships. Points can be\n        counted or have their attribute values aggregated using various statistical methods.\n\n        Args:\n            points (Union[pd.DataFrame, gpd.GeoDataFrame]): The point data to map.\n                Must contain geometry information if DataFrame.\n            value_columns (Union[str, List[str]], optional): Column name(s) containing\n                values to aggregate. If None, only point counts are performed.\n            aggregation (Union[str, Dict[str, str]]): Aggregation method(s) to use.\n                Can be a single string (\"count\", \"mean\", \"sum\", \"min\", \"max\", etc.)\n                or a dictionary mapping column names to aggregation methods.\n            predicate (str): Spatial predicate for point-to-zone relationship.\n                Options include \"within\", \"intersects\", \"contains\". Defaults to \"within\".\n            output_suffix (str): Suffix to add to output column names. Defaults to empty string.\n            mapping_function (Callable, optional): Custom function for mapping points to zones.\n                If provided, signature should be mapping_function(self, points, **mapping_kwargs).\n                When used, all other parameters except mapping_kwargs are ignored.\n            **mapping_kwargs: Additional keyword arguments passed to the mapping function.\n\n        Returns:\n            Dict: Dictionary with zone IDs as keys and aggregated values as values.\n                If value_columns is None, returns point counts per zone.\n                If value_columns is specified, returns aggregated values per zone.\n        \"\"\"\n        if mapping_function is not None:\n            return mapping_function(self, points, **mapping_kwargs)\n\n        else:\n            self.logger.warning(\n                \"Using default points mapping implementation. Consider creating a specialized mapping function.\"\n            )\n            result = aggregate_points_to_zones(\n                points=points,\n                zones=self.zone_gdf,\n                value_columns=value_columns,\n                aggregation=aggregation,\n                point_zone_predicate=predicate,\n                zone_id_column=\"zone_id\",\n                output_suffix=output_suffix,\n            )\n\n            if not value_columns:\n                return result[\"point_count\"].to_dict()\n\n            return result[value_columns].to_dict()\n\n    def map_polygons(\n        self,\n        polygons: Union[pd.DataFrame, gpd.GeoDataFrame],\n        value_columns: Optional[Union[str, List[str]]] = None,\n        aggregation: Union[str, Dict[str, str]] = \"sum\",\n        area_weighted: bool = False,\n        area_column: str = \"area_in_meters\",\n        mapping_function: Optional[Callable] = None,\n        **mapping_kwargs,\n    ) -&gt; Dict:\n        \"\"\"Map polygon data to zones with optional area weighting.\n\n        Aggregates polygon data to zones based on spatial intersections. Values can be\n        weighted by the fractional area of intersection between polygons and zones.\n\n        Args:\n            polygons (Union[pd.DataFrame, gpd.GeoDataFrame]): The polygon data to map.\n                Must contain geometry information if DataFrame.\n            value_columns (Union[str, List[str]], optional): Column name(s) to aggregate.\n                If None, only intersection areas will be calculated.\n            aggregation (Union[str, Dict[str, str]]): Aggregation method(s) to use.\n                Can be a single string (\"sum\", \"mean\", \"max\", \"min\") or a dictionary\n                mapping column names to specific aggregation methods. Defaults to \"sum\".\n            area_weighted (bool): Whether to weight values by fractional area of\n                intersection. Defaults to False.\n            area_column (str): Name of column to store calculated areas. Only used\n                if area calculation is needed. Defaults to \"area_in_meters\".\n            mapping_function (Callable, optional): Custom function for mapping polygons\n                to zones. If provided, signature should be mapping_function(self, polygons, **mapping_kwargs).\n                When used, all other parameters except mapping_kwargs are ignored.\n            **mapping_kwargs: Additional keyword arguments passed to the mapping function.\n\n        Returns:\n            Dict: Dictionary with zone IDs as keys and aggregated values as values.\n                Returns aggregated values for the specified value_columns.\n\n        Raises:\n            TypeError: If polygons cannot be converted to a GeoDataFrame.\n        \"\"\"\n        if mapping_function is not None:\n            return mapping_function(self, polygons, **mapping_kwargs)\n\n        if area_column not in polygons_gdf:\n            if not isinstance(polygons, gpd.GeoDataFrame):\n                try:\n                    polygons_gdf = convert_to_geodataframe(polygons)\n                except:\n                    raise TypeError(\n                        \"polygons must be a GeoDataFrame or convertible to one\"\n                    )\n            else:\n                polygons_gdf = polygons.copy()\n\n            polygons_gdf[area_column] = polygons_gdf.to_crs(\n                polygons_gdf.estimate_utm_crs()\n            ).geometry.area\n\n        if value_columns is None:\n            self.logger.warning(\n                \"Using default polygon mapping implementation. Consider providing value_columns.\"\n            )\n            value_columns = area_column\n\n        result = aggregate_polygons_to_zones(\n            polygons=polygons_gdf,\n            zones=self.zone_gdf,\n            value_columns=value_columns,\n            aggregation=aggregation,\n            area_weighted=area_weighted,\n            zone_id_column=\"zone_id\",\n        )\n\n        return result[value_columns].to_dict()\n\n    def map_rasters(\n        self,\n        tif_processors: List[TifProcessor],\n        mapping_function: Optional[Callable] = None,\n        stat: str = \"mean\",\n        **mapping_kwargs,\n    ) -&gt; Union[np.ndarray, Dict]:\n        \"\"\"Map raster data to zones using zonal statistics.\n\n        Samples raster values within each zone and computes statistics. Automatically\n        handles coordinate reference system transformations between raster and zone data.\n\n        Args:\n            tif_processors (List[TifProcessor]): List of TifProcessor objects for\n                accessing raster data. All processors should have the same CRS.\n            mapping_function (Callable, optional): Custom function for mapping rasters\n                to zones. If provided, signature should be mapping_function(self, tif_processors, **mapping_kwargs).\n                When used, stat and other parameters except mapping_kwargs are ignored.\n            stat (str): Statistic to calculate when aggregating raster values within\n                each zone. Options include \"mean\", \"sum\", \"min\", \"max\", \"std\", etc.\n                Defaults to \"mean\".\n            **mapping_kwargs: Additional keyword arguments passed to the mapping function.\n\n        Returns:\n            Union[np.ndarray, Dict]: By default, returns a NumPy array of sampled values\n                with shape (n_zones, n_rasters), taking the first non-nodata value encountered.\n                Custom mapping functions may return different data structures.\n\n        Note:\n            If the coordinate reference system of the rasters differs from the zones,\n            the zone geometries will be automatically transformed to match the raster CRS.\n        \"\"\"\n        if mapping_function is not None:\n            return mapping_function(self, tif_processors, **mapping_kwargs)\n\n        self.logger.warning(\n            \"Using default raster mapping implementation. Consider creating a specialized mapping function.\"\n        )\n\n        raster_crs = tif_processors[0].crs\n\n        if raster_crs != self.zone_gdf.crs:\n            self.logger.info(f\"Projecting zones to raster CRS: {raster_crs}\")\n            zone_geoms = self._get_transformed_geometries(raster_crs)\n        else:\n            zone_geoms = self.get_zonal_geometries()\n\n        # Sample raster values\n        sampled_values = sample_multiple_tifs_by_polygons(\n            tif_processors=tif_processors, polygon_list=zone_geoms, stat=stat\n        )\n\n        return sampled_values\n\n    @lru_cache(maxsize=32)\n    def _get_transformed_geometries(self, target_crs):\n        \"\"\"Get zone geometries transformed to target coordinate reference system.\n\n        This method is cached to avoid repeated coordinate transformations for\n        the same target CRS.\n\n        Args:\n            target_crs: Target coordinate reference system for transformation.\n\n        Returns:\n            List[Polygon]: List of zone geometries transformed to the target CRS.\n        \"\"\"\n        return self.zone_gdf.to_crs(target_crs).geometry.tolist()\n\n    def save_view(\n        self,\n        view_data: gpd.GeoDataFrame,\n        name: str,\n        output_format: Optional[str] = None,\n    ) -&gt; Path:\n        \"\"\"Save the generated zonal view to disk.\n\n        Args:\n            view_data (gpd.GeoDataFrame): The zonal view data to save.\n            name (str): Base name for the output file (without extension).\n            output_format (str, optional): File format to save in (e.g., \"parquet\",\n                \"geojson\", \"shp\"). If None, uses the format specified in generator_config.\n\n        Returns:\n            Path: The full path where the view was saved.\n\n        Note:\n            The output directory is determined by the generator_config.base_path setting.\n            The file extension is automatically added based on the output format.\n        \"\"\"\n        format_to_use = output_format or self.config.output_format\n        output_path = self.config.base_path / f\"{name}.{format_to_use}\"\n\n        self.logger.info(f\"Saving zonal view to {output_path}\")\n        write_dataset(\n            df=view_data,\n            path=str(output_path),\n            data_store=self.data_store,\n            format=format_to_use,\n        )\n\n        return output_path\n</code></pre>"},{"location":"api/generators/#gigaspatial.generators.zonal.base.ZonalViewGenerator.zone_gdf","title":"<code>zone_gdf: gpd.GeoDataFrame</code>  <code>property</code>","text":"<p>Cached GeoDataFrame of zones.</p> <p>Returns:</p> Type Description <code>GeoDataFrame</code> <p>gpd.GeoDataFrame: Lazily-computed and cached GeoDataFrame of zone geometries and identifiers.</p>"},{"location":"api/generators/#gigaspatial.generators.zonal.base.ZonalViewGenerator.__init__","title":"<code>__init__(config=None, data_store=None, logger=None)</code>","text":"<p>Initialize the ZonalViewGenerator.</p> <p>Parameters:</p> Name Type Description Default <code>generator_config</code> <code>ZonalViewGeneratorConfig</code> <p>Configuration for the generator. If None, uses default configuration.</p> required <code>data_store</code> <code>DataStore</code> <p>The data store for accessing input data. If None, uses LocalDataStore.</p> <code>None</code> Source code in <code>gigaspatial/generators/zonal/base.py</code> <pre><code>def __init__(\n    self,\n    config: Optional[ZonalViewGeneratorConfig] = None,\n    data_store: Optional[DataStore] = None,\n    logger: logging.Logger = None,\n):\n    \"\"\"Initialize the ZonalViewGenerator.\n\n    Args:\n        generator_config (ZonalViewGeneratorConfig, optional): Configuration for the generator.\n            If None, uses default configuration.\n        data_store (DataStore, optional): The data store for accessing input data.\n            If None, uses LocalDataStore.\n    \"\"\"\n    self.config = config or ZonalViewGeneratorConfig()\n    self.data_store = data_store or LocalDataStore()\n    self.logger = logger or global_config.get_logger(self.__class__.__name__)\n</code></pre>"},{"location":"api/generators/#gigaspatial.generators.zonal.base.ZonalViewGenerator.get_zonal_geometries","title":"<code>get_zonal_geometries()</code>  <code>abstractmethod</code>","text":"<p>Get the geometries of the zones.</p> <p>This method must be implemented by subclasses to return the actual geometric shapes of the zones (e.g., grid tiles, catchment boundaries, administrative areas).</p> <p>Returns:</p> Type Description <code>List[Polygon]</code> <p>List[Polygon]: A list of Shapely Polygon objects representing zone geometries.</p> Source code in <code>gigaspatial/generators/zonal/base.py</code> <pre><code>@abstractmethod\ndef get_zonal_geometries(self) -&gt; List[Polygon]:\n    \"\"\"Get the geometries of the zones.\n\n    This method must be implemented by subclasses to return the actual geometric\n    shapes of the zones (e.g., grid tiles, catchment boundaries, administrative areas).\n\n    Returns:\n        List[Polygon]: A list of Shapely Polygon objects representing zone geometries.\n    \"\"\"\n    pass\n</code></pre>"},{"location":"api/generators/#gigaspatial.generators.zonal.base.ZonalViewGenerator.get_zone_identifiers","title":"<code>get_zone_identifiers()</code>  <code>abstractmethod</code>","text":"<p>Get unique identifiers for each zone.</p> <p>This method must be implemented by subclasses to return identifiers that correspond one-to-one with the geometries returned by get_zonal_geometries().</p> <p>Returns:</p> Type Description <code>List[T]</code> <p>List[T]: A list of zone identifiers (e.g., quadkeys, H3 indices, tile IDs). The type T is determined by the specific zonal system implementation.</p> Source code in <code>gigaspatial/generators/zonal/base.py</code> <pre><code>@abstractmethod\ndef get_zone_identifiers(self) -&gt; List[T]:\n    \"\"\"Get unique identifiers for each zone.\n\n    This method must be implemented by subclasses to return identifiers that\n    correspond one-to-one with the geometries returned by get_zonal_geometries().\n\n    Returns:\n        List[T]: A list of zone identifiers (e.g., quadkeys, H3 indices, tile IDs).\n            The type T is determined by the specific zonal system implementation.\n    \"\"\"\n    pass\n</code></pre>"},{"location":"api/generators/#gigaspatial.generators.zonal.base.ZonalViewGenerator.map_points","title":"<code>map_points(points, value_columns=None, aggregation='count', predicate='within', output_suffix='', mapping_function=None, **mapping_kwargs)</code>","text":"<p>Map point data to zones with spatial aggregation.</p> <p>Aggregates point data to zones using spatial relationships. Points can be counted or have their attribute values aggregated using various statistical methods.</p> <p>Parameters:</p> Name Type Description Default <code>points</code> <code>Union[DataFrame, GeoDataFrame]</code> <p>The point data to map. Must contain geometry information if DataFrame.</p> required <code>value_columns</code> <code>Union[str, List[str]]</code> <p>Column name(s) containing values to aggregate. If None, only point counts are performed.</p> <code>None</code> <code>aggregation</code> <code>Union[str, Dict[str, str]]</code> <p>Aggregation method(s) to use. Can be a single string (\"count\", \"mean\", \"sum\", \"min\", \"max\", etc.) or a dictionary mapping column names to aggregation methods.</p> <code>'count'</code> <code>predicate</code> <code>str</code> <p>Spatial predicate for point-to-zone relationship. Options include \"within\", \"intersects\", \"contains\". Defaults to \"within\".</p> <code>'within'</code> <code>output_suffix</code> <code>str</code> <p>Suffix to add to output column names. Defaults to empty string.</p> <code>''</code> <code>mapping_function</code> <code>Callable</code> <p>Custom function for mapping points to zones. If provided, signature should be mapping_function(self, points, **mapping_kwargs). When used, all other parameters except mapping_kwargs are ignored.</p> <code>None</code> <code>**mapping_kwargs</code> <p>Additional keyword arguments passed to the mapping function.</p> <code>{}</code> <p>Returns:</p> Name Type Description <code>Dict</code> <code>Dict</code> <p>Dictionary with zone IDs as keys and aggregated values as values. If value_columns is None, returns point counts per zone. If value_columns is specified, returns aggregated values per zone.</p> Source code in <code>gigaspatial/generators/zonal/base.py</code> <pre><code>def map_points(\n    self,\n    points: Union[pd.DataFrame, gpd.GeoDataFrame],\n    value_columns: Optional[Union[str, List[str]]] = None,\n    aggregation: Union[str, Dict[str, str]] = \"count\",\n    predicate: str = \"within\",\n    output_suffix: str = \"\",\n    mapping_function: Optional[Callable] = None,\n    **mapping_kwargs,\n) -&gt; Dict:\n    \"\"\"Map point data to zones with spatial aggregation.\n\n    Aggregates point data to zones using spatial relationships. Points can be\n    counted or have their attribute values aggregated using various statistical methods.\n\n    Args:\n        points (Union[pd.DataFrame, gpd.GeoDataFrame]): The point data to map.\n            Must contain geometry information if DataFrame.\n        value_columns (Union[str, List[str]], optional): Column name(s) containing\n            values to aggregate. If None, only point counts are performed.\n        aggregation (Union[str, Dict[str, str]]): Aggregation method(s) to use.\n            Can be a single string (\"count\", \"mean\", \"sum\", \"min\", \"max\", etc.)\n            or a dictionary mapping column names to aggregation methods.\n        predicate (str): Spatial predicate for point-to-zone relationship.\n            Options include \"within\", \"intersects\", \"contains\". Defaults to \"within\".\n        output_suffix (str): Suffix to add to output column names. Defaults to empty string.\n        mapping_function (Callable, optional): Custom function for mapping points to zones.\n            If provided, signature should be mapping_function(self, points, **mapping_kwargs).\n            When used, all other parameters except mapping_kwargs are ignored.\n        **mapping_kwargs: Additional keyword arguments passed to the mapping function.\n\n    Returns:\n        Dict: Dictionary with zone IDs as keys and aggregated values as values.\n            If value_columns is None, returns point counts per zone.\n            If value_columns is specified, returns aggregated values per zone.\n    \"\"\"\n    if mapping_function is not None:\n        return mapping_function(self, points, **mapping_kwargs)\n\n    else:\n        self.logger.warning(\n            \"Using default points mapping implementation. Consider creating a specialized mapping function.\"\n        )\n        result = aggregate_points_to_zones(\n            points=points,\n            zones=self.zone_gdf,\n            value_columns=value_columns,\n            aggregation=aggregation,\n            point_zone_predicate=predicate,\n            zone_id_column=\"zone_id\",\n            output_suffix=output_suffix,\n        )\n\n        if not value_columns:\n            return result[\"point_count\"].to_dict()\n\n        return result[value_columns].to_dict()\n</code></pre>"},{"location":"api/generators/#gigaspatial.generators.zonal.base.ZonalViewGenerator.map_polygons","title":"<code>map_polygons(polygons, value_columns=None, aggregation='sum', area_weighted=False, area_column='area_in_meters', mapping_function=None, **mapping_kwargs)</code>","text":"<p>Map polygon data to zones with optional area weighting.</p> <p>Aggregates polygon data to zones based on spatial intersections. Values can be weighted by the fractional area of intersection between polygons and zones.</p> <p>Parameters:</p> Name Type Description Default <code>polygons</code> <code>Union[DataFrame, GeoDataFrame]</code> <p>The polygon data to map. Must contain geometry information if DataFrame.</p> required <code>value_columns</code> <code>Union[str, List[str]]</code> <p>Column name(s) to aggregate. If None, only intersection areas will be calculated.</p> <code>None</code> <code>aggregation</code> <code>Union[str, Dict[str, str]]</code> <p>Aggregation method(s) to use. Can be a single string (\"sum\", \"mean\", \"max\", \"min\") or a dictionary mapping column names to specific aggregation methods. Defaults to \"sum\".</p> <code>'sum'</code> <code>area_weighted</code> <code>bool</code> <p>Whether to weight values by fractional area of intersection. Defaults to False.</p> <code>False</code> <code>area_column</code> <code>str</code> <p>Name of column to store calculated areas. Only used if area calculation is needed. Defaults to \"area_in_meters\".</p> <code>'area_in_meters'</code> <code>mapping_function</code> <code>Callable</code> <p>Custom function for mapping polygons to zones. If provided, signature should be mapping_function(self, polygons, **mapping_kwargs). When used, all other parameters except mapping_kwargs are ignored.</p> <code>None</code> <code>**mapping_kwargs</code> <p>Additional keyword arguments passed to the mapping function.</p> <code>{}</code> <p>Returns:</p> Name Type Description <code>Dict</code> <code>Dict</code> <p>Dictionary with zone IDs as keys and aggregated values as values. Returns aggregated values for the specified value_columns.</p> <p>Raises:</p> Type Description <code>TypeError</code> <p>If polygons cannot be converted to a GeoDataFrame.</p> Source code in <code>gigaspatial/generators/zonal/base.py</code> <pre><code>def map_polygons(\n    self,\n    polygons: Union[pd.DataFrame, gpd.GeoDataFrame],\n    value_columns: Optional[Union[str, List[str]]] = None,\n    aggregation: Union[str, Dict[str, str]] = \"sum\",\n    area_weighted: bool = False,\n    area_column: str = \"area_in_meters\",\n    mapping_function: Optional[Callable] = None,\n    **mapping_kwargs,\n) -&gt; Dict:\n    \"\"\"Map polygon data to zones with optional area weighting.\n\n    Aggregates polygon data to zones based on spatial intersections. Values can be\n    weighted by the fractional area of intersection between polygons and zones.\n\n    Args:\n        polygons (Union[pd.DataFrame, gpd.GeoDataFrame]): The polygon data to map.\n            Must contain geometry information if DataFrame.\n        value_columns (Union[str, List[str]], optional): Column name(s) to aggregate.\n            If None, only intersection areas will be calculated.\n        aggregation (Union[str, Dict[str, str]]): Aggregation method(s) to use.\n            Can be a single string (\"sum\", \"mean\", \"max\", \"min\") or a dictionary\n            mapping column names to specific aggregation methods. Defaults to \"sum\".\n        area_weighted (bool): Whether to weight values by fractional area of\n            intersection. Defaults to False.\n        area_column (str): Name of column to store calculated areas. Only used\n            if area calculation is needed. Defaults to \"area_in_meters\".\n        mapping_function (Callable, optional): Custom function for mapping polygons\n            to zones. If provided, signature should be mapping_function(self, polygons, **mapping_kwargs).\n            When used, all other parameters except mapping_kwargs are ignored.\n        **mapping_kwargs: Additional keyword arguments passed to the mapping function.\n\n    Returns:\n        Dict: Dictionary with zone IDs as keys and aggregated values as values.\n            Returns aggregated values for the specified value_columns.\n\n    Raises:\n        TypeError: If polygons cannot be converted to a GeoDataFrame.\n    \"\"\"\n    if mapping_function is not None:\n        return mapping_function(self, polygons, **mapping_kwargs)\n\n    if area_column not in polygons_gdf:\n        if not isinstance(polygons, gpd.GeoDataFrame):\n            try:\n                polygons_gdf = convert_to_geodataframe(polygons)\n            except:\n                raise TypeError(\n                    \"polygons must be a GeoDataFrame or convertible to one\"\n                )\n        else:\n            polygons_gdf = polygons.copy()\n\n        polygons_gdf[area_column] = polygons_gdf.to_crs(\n            polygons_gdf.estimate_utm_crs()\n        ).geometry.area\n\n    if value_columns is None:\n        self.logger.warning(\n            \"Using default polygon mapping implementation. Consider providing value_columns.\"\n        )\n        value_columns = area_column\n\n    result = aggregate_polygons_to_zones(\n        polygons=polygons_gdf,\n        zones=self.zone_gdf,\n        value_columns=value_columns,\n        aggregation=aggregation,\n        area_weighted=area_weighted,\n        zone_id_column=\"zone_id\",\n    )\n\n    return result[value_columns].to_dict()\n</code></pre>"},{"location":"api/generators/#gigaspatial.generators.zonal.base.ZonalViewGenerator.map_rasters","title":"<code>map_rasters(tif_processors, mapping_function=None, stat='mean', **mapping_kwargs)</code>","text":"<p>Map raster data to zones using zonal statistics.</p> <p>Samples raster values within each zone and computes statistics. Automatically handles coordinate reference system transformations between raster and zone data.</p> <p>Parameters:</p> Name Type Description Default <code>tif_processors</code> <code>List[TifProcessor]</code> <p>List of TifProcessor objects for accessing raster data. All processors should have the same CRS.</p> required <code>mapping_function</code> <code>Callable</code> <p>Custom function for mapping rasters to zones. If provided, signature should be mapping_function(self, tif_processors, **mapping_kwargs). When used, stat and other parameters except mapping_kwargs are ignored.</p> <code>None</code> <code>stat</code> <code>str</code> <p>Statistic to calculate when aggregating raster values within each zone. Options include \"mean\", \"sum\", \"min\", \"max\", \"std\", etc. Defaults to \"mean\".</p> <code>'mean'</code> <code>**mapping_kwargs</code> <p>Additional keyword arguments passed to the mapping function.</p> <code>{}</code> <p>Returns:</p> Type Description <code>Union[ndarray, Dict]</code> <p>Union[np.ndarray, Dict]: By default, returns a NumPy array of sampled values with shape (n_zones, n_rasters), taking the first non-nodata value encountered. Custom mapping functions may return different data structures.</p> Note <p>If the coordinate reference system of the rasters differs from the zones, the zone geometries will be automatically transformed to match the raster CRS.</p> Source code in <code>gigaspatial/generators/zonal/base.py</code> <pre><code>def map_rasters(\n    self,\n    tif_processors: List[TifProcessor],\n    mapping_function: Optional[Callable] = None,\n    stat: str = \"mean\",\n    **mapping_kwargs,\n) -&gt; Union[np.ndarray, Dict]:\n    \"\"\"Map raster data to zones using zonal statistics.\n\n    Samples raster values within each zone and computes statistics. Automatically\n    handles coordinate reference system transformations between raster and zone data.\n\n    Args:\n        tif_processors (List[TifProcessor]): List of TifProcessor objects for\n            accessing raster data. All processors should have the same CRS.\n        mapping_function (Callable, optional): Custom function for mapping rasters\n            to zones. If provided, signature should be mapping_function(self, tif_processors, **mapping_kwargs).\n            When used, stat and other parameters except mapping_kwargs are ignored.\n        stat (str): Statistic to calculate when aggregating raster values within\n            each zone. Options include \"mean\", \"sum\", \"min\", \"max\", \"std\", etc.\n            Defaults to \"mean\".\n        **mapping_kwargs: Additional keyword arguments passed to the mapping function.\n\n    Returns:\n        Union[np.ndarray, Dict]: By default, returns a NumPy array of sampled values\n            with shape (n_zones, n_rasters), taking the first non-nodata value encountered.\n            Custom mapping functions may return different data structures.\n\n    Note:\n        If the coordinate reference system of the rasters differs from the zones,\n        the zone geometries will be automatically transformed to match the raster CRS.\n    \"\"\"\n    if mapping_function is not None:\n        return mapping_function(self, tif_processors, **mapping_kwargs)\n\n    self.logger.warning(\n        \"Using default raster mapping implementation. Consider creating a specialized mapping function.\"\n    )\n\n    raster_crs = tif_processors[0].crs\n\n    if raster_crs != self.zone_gdf.crs:\n        self.logger.info(f\"Projecting zones to raster CRS: {raster_crs}\")\n        zone_geoms = self._get_transformed_geometries(raster_crs)\n    else:\n        zone_geoms = self.get_zonal_geometries()\n\n    # Sample raster values\n    sampled_values = sample_multiple_tifs_by_polygons(\n        tif_processors=tif_processors, polygon_list=zone_geoms, stat=stat\n    )\n\n    return sampled_values\n</code></pre>"},{"location":"api/generators/#gigaspatial.generators.zonal.base.ZonalViewGenerator.save_view","title":"<code>save_view(view_data, name, output_format=None)</code>","text":"<p>Save the generated zonal view to disk.</p> <p>Parameters:</p> Name Type Description Default <code>view_data</code> <code>GeoDataFrame</code> <p>The zonal view data to save.</p> required <code>name</code> <code>str</code> <p>Base name for the output file (without extension).</p> required <code>output_format</code> <code>str</code> <p>File format to save in (e.g., \"parquet\", \"geojson\", \"shp\"). If None, uses the format specified in generator_config.</p> <code>None</code> <p>Returns:</p> Name Type Description <code>Path</code> <code>Path</code> <p>The full path where the view was saved.</p> Note <p>The output directory is determined by the generator_config.base_path setting. The file extension is automatically added based on the output format.</p> Source code in <code>gigaspatial/generators/zonal/base.py</code> <pre><code>def save_view(\n    self,\n    view_data: gpd.GeoDataFrame,\n    name: str,\n    output_format: Optional[str] = None,\n) -&gt; Path:\n    \"\"\"Save the generated zonal view to disk.\n\n    Args:\n        view_data (gpd.GeoDataFrame): The zonal view data to save.\n        name (str): Base name for the output file (without extension).\n        output_format (str, optional): File format to save in (e.g., \"parquet\",\n            \"geojson\", \"shp\"). If None, uses the format specified in generator_config.\n\n    Returns:\n        Path: The full path where the view was saved.\n\n    Note:\n        The output directory is determined by the generator_config.base_path setting.\n        The file extension is automatically added based on the output format.\n    \"\"\"\n    format_to_use = output_format or self.config.output_format\n    output_path = self.config.base_path / f\"{name}.{format_to_use}\"\n\n    self.logger.info(f\"Saving zonal view to {output_path}\")\n    write_dataset(\n        df=view_data,\n        path=str(output_path),\n        data_store=self.data_store,\n        format=format_to_use,\n    )\n\n    return output_path\n</code></pre>"},{"location":"api/generators/#gigaspatial.generators.zonal.base.ZonalViewGenerator.to_geodataframe","title":"<code>to_geodataframe()</code>","text":"<p>Convert zones to a GeoDataFrame.</p> <p>Creates a GeoDataFrame containing zone identifiers and their corresponding geometries in WGS84 (EPSG:4326) coordinate reference system.</p> <p>Returns:</p> Type Description <code>GeoDataFrame</code> <p>gpd.GeoDataFrame: A GeoDataFrame with 'zone_id' and 'geometry' columns, where zone_id contains the identifiers and geometry contains the corresponding Polygon objects.</p> Source code in <code>gigaspatial/generators/zonal/base.py</code> <pre><code>def to_geodataframe(self) -&gt; gpd.GeoDataFrame:\n    \"\"\"Convert zones to a GeoDataFrame.\n\n    Creates a GeoDataFrame containing zone identifiers and their corresponding\n    geometries in WGS84 (EPSG:4326) coordinate reference system.\n\n    Returns:\n        gpd.GeoDataFrame: A GeoDataFrame with 'zone_id' and 'geometry' columns,\n            where zone_id contains the identifiers and geometry contains the\n            corresponding Polygon objects.\n    \"\"\"\n    return gpd.GeoDataFrame(\n        {\n            \"zone_id\": self.get_zone_identifiers(),\n            \"geometry\": self.get_zonal_geometries(),\n        },\n        crs=\"EPSG:4326\",\n    )\n</code></pre>"},{"location":"api/generators/#gigaspatial.generators.zonal.base.ZonalViewGeneratorConfig","title":"<code>ZonalViewGeneratorConfig</code>","text":"<p>               Bases: <code>BaseModel</code></p> <p>Configuration for zonal view generation.</p> <p>Attributes:</p> Name Type Description <code>base_path</code> <code>Path</code> <p>Base directory path for storing zonal views. Defaults to configured zonal views path.</p> <code>output_format</code> <code>str</code> <p>Default output format for saved views. Defaults to \"parquet\".</p> Source code in <code>gigaspatial/generators/zonal/base.py</code> <pre><code>class ZonalViewGeneratorConfig(BaseModel):\n    \"\"\"Configuration for zonal view generation.\n\n    Attributes:\n        base_path (Path): Base directory path for storing zonal views. Defaults to\n            configured zonal views path.\n        output_format (str): Default output format for saved views. Defaults to \"parquet\".\n    \"\"\"\n\n    base_path: Path = Field(default=global_config.get_path(\"zonal\", \"views\"))\n    output_format: str = \"parquet\"\n    ensure_available: bool = True\n</code></pre>"},{"location":"api/generators/#gigaspatial.generators.zonal.geometry","title":"<code>geometry</code>","text":""},{"location":"api/generators/#gigaspatial.generators.zonal.geometry.GeometryBasedZonalViewGenerator","title":"<code>GeometryBasedZonalViewGenerator</code>","text":"<p>               Bases: <code>ZonalViewGenerator[T]</code></p> <p>Mid-level class for zonal view generation based on geometries with identifiers.</p> <p>This class serves as an intermediate between the abstract ZonalViewGenerator and specific implementations like MercatorViewGenerator or H3ViewGenerator. It handles the common case where zones are defined by a mapping between zone identifiers and geometries, either provided as a dictionary or as a GeoDataFrame.</p> <p>The class extends the base functionality with methods for mapping common geospatial datasets including GHSL (Global Human Settlement Layer), Google Open Buildings, and Microsoft Global Buildings data.</p> <p>Attributes:</p> Name Type Description <code>zone_dict</code> <code>Dict[T, Polygon]</code> <p>Mapping of zone identifiers to geometries.</p> <code>zone_id_column</code> <code>str</code> <p>Name of the column containing zone identifiers.</p> <code>zone_data_crs</code> <code>str</code> <p>Coordinate reference system of the zone data.</p> <code>_zone_gdf</code> <code>GeoDataFrame</code> <p>Cached GeoDataFrame representation of zones.</p> <code>data_store</code> <code>DataStore</code> <p>For accessing input data.</p> <code>config</code> <code>ZonalViewGeneratorConfig</code> <p>Configuration for view generation.</p> <code>logger</code> <code>ZonalViewGeneratorConfig</code> <p>Logger instance for this class.</p> Source code in <code>gigaspatial/generators/zonal/geometry.py</code> <pre><code>class GeometryBasedZonalViewGenerator(ZonalViewGenerator[T]):\n    \"\"\"Mid-level class for zonal view generation based on geometries with identifiers.\n\n    This class serves as an intermediate between the abstract ZonalViewGenerator and specific\n    implementations like MercatorViewGenerator or H3ViewGenerator. It handles the common case\n    where zones are defined by a mapping between zone identifiers and geometries, either\n    provided as a dictionary or as a GeoDataFrame.\n\n    The class extends the base functionality with methods for mapping common geospatial\n    datasets including GHSL (Global Human Settlement Layer), Google Open Buildings,\n    and Microsoft Global Buildings data.\n\n    Attributes:\n        zone_dict (Dict[T, Polygon]): Mapping of zone identifiers to geometries.\n        zone_id_column (str): Name of the column containing zone identifiers.\n        zone_data_crs (str): Coordinate reference system of the zone data.\n        _zone_gdf (gpd.GeoDataFrame): Cached GeoDataFrame representation of zones.\n        data_store (DataStore): For accessing input data.\n        config (ZonalViewGeneratorConfig): Configuration for view generation.\n        logger: Logger instance for this class.\n    \"\"\"\n\n    def __init__(\n        self,\n        zone_data: Union[Dict[T, Polygon], gpd.GeoDataFrame],\n        zone_id_column: str = \"zone_id\",\n        zone_data_crs: str = \"EPSG:4326\",\n        config: Optional[ZonalViewGeneratorConfig] = None,\n        data_store: Optional[DataStore] = None,\n        logger: logging.Logger = None,\n    ):\n        \"\"\"Initialize with zone geometries and identifiers.\n\n        Args:\n            zone_data (Union[Dict[T, Polygon], gpd.GeoDataFrame]): Zone definitions.\n                Either a dictionary mapping zone identifiers to Polygon/MultiPolygon geometries,\n                or a GeoDataFrame with geometries and a zone identifier column.\n            zone_id_column (str): Name of the column containing zone identifiers.\n                Only used if zone_data is a GeoDataFrame. Defaults to \"zone_id\".\n            zone_data_crs (str): Coordinate reference system of the zone data.\n                Defaults to \"EPSG:4326\" (WGS84).\n            config (ZonalViewGeneratorConfig, optional): Generator configuration.\n                If None, uses default configuration.\n            data_store (DataStore, optional): Data store for accessing input data.\n                If None, uses LocalDataStore.\n\n        Raises:\n            TypeError: If zone_data is not a dictionary or GeoDataFrame, or if dictionary\n                values are not Polygon/MultiPolygon geometries.\n            ValueError: If zone_id_column is not found in GeoDataFrame, or if the provided\n                CRS doesn't match the GeoDataFrame's CRS.\n        \"\"\"\n        super().__init__(config=config, data_store=data_store, logger=logger)\n\n        self.zone_id_column = zone_id_column\n        self.zone_data_crs = zone_data_crs\n\n        # Store zone data based on input type\n        if isinstance(zone_data, dict):\n            for zone_id, geom in zone_data.items():\n                if not isinstance(geom, (Polygon, MultiPolygon)):\n                    raise TypeError(\n                        f\"Zone {zone_id}: Expected (Multi)Polygon, got {type(geom).__name__}\"\n                    )\n\n            # Store the original dictionary\n            self.zone_dict = zone_data\n\n            # Also create a GeoDataFrame for consistent access\n            self._zone_gdf = gpd.GeoDataFrame(\n                {\n                    \"zone_id\": list(zone_data.keys()),\n                    \"geometry\": list(zone_data.values()),\n                },\n                crs=zone_data_crs,\n            )\n            self.zone_id_column = \"zone_id\"\n        else:\n            if not isinstance(zone_data, gpd.GeoDataFrame):\n                raise TypeError(\n                    \"zone_data must be either a Dict[T, Polygon] or a GeoDataFrame\"\n                )\n\n            if zone_id_column not in zone_data.columns:\n                raise ValueError(\n                    f\"Zone ID column '{zone_id_column}' not found in GeoDataFrame\"\n                )\n\n            if zone_data_crs != zone_data.crs:\n                raise ValueError(\n                    f\"Provided data crs '{zone_data_crs}' does not match to the crs of the data '{zone_data.crs}'\"\n                )\n\n            # Store the GeoDataFrame\n            self._zone_gdf = zone_data.rename(columns={zone_id_column: \"zone_id\"})\n\n            # Also create a dictionary for fast lookups\n            self.zone_dict = dict(zip(zone_data[zone_id_column], zone_data.geometry))\n\n    def get_zonal_geometries(self) -&gt; List[Polygon]:\n        \"\"\"Get the geometry of each zone.\n\n        Returns:\n            List[Polygon]: A list of zone geometries in the order they appear in the\n                underlying GeoDataFrame.\n        \"\"\"\n        return self._zone_gdf.geometry.tolist()\n\n    def get_zone_identifiers(self) -&gt; List[T]:\n        \"\"\"Get the identifier for each zone.\n\n        Returns:\n            List[T]: A list of zone identifiers in the order they appear in the\n                underlying GeoDataFrame.\n        \"\"\"\n        return self._zone_gdf[self.zone_id_column].tolist()\n\n    def to_geodataframe(self) -&gt; gpd.GeoDataFrame:\n        \"\"\"Convert zones to a GeoDataFrame with standardized column names.\n\n        Returns:\n            gpd.GeoDataFrame: A GeoDataFrame with 'zone_id' and 'geometry' columns.\n                The zone_id column is renamed from the original zone_id_column if different.\n        \"\"\"\n        # If we already have a GeoDataFrame, just rename the ID column if needed\n        result = self._zone_gdf.copy()\n        if self.zone_id_column != \"zone_id\":\n            result = result.rename(columns={self.zone_id_column: \"zone_id\"})\n        return result\n\n    def map_built_s(\n        self,\n        year=2020,\n        resolution=100,\n        stat: str = \"sum\",\n        name_prefix: str = \"built_surface_m2_\",\n        **kwargs,\n    ) -&gt; gpd.GeoDataFrame:\n        \"\"\"Map GHSL Built-up Surface data to zones.\n\n        Convenience method for mapping Global Human Settlement Layer Built-up Surface\n        data using appropriate default parameters for built surface analysis.\n\n        Args:\n            ghsl_data_config (GHSLDataConfig): Configuration for GHSL Built-up Surface data.\n                Defaults to GHS_BUILT_S product for 2020 at 100m resolution.\n            stat (str): Statistic to calculate for built surface values within each zone.\n                Defaults to \"sum\" which gives total built surface area.\n            name_prefix (str): Prefix for the output column name. Defaults to \"built_surface_m2_\".\n\n        Returns:\n            gpd.GeoDataFrame: Updated GeoDataFrame with zones and built surface metrics.\n                Adds a column named \"{name_prefix}{stat}\" containing the aggregated values.\n        \"\"\"\n        handler = GHSLDataHandler(\n            product=\"GHS_BUILT_S\",\n            year=year,\n            resolution=resolution,\n            data_store=self.data_store,\n            **kwargs,\n        )\n\n        return self.map_ghsl(\n            handler=handler, stat=stat, name_prefix=name_prefix, **kwargs\n        )\n\n    def map_smod(\n        self,\n        year=2020,\n        resolution=100,\n        stat: str = \"median\",\n        name_prefix: str = \"smod_class_\",\n        **kwargs,\n    ) -&gt; gpd.GeoDataFrame:\n        \"\"\"Map GHSL Settlement Model data to zones.\n\n        Convenience method for mapping Global Human Settlement Layer Settlement Model\n        data using appropriate default parameters for settlement classification analysis.\n\n        Args:\n            ghsl_data_config (GHSLDataConfig): Configuration for GHSL Settlement Model data.\n                Defaults to GHS_SMOD product for 2020 at 1000m resolution in Mollweide projection.\n            stat (str): Statistic to calculate for settlement class values within each zone.\n                Defaults to \"median\" which gives the predominant settlement class.\n            name_prefix (str): Prefix for the output column name. Defaults to \"smod_class_\".\n\n        Returns:\n            gpd.GeoDataFrame: Updated GeoDataFrame with zones and settlement classification.\n                Adds a column named \"{name_prefix}{stat}\" containing the aggregated values.\n        \"\"\"\n        handler = GHSLDataHandler(\n            product=\"GHS_SMOD\",\n            year=year,\n            resolution=resolution,\n            data_store=self.data_store,\n            coord_system=54009,\n            **kwargs,\n        )\n\n        return self.map_ghsl(\n            handler=handler, stat=stat, name_prefix=name_prefix, **kwargs\n        )\n\n    def map_ghsl(\n        self,\n        handler: GHSLDataHandler,\n        stat: str,\n        name_prefix: Optional[str] = None,\n        **kwargs,\n    ) -&gt; gpd.GeoDataFrame:\n        \"\"\"Map Global Human Settlement Layer data to zones.\n\n        Loads and processes GHSL raster data for the intersecting tiles, then samples\n        the raster values within each zone using the specified statistic.\n\n        Args:\n            ghsl_data_config (GHSLDataConfig): Configuration specifying which GHSL\n                product, year, resolution, and coordinate system to use.\n            stat (str): Statistic to calculate for raster values within each zone.\n                Common options: \"mean\", \"sum\", \"median\", \"min\", \"max\".\n            name_prefix (str, optional): Prefix for the output column name.\n                If None, uses the GHSL product name in lowercase followed by underscore.\n\n        Returns:\n            gpd.GeoDataFrame: Updated GeoDataFrame with zones and GHSL metrics.\n                Adds a column named \"{name_prefix}{stat}\" containing the sampled values.\n\n        Note:\n            The method automatically determines which GHSL tiles intersect with the zones\n            and loads only the necessary data for efficient processing.\n        \"\"\"\n        handler = handler or GHSLDataHandler(data_store=self.data_store, **kwargs)\n        self.logger.info(\n            f\"Mapping {handler.config.product} data (year: {handler.config.year}, resolution: {handler.config.resolution}m)\"\n        )\n        tif_processors = handler.load_data(\n            self.zone_gdf, ensure_available=self.config.ensure_available\n        )\n\n        self.logger.info(\n            f\"Sampling {handler.config.product} data using '{stat}' statistic\"\n        )\n        sampled_values = self.map_rasters(tif_processors=tif_processors, stat=stat)\n\n        name_prefix = (\n            name_prefix if name_prefix else handler.config.product.lower() + \"_\"\n        )\n        column_name = f\"{name_prefix}{stat}\"\n        self._zone_gdf[column_name] = sampled_values\n\n        self.logger.info(f\"Added {column_name} column\")\n\n        return self._zone_gdf.copy()\n\n    def map_google_buildings(\n        self,\n        handler: Optional[GoogleOpenBuildingsHandler] = None,\n        use_polygons: bool = False,\n    ) -&gt; gpd.GeoDataFrame:\n        \"\"\"Map Google Open Buildings data to zones.\n\n        Processes Google Open Buildings dataset to calculate building counts and total\n        building area within each zone. Can use either point centroids (faster) or\n        polygon geometries (more accurate) for spatial operations.\n\n        Args:\n            google_open_buildings_config (GoogleOpenBuildingsConfig): Configuration\n                for accessing Google Open Buildings data. Uses default configuration if not provided.\n            use_polygons (bool): Whether to use polygon geometries for buildings.\n                If True, uses actual building polygons for more accurate area calculations\n                but with slower performance. If False, uses building centroids with\n                area values from attributes for faster processing. Defaults to False.\n\n        Returns:\n            gpd.GeoDataFrame: Updated GeoDataFrame with zones and building metrics.\n                Adds columns:\n                - 'google_buildings_count': Number of buildings in each zone\n                - 'google_buildings_area_in_meters': Total building area in square meters\n\n        Note:\n            If no Google Buildings data is found for the zones, returns the original\n            GeoDataFrame unchanged with a warning logged.\n        \"\"\"\n        self.logger.info(\n            f\"Mapping Google Open Buildings data (use_polygons={use_polygons})\"\n        )\n\n        self.logger.info(\"Loading Google Buildings point data\")\n        handler = handler or GoogleOpenBuildingsHandler(data_store=self.data_store)\n        buildings_df = handler.load_points(\n            self.zone_gdf, ensure_available=self.config.ensure_available\n        )\n\n        if buildings_df.empty:\n            self.logger.warning(\"No Google buildings data found for the provided zones\")\n            return self._zone_gdf.copy()\n\n        if not use_polygons:\n            self.logger.info(\"Aggregating building data using points with attributes\")\n            result = self.map_points(\n                points=buildings_df,\n                value_columns=[\"full_plus_code\", \"area_in_meters\"],\n                aggregation={\"full_plus_code\": \"count\", \"area_in_meters\": \"sum\"},\n                predicate=\"within\",\n            )\n\n            count_result = result[\"full_plus_code\"]\n            area_result = result[\"area_in_meters\"]\n\n        else:\n            self.logger.info(\n                \"Loading Google Buildings polygon data for more accurate mapping\"\n            )\n            buildings_gdf = handler.load_polygons(\n                self.zone_gdf, self.config.ensure_available\n            )\n\n            self.logger.info(\n                \"Calculating building areas with area-weighted aggregation\"\n            )\n            area_result = self.map_polygons(buildings_gdf, area_weighted=True)\n\n            self.logger.info(\"Counting buildings using points data\")\n            count_result = self.map_points(points=buildings_df, predicate=\"within\")\n\n        self._zone_gdf[\"google_buildings_count\"] = self.zone_gdf.index.map(count_result)\n        self._zone_gdf[\"google_buildings_area_in_meters\"] = self.zone_gdf.index.map(\n            area_result\n        )\n\n        self.logger.info(f\"Added Google building data\")\n\n        return self._zone_gdf.copy()\n\n    def map_ms_buildings(\n        self,\n        handler: Optional[MSBuildingsHandler] = None,\n        use_polygons: bool = False,\n    ) -&gt; gpd.GeoDataFrame:\n        \"\"\"Map Microsoft Global Buildings data to zones.\n\n        Processes Microsoft Global Buildings dataset to calculate building counts and\n        total building area within each zone. Can use either centroid points (faster)\n        or polygon geometries (more accurate) for spatial operations.\n\n        Args:\n            ms_buildings_config (MSBuildingsConfig, optional): Configuration for\n                accessing Microsoft Global Buildings data. If None, uses default configuration.\n            use_polygons (bool): Whether to use polygon geometries for buildings.\n                If True, uses actual building polygons for more accurate area calculations\n                but with slower performance. If False, uses building centroids with\n                area values from attributes for faster processing. Defaults to False.\n\n        Returns:\n            gpd.GeoDataFrame: Updated GeoDataFrame with zones and building metrics.\n                Adds columns:\n                - 'ms_buildings_count': Number of buildings in each zone\n                - 'ms_buildings_area_in_meters': Total building area in square meters\n\n        Note:\n            If no Microsoft Buildings data is found for the zones, returns the original\n            GeoDataFrame unchanged with a warning logged. Building areas are calculated\n            in meters using appropriate UTM projections.\n        \"\"\"\n        self.logger.info(\"Mapping Microsoft Global Buildings data\")\n\n        self.logger.info(\"Loading Microsoft Buildings polygon data\")\n        handler = MSBuildingsHandler(data_store=self.data_store)\n        buildings_gdf = handler.load_data(\n            self.zone_gdf, ensure_available=self.config.ensure_available\n        )\n\n        # Check if we found any buildings\n        if buildings_gdf.empty:\n            self.logger.warning(\n                \"No Microsoft buildings data found for the provided zones\"\n            )\n            return self._zone_gdf.copy()\n\n        buildings_gdf = add_area_in_meters(buildings_gdf)\n\n        building_centroids = get_centroids(buildings_gdf)\n\n        if not use_polygons:\n            self.logger.info(\"Aggregating building data using points with attributes\")\n\n            result = self.map_points(\n                points=building_centroids,\n                value_columns=[\"type\", \"area_in_meters\"],\n                aggregation={\"type\": \"count\", \"area_in_meters\": \"sum\"},\n                predicate=\"within\",\n            )\n\n            count_result = result[\"type\"]\n            area_result = result[\"area_in_meters\"]\n        else:\n\n            self.logger.info(\n                \"Calculating building areas with area-weighted aggregation\"\n            )\n            area_result = self.map_polygons(buildings_gdf, area_weighted=True)\n\n            self.logger.info(\"Counting Microsoft buildings per zone\")\n\n            count_result = self.map_points(\n                points=building_centroids, predicate=\"within\"\n            )\n\n        self._zone_gdf[\"ms_buildings_count\"] = self.zone_gdf.index.map(count_result)\n        self._zone_gdf[\"ms_buildings_area_in_meters\"] = self.zone_gdf.index.map(\n            area_result\n        )\n\n        self.logger.info(f\"Added Microsoft building data\")\n\n        return self._zone_gdf.copy()\n</code></pre>"},{"location":"api/generators/#gigaspatial.generators.zonal.geometry.GeometryBasedZonalViewGenerator.__init__","title":"<code>__init__(zone_data, zone_id_column='zone_id', zone_data_crs='EPSG:4326', config=None, data_store=None, logger=None)</code>","text":"<p>Initialize with zone geometries and identifiers.</p> <p>Parameters:</p> Name Type Description Default <code>zone_data</code> <code>Union[Dict[T, Polygon], GeoDataFrame]</code> <p>Zone definitions. Either a dictionary mapping zone identifiers to Polygon/MultiPolygon geometries, or a GeoDataFrame with geometries and a zone identifier column.</p> required <code>zone_id_column</code> <code>str</code> <p>Name of the column containing zone identifiers. Only used if zone_data is a GeoDataFrame. Defaults to \"zone_id\".</p> <code>'zone_id'</code> <code>zone_data_crs</code> <code>str</code> <p>Coordinate reference system of the zone data. Defaults to \"EPSG:4326\" (WGS84).</p> <code>'EPSG:4326'</code> <code>config</code> <code>ZonalViewGeneratorConfig</code> <p>Generator configuration. If None, uses default configuration.</p> <code>None</code> <code>data_store</code> <code>DataStore</code> <p>Data store for accessing input data. If None, uses LocalDataStore.</p> <code>None</code> <p>Raises:</p> Type Description <code>TypeError</code> <p>If zone_data is not a dictionary or GeoDataFrame, or if dictionary values are not Polygon/MultiPolygon geometries.</p> <code>ValueError</code> <p>If zone_id_column is not found in GeoDataFrame, or if the provided CRS doesn't match the GeoDataFrame's CRS.</p> Source code in <code>gigaspatial/generators/zonal/geometry.py</code> <pre><code>def __init__(\n    self,\n    zone_data: Union[Dict[T, Polygon], gpd.GeoDataFrame],\n    zone_id_column: str = \"zone_id\",\n    zone_data_crs: str = \"EPSG:4326\",\n    config: Optional[ZonalViewGeneratorConfig] = None,\n    data_store: Optional[DataStore] = None,\n    logger: logging.Logger = None,\n):\n    \"\"\"Initialize with zone geometries and identifiers.\n\n    Args:\n        zone_data (Union[Dict[T, Polygon], gpd.GeoDataFrame]): Zone definitions.\n            Either a dictionary mapping zone identifiers to Polygon/MultiPolygon geometries,\n            or a GeoDataFrame with geometries and a zone identifier column.\n        zone_id_column (str): Name of the column containing zone identifiers.\n            Only used if zone_data is a GeoDataFrame. Defaults to \"zone_id\".\n        zone_data_crs (str): Coordinate reference system of the zone data.\n            Defaults to \"EPSG:4326\" (WGS84).\n        config (ZonalViewGeneratorConfig, optional): Generator configuration.\n            If None, uses default configuration.\n        data_store (DataStore, optional): Data store for accessing input data.\n            If None, uses LocalDataStore.\n\n    Raises:\n        TypeError: If zone_data is not a dictionary or GeoDataFrame, or if dictionary\n            values are not Polygon/MultiPolygon geometries.\n        ValueError: If zone_id_column is not found in GeoDataFrame, or if the provided\n            CRS doesn't match the GeoDataFrame's CRS.\n    \"\"\"\n    super().__init__(config=config, data_store=data_store, logger=logger)\n\n    self.zone_id_column = zone_id_column\n    self.zone_data_crs = zone_data_crs\n\n    # Store zone data based on input type\n    if isinstance(zone_data, dict):\n        for zone_id, geom in zone_data.items():\n            if not isinstance(geom, (Polygon, MultiPolygon)):\n                raise TypeError(\n                    f\"Zone {zone_id}: Expected (Multi)Polygon, got {type(geom).__name__}\"\n                )\n\n        # Store the original dictionary\n        self.zone_dict = zone_data\n\n        # Also create a GeoDataFrame for consistent access\n        self._zone_gdf = gpd.GeoDataFrame(\n            {\n                \"zone_id\": list(zone_data.keys()),\n                \"geometry\": list(zone_data.values()),\n            },\n            crs=zone_data_crs,\n        )\n        self.zone_id_column = \"zone_id\"\n    else:\n        if not isinstance(zone_data, gpd.GeoDataFrame):\n            raise TypeError(\n                \"zone_data must be either a Dict[T, Polygon] or a GeoDataFrame\"\n            )\n\n        if zone_id_column not in zone_data.columns:\n            raise ValueError(\n                f\"Zone ID column '{zone_id_column}' not found in GeoDataFrame\"\n            )\n\n        if zone_data_crs != zone_data.crs:\n            raise ValueError(\n                f\"Provided data crs '{zone_data_crs}' does not match to the crs of the data '{zone_data.crs}'\"\n            )\n\n        # Store the GeoDataFrame\n        self._zone_gdf = zone_data.rename(columns={zone_id_column: \"zone_id\"})\n\n        # Also create a dictionary for fast lookups\n        self.zone_dict = dict(zip(zone_data[zone_id_column], zone_data.geometry))\n</code></pre>"},{"location":"api/generators/#gigaspatial.generators.zonal.geometry.GeometryBasedZonalViewGenerator.get_zonal_geometries","title":"<code>get_zonal_geometries()</code>","text":"<p>Get the geometry of each zone.</p> <p>Returns:</p> Type Description <code>List[Polygon]</code> <p>List[Polygon]: A list of zone geometries in the order they appear in the underlying GeoDataFrame.</p> Source code in <code>gigaspatial/generators/zonal/geometry.py</code> <pre><code>def get_zonal_geometries(self) -&gt; List[Polygon]:\n    \"\"\"Get the geometry of each zone.\n\n    Returns:\n        List[Polygon]: A list of zone geometries in the order they appear in the\n            underlying GeoDataFrame.\n    \"\"\"\n    return self._zone_gdf.geometry.tolist()\n</code></pre>"},{"location":"api/generators/#gigaspatial.generators.zonal.geometry.GeometryBasedZonalViewGenerator.get_zone_identifiers","title":"<code>get_zone_identifiers()</code>","text":"<p>Get the identifier for each zone.</p> <p>Returns:</p> Type Description <code>List[T]</code> <p>List[T]: A list of zone identifiers in the order they appear in the underlying GeoDataFrame.</p> Source code in <code>gigaspatial/generators/zonal/geometry.py</code> <pre><code>def get_zone_identifiers(self) -&gt; List[T]:\n    \"\"\"Get the identifier for each zone.\n\n    Returns:\n        List[T]: A list of zone identifiers in the order they appear in the\n            underlying GeoDataFrame.\n    \"\"\"\n    return self._zone_gdf[self.zone_id_column].tolist()\n</code></pre>"},{"location":"api/generators/#gigaspatial.generators.zonal.geometry.GeometryBasedZonalViewGenerator.map_built_s","title":"<code>map_built_s(year=2020, resolution=100, stat='sum', name_prefix='built_surface_m2_', **kwargs)</code>","text":"<p>Map GHSL Built-up Surface data to zones.</p> <p>Convenience method for mapping Global Human Settlement Layer Built-up Surface data using appropriate default parameters for built surface analysis.</p> <p>Parameters:</p> Name Type Description Default <code>ghsl_data_config</code> <code>GHSLDataConfig</code> <p>Configuration for GHSL Built-up Surface data. Defaults to GHS_BUILT_S product for 2020 at 100m resolution.</p> required <code>stat</code> <code>str</code> <p>Statistic to calculate for built surface values within each zone. Defaults to \"sum\" which gives total built surface area.</p> <code>'sum'</code> <code>name_prefix</code> <code>str</code> <p>Prefix for the output column name. Defaults to \"built_surface_m2_\".</p> <code>'built_surface_m2_'</code> <p>Returns:</p> Type Description <code>GeoDataFrame</code> <p>gpd.GeoDataFrame: Updated GeoDataFrame with zones and built surface metrics. Adds a column named \"{name_prefix}{stat}\" containing the aggregated values.</p> Source code in <code>gigaspatial/generators/zonal/geometry.py</code> <pre><code>def map_built_s(\n    self,\n    year=2020,\n    resolution=100,\n    stat: str = \"sum\",\n    name_prefix: str = \"built_surface_m2_\",\n    **kwargs,\n) -&gt; gpd.GeoDataFrame:\n    \"\"\"Map GHSL Built-up Surface data to zones.\n\n    Convenience method for mapping Global Human Settlement Layer Built-up Surface\n    data using appropriate default parameters for built surface analysis.\n\n    Args:\n        ghsl_data_config (GHSLDataConfig): Configuration for GHSL Built-up Surface data.\n            Defaults to GHS_BUILT_S product for 2020 at 100m resolution.\n        stat (str): Statistic to calculate for built surface values within each zone.\n            Defaults to \"sum\" which gives total built surface area.\n        name_prefix (str): Prefix for the output column name. Defaults to \"built_surface_m2_\".\n\n    Returns:\n        gpd.GeoDataFrame: Updated GeoDataFrame with zones and built surface metrics.\n            Adds a column named \"{name_prefix}{stat}\" containing the aggregated values.\n    \"\"\"\n    handler = GHSLDataHandler(\n        product=\"GHS_BUILT_S\",\n        year=year,\n        resolution=resolution,\n        data_store=self.data_store,\n        **kwargs,\n    )\n\n    return self.map_ghsl(\n        handler=handler, stat=stat, name_prefix=name_prefix, **kwargs\n    )\n</code></pre>"},{"location":"api/generators/#gigaspatial.generators.zonal.geometry.GeometryBasedZonalViewGenerator.map_ghsl","title":"<code>map_ghsl(handler, stat, name_prefix=None, **kwargs)</code>","text":"<p>Map Global Human Settlement Layer data to zones.</p> <p>Loads and processes GHSL raster data for the intersecting tiles, then samples the raster values within each zone using the specified statistic.</p> <p>Parameters:</p> Name Type Description Default <code>ghsl_data_config</code> <code>GHSLDataConfig</code> <p>Configuration specifying which GHSL product, year, resolution, and coordinate system to use.</p> required <code>stat</code> <code>str</code> <p>Statistic to calculate for raster values within each zone. Common options: \"mean\", \"sum\", \"median\", \"min\", \"max\".</p> required <code>name_prefix</code> <code>str</code> <p>Prefix for the output column name. If None, uses the GHSL product name in lowercase followed by underscore.</p> <code>None</code> <p>Returns:</p> Type Description <code>GeoDataFrame</code> <p>gpd.GeoDataFrame: Updated GeoDataFrame with zones and GHSL metrics. Adds a column named \"{name_prefix}{stat}\" containing the sampled values.</p> Note <p>The method automatically determines which GHSL tiles intersect with the zones and loads only the necessary data for efficient processing.</p> Source code in <code>gigaspatial/generators/zonal/geometry.py</code> <pre><code>def map_ghsl(\n    self,\n    handler: GHSLDataHandler,\n    stat: str,\n    name_prefix: Optional[str] = None,\n    **kwargs,\n) -&gt; gpd.GeoDataFrame:\n    \"\"\"Map Global Human Settlement Layer data to zones.\n\n    Loads and processes GHSL raster data for the intersecting tiles, then samples\n    the raster values within each zone using the specified statistic.\n\n    Args:\n        ghsl_data_config (GHSLDataConfig): Configuration specifying which GHSL\n            product, year, resolution, and coordinate system to use.\n        stat (str): Statistic to calculate for raster values within each zone.\n            Common options: \"mean\", \"sum\", \"median\", \"min\", \"max\".\n        name_prefix (str, optional): Prefix for the output column name.\n            If None, uses the GHSL product name in lowercase followed by underscore.\n\n    Returns:\n        gpd.GeoDataFrame: Updated GeoDataFrame with zones and GHSL metrics.\n            Adds a column named \"{name_prefix}{stat}\" containing the sampled values.\n\n    Note:\n        The method automatically determines which GHSL tiles intersect with the zones\n        and loads only the necessary data for efficient processing.\n    \"\"\"\n    handler = handler or GHSLDataHandler(data_store=self.data_store, **kwargs)\n    self.logger.info(\n        f\"Mapping {handler.config.product} data (year: {handler.config.year}, resolution: {handler.config.resolution}m)\"\n    )\n    tif_processors = handler.load_data(\n        self.zone_gdf, ensure_available=self.config.ensure_available\n    )\n\n    self.logger.info(\n        f\"Sampling {handler.config.product} data using '{stat}' statistic\"\n    )\n    sampled_values = self.map_rasters(tif_processors=tif_processors, stat=stat)\n\n    name_prefix = (\n        name_prefix if name_prefix else handler.config.product.lower() + \"_\"\n    )\n    column_name = f\"{name_prefix}{stat}\"\n    self._zone_gdf[column_name] = sampled_values\n\n    self.logger.info(f\"Added {column_name} column\")\n\n    return self._zone_gdf.copy()\n</code></pre>"},{"location":"api/generators/#gigaspatial.generators.zonal.geometry.GeometryBasedZonalViewGenerator.map_google_buildings","title":"<code>map_google_buildings(handler=None, use_polygons=False)</code>","text":"<p>Map Google Open Buildings data to zones.</p> <p>Processes Google Open Buildings dataset to calculate building counts and total building area within each zone. Can use either point centroids (faster) or polygon geometries (more accurate) for spatial operations.</p> <p>Parameters:</p> Name Type Description Default <code>google_open_buildings_config</code> <code>GoogleOpenBuildingsConfig</code> <p>Configuration for accessing Google Open Buildings data. Uses default configuration if not provided.</p> required <code>use_polygons</code> <code>bool</code> <p>Whether to use polygon geometries for buildings. If True, uses actual building polygons for more accurate area calculations but with slower performance. If False, uses building centroids with area values from attributes for faster processing. Defaults to False.</p> <code>False</code> <p>Returns:</p> Type Description <code>GeoDataFrame</code> <p>gpd.GeoDataFrame: Updated GeoDataFrame with zones and building metrics. Adds columns: - 'google_buildings_count': Number of buildings in each zone - 'google_buildings_area_in_meters': Total building area in square meters</p> Note <p>If no Google Buildings data is found for the zones, returns the original GeoDataFrame unchanged with a warning logged.</p> Source code in <code>gigaspatial/generators/zonal/geometry.py</code> <pre><code>def map_google_buildings(\n    self,\n    handler: Optional[GoogleOpenBuildingsHandler] = None,\n    use_polygons: bool = False,\n) -&gt; gpd.GeoDataFrame:\n    \"\"\"Map Google Open Buildings data to zones.\n\n    Processes Google Open Buildings dataset to calculate building counts and total\n    building area within each zone. Can use either point centroids (faster) or\n    polygon geometries (more accurate) for spatial operations.\n\n    Args:\n        google_open_buildings_config (GoogleOpenBuildingsConfig): Configuration\n            for accessing Google Open Buildings data. Uses default configuration if not provided.\n        use_polygons (bool): Whether to use polygon geometries for buildings.\n            If True, uses actual building polygons for more accurate area calculations\n            but with slower performance. If False, uses building centroids with\n            area values from attributes for faster processing. Defaults to False.\n\n    Returns:\n        gpd.GeoDataFrame: Updated GeoDataFrame with zones and building metrics.\n            Adds columns:\n            - 'google_buildings_count': Number of buildings in each zone\n            - 'google_buildings_area_in_meters': Total building area in square meters\n\n    Note:\n        If no Google Buildings data is found for the zones, returns the original\n        GeoDataFrame unchanged with a warning logged.\n    \"\"\"\n    self.logger.info(\n        f\"Mapping Google Open Buildings data (use_polygons={use_polygons})\"\n    )\n\n    self.logger.info(\"Loading Google Buildings point data\")\n    handler = handler or GoogleOpenBuildingsHandler(data_store=self.data_store)\n    buildings_df = handler.load_points(\n        self.zone_gdf, ensure_available=self.config.ensure_available\n    )\n\n    if buildings_df.empty:\n        self.logger.warning(\"No Google buildings data found for the provided zones\")\n        return self._zone_gdf.copy()\n\n    if not use_polygons:\n        self.logger.info(\"Aggregating building data using points with attributes\")\n        result = self.map_points(\n            points=buildings_df,\n            value_columns=[\"full_plus_code\", \"area_in_meters\"],\n            aggregation={\"full_plus_code\": \"count\", \"area_in_meters\": \"sum\"},\n            predicate=\"within\",\n        )\n\n        count_result = result[\"full_plus_code\"]\n        area_result = result[\"area_in_meters\"]\n\n    else:\n        self.logger.info(\n            \"Loading Google Buildings polygon data for more accurate mapping\"\n        )\n        buildings_gdf = handler.load_polygons(\n            self.zone_gdf, self.config.ensure_available\n        )\n\n        self.logger.info(\n            \"Calculating building areas with area-weighted aggregation\"\n        )\n        area_result = self.map_polygons(buildings_gdf, area_weighted=True)\n\n        self.logger.info(\"Counting buildings using points data\")\n        count_result = self.map_points(points=buildings_df, predicate=\"within\")\n\n    self._zone_gdf[\"google_buildings_count\"] = self.zone_gdf.index.map(count_result)\n    self._zone_gdf[\"google_buildings_area_in_meters\"] = self.zone_gdf.index.map(\n        area_result\n    )\n\n    self.logger.info(f\"Added Google building data\")\n\n    return self._zone_gdf.copy()\n</code></pre>"},{"location":"api/generators/#gigaspatial.generators.zonal.geometry.GeometryBasedZonalViewGenerator.map_ms_buildings","title":"<code>map_ms_buildings(handler=None, use_polygons=False)</code>","text":"<p>Map Microsoft Global Buildings data to zones.</p> <p>Processes Microsoft Global Buildings dataset to calculate building counts and total building area within each zone. Can use either centroid points (faster) or polygon geometries (more accurate) for spatial operations.</p> <p>Parameters:</p> Name Type Description Default <code>ms_buildings_config</code> <code>MSBuildingsConfig</code> <p>Configuration for accessing Microsoft Global Buildings data. If None, uses default configuration.</p> required <code>use_polygons</code> <code>bool</code> <p>Whether to use polygon geometries for buildings. If True, uses actual building polygons for more accurate area calculations but with slower performance. If False, uses building centroids with area values from attributes for faster processing. Defaults to False.</p> <code>False</code> <p>Returns:</p> Type Description <code>GeoDataFrame</code> <p>gpd.GeoDataFrame: Updated GeoDataFrame with zones and building metrics. Adds columns: - 'ms_buildings_count': Number of buildings in each zone - 'ms_buildings_area_in_meters': Total building area in square meters</p> Note <p>If no Microsoft Buildings data is found for the zones, returns the original GeoDataFrame unchanged with a warning logged. Building areas are calculated in meters using appropriate UTM projections.</p> Source code in <code>gigaspatial/generators/zonal/geometry.py</code> <pre><code>def map_ms_buildings(\n    self,\n    handler: Optional[MSBuildingsHandler] = None,\n    use_polygons: bool = False,\n) -&gt; gpd.GeoDataFrame:\n    \"\"\"Map Microsoft Global Buildings data to zones.\n\n    Processes Microsoft Global Buildings dataset to calculate building counts and\n    total building area within each zone. Can use either centroid points (faster)\n    or polygon geometries (more accurate) for spatial operations.\n\n    Args:\n        ms_buildings_config (MSBuildingsConfig, optional): Configuration for\n            accessing Microsoft Global Buildings data. If None, uses default configuration.\n        use_polygons (bool): Whether to use polygon geometries for buildings.\n            If True, uses actual building polygons for more accurate area calculations\n            but with slower performance. If False, uses building centroids with\n            area values from attributes for faster processing. Defaults to False.\n\n    Returns:\n        gpd.GeoDataFrame: Updated GeoDataFrame with zones and building metrics.\n            Adds columns:\n            - 'ms_buildings_count': Number of buildings in each zone\n            - 'ms_buildings_area_in_meters': Total building area in square meters\n\n    Note:\n        If no Microsoft Buildings data is found for the zones, returns the original\n        GeoDataFrame unchanged with a warning logged. Building areas are calculated\n        in meters using appropriate UTM projections.\n    \"\"\"\n    self.logger.info(\"Mapping Microsoft Global Buildings data\")\n\n    self.logger.info(\"Loading Microsoft Buildings polygon data\")\n    handler = MSBuildingsHandler(data_store=self.data_store)\n    buildings_gdf = handler.load_data(\n        self.zone_gdf, ensure_available=self.config.ensure_available\n    )\n\n    # Check if we found any buildings\n    if buildings_gdf.empty:\n        self.logger.warning(\n            \"No Microsoft buildings data found for the provided zones\"\n        )\n        return self._zone_gdf.copy()\n\n    buildings_gdf = add_area_in_meters(buildings_gdf)\n\n    building_centroids = get_centroids(buildings_gdf)\n\n    if not use_polygons:\n        self.logger.info(\"Aggregating building data using points with attributes\")\n\n        result = self.map_points(\n            points=building_centroids,\n            value_columns=[\"type\", \"area_in_meters\"],\n            aggregation={\"type\": \"count\", \"area_in_meters\": \"sum\"},\n            predicate=\"within\",\n        )\n\n        count_result = result[\"type\"]\n        area_result = result[\"area_in_meters\"]\n    else:\n\n        self.logger.info(\n            \"Calculating building areas with area-weighted aggregation\"\n        )\n        area_result = self.map_polygons(buildings_gdf, area_weighted=True)\n\n        self.logger.info(\"Counting Microsoft buildings per zone\")\n\n        count_result = self.map_points(\n            points=building_centroids, predicate=\"within\"\n        )\n\n    self._zone_gdf[\"ms_buildings_count\"] = self.zone_gdf.index.map(count_result)\n    self._zone_gdf[\"ms_buildings_area_in_meters\"] = self.zone_gdf.index.map(\n        area_result\n    )\n\n    self.logger.info(f\"Added Microsoft building data\")\n\n    return self._zone_gdf.copy()\n</code></pre>"},{"location":"api/generators/#gigaspatial.generators.zonal.geometry.GeometryBasedZonalViewGenerator.map_smod","title":"<code>map_smod(year=2020, resolution=100, stat='median', name_prefix='smod_class_', **kwargs)</code>","text":"<p>Map GHSL Settlement Model data to zones.</p> <p>Convenience method for mapping Global Human Settlement Layer Settlement Model data using appropriate default parameters for settlement classification analysis.</p> <p>Parameters:</p> Name Type Description Default <code>ghsl_data_config</code> <code>GHSLDataConfig</code> <p>Configuration for GHSL Settlement Model data. Defaults to GHS_SMOD product for 2020 at 1000m resolution in Mollweide projection.</p> required <code>stat</code> <code>str</code> <p>Statistic to calculate for settlement class values within each zone. Defaults to \"median\" which gives the predominant settlement class.</p> <code>'median'</code> <code>name_prefix</code> <code>str</code> <p>Prefix for the output column name. Defaults to \"smod_class_\".</p> <code>'smod_class_'</code> <p>Returns:</p> Type Description <code>GeoDataFrame</code> <p>gpd.GeoDataFrame: Updated GeoDataFrame with zones and settlement classification. Adds a column named \"{name_prefix}{stat}\" containing the aggregated values.</p> Source code in <code>gigaspatial/generators/zonal/geometry.py</code> <pre><code>def map_smod(\n    self,\n    year=2020,\n    resolution=100,\n    stat: str = \"median\",\n    name_prefix: str = \"smod_class_\",\n    **kwargs,\n) -&gt; gpd.GeoDataFrame:\n    \"\"\"Map GHSL Settlement Model data to zones.\n\n    Convenience method for mapping Global Human Settlement Layer Settlement Model\n    data using appropriate default parameters for settlement classification analysis.\n\n    Args:\n        ghsl_data_config (GHSLDataConfig): Configuration for GHSL Settlement Model data.\n            Defaults to GHS_SMOD product for 2020 at 1000m resolution in Mollweide projection.\n        stat (str): Statistic to calculate for settlement class values within each zone.\n            Defaults to \"median\" which gives the predominant settlement class.\n        name_prefix (str): Prefix for the output column name. Defaults to \"smod_class_\".\n\n    Returns:\n        gpd.GeoDataFrame: Updated GeoDataFrame with zones and settlement classification.\n            Adds a column named \"{name_prefix}{stat}\" containing the aggregated values.\n    \"\"\"\n    handler = GHSLDataHandler(\n        product=\"GHS_SMOD\",\n        year=year,\n        resolution=resolution,\n        data_store=self.data_store,\n        coord_system=54009,\n        **kwargs,\n    )\n\n    return self.map_ghsl(\n        handler=handler, stat=stat, name_prefix=name_prefix, **kwargs\n    )\n</code></pre>"},{"location":"api/generators/#gigaspatial.generators.zonal.geometry.GeometryBasedZonalViewGenerator.to_geodataframe","title":"<code>to_geodataframe()</code>","text":"<p>Convert zones to a GeoDataFrame with standardized column names.</p> <p>Returns:</p> Type Description <code>GeoDataFrame</code> <p>gpd.GeoDataFrame: A GeoDataFrame with 'zone_id' and 'geometry' columns. The zone_id column is renamed from the original zone_id_column if different.</p> Source code in <code>gigaspatial/generators/zonal/geometry.py</code> <pre><code>def to_geodataframe(self) -&gt; gpd.GeoDataFrame:\n    \"\"\"Convert zones to a GeoDataFrame with standardized column names.\n\n    Returns:\n        gpd.GeoDataFrame: A GeoDataFrame with 'zone_id' and 'geometry' columns.\n            The zone_id column is renamed from the original zone_id_column if different.\n    \"\"\"\n    # If we already have a GeoDataFrame, just rename the ID column if needed\n    result = self._zone_gdf.copy()\n    if self.zone_id_column != \"zone_id\":\n        result = result.rename(columns={self.zone_id_column: \"zone_id\"})\n    return result\n</code></pre>"},{"location":"api/generators/#gigaspatial.generators.zonal.mercator","title":"<code>mercator</code>","text":""},{"location":"api/generators/#gigaspatial.generators.zonal.mercator.MercatorViewGenerator","title":"<code>MercatorViewGenerator</code>","text":"<p>               Bases: <code>GeometryBasedZonalViewGenerator[T]</code></p> <p>Mid-level class for zonal view generation based on geometries with identifiers.</p> <p>This class serves as an intermediate between the abstract ZonalViewGenerator and specific implementations like MercatorViewGenerator or H3ViewGenerator. It handles the common case where zones are defined by a mapping between zone identifiers and geometries, either provided as a dictionary or as a GeoDataFrame.</p> <p>The class extends the base functionality with methods for mapping common geospatial datasets including GHSL (Global Human Settlement Layer), Google Open Buildings, and Microsoft Global Buildings data.</p> <p>Attributes:</p> Name Type Description <code>zone_dict</code> <code>Dict[T, Polygon]</code> <p>Mapping of zone identifiers to geometries.</p> <code>zone_id_column</code> <code>str</code> <p>Name of the column containing zone identifiers.</p> <code>zone_data_crs</code> <code>str</code> <p>Coordinate reference system of the zone data.</p> <code>_zone_gdf</code> <code>GeoDataFrame</code> <p>Cached GeoDataFrame representation of zones.</p> <code>data_store</code> <code>DataStore</code> <p>For accessing input data.</p> <code>generator_config</code> <code>ZonalViewGeneratorConfig</code> <p>Configuration for view generation.</p> <code>logger</code> <code>ZonalViewGeneratorConfig</code> <p>Logger instance for this class.</p> Source code in <code>gigaspatial/generators/zonal/mercator.py</code> <pre><code>class MercatorViewGenerator(GeometryBasedZonalViewGenerator[T]):\n    \"\"\"Mid-level class for zonal view generation based on geometries with identifiers.\n\n    This class serves as an intermediate between the abstract ZonalViewGenerator and specific\n    implementations like MercatorViewGenerator or H3ViewGenerator. It handles the common case\n    where zones are defined by a mapping between zone identifiers and geometries, either\n    provided as a dictionary or as a GeoDataFrame.\n\n    The class extends the base functionality with methods for mapping common geospatial\n    datasets including GHSL (Global Human Settlement Layer), Google Open Buildings,\n    and Microsoft Global Buildings data.\n\n    Attributes:\n        zone_dict (Dict[T, Polygon]): Mapping of zone identifiers to geometries.\n        zone_id_column (str): Name of the column containing zone identifiers.\n        zone_data_crs (str): Coordinate reference system of the zone data.\n        _zone_gdf (gpd.GeoDataFrame): Cached GeoDataFrame representation of zones.\n        data_store (DataStore): For accessing input data.\n        generator_config (ZonalViewGeneratorConfig): Configuration for view generation.\n        logger: Logger instance for this class.\n    \"\"\"\n\n    def __init__(\n        self,\n        source: Union[\n            str,  # country\n            BaseGeometry,  # shapely geom\n            gpd.GeoDataFrame,\n            List[Union[Point, Tuple[float, float]]],  # points\n            List[str],  # quadkeys\n        ],\n        zoom_level: int,\n        predicate=\"intersects\",\n        config: Optional[ZonalViewGeneratorConfig] = None,\n        data_store: Optional[DataStore] = None,\n        logger: logging.Logger = None,\n    ):\n\n        super().__init__(\n            zone_data=self._init_zone_data(source, zoom_level, predicate),\n            zone_id_column=\"quadkey\",\n            config=config,\n            data_store=data_store,\n            logger=logger,\n        )\n\n    def _init_zone_data(self, source, zoom_level, predicate):\n        if isinstance(source, str):\n            tiles = CountryMercatorTiles.create(country=source, zoom_level=zoom_level)\n        elif isinstance(source, (BaseGeometry, Iterable)):\n            if isinstance(source, Iterable) and all(\n                isinstance(qk, str) for qk in source\n            ):\n                tiles = MercatorTiles.from_quadkeys(source)\n            else:\n                tiles = MercatorTiles.from_spatial(\n                    source=source, zoom_level=zoom_level, predicate=predicate\n                )\n        else:\n            raise ValueError(\"sadadasfasfkasmf\")\n\n        return tiles.to_geodataframe()\n</code></pre>"},{"location":"api/grid/","title":"Grid Module","text":""},{"location":"api/grid/#gigaspatial.grid","title":"<code>gigaspatial.grid</code>","text":""},{"location":"api/grid/#gigaspatial.grid.CountryMercatorTiles","title":"<code>CountryMercatorTiles</code>","text":"<p>               Bases: <code>MercatorTiles</code></p> <p>MercatorTiles specialized for country-level operations.</p> <p>This class extends MercatorTiles to work specifically with country boundaries. It can only be instantiated through the create() classmethod.</p> Source code in <code>gigaspatial/grid/mercator_tiles.py</code> <pre><code>class CountryMercatorTiles(MercatorTiles):\n    \"\"\"MercatorTiles specialized for country-level operations.\n\n    This class extends MercatorTiles to work specifically with country boundaries.\n    It can only be instantiated through the create() classmethod.\n    \"\"\"\n\n    country: str = Field(..., exclude=True)\n\n    def __init__(self, *args, **kwargs):\n        raise TypeError(\n            \"CountryMercatorTiles cannot be instantiated directly. \"\n            \"Use CountryMercatorTiles.create() instead.\"\n        )\n\n    @classmethod\n    def create(\n        cls,\n        country: str,\n        zoom_level: int,\n        predicate: str = \"intersects\",\n        data_store: Optional[DataStore] = None,\n        country_geom_path: Optional[Union[str, Path]] = None,\n    ):\n        \"\"\"Create CountryMercatorTiles for a specific country.\"\"\"\n        from gigaspatial.handlers.boundaries import AdminBoundaries\n\n        instance = super().__new__(cls)\n        super(CountryMercatorTiles, instance).__init__(\n            zoom_level=zoom_level,\n            quadkeys=[],\n            data_store=data_store or LocalDataStore(),\n            country=pycountry.countries.lookup(country).alpha_3,\n        )\n\n        country_geom = (\n            AdminBoundaries.create(\n                country_code=country,\n                data_store=data_store,\n                path=country_geom_path,\n            )\n            .boundaries[0]\n            .geometry\n        )\n\n        tiles = MercatorTiles.from_geometry(country_geom, zoom_level, predicate)\n\n        instance.quadkeys = tiles.quadkeys\n        return instance\n</code></pre>"},{"location":"api/grid/#gigaspatial.grid.CountryMercatorTiles.create","title":"<code>create(country, zoom_level, predicate='intersects', data_store=None, country_geom_path=None)</code>  <code>classmethod</code>","text":"<p>Create CountryMercatorTiles for a specific country.</p> Source code in <code>gigaspatial/grid/mercator_tiles.py</code> <pre><code>@classmethod\ndef create(\n    cls,\n    country: str,\n    zoom_level: int,\n    predicate: str = \"intersects\",\n    data_store: Optional[DataStore] = None,\n    country_geom_path: Optional[Union[str, Path]] = None,\n):\n    \"\"\"Create CountryMercatorTiles for a specific country.\"\"\"\n    from gigaspatial.handlers.boundaries import AdminBoundaries\n\n    instance = super().__new__(cls)\n    super(CountryMercatorTiles, instance).__init__(\n        zoom_level=zoom_level,\n        quadkeys=[],\n        data_store=data_store or LocalDataStore(),\n        country=pycountry.countries.lookup(country).alpha_3,\n    )\n\n    country_geom = (\n        AdminBoundaries.create(\n            country_code=country,\n            data_store=data_store,\n            path=country_geom_path,\n        )\n        .boundaries[0]\n        .geometry\n    )\n\n    tiles = MercatorTiles.from_geometry(country_geom, zoom_level, predicate)\n\n    instance.quadkeys = tiles.quadkeys\n    return instance\n</code></pre>"},{"location":"api/grid/#gigaspatial.grid.DataStore","title":"<code>DataStore</code>","text":"<p>               Bases: <code>ABC</code></p> <p>Abstract base class defining the interface for data store implementations. This class serves as a parent for both local and cloud-based storage solutions.</p> Source code in <code>gigaspatial/core/io/data_store.py</code> <pre><code>class DataStore(ABC):\n    \"\"\"\n    Abstract base class defining the interface for data store implementations.\n    This class serves as a parent for both local and cloud-based storage solutions.\n    \"\"\"\n\n    @abstractmethod\n    def read_file(self, path: str) -&gt; Any:\n        \"\"\"\n        Read contents of a file from the data store.\n\n        Args:\n            path: Path to the file to read\n\n        Returns:\n            Contents of the file\n\n        Raises:\n            IOError: If file cannot be read\n        \"\"\"\n        pass\n\n    @abstractmethod\n    def write_file(self, path: str, data: Any) -&gt; None:\n        \"\"\"\n        Write data to a file in the data store.\n\n        Args:\n            path: Path where to write the file\n            data: Data to write to the file\n\n        Raises:\n            IOError: If file cannot be written\n        \"\"\"\n        pass\n\n    @abstractmethod\n    def file_exists(self, path: str) -&gt; bool:\n        \"\"\"\n        Check if a file exists in the data store.\n\n        Args:\n            path: Path to check\n\n        Returns:\n            True if file exists, False otherwise\n        \"\"\"\n        pass\n\n    @abstractmethod\n    def list_files(self, path: str) -&gt; List[str]:\n        \"\"\"\n        List all files in a directory.\n\n        Args:\n            path: Directory path to list\n\n        Returns:\n            List of file paths in the directory\n        \"\"\"\n        pass\n\n    @abstractmethod\n    def walk(self, top: str) -&gt; Generator:\n        \"\"\"\n        Walk through directory tree, similar to os.walk().\n\n        Args:\n            top: Starting directory for the walk\n\n        Returns:\n            Generator yielding tuples of (dirpath, dirnames, filenames)\n        \"\"\"\n        pass\n\n    @abstractmethod\n    def open(self, file: str, mode: str = \"r\") -&gt; Union[str, bytes]:\n        \"\"\"\n        Context manager for file operations.\n\n        Args:\n            file: Path to the file\n            mode: File mode ('r', 'w', 'rb', 'wb')\n\n        Yields:\n            File-like object\n\n        Raises:\n            IOError: If file cannot be opened\n        \"\"\"\n        pass\n\n    @abstractmethod\n    def is_file(self, path: str) -&gt; bool:\n        \"\"\"\n        Check if path points to a file.\n\n        Args:\n            path: Path to check\n\n        Returns:\n            True if path is a file, False otherwise\n        \"\"\"\n        pass\n\n    @abstractmethod\n    def is_dir(self, path: str) -&gt; bool:\n        \"\"\"\n        Check if path points to a directory.\n\n        Args:\n            path: Path to check\n\n        Returns:\n            True if path is a directory, False otherwise\n        \"\"\"\n        pass\n\n    @abstractmethod\n    def remove(self, path: str) -&gt; None:\n        \"\"\"\n        Remove a file.\n\n        Args:\n            path: Path to the file to remove\n\n        Raises:\n            IOError: If file cannot be removed\n        \"\"\"\n        pass\n\n    @abstractmethod\n    def rmdir(self, dir: str) -&gt; None:\n        \"\"\"\n        Remove a directory and all its contents.\n\n        Args:\n            dir: Path to the directory to remove\n\n        Raises:\n            IOError: If directory cannot be removed\n        \"\"\"\n        pass\n</code></pre>"},{"location":"api/grid/#gigaspatial.grid.DataStore.file_exists","title":"<code>file_exists(path)</code>  <code>abstractmethod</code>","text":"<p>Check if a file exists in the data store.</p> <p>Parameters:</p> Name Type Description Default <code>path</code> <code>str</code> <p>Path to check</p> required <p>Returns:</p> Type Description <code>bool</code> <p>True if file exists, False otherwise</p> Source code in <code>gigaspatial/core/io/data_store.py</code> <pre><code>@abstractmethod\ndef file_exists(self, path: str) -&gt; bool:\n    \"\"\"\n    Check if a file exists in the data store.\n\n    Args:\n        path: Path to check\n\n    Returns:\n        True if file exists, False otherwise\n    \"\"\"\n    pass\n</code></pre>"},{"location":"api/grid/#gigaspatial.grid.DataStore.is_dir","title":"<code>is_dir(path)</code>  <code>abstractmethod</code>","text":"<p>Check if path points to a directory.</p> <p>Parameters:</p> Name Type Description Default <code>path</code> <code>str</code> <p>Path to check</p> required <p>Returns:</p> Type Description <code>bool</code> <p>True if path is a directory, False otherwise</p> Source code in <code>gigaspatial/core/io/data_store.py</code> <pre><code>@abstractmethod\ndef is_dir(self, path: str) -&gt; bool:\n    \"\"\"\n    Check if path points to a directory.\n\n    Args:\n        path: Path to check\n\n    Returns:\n        True if path is a directory, False otherwise\n    \"\"\"\n    pass\n</code></pre>"},{"location":"api/grid/#gigaspatial.grid.DataStore.is_file","title":"<code>is_file(path)</code>  <code>abstractmethod</code>","text":"<p>Check if path points to a file.</p> <p>Parameters:</p> Name Type Description Default <code>path</code> <code>str</code> <p>Path to check</p> required <p>Returns:</p> Type Description <code>bool</code> <p>True if path is a file, False otherwise</p> Source code in <code>gigaspatial/core/io/data_store.py</code> <pre><code>@abstractmethod\ndef is_file(self, path: str) -&gt; bool:\n    \"\"\"\n    Check if path points to a file.\n\n    Args:\n        path: Path to check\n\n    Returns:\n        True if path is a file, False otherwise\n    \"\"\"\n    pass\n</code></pre>"},{"location":"api/grid/#gigaspatial.grid.DataStore.list_files","title":"<code>list_files(path)</code>  <code>abstractmethod</code>","text":"<p>List all files in a directory.</p> <p>Parameters:</p> Name Type Description Default <code>path</code> <code>str</code> <p>Directory path to list</p> required <p>Returns:</p> Type Description <code>List[str]</code> <p>List of file paths in the directory</p> Source code in <code>gigaspatial/core/io/data_store.py</code> <pre><code>@abstractmethod\ndef list_files(self, path: str) -&gt; List[str]:\n    \"\"\"\n    List all files in a directory.\n\n    Args:\n        path: Directory path to list\n\n    Returns:\n        List of file paths in the directory\n    \"\"\"\n    pass\n</code></pre>"},{"location":"api/grid/#gigaspatial.grid.DataStore.open","title":"<code>open(file, mode='r')</code>  <code>abstractmethod</code>","text":"<p>Context manager for file operations.</p> <p>Parameters:</p> Name Type Description Default <code>file</code> <code>str</code> <p>Path to the file</p> required <code>mode</code> <code>str</code> <p>File mode ('r', 'w', 'rb', 'wb')</p> <code>'r'</code> <p>Yields:</p> Type Description <code>Union[str, bytes]</code> <p>File-like object</p> <p>Raises:</p> Type Description <code>IOError</code> <p>If file cannot be opened</p> Source code in <code>gigaspatial/core/io/data_store.py</code> <pre><code>@abstractmethod\ndef open(self, file: str, mode: str = \"r\") -&gt; Union[str, bytes]:\n    \"\"\"\n    Context manager for file operations.\n\n    Args:\n        file: Path to the file\n        mode: File mode ('r', 'w', 'rb', 'wb')\n\n    Yields:\n        File-like object\n\n    Raises:\n        IOError: If file cannot be opened\n    \"\"\"\n    pass\n</code></pre>"},{"location":"api/grid/#gigaspatial.grid.DataStore.read_file","title":"<code>read_file(path)</code>  <code>abstractmethod</code>","text":"<p>Read contents of a file from the data store.</p> <p>Parameters:</p> Name Type Description Default <code>path</code> <code>str</code> <p>Path to the file to read</p> required <p>Returns:</p> Type Description <code>Any</code> <p>Contents of the file</p> <p>Raises:</p> Type Description <code>IOError</code> <p>If file cannot be read</p> Source code in <code>gigaspatial/core/io/data_store.py</code> <pre><code>@abstractmethod\ndef read_file(self, path: str) -&gt; Any:\n    \"\"\"\n    Read contents of a file from the data store.\n\n    Args:\n        path: Path to the file to read\n\n    Returns:\n        Contents of the file\n\n    Raises:\n        IOError: If file cannot be read\n    \"\"\"\n    pass\n</code></pre>"},{"location":"api/grid/#gigaspatial.grid.DataStore.remove","title":"<code>remove(path)</code>  <code>abstractmethod</code>","text":"<p>Remove a file.</p> <p>Parameters:</p> Name Type Description Default <code>path</code> <code>str</code> <p>Path to the file to remove</p> required <p>Raises:</p> Type Description <code>IOError</code> <p>If file cannot be removed</p> Source code in <code>gigaspatial/core/io/data_store.py</code> <pre><code>@abstractmethod\ndef remove(self, path: str) -&gt; None:\n    \"\"\"\n    Remove a file.\n\n    Args:\n        path: Path to the file to remove\n\n    Raises:\n        IOError: If file cannot be removed\n    \"\"\"\n    pass\n</code></pre>"},{"location":"api/grid/#gigaspatial.grid.DataStore.rmdir","title":"<code>rmdir(dir)</code>  <code>abstractmethod</code>","text":"<p>Remove a directory and all its contents.</p> <p>Parameters:</p> Name Type Description Default <code>dir</code> <code>str</code> <p>Path to the directory to remove</p> required <p>Raises:</p> Type Description <code>IOError</code> <p>If directory cannot be removed</p> Source code in <code>gigaspatial/core/io/data_store.py</code> <pre><code>@abstractmethod\ndef rmdir(self, dir: str) -&gt; None:\n    \"\"\"\n    Remove a directory and all its contents.\n\n    Args:\n        dir: Path to the directory to remove\n\n    Raises:\n        IOError: If directory cannot be removed\n    \"\"\"\n    pass\n</code></pre>"},{"location":"api/grid/#gigaspatial.grid.DataStore.walk","title":"<code>walk(top)</code>  <code>abstractmethod</code>","text":"<p>Walk through directory tree, similar to os.walk().</p> <p>Parameters:</p> Name Type Description Default <code>top</code> <code>str</code> <p>Starting directory for the walk</p> required <p>Returns:</p> Type Description <code>Generator</code> <p>Generator yielding tuples of (dirpath, dirnames, filenames)</p> Source code in <code>gigaspatial/core/io/data_store.py</code> <pre><code>@abstractmethod\ndef walk(self, top: str) -&gt; Generator:\n    \"\"\"\n    Walk through directory tree, similar to os.walk().\n\n    Args:\n        top: Starting directory for the walk\n\n    Returns:\n        Generator yielding tuples of (dirpath, dirnames, filenames)\n    \"\"\"\n    pass\n</code></pre>"},{"location":"api/grid/#gigaspatial.grid.DataStore.write_file","title":"<code>write_file(path, data)</code>  <code>abstractmethod</code>","text":"<p>Write data to a file in the data store.</p> <p>Parameters:</p> Name Type Description Default <code>path</code> <code>str</code> <p>Path where to write the file</p> required <code>data</code> <code>Any</code> <p>Data to write to the file</p> required <p>Raises:</p> Type Description <code>IOError</code> <p>If file cannot be written</p> Source code in <code>gigaspatial/core/io/data_store.py</code> <pre><code>@abstractmethod\ndef write_file(self, path: str, data: Any) -&gt; None:\n    \"\"\"\n    Write data to a file in the data store.\n\n    Args:\n        path: Path where to write the file\n        data: Data to write to the file\n\n    Raises:\n        IOError: If file cannot be written\n    \"\"\"\n    pass\n</code></pre>"},{"location":"api/grid/#gigaspatial.grid.LocalDataStore","title":"<code>LocalDataStore</code>","text":"<p>               Bases: <code>DataStore</code></p> <p>Implementation for local filesystem storage.</p> Source code in <code>gigaspatial/core/io/local_data_store.py</code> <pre><code>class LocalDataStore(DataStore):\n    \"\"\"Implementation for local filesystem storage.\"\"\"\n\n    def __init__(self, base_path: Union[str, Path] = \"\"):\n        super().__init__()\n        self.base_path = Path(base_path).resolve()\n\n    def _resolve_path(self, path: str) -&gt; Path:\n        \"\"\"Resolve path relative to base directory.\"\"\"\n        return self.base_path / path\n\n    def read_file(self, path: str) -&gt; bytes:\n        full_path = self._resolve_path(path)\n        with open(full_path, \"rb\") as f:\n            return f.read()\n\n    def write_file(self, path: str, data: Union[bytes, str]) -&gt; None:\n        full_path = self._resolve_path(path)\n        self.mkdir(str(full_path.parent), exist_ok=True)\n\n        if isinstance(data, str):\n            mode = \"w\"\n            encoding = \"utf-8\"\n        else:\n            mode = \"wb\"\n            encoding = None\n\n        with open(full_path, mode, encoding=encoding) as f:\n            f.write(data)\n\n    def file_exists(self, path: str) -&gt; bool:\n        return self._resolve_path(path).is_file()\n\n    def list_files(self, path: str) -&gt; List[str]:\n        full_path = self._resolve_path(path)\n        return [\n            str(f.relative_to(self.base_path))\n            for f in full_path.iterdir()\n            if f.is_file()\n        ]\n\n    def walk(self, top: str) -&gt; Generator[Tuple[str, List[str], List[str]], None, None]:\n        full_path = self._resolve_path(top)\n        for root, dirs, files in os.walk(full_path):\n            rel_root = str(Path(root).relative_to(self.base_path))\n            yield rel_root, dirs, files\n\n    def list_directories(self, path: str) -&gt; List[str]:\n        full_path = self._resolve_path(path)\n\n        if not full_path.exists():\n            return []\n\n        if not full_path.is_dir():\n            return []\n\n        return [d.name for d in full_path.iterdir() if d.is_dir()]\n\n    def open(self, path: str, mode: str = \"r\") -&gt; IO:\n        full_path = self._resolve_path(path)\n        self.mkdir(str(full_path.parent), exist_ok=True)\n        return open(full_path, mode)\n\n    def is_file(self, path: str) -&gt; bool:\n        return self._resolve_path(path).is_file()\n\n    def is_dir(self, path: str) -&gt; bool:\n        return self._resolve_path(path).is_dir()\n\n    def remove(self, path: str) -&gt; None:\n        full_path = self._resolve_path(path)\n        if full_path.is_file():\n            os.remove(full_path)\n\n    def rmdir(self, directory: str) -&gt; None:\n        full_path = self._resolve_path(directory)\n        if full_path.is_dir():\n            os.rmdir(full_path)\n\n    def mkdir(self, path: str, exist_ok: bool = False) -&gt; None:\n        full_path = self._resolve_path(path)\n        full_path.mkdir(parents=True, exist_ok=exist_ok)\n\n    def exists(self, path: str) -&gt; bool:\n        return self._resolve_path(path).exists()\n</code></pre>"},{"location":"api/grid/#gigaspatial.grid.MercatorTiles","title":"<code>MercatorTiles</code>","text":"<p>               Bases: <code>BaseModel</code></p> Source code in <code>gigaspatial/grid/mercator_tiles.py</code> <pre><code>class MercatorTiles(BaseModel):\n    zoom_level: int = Field(..., ge=0, le=20)\n    quadkeys: List[str] = Field(default_factory=list)\n    data_store: DataStore = Field(default_factory=LocalDataStore, exclude=True)\n    logger: ClassVar = config.get_logger(\"MercatorTiles\")\n\n    class Config:\n        arbitrary_types_allowed = True\n\n    @classmethod\n    def from_quadkeys(cls, quadkeys: List[str]):\n        \"\"\"Create MercatorTiles from list of quadkeys.\"\"\"\n        if not quadkeys:\n            cls.logger.warning(\"No quadkeys provided to from_quadkeys.\")\n            return cls(zoom_level=0, quadkeys=[])\n        return cls(zoom_level=len(quadkeys[0]), quadkeys=set(quadkeys))\n\n    @classmethod\n    def from_bounds(\n        cls, xmin: float, ymin: float, xmax: float, ymax: float, zoom_level: int\n    ):\n        \"\"\"Create MercatorTiles from boundary coordinates.\"\"\"\n        cls.logger.info(\n            f\"Creating MercatorTiles from bounds: ({xmin}, {ymin}, {xmax}, {ymax}) at zoom level: {zoom_level}\"\n        )\n        return cls(\n            zoom_level=zoom_level,\n            quadkeys=[\n                mercantile.quadkey(tile)\n                for tile in mercantile.tiles(xmin, ymin, xmax, ymax, zoom_level)\n            ],\n        )\n\n    @classmethod\n    def from_spatial(\n        cls,\n        source: Union[\n            BaseGeometry,\n            gpd.GeoDataFrame,\n            List[Union[Point, Tuple[float, float]]],  # points\n        ],\n        zoom_level: int,\n        predicate: str = \"intersects\",\n        **kwargs,\n    ):\n        cls.logger.info(\n            f\"Creating MercatorTiles from spatial source (type: {type(source)}) at zoom level: {zoom_level} with predicate: {predicate}\"\n        )\n        if isinstance(source, gpd.GeoDataFrame):\n            if source.crs != \"EPSG:4326\":\n                source = source.to_crs(\"EPSG:4326\")\n            source = source.geometry.unary_union\n\n        if isinstance(source, BaseGeometry):\n            return cls.from_geometry(\n                geometry=source, zoom_level=zoom_level, predicate=predicate, **kwargs\n            )\n        elif isinstance(source, Iterable) and all(\n            len(pt) == 2 or isinstance(pt, Point) for pt in source\n        ):\n            return cls.from_points(geometry=source, zoom_level=zoom_level, **kwargs)\n        else:\n            raise\n\n    @classmethod\n    def from_geometry(\n        cls,\n        geometry: BaseGeometry,\n        zoom_level: int,\n        predicate: str = \"intersects\",\n        **kwargs,\n    ):\n        \"\"\"Create MercatorTiles from a polygon.\"\"\"\n        cls.logger.info(\n            f\"Creating MercatorTiles from geometry (bounds: {geometry.bounds}) at zoom level: {zoom_level} with predicate: {predicate}\"\n        )\n        tiles = list(mercantile.tiles(*geometry.bounds, zoom_level))\n        quadkeys_boxes = [\n            (mercantile.quadkey(t), box(*mercantile.bounds(t))) for t in tiles\n        ]\n        quadkeys, boxes = zip(*quadkeys_boxes) if quadkeys_boxes else ([], [])\n\n        if not boxes:\n            cls.logger.warning(\n                \"No boxes generated from geometry bounds. Returning empty MercatorTiles.\"\n            )\n            return MercatorTiles(zoom_level=zoom_level, quadkeys=[])\n\n        s = STRtree(boxes)\n        result_indices = s.query(geometry, predicate=predicate)\n        filtered_quadkeys = [quadkeys[i] for i in result_indices]\n        cls.logger.info(\n            f\"Filtered down to {len(filtered_quadkeys)} quadkeys using spatial predicate.\"\n        )\n        return cls(zoom_level=zoom_level, quadkeys=filtered_quadkeys, **kwargs)\n\n    @classmethod\n    def from_points(\n        cls, points: List[Union[Point, Tuple[float, float]]], zoom_level: int, **kwargs\n    ) -&gt; \"MercatorTiles\":\n        \"\"\"Create MercatorTiles from a list of points or lat-lon pairs.\"\"\"\n        cls.logger.info(\n            f\"Creating MercatorTiles from {len(points)} points at zoom level: {zoom_level}\"\n        )\n        quadkeys = {\n            (\n                mercantile.quadkey(mercantile.tile(p.x, p.y, zoom_level))\n                if isinstance(p, Point)\n                else mercantile.quadkey(mercantile.tile(p[1], p[0], zoom_level))\n            )\n            for p in points\n        }\n        cls.logger.info(f\"Generated {len(quadkeys)} unique quadkeys from points.\")\n        return cls(zoom_level=zoom_level, quadkeys=list(quadkeys), **kwargs)\n\n    @classmethod\n    def from_json(\n        cls, data_store: DataStore, file: Union[str, Path], **kwargs\n    ) -&gt; \"MercatorTiles\":\n        \"\"\"Load MercatorTiles from a JSON file.\"\"\"\n        cls.logger.info(\n            f\"Loading MercatorTiles from JSON file: {file} using data store: {type(data_store).__name__}\"\n        )\n        with data_store.open(str(file), \"r\") as f:\n            data = json.load(f)\n            if isinstance(data, list):  # If file contains only quadkeys\n                data = {\n                    \"zoom_level\": len(data[0]) if data else 0,\n                    \"quadkeys\": data,\n                    **kwargs,\n                }\n            else:\n                data.update(kwargs)\n            instance = cls(**data)\n            instance.data_store = data_store\n            cls.logger.info(\n                f\"Successfully loaded {len(instance.quadkeys)} quadkeys from JSON file.\"\n            )\n            return instance\n\n    def filter_quadkeys(self, quadkeys: Iterable[str]) -&gt; \"MercatorTiles\":\n        \"\"\"Filter quadkeys by a given set of quadkeys.\"\"\"\n        original_count = len(self.quadkeys)\n        incoming_count = len(\n            list(quadkeys)\n        )  # Convert to list to get length if it's an iterator\n\n        self.logger.info(\n            f\"Filtering {original_count} quadkeys with an incoming set of {incoming_count} quadkeys.\"\n        )\n        filtered_quadkeys = list(set(self.quadkeys) &amp; set(quadkeys))\n        self.logger.info(f\"Resulting in {len(filtered_quadkeys)} filtered quadkeys.\")\n        return MercatorTiles(\n            zoom_level=self.zoom_level,\n            quadkeys=filtered_quadkeys,\n        )\n\n    def to_dataframe(self) -&gt; pd.DataFrame:\n        \"\"\"Convert to pandas DataFrame with quadkey and centroid coordinates.\"\"\"\n        self.logger.info(\n            f\"Converting {len(self.quadkeys)} quadkeys to pandas DataFrame.\"\n        )\n        if not self.quadkeys:\n            self.logger.warning(\n                \"No quadkeys to convert to DataFrame. Returning empty DataFrame.\"\n            )\n            return pd.DataFrame(columns=[\"quadkey\", \"latitude\", \"longitude\"])\n        tiles_data = [mercantile.quadkey_to_tile(q) for q in self.quadkeys]\n        bounds_data = [mercantile.bounds(tile) for tile in tiles_data]\n\n        centroids = [\n            (\n                (bounds.south + bounds.north) / 2,  # latitude\n                (bounds.west + bounds.east) / 2,  # longitude\n            )\n            for bounds in bounds_data\n        ]\n\n        self.logger.info(f\"Successfully converted to DataFrame.\")\n\n        return pd.DataFrame(\n            {\n                \"quadkey\": self.quadkeys,\n                \"latitude\": [c[0] for c in centroids],\n                \"longitude\": [c[1] for c in centroids],\n            }\n        )\n\n    def to_geoms(self) -&gt; List[box]:\n        self.logger.info(\n            f\"Converting {len(self.quadkeys)} quadkeys to shapely box geometries.\"\n        )\n        return [\n            box(*mercantile.bounds(mercantile.quadkey_to_tile(q)))\n            for q in self.quadkeys\n        ]\n\n    def to_geodataframe(self) -&gt; gpd.GeoDataFrame:\n        \"\"\"Convert to GeoPandas GeoDataFrame.\"\"\"\n        return gpd.GeoDataFrame(\n            {\"quadkey\": self.quadkeys, \"geometry\": self.to_geoms()}, crs=\"EPSG:4326\"\n        )\n\n    def save(self, file: Union[str, Path], format: str = \"json\") -&gt; None:\n        \"\"\"Save MercatorTiles to file in specified format.\"\"\"\n        with self.data_store.open(str(file), \"wb\" if format == \"parquet\" else \"w\") as f:\n            if format == \"parquet\":\n                self.to_geodataframe().to_parquet(f, index=False)\n            elif format == \"geojson\":\n                f.write(self.to_geodataframe().to_json(drop_id=True))\n            elif format == \"json\":\n                json.dump(self.quadkeys, f)\n            else:\n                raise ValueError(f\"Unsupported format: {format}\")\n\n    def __len__(self) -&gt; int:\n        return len(self.quadkeys)\n</code></pre>"},{"location":"api/grid/#gigaspatial.grid.MercatorTiles.filter_quadkeys","title":"<code>filter_quadkeys(quadkeys)</code>","text":"<p>Filter quadkeys by a given set of quadkeys.</p> Source code in <code>gigaspatial/grid/mercator_tiles.py</code> <pre><code>def filter_quadkeys(self, quadkeys: Iterable[str]) -&gt; \"MercatorTiles\":\n    \"\"\"Filter quadkeys by a given set of quadkeys.\"\"\"\n    original_count = len(self.quadkeys)\n    incoming_count = len(\n        list(quadkeys)\n    )  # Convert to list to get length if it's an iterator\n\n    self.logger.info(\n        f\"Filtering {original_count} quadkeys with an incoming set of {incoming_count} quadkeys.\"\n    )\n    filtered_quadkeys = list(set(self.quadkeys) &amp; set(quadkeys))\n    self.logger.info(f\"Resulting in {len(filtered_quadkeys)} filtered quadkeys.\")\n    return MercatorTiles(\n        zoom_level=self.zoom_level,\n        quadkeys=filtered_quadkeys,\n    )\n</code></pre>"},{"location":"api/grid/#gigaspatial.grid.MercatorTiles.from_bounds","title":"<code>from_bounds(xmin, ymin, xmax, ymax, zoom_level)</code>  <code>classmethod</code>","text":"<p>Create MercatorTiles from boundary coordinates.</p> Source code in <code>gigaspatial/grid/mercator_tiles.py</code> <pre><code>@classmethod\ndef from_bounds(\n    cls, xmin: float, ymin: float, xmax: float, ymax: float, zoom_level: int\n):\n    \"\"\"Create MercatorTiles from boundary coordinates.\"\"\"\n    cls.logger.info(\n        f\"Creating MercatorTiles from bounds: ({xmin}, {ymin}, {xmax}, {ymax}) at zoom level: {zoom_level}\"\n    )\n    return cls(\n        zoom_level=zoom_level,\n        quadkeys=[\n            mercantile.quadkey(tile)\n            for tile in mercantile.tiles(xmin, ymin, xmax, ymax, zoom_level)\n        ],\n    )\n</code></pre>"},{"location":"api/grid/#gigaspatial.grid.MercatorTiles.from_geometry","title":"<code>from_geometry(geometry, zoom_level, predicate='intersects', **kwargs)</code>  <code>classmethod</code>","text":"<p>Create MercatorTiles from a polygon.</p> Source code in <code>gigaspatial/grid/mercator_tiles.py</code> <pre><code>@classmethod\ndef from_geometry(\n    cls,\n    geometry: BaseGeometry,\n    zoom_level: int,\n    predicate: str = \"intersects\",\n    **kwargs,\n):\n    \"\"\"Create MercatorTiles from a polygon.\"\"\"\n    cls.logger.info(\n        f\"Creating MercatorTiles from geometry (bounds: {geometry.bounds}) at zoom level: {zoom_level} with predicate: {predicate}\"\n    )\n    tiles = list(mercantile.tiles(*geometry.bounds, zoom_level))\n    quadkeys_boxes = [\n        (mercantile.quadkey(t), box(*mercantile.bounds(t))) for t in tiles\n    ]\n    quadkeys, boxes = zip(*quadkeys_boxes) if quadkeys_boxes else ([], [])\n\n    if not boxes:\n        cls.logger.warning(\n            \"No boxes generated from geometry bounds. Returning empty MercatorTiles.\"\n        )\n        return MercatorTiles(zoom_level=zoom_level, quadkeys=[])\n\n    s = STRtree(boxes)\n    result_indices = s.query(geometry, predicate=predicate)\n    filtered_quadkeys = [quadkeys[i] for i in result_indices]\n    cls.logger.info(\n        f\"Filtered down to {len(filtered_quadkeys)} quadkeys using spatial predicate.\"\n    )\n    return cls(zoom_level=zoom_level, quadkeys=filtered_quadkeys, **kwargs)\n</code></pre>"},{"location":"api/grid/#gigaspatial.grid.MercatorTiles.from_json","title":"<code>from_json(data_store, file, **kwargs)</code>  <code>classmethod</code>","text":"<p>Load MercatorTiles from a JSON file.</p> Source code in <code>gigaspatial/grid/mercator_tiles.py</code> <pre><code>@classmethod\ndef from_json(\n    cls, data_store: DataStore, file: Union[str, Path], **kwargs\n) -&gt; \"MercatorTiles\":\n    \"\"\"Load MercatorTiles from a JSON file.\"\"\"\n    cls.logger.info(\n        f\"Loading MercatorTiles from JSON file: {file} using data store: {type(data_store).__name__}\"\n    )\n    with data_store.open(str(file), \"r\") as f:\n        data = json.load(f)\n        if isinstance(data, list):  # If file contains only quadkeys\n            data = {\n                \"zoom_level\": len(data[0]) if data else 0,\n                \"quadkeys\": data,\n                **kwargs,\n            }\n        else:\n            data.update(kwargs)\n        instance = cls(**data)\n        instance.data_store = data_store\n        cls.logger.info(\n            f\"Successfully loaded {len(instance.quadkeys)} quadkeys from JSON file.\"\n        )\n        return instance\n</code></pre>"},{"location":"api/grid/#gigaspatial.grid.MercatorTiles.from_points","title":"<code>from_points(points, zoom_level, **kwargs)</code>  <code>classmethod</code>","text":"<p>Create MercatorTiles from a list of points or lat-lon pairs.</p> Source code in <code>gigaspatial/grid/mercator_tiles.py</code> <pre><code>@classmethod\ndef from_points(\n    cls, points: List[Union[Point, Tuple[float, float]]], zoom_level: int, **kwargs\n) -&gt; \"MercatorTiles\":\n    \"\"\"Create MercatorTiles from a list of points or lat-lon pairs.\"\"\"\n    cls.logger.info(\n        f\"Creating MercatorTiles from {len(points)} points at zoom level: {zoom_level}\"\n    )\n    quadkeys = {\n        (\n            mercantile.quadkey(mercantile.tile(p.x, p.y, zoom_level))\n            if isinstance(p, Point)\n            else mercantile.quadkey(mercantile.tile(p[1], p[0], zoom_level))\n        )\n        for p in points\n    }\n    cls.logger.info(f\"Generated {len(quadkeys)} unique quadkeys from points.\")\n    return cls(zoom_level=zoom_level, quadkeys=list(quadkeys), **kwargs)\n</code></pre>"},{"location":"api/grid/#gigaspatial.grid.MercatorTiles.from_quadkeys","title":"<code>from_quadkeys(quadkeys)</code>  <code>classmethod</code>","text":"<p>Create MercatorTiles from list of quadkeys.</p> Source code in <code>gigaspatial/grid/mercator_tiles.py</code> <pre><code>@classmethod\ndef from_quadkeys(cls, quadkeys: List[str]):\n    \"\"\"Create MercatorTiles from list of quadkeys.\"\"\"\n    if not quadkeys:\n        cls.logger.warning(\"No quadkeys provided to from_quadkeys.\")\n        return cls(zoom_level=0, quadkeys=[])\n    return cls(zoom_level=len(quadkeys[0]), quadkeys=set(quadkeys))\n</code></pre>"},{"location":"api/grid/#gigaspatial.grid.MercatorTiles.save","title":"<code>save(file, format='json')</code>","text":"<p>Save MercatorTiles to file in specified format.</p> Source code in <code>gigaspatial/grid/mercator_tiles.py</code> <pre><code>def save(self, file: Union[str, Path], format: str = \"json\") -&gt; None:\n    \"\"\"Save MercatorTiles to file in specified format.\"\"\"\n    with self.data_store.open(str(file), \"wb\" if format == \"parquet\" else \"w\") as f:\n        if format == \"parquet\":\n            self.to_geodataframe().to_parquet(f, index=False)\n        elif format == \"geojson\":\n            f.write(self.to_geodataframe().to_json(drop_id=True))\n        elif format == \"json\":\n            json.dump(self.quadkeys, f)\n        else:\n            raise ValueError(f\"Unsupported format: {format}\")\n</code></pre>"},{"location":"api/grid/#gigaspatial.grid.MercatorTiles.to_dataframe","title":"<code>to_dataframe()</code>","text":"<p>Convert to pandas DataFrame with quadkey and centroid coordinates.</p> Source code in <code>gigaspatial/grid/mercator_tiles.py</code> <pre><code>def to_dataframe(self) -&gt; pd.DataFrame:\n    \"\"\"Convert to pandas DataFrame with quadkey and centroid coordinates.\"\"\"\n    self.logger.info(\n        f\"Converting {len(self.quadkeys)} quadkeys to pandas DataFrame.\"\n    )\n    if not self.quadkeys:\n        self.logger.warning(\n            \"No quadkeys to convert to DataFrame. Returning empty DataFrame.\"\n        )\n        return pd.DataFrame(columns=[\"quadkey\", \"latitude\", \"longitude\"])\n    tiles_data = [mercantile.quadkey_to_tile(q) for q in self.quadkeys]\n    bounds_data = [mercantile.bounds(tile) for tile in tiles_data]\n\n    centroids = [\n        (\n            (bounds.south + bounds.north) / 2,  # latitude\n            (bounds.west + bounds.east) / 2,  # longitude\n        )\n        for bounds in bounds_data\n    ]\n\n    self.logger.info(f\"Successfully converted to DataFrame.\")\n\n    return pd.DataFrame(\n        {\n            \"quadkey\": self.quadkeys,\n            \"latitude\": [c[0] for c in centroids],\n            \"longitude\": [c[1] for c in centroids],\n        }\n    )\n</code></pre>"},{"location":"api/grid/#gigaspatial.grid.MercatorTiles.to_geodataframe","title":"<code>to_geodataframe()</code>","text":"<p>Convert to GeoPandas GeoDataFrame.</p> Source code in <code>gigaspatial/grid/mercator_tiles.py</code> <pre><code>def to_geodataframe(self) -&gt; gpd.GeoDataFrame:\n    \"\"\"Convert to GeoPandas GeoDataFrame.\"\"\"\n    return gpd.GeoDataFrame(\n        {\"quadkey\": self.quadkeys, \"geometry\": self.to_geoms()}, crs=\"EPSG:4326\"\n    )\n</code></pre>"},{"location":"api/grid/#gigaspatial.grid.mercator_tiles","title":"<code>mercator_tiles</code>","text":""},{"location":"api/grid/#gigaspatial.grid.mercator_tiles.CountryMercatorTiles","title":"<code>CountryMercatorTiles</code>","text":"<p>               Bases: <code>MercatorTiles</code></p> <p>MercatorTiles specialized for country-level operations.</p> <p>This class extends MercatorTiles to work specifically with country boundaries. It can only be instantiated through the create() classmethod.</p> Source code in <code>gigaspatial/grid/mercator_tiles.py</code> <pre><code>class CountryMercatorTiles(MercatorTiles):\n    \"\"\"MercatorTiles specialized for country-level operations.\n\n    This class extends MercatorTiles to work specifically with country boundaries.\n    It can only be instantiated through the create() classmethod.\n    \"\"\"\n\n    country: str = Field(..., exclude=True)\n\n    def __init__(self, *args, **kwargs):\n        raise TypeError(\n            \"CountryMercatorTiles cannot be instantiated directly. \"\n            \"Use CountryMercatorTiles.create() instead.\"\n        )\n\n    @classmethod\n    def create(\n        cls,\n        country: str,\n        zoom_level: int,\n        predicate: str = \"intersects\",\n        data_store: Optional[DataStore] = None,\n        country_geom_path: Optional[Union[str, Path]] = None,\n    ):\n        \"\"\"Create CountryMercatorTiles for a specific country.\"\"\"\n        from gigaspatial.handlers.boundaries import AdminBoundaries\n\n        instance = super().__new__(cls)\n        super(CountryMercatorTiles, instance).__init__(\n            zoom_level=zoom_level,\n            quadkeys=[],\n            data_store=data_store or LocalDataStore(),\n            country=pycountry.countries.lookup(country).alpha_3,\n        )\n\n        country_geom = (\n            AdminBoundaries.create(\n                country_code=country,\n                data_store=data_store,\n                path=country_geom_path,\n            )\n            .boundaries[0]\n            .geometry\n        )\n\n        tiles = MercatorTiles.from_geometry(country_geom, zoom_level, predicate)\n\n        instance.quadkeys = tiles.quadkeys\n        return instance\n</code></pre>"},{"location":"api/grid/#gigaspatial.grid.mercator_tiles.CountryMercatorTiles.create","title":"<code>create(country, zoom_level, predicate='intersects', data_store=None, country_geom_path=None)</code>  <code>classmethod</code>","text":"<p>Create CountryMercatorTiles for a specific country.</p> Source code in <code>gigaspatial/grid/mercator_tiles.py</code> <pre><code>@classmethod\ndef create(\n    cls,\n    country: str,\n    zoom_level: int,\n    predicate: str = \"intersects\",\n    data_store: Optional[DataStore] = None,\n    country_geom_path: Optional[Union[str, Path]] = None,\n):\n    \"\"\"Create CountryMercatorTiles for a specific country.\"\"\"\n    from gigaspatial.handlers.boundaries import AdminBoundaries\n\n    instance = super().__new__(cls)\n    super(CountryMercatorTiles, instance).__init__(\n        zoom_level=zoom_level,\n        quadkeys=[],\n        data_store=data_store or LocalDataStore(),\n        country=pycountry.countries.lookup(country).alpha_3,\n    )\n\n    country_geom = (\n        AdminBoundaries.create(\n            country_code=country,\n            data_store=data_store,\n            path=country_geom_path,\n        )\n        .boundaries[0]\n        .geometry\n    )\n\n    tiles = MercatorTiles.from_geometry(country_geom, zoom_level, predicate)\n\n    instance.quadkeys = tiles.quadkeys\n    return instance\n</code></pre>"},{"location":"api/grid/#gigaspatial.grid.mercator_tiles.MercatorTiles","title":"<code>MercatorTiles</code>","text":"<p>               Bases: <code>BaseModel</code></p> Source code in <code>gigaspatial/grid/mercator_tiles.py</code> <pre><code>class MercatorTiles(BaseModel):\n    zoom_level: int = Field(..., ge=0, le=20)\n    quadkeys: List[str] = Field(default_factory=list)\n    data_store: DataStore = Field(default_factory=LocalDataStore, exclude=True)\n    logger: ClassVar = config.get_logger(\"MercatorTiles\")\n\n    class Config:\n        arbitrary_types_allowed = True\n\n    @classmethod\n    def from_quadkeys(cls, quadkeys: List[str]):\n        \"\"\"Create MercatorTiles from list of quadkeys.\"\"\"\n        if not quadkeys:\n            cls.logger.warning(\"No quadkeys provided to from_quadkeys.\")\n            return cls(zoom_level=0, quadkeys=[])\n        return cls(zoom_level=len(quadkeys[0]), quadkeys=set(quadkeys))\n\n    @classmethod\n    def from_bounds(\n        cls, xmin: float, ymin: float, xmax: float, ymax: float, zoom_level: int\n    ):\n        \"\"\"Create MercatorTiles from boundary coordinates.\"\"\"\n        cls.logger.info(\n            f\"Creating MercatorTiles from bounds: ({xmin}, {ymin}, {xmax}, {ymax}) at zoom level: {zoom_level}\"\n        )\n        return cls(\n            zoom_level=zoom_level,\n            quadkeys=[\n                mercantile.quadkey(tile)\n                for tile in mercantile.tiles(xmin, ymin, xmax, ymax, zoom_level)\n            ],\n        )\n\n    @classmethod\n    def from_spatial(\n        cls,\n        source: Union[\n            BaseGeometry,\n            gpd.GeoDataFrame,\n            List[Union[Point, Tuple[float, float]]],  # points\n        ],\n        zoom_level: int,\n        predicate: str = \"intersects\",\n        **kwargs,\n    ):\n        cls.logger.info(\n            f\"Creating MercatorTiles from spatial source (type: {type(source)}) at zoom level: {zoom_level} with predicate: {predicate}\"\n        )\n        if isinstance(source, gpd.GeoDataFrame):\n            if source.crs != \"EPSG:4326\":\n                source = source.to_crs(\"EPSG:4326\")\n            source = source.geometry.unary_union\n\n        if isinstance(source, BaseGeometry):\n            return cls.from_geometry(\n                geometry=source, zoom_level=zoom_level, predicate=predicate, **kwargs\n            )\n        elif isinstance(source, Iterable) and all(\n            len(pt) == 2 or isinstance(pt, Point) for pt in source\n        ):\n            return cls.from_points(geometry=source, zoom_level=zoom_level, **kwargs)\n        else:\n            raise\n\n    @classmethod\n    def from_geometry(\n        cls,\n        geometry: BaseGeometry,\n        zoom_level: int,\n        predicate: str = \"intersects\",\n        **kwargs,\n    ):\n        \"\"\"Create MercatorTiles from a polygon.\"\"\"\n        cls.logger.info(\n            f\"Creating MercatorTiles from geometry (bounds: {geometry.bounds}) at zoom level: {zoom_level} with predicate: {predicate}\"\n        )\n        tiles = list(mercantile.tiles(*geometry.bounds, zoom_level))\n        quadkeys_boxes = [\n            (mercantile.quadkey(t), box(*mercantile.bounds(t))) for t in tiles\n        ]\n        quadkeys, boxes = zip(*quadkeys_boxes) if quadkeys_boxes else ([], [])\n\n        if not boxes:\n            cls.logger.warning(\n                \"No boxes generated from geometry bounds. Returning empty MercatorTiles.\"\n            )\n            return MercatorTiles(zoom_level=zoom_level, quadkeys=[])\n\n        s = STRtree(boxes)\n        result_indices = s.query(geometry, predicate=predicate)\n        filtered_quadkeys = [quadkeys[i] for i in result_indices]\n        cls.logger.info(\n            f\"Filtered down to {len(filtered_quadkeys)} quadkeys using spatial predicate.\"\n        )\n        return cls(zoom_level=zoom_level, quadkeys=filtered_quadkeys, **kwargs)\n\n    @classmethod\n    def from_points(\n        cls, points: List[Union[Point, Tuple[float, float]]], zoom_level: int, **kwargs\n    ) -&gt; \"MercatorTiles\":\n        \"\"\"Create MercatorTiles from a list of points or lat-lon pairs.\"\"\"\n        cls.logger.info(\n            f\"Creating MercatorTiles from {len(points)} points at zoom level: {zoom_level}\"\n        )\n        quadkeys = {\n            (\n                mercantile.quadkey(mercantile.tile(p.x, p.y, zoom_level))\n                if isinstance(p, Point)\n                else mercantile.quadkey(mercantile.tile(p[1], p[0], zoom_level))\n            )\n            for p in points\n        }\n        cls.logger.info(f\"Generated {len(quadkeys)} unique quadkeys from points.\")\n        return cls(zoom_level=zoom_level, quadkeys=list(quadkeys), **kwargs)\n\n    @classmethod\n    def from_json(\n        cls, data_store: DataStore, file: Union[str, Path], **kwargs\n    ) -&gt; \"MercatorTiles\":\n        \"\"\"Load MercatorTiles from a JSON file.\"\"\"\n        cls.logger.info(\n            f\"Loading MercatorTiles from JSON file: {file} using data store: {type(data_store).__name__}\"\n        )\n        with data_store.open(str(file), \"r\") as f:\n            data = json.load(f)\n            if isinstance(data, list):  # If file contains only quadkeys\n                data = {\n                    \"zoom_level\": len(data[0]) if data else 0,\n                    \"quadkeys\": data,\n                    **kwargs,\n                }\n            else:\n                data.update(kwargs)\n            instance = cls(**data)\n            instance.data_store = data_store\n            cls.logger.info(\n                f\"Successfully loaded {len(instance.quadkeys)} quadkeys from JSON file.\"\n            )\n            return instance\n\n    def filter_quadkeys(self, quadkeys: Iterable[str]) -&gt; \"MercatorTiles\":\n        \"\"\"Filter quadkeys by a given set of quadkeys.\"\"\"\n        original_count = len(self.quadkeys)\n        incoming_count = len(\n            list(quadkeys)\n        )  # Convert to list to get length if it's an iterator\n\n        self.logger.info(\n            f\"Filtering {original_count} quadkeys with an incoming set of {incoming_count} quadkeys.\"\n        )\n        filtered_quadkeys = list(set(self.quadkeys) &amp; set(quadkeys))\n        self.logger.info(f\"Resulting in {len(filtered_quadkeys)} filtered quadkeys.\")\n        return MercatorTiles(\n            zoom_level=self.zoom_level,\n            quadkeys=filtered_quadkeys,\n        )\n\n    def to_dataframe(self) -&gt; pd.DataFrame:\n        \"\"\"Convert to pandas DataFrame with quadkey and centroid coordinates.\"\"\"\n        self.logger.info(\n            f\"Converting {len(self.quadkeys)} quadkeys to pandas DataFrame.\"\n        )\n        if not self.quadkeys:\n            self.logger.warning(\n                \"No quadkeys to convert to DataFrame. Returning empty DataFrame.\"\n            )\n            return pd.DataFrame(columns=[\"quadkey\", \"latitude\", \"longitude\"])\n        tiles_data = [mercantile.quadkey_to_tile(q) for q in self.quadkeys]\n        bounds_data = [mercantile.bounds(tile) for tile in tiles_data]\n\n        centroids = [\n            (\n                (bounds.south + bounds.north) / 2,  # latitude\n                (bounds.west + bounds.east) / 2,  # longitude\n            )\n            for bounds in bounds_data\n        ]\n\n        self.logger.info(f\"Successfully converted to DataFrame.\")\n\n        return pd.DataFrame(\n            {\n                \"quadkey\": self.quadkeys,\n                \"latitude\": [c[0] for c in centroids],\n                \"longitude\": [c[1] for c in centroids],\n            }\n        )\n\n    def to_geoms(self) -&gt; List[box]:\n        self.logger.info(\n            f\"Converting {len(self.quadkeys)} quadkeys to shapely box geometries.\"\n        )\n        return [\n            box(*mercantile.bounds(mercantile.quadkey_to_tile(q)))\n            for q in self.quadkeys\n        ]\n\n    def to_geodataframe(self) -&gt; gpd.GeoDataFrame:\n        \"\"\"Convert to GeoPandas GeoDataFrame.\"\"\"\n        return gpd.GeoDataFrame(\n            {\"quadkey\": self.quadkeys, \"geometry\": self.to_geoms()}, crs=\"EPSG:4326\"\n        )\n\n    def save(self, file: Union[str, Path], format: str = \"json\") -&gt; None:\n        \"\"\"Save MercatorTiles to file in specified format.\"\"\"\n        with self.data_store.open(str(file), \"wb\" if format == \"parquet\" else \"w\") as f:\n            if format == \"parquet\":\n                self.to_geodataframe().to_parquet(f, index=False)\n            elif format == \"geojson\":\n                f.write(self.to_geodataframe().to_json(drop_id=True))\n            elif format == \"json\":\n                json.dump(self.quadkeys, f)\n            else:\n                raise ValueError(f\"Unsupported format: {format}\")\n\n    def __len__(self) -&gt; int:\n        return len(self.quadkeys)\n</code></pre>"},{"location":"api/grid/#gigaspatial.grid.mercator_tiles.MercatorTiles.filter_quadkeys","title":"<code>filter_quadkeys(quadkeys)</code>","text":"<p>Filter quadkeys by a given set of quadkeys.</p> Source code in <code>gigaspatial/grid/mercator_tiles.py</code> <pre><code>def filter_quadkeys(self, quadkeys: Iterable[str]) -&gt; \"MercatorTiles\":\n    \"\"\"Filter quadkeys by a given set of quadkeys.\"\"\"\n    original_count = len(self.quadkeys)\n    incoming_count = len(\n        list(quadkeys)\n    )  # Convert to list to get length if it's an iterator\n\n    self.logger.info(\n        f\"Filtering {original_count} quadkeys with an incoming set of {incoming_count} quadkeys.\"\n    )\n    filtered_quadkeys = list(set(self.quadkeys) &amp; set(quadkeys))\n    self.logger.info(f\"Resulting in {len(filtered_quadkeys)} filtered quadkeys.\")\n    return MercatorTiles(\n        zoom_level=self.zoom_level,\n        quadkeys=filtered_quadkeys,\n    )\n</code></pre>"},{"location":"api/grid/#gigaspatial.grid.mercator_tiles.MercatorTiles.from_bounds","title":"<code>from_bounds(xmin, ymin, xmax, ymax, zoom_level)</code>  <code>classmethod</code>","text":"<p>Create MercatorTiles from boundary coordinates.</p> Source code in <code>gigaspatial/grid/mercator_tiles.py</code> <pre><code>@classmethod\ndef from_bounds(\n    cls, xmin: float, ymin: float, xmax: float, ymax: float, zoom_level: int\n):\n    \"\"\"Create MercatorTiles from boundary coordinates.\"\"\"\n    cls.logger.info(\n        f\"Creating MercatorTiles from bounds: ({xmin}, {ymin}, {xmax}, {ymax}) at zoom level: {zoom_level}\"\n    )\n    return cls(\n        zoom_level=zoom_level,\n        quadkeys=[\n            mercantile.quadkey(tile)\n            for tile in mercantile.tiles(xmin, ymin, xmax, ymax, zoom_level)\n        ],\n    )\n</code></pre>"},{"location":"api/grid/#gigaspatial.grid.mercator_tiles.MercatorTiles.from_geometry","title":"<code>from_geometry(geometry, zoom_level, predicate='intersects', **kwargs)</code>  <code>classmethod</code>","text":"<p>Create MercatorTiles from a polygon.</p> Source code in <code>gigaspatial/grid/mercator_tiles.py</code> <pre><code>@classmethod\ndef from_geometry(\n    cls,\n    geometry: BaseGeometry,\n    zoom_level: int,\n    predicate: str = \"intersects\",\n    **kwargs,\n):\n    \"\"\"Create MercatorTiles from a polygon.\"\"\"\n    cls.logger.info(\n        f\"Creating MercatorTiles from geometry (bounds: {geometry.bounds}) at zoom level: {zoom_level} with predicate: {predicate}\"\n    )\n    tiles = list(mercantile.tiles(*geometry.bounds, zoom_level))\n    quadkeys_boxes = [\n        (mercantile.quadkey(t), box(*mercantile.bounds(t))) for t in tiles\n    ]\n    quadkeys, boxes = zip(*quadkeys_boxes) if quadkeys_boxes else ([], [])\n\n    if not boxes:\n        cls.logger.warning(\n            \"No boxes generated from geometry bounds. Returning empty MercatorTiles.\"\n        )\n        return MercatorTiles(zoom_level=zoom_level, quadkeys=[])\n\n    s = STRtree(boxes)\n    result_indices = s.query(geometry, predicate=predicate)\n    filtered_quadkeys = [quadkeys[i] for i in result_indices]\n    cls.logger.info(\n        f\"Filtered down to {len(filtered_quadkeys)} quadkeys using spatial predicate.\"\n    )\n    return cls(zoom_level=zoom_level, quadkeys=filtered_quadkeys, **kwargs)\n</code></pre>"},{"location":"api/grid/#gigaspatial.grid.mercator_tiles.MercatorTiles.from_json","title":"<code>from_json(data_store, file, **kwargs)</code>  <code>classmethod</code>","text":"<p>Load MercatorTiles from a JSON file.</p> Source code in <code>gigaspatial/grid/mercator_tiles.py</code> <pre><code>@classmethod\ndef from_json(\n    cls, data_store: DataStore, file: Union[str, Path], **kwargs\n) -&gt; \"MercatorTiles\":\n    \"\"\"Load MercatorTiles from a JSON file.\"\"\"\n    cls.logger.info(\n        f\"Loading MercatorTiles from JSON file: {file} using data store: {type(data_store).__name__}\"\n    )\n    with data_store.open(str(file), \"r\") as f:\n        data = json.load(f)\n        if isinstance(data, list):  # If file contains only quadkeys\n            data = {\n                \"zoom_level\": len(data[0]) if data else 0,\n                \"quadkeys\": data,\n                **kwargs,\n            }\n        else:\n            data.update(kwargs)\n        instance = cls(**data)\n        instance.data_store = data_store\n        cls.logger.info(\n            f\"Successfully loaded {len(instance.quadkeys)} quadkeys from JSON file.\"\n        )\n        return instance\n</code></pre>"},{"location":"api/grid/#gigaspatial.grid.mercator_tiles.MercatorTiles.from_points","title":"<code>from_points(points, zoom_level, **kwargs)</code>  <code>classmethod</code>","text":"<p>Create MercatorTiles from a list of points or lat-lon pairs.</p> Source code in <code>gigaspatial/grid/mercator_tiles.py</code> <pre><code>@classmethod\ndef from_points(\n    cls, points: List[Union[Point, Tuple[float, float]]], zoom_level: int, **kwargs\n) -&gt; \"MercatorTiles\":\n    \"\"\"Create MercatorTiles from a list of points or lat-lon pairs.\"\"\"\n    cls.logger.info(\n        f\"Creating MercatorTiles from {len(points)} points at zoom level: {zoom_level}\"\n    )\n    quadkeys = {\n        (\n            mercantile.quadkey(mercantile.tile(p.x, p.y, zoom_level))\n            if isinstance(p, Point)\n            else mercantile.quadkey(mercantile.tile(p[1], p[0], zoom_level))\n        )\n        for p in points\n    }\n    cls.logger.info(f\"Generated {len(quadkeys)} unique quadkeys from points.\")\n    return cls(zoom_level=zoom_level, quadkeys=list(quadkeys), **kwargs)\n</code></pre>"},{"location":"api/grid/#gigaspatial.grid.mercator_tiles.MercatorTiles.from_quadkeys","title":"<code>from_quadkeys(quadkeys)</code>  <code>classmethod</code>","text":"<p>Create MercatorTiles from list of quadkeys.</p> Source code in <code>gigaspatial/grid/mercator_tiles.py</code> <pre><code>@classmethod\ndef from_quadkeys(cls, quadkeys: List[str]):\n    \"\"\"Create MercatorTiles from list of quadkeys.\"\"\"\n    if not quadkeys:\n        cls.logger.warning(\"No quadkeys provided to from_quadkeys.\")\n        return cls(zoom_level=0, quadkeys=[])\n    return cls(zoom_level=len(quadkeys[0]), quadkeys=set(quadkeys))\n</code></pre>"},{"location":"api/grid/#gigaspatial.grid.mercator_tiles.MercatorTiles.save","title":"<code>save(file, format='json')</code>","text":"<p>Save MercatorTiles to file in specified format.</p> Source code in <code>gigaspatial/grid/mercator_tiles.py</code> <pre><code>def save(self, file: Union[str, Path], format: str = \"json\") -&gt; None:\n    \"\"\"Save MercatorTiles to file in specified format.\"\"\"\n    with self.data_store.open(str(file), \"wb\" if format == \"parquet\" else \"w\") as f:\n        if format == \"parquet\":\n            self.to_geodataframe().to_parquet(f, index=False)\n        elif format == \"geojson\":\n            f.write(self.to_geodataframe().to_json(drop_id=True))\n        elif format == \"json\":\n            json.dump(self.quadkeys, f)\n        else:\n            raise ValueError(f\"Unsupported format: {format}\")\n</code></pre>"},{"location":"api/grid/#gigaspatial.grid.mercator_tiles.MercatorTiles.to_dataframe","title":"<code>to_dataframe()</code>","text":"<p>Convert to pandas DataFrame with quadkey and centroid coordinates.</p> Source code in <code>gigaspatial/grid/mercator_tiles.py</code> <pre><code>def to_dataframe(self) -&gt; pd.DataFrame:\n    \"\"\"Convert to pandas DataFrame with quadkey and centroid coordinates.\"\"\"\n    self.logger.info(\n        f\"Converting {len(self.quadkeys)} quadkeys to pandas DataFrame.\"\n    )\n    if not self.quadkeys:\n        self.logger.warning(\n            \"No quadkeys to convert to DataFrame. Returning empty DataFrame.\"\n        )\n        return pd.DataFrame(columns=[\"quadkey\", \"latitude\", \"longitude\"])\n    tiles_data = [mercantile.quadkey_to_tile(q) for q in self.quadkeys]\n    bounds_data = [mercantile.bounds(tile) for tile in tiles_data]\n\n    centroids = [\n        (\n            (bounds.south + bounds.north) / 2,  # latitude\n            (bounds.west + bounds.east) / 2,  # longitude\n        )\n        for bounds in bounds_data\n    ]\n\n    self.logger.info(f\"Successfully converted to DataFrame.\")\n\n    return pd.DataFrame(\n        {\n            \"quadkey\": self.quadkeys,\n            \"latitude\": [c[0] for c in centroids],\n            \"longitude\": [c[1] for c in centroids],\n        }\n    )\n</code></pre>"},{"location":"api/grid/#gigaspatial.grid.mercator_tiles.MercatorTiles.to_geodataframe","title":"<code>to_geodataframe()</code>","text":"<p>Convert to GeoPandas GeoDataFrame.</p> Source code in <code>gigaspatial/grid/mercator_tiles.py</code> <pre><code>def to_geodataframe(self) -&gt; gpd.GeoDataFrame:\n    \"\"\"Convert to GeoPandas GeoDataFrame.\"\"\"\n    return gpd.GeoDataFrame(\n        {\"quadkey\": self.quadkeys, \"geometry\": self.to_geoms()}, crs=\"EPSG:4326\"\n    )\n</code></pre>"},{"location":"api/handlers/","title":"Handlers Module","text":""},{"location":"api/handlers/#gigaspatial.handlers","title":"<code>gigaspatial.handlers</code>","text":""},{"location":"api/handlers/#gigaspatial.handlers.base","title":"<code>base</code>","text":""},{"location":"api/handlers/#gigaspatial.handlers.base.BaseHandler","title":"<code>BaseHandler</code>","text":"<p>               Bases: <code>ABC</code></p> <p>Abstract base class that orchestrates configuration, downloading, and reading functionality.</p> <p>This class serves as the main entry point for dataset handlers, providing a unified interface for data acquisition and loading. It manages the lifecycle of config, downloader, and reader components.</p> <p>Subclasses should implement the abstract methods to provide specific handler types and define how components are created and interact.</p> Source code in <code>gigaspatial/handlers/base.py</code> <pre><code>class BaseHandler(ABC):\n    \"\"\"\n    Abstract base class that orchestrates configuration, downloading, and reading functionality.\n\n    This class serves as the main entry point for dataset handlers, providing a unified\n    interface for data acquisition and loading. It manages the lifecycle of config,\n    downloader, and reader components.\n\n    Subclasses should implement the abstract methods to provide specific handler types\n    and define how components are created and interact.\n    \"\"\"\n\n    def __init__(\n        self,\n        config: Optional[BaseHandlerConfig] = None,\n        downloader: Optional[BaseHandlerDownloader] = None,\n        reader: Optional[BaseHandlerReader] = None,\n        data_store: Optional[DataStore] = None,\n        logger: Optional[logging.Logger] = None,\n    ):\n        \"\"\"\n        Initialize the BaseHandler with optional components.\n\n        Args:\n            config: Configuration object. If None, will be created via create_config()\n            downloader: Downloader instance. If None, will be created via create_downloader()\n            reader: Reader instance. If None, will be created via create_reader()\n            data_store: Data store instance. Defaults to LocalDataStore if not provided\n            logger: Logger instance. If not provided, creates one based on class name\n        \"\"\"\n        # Initialize data store first as it's used by other components\n        self.data_store = data_store or LocalDataStore()\n\n        # Initialize logger\n        self.logger = logger or global_config.get_logger(self.__class__.__name__)\n\n        # Initialize or create config\n        self._config = config\n        if self._config is None:\n            self._config = self.create_config(\n                data_store=self.data_store, logger=self.logger\n            )\n\n        # Initialize or create downloader\n        self._downloader = downloader\n        if self._downloader is None:\n            self._downloader = self.create_downloader(\n                config=self._config, data_store=self.data_store, logger=self.logger\n            )\n\n        # Initialize or create reader\n        self._reader = reader\n        if self._reader is None:\n            self._reader = self.create_reader(\n                config=self._config, data_store=self.data_store, logger=self.logger\n            )\n\n    @property\n    def config(self) -&gt; BaseHandlerConfig:\n        \"\"\"Get the configuration object.\"\"\"\n        return self._config\n\n    @property\n    def downloader(self) -&gt; BaseHandlerDownloader:\n        \"\"\"Get the downloader object.\"\"\"\n        return self._downloader\n\n    @property\n    def reader(self) -&gt; BaseHandlerReader:\n        \"\"\"Get the reader object.\"\"\"\n        return self._reader\n\n    # Abstract factory methods for creating components\n    @abstractmethod\n    def create_config(\n        self, data_store: DataStore, logger: logging.Logger, **kwargs\n    ) -&gt; BaseHandlerConfig:\n        \"\"\"\n        Create and return a configuration object for this handler.\n\n        Args:\n            data_store: The data store instance to use\n            logger: The logger instance to use\n            **kwargs: Additional configuration parameters\n\n        Returns:\n            Configured BaseHandlerConfig instance\n        \"\"\"\n        pass\n\n    @abstractmethod\n    def create_downloader(\n        self,\n        config: BaseHandlerConfig,\n        data_store: DataStore,\n        logger: logging.Logger,\n        **kwargs,\n    ) -&gt; BaseHandlerDownloader:\n        \"\"\"\n        Create and return a downloader object for this handler.\n\n        Args:\n            config: The configuration object\n            data_store: The data store instance to use\n            logger: The logger instance to use\n            **kwargs: Additional downloader parameters\n\n        Returns:\n            Configured BaseHandlerDownloader instance\n        \"\"\"\n        pass\n\n    @abstractmethod\n    def create_reader(\n        self,\n        config: BaseHandlerConfig,\n        data_store: DataStore,\n        logger: logging.Logger,\n        **kwargs,\n    ) -&gt; BaseHandlerReader:\n        \"\"\"\n        Create and return a reader object for this handler.\n\n        Args:\n            config: The configuration object\n            data_store: The data store instance to use\n            logger: The logger instance to use\n            **kwargs: Additional reader parameters\n\n        Returns:\n            Configured BaseHandlerReader instance\n        \"\"\"\n        pass\n\n    # High-level interface methods\n    def ensure_data_available(\n        self,\n        source: Union[\n            str,  # country\n            List[Union[tuple, Point]],  # points\n            BaseGeometry,  # geometry\n            gpd.GeoDataFrame,  # geodataframe\n            Path,  # path\n            List[Union[str, Path]],  # list of paths\n        ],\n        force_download: bool = False,\n        **kwargs,\n    ) -&gt; bool:\n        \"\"\"\n        Ensure that data is available for the given source.\n\n        This method checks if the required data exists locally, and if not (or if\n        force_download is True), downloads it using the downloader.\n\n        Args:\n            source: The data source specification\n            force_download: If True, download even if data exists locally\n            **kwargs: Additional parameters passed to download methods\n\n        Returns:\n            bool: True if data is available after this operation\n        \"\"\"\n        try:\n            # Resolve what data units are needed\n            if hasattr(self.config, \"get_relevant_data_units\"):\n                data_units = self.config.get_relevant_data_units(source, **kwargs)\n                data_paths = self.config.get_data_unit_paths(data_units, **kwargs)\n            else:\n                # Fallback: try to resolve paths directly\n                if hasattr(self.reader, \"resolve_source_paths\"):\n                    data_paths = self.reader.resolve_source_paths(source, **kwargs)\n                else:\n                    self.logger.warning(\"Cannot determine required data paths\")\n                    return False\n\n            # Check if data exists (unless force download)\n            if not force_download:\n                missing_paths = [\n                    path\n                    for path in data_paths\n                    if not self.data_store.file_exists(str(path))\n                ]\n                if not missing_paths:\n                    self.logger.info(\"All required data is already available\")\n                    return True\n\n            # Download missing or all data\n            if hasattr(self.config, \"get_relevant_data_units\"):\n                data_units = self.config.get_relevant_data_units(source, **kwargs)\n                self.downloader.download_data_units(data_units, **kwargs)\n            else:\n                self.downloader.download(source, **kwargs)\n\n            return True\n\n        except Exception as e:\n            self.logger.error(f\"Failed to ensure data availability: {e}\")\n            return False\n\n    def load_data(\n        self,\n        source: Union[\n            str,  # country\n            List[Union[tuple, Point]],  # points\n            BaseGeometry,  # geometry\n            gpd.GeoDataFrame,  # geodataframe\n            Path,  # path\n            List[Union[str, Path]],  # list of paths\n        ],\n        ensure_available: bool = True,\n        **kwargs,\n    ) -&gt; Any:\n        \"\"\"\n        Load data from the given source.\n\n        Args:\n            source: The data source specification\n            ensure_available: If True, ensure data is downloaded before loading\n            **kwargs: Additional parameters passed to load methods\n\n        Returns:\n            Loaded data (type depends on specific handler implementation)\n        \"\"\"\n        if ensure_available:\n            if not self.ensure_data_available(source, **kwargs):\n                raise RuntimeError(\"Could not ensure data availability for loading\")\n\n        return self.reader.load(source, **kwargs)\n\n    def download_and_load(\n        self,\n        source: Union[\n            str,  # country\n            List[Union[tuple, Point]],  # points\n            BaseGeometry,  # geometry\n            gpd.GeoDataFrame,  # geodataframe\n            Path,  # path\n            List[Union[str, Path]],  # list of paths\n        ],\n        force_download: bool = False,\n        **kwargs,\n    ) -&gt; Any:\n        \"\"\"\n        Convenience method to download (if needed) and load data in one call.\n\n        Args:\n            source: The data source specification\n            force_download: If True, download even if data exists locally\n            **kwargs: Additional parameters\n\n        Returns:\n            Loaded data\n        \"\"\"\n        self.ensure_data_available(source, force_download=force_download, **kwargs)\n        return self.reader.load(source, **kwargs)\n\n    def get_available_data_info(\n        self,\n        source: Union[\n            str,  # country\n            List[Union[tuple, Point]],  # points\n            BaseGeometry,  # geometry\n            gpd.GeoDataFrame,  # geodataframe\n        ],\n        **kwargs,\n    ) -&gt; dict:\n        \"\"\"\n        Get information about available data for the given source.\n\n        Args:\n            source: The data source specification\n            **kwargs: Additional parameters\n\n        Returns:\n            dict: Information about data availability, paths, etc.\n        \"\"\"\n        try:\n            if hasattr(self.config, \"get_relevant_data_units\"):\n                data_units = self.config.get_relevant_data_units(source, **kwargs)\n                data_paths = self.config.get_data_unit_paths(data_units, **kwargs)\n            else:\n                data_paths = self.reader.resolve_source_paths(source, **kwargs)\n\n            existing_paths = [\n                path for path in data_paths if self.data_store.file_exists(str(path))\n            ]\n            missing_paths = [\n                path\n                for path in data_paths\n                if not self.data_store.file_exists(str(path))\n            ]\n\n            return {\n                \"total_data_units\": len(data_paths),\n                \"available_data_units\": len(existing_paths),\n                \"missing_data_units\": len(missing_paths),\n                \"available_paths\": existing_paths,\n                \"missing_paths\": missing_paths,\n                \"all_available\": len(missing_paths) == 0,\n            }\n\n        except Exception as e:\n            self.logger.error(f\"Failed to get data info: {e}\")\n            return {\n                \"error\": str(e),\n                \"total_data_units\": 0,\n                \"available_data_units\": 0,\n                \"missing_data_units\": 0,\n                \"available_paths\": [],\n                \"missing_paths\": [],\n                \"all_available\": False,\n            }\n\n    def cleanup(self):\n        \"\"\"\n        Cleanup resources used by the handler.\n\n        Override in subclasses if specific cleanup is needed.\n        \"\"\"\n        self.logger.info(f\"Cleaning up {self.__class__.__name__}\")\n        # Subclasses can override to add specific cleanup logic\n\n    def __enter__(self):\n        \"\"\"Context manager entry.\"\"\"\n        return self\n\n    def __exit__(self, exc_type, exc_val, exc_tb):\n        \"\"\"Context manager exit.\"\"\"\n        self.cleanup()\n\n    def __repr__(self) -&gt; str:\n        \"\"\"String representation of the handler.\"\"\"\n        return (\n            f\"{self.__class__.__name__}(\"\n            f\"config={self.config.__class__.__name__}, \"\n            f\"downloader={self.downloader.__class__.__name__}, \"\n            f\"reader={self.reader.__class__.__name__})\"\n        )\n</code></pre>"},{"location":"api/handlers/#gigaspatial.handlers.base.BaseHandler.config","title":"<code>config: BaseHandlerConfig</code>  <code>property</code>","text":"<p>Get the configuration object.</p>"},{"location":"api/handlers/#gigaspatial.handlers.base.BaseHandler.downloader","title":"<code>downloader: BaseHandlerDownloader</code>  <code>property</code>","text":"<p>Get the downloader object.</p>"},{"location":"api/handlers/#gigaspatial.handlers.base.BaseHandler.reader","title":"<code>reader: BaseHandlerReader</code>  <code>property</code>","text":"<p>Get the reader object.</p>"},{"location":"api/handlers/#gigaspatial.handlers.base.BaseHandler.__enter__","title":"<code>__enter__()</code>","text":"<p>Context manager entry.</p> Source code in <code>gigaspatial/handlers/base.py</code> <pre><code>def __enter__(self):\n    \"\"\"Context manager entry.\"\"\"\n    return self\n</code></pre>"},{"location":"api/handlers/#gigaspatial.handlers.base.BaseHandler.__exit__","title":"<code>__exit__(exc_type, exc_val, exc_tb)</code>","text":"<p>Context manager exit.</p> Source code in <code>gigaspatial/handlers/base.py</code> <pre><code>def __exit__(self, exc_type, exc_val, exc_tb):\n    \"\"\"Context manager exit.\"\"\"\n    self.cleanup()\n</code></pre>"},{"location":"api/handlers/#gigaspatial.handlers.base.BaseHandler.__init__","title":"<code>__init__(config=None, downloader=None, reader=None, data_store=None, logger=None)</code>","text":"<p>Initialize the BaseHandler with optional components.</p> <p>Parameters:</p> Name Type Description Default <code>config</code> <code>Optional[BaseHandlerConfig]</code> <p>Configuration object. If None, will be created via create_config()</p> <code>None</code> <code>downloader</code> <code>Optional[BaseHandlerDownloader]</code> <p>Downloader instance. If None, will be created via create_downloader()</p> <code>None</code> <code>reader</code> <code>Optional[BaseHandlerReader]</code> <p>Reader instance. If None, will be created via create_reader()</p> <code>None</code> <code>data_store</code> <code>Optional[DataStore]</code> <p>Data store instance. Defaults to LocalDataStore if not provided</p> <code>None</code> <code>logger</code> <code>Optional[Logger]</code> <p>Logger instance. If not provided, creates one based on class name</p> <code>None</code> Source code in <code>gigaspatial/handlers/base.py</code> <pre><code>def __init__(\n    self,\n    config: Optional[BaseHandlerConfig] = None,\n    downloader: Optional[BaseHandlerDownloader] = None,\n    reader: Optional[BaseHandlerReader] = None,\n    data_store: Optional[DataStore] = None,\n    logger: Optional[logging.Logger] = None,\n):\n    \"\"\"\n    Initialize the BaseHandler with optional components.\n\n    Args:\n        config: Configuration object. If None, will be created via create_config()\n        downloader: Downloader instance. If None, will be created via create_downloader()\n        reader: Reader instance. If None, will be created via create_reader()\n        data_store: Data store instance. Defaults to LocalDataStore if not provided\n        logger: Logger instance. If not provided, creates one based on class name\n    \"\"\"\n    # Initialize data store first as it's used by other components\n    self.data_store = data_store or LocalDataStore()\n\n    # Initialize logger\n    self.logger = logger or global_config.get_logger(self.__class__.__name__)\n\n    # Initialize or create config\n    self._config = config\n    if self._config is None:\n        self._config = self.create_config(\n            data_store=self.data_store, logger=self.logger\n        )\n\n    # Initialize or create downloader\n    self._downloader = downloader\n    if self._downloader is None:\n        self._downloader = self.create_downloader(\n            config=self._config, data_store=self.data_store, logger=self.logger\n        )\n\n    # Initialize or create reader\n    self._reader = reader\n    if self._reader is None:\n        self._reader = self.create_reader(\n            config=self._config, data_store=self.data_store, logger=self.logger\n        )\n</code></pre>"},{"location":"api/handlers/#gigaspatial.handlers.base.BaseHandler.__repr__","title":"<code>__repr__()</code>","text":"<p>String representation of the handler.</p> Source code in <code>gigaspatial/handlers/base.py</code> <pre><code>def __repr__(self) -&gt; str:\n    \"\"\"String representation of the handler.\"\"\"\n    return (\n        f\"{self.__class__.__name__}(\"\n        f\"config={self.config.__class__.__name__}, \"\n        f\"downloader={self.downloader.__class__.__name__}, \"\n        f\"reader={self.reader.__class__.__name__})\"\n    )\n</code></pre>"},{"location":"api/handlers/#gigaspatial.handlers.base.BaseHandler.cleanup","title":"<code>cleanup()</code>","text":"<p>Cleanup resources used by the handler.</p> <p>Override in subclasses if specific cleanup is needed.</p> Source code in <code>gigaspatial/handlers/base.py</code> <pre><code>def cleanup(self):\n    \"\"\"\n    Cleanup resources used by the handler.\n\n    Override in subclasses if specific cleanup is needed.\n    \"\"\"\n    self.logger.info(f\"Cleaning up {self.__class__.__name__}\")\n</code></pre>"},{"location":"api/handlers/#gigaspatial.handlers.base.BaseHandler.create_config","title":"<code>create_config(data_store, logger, **kwargs)</code>  <code>abstractmethod</code>","text":"<p>Create and return a configuration object for this handler.</p> <p>Parameters:</p> Name Type Description Default <code>data_store</code> <code>DataStore</code> <p>The data store instance to use</p> required <code>logger</code> <code>Logger</code> <p>The logger instance to use</p> required <code>**kwargs</code> <p>Additional configuration parameters</p> <code>{}</code> <p>Returns:</p> Type Description <code>BaseHandlerConfig</code> <p>Configured BaseHandlerConfig instance</p> Source code in <code>gigaspatial/handlers/base.py</code> <pre><code>@abstractmethod\ndef create_config(\n    self, data_store: DataStore, logger: logging.Logger, **kwargs\n) -&gt; BaseHandlerConfig:\n    \"\"\"\n    Create and return a configuration object for this handler.\n\n    Args:\n        data_store: The data store instance to use\n        logger: The logger instance to use\n        **kwargs: Additional configuration parameters\n\n    Returns:\n        Configured BaseHandlerConfig instance\n    \"\"\"\n    pass\n</code></pre>"},{"location":"api/handlers/#gigaspatial.handlers.base.BaseHandler.create_downloader","title":"<code>create_downloader(config, data_store, logger, **kwargs)</code>  <code>abstractmethod</code>","text":"<p>Create and return a downloader object for this handler.</p> <p>Parameters:</p> Name Type Description Default <code>config</code> <code>BaseHandlerConfig</code> <p>The configuration object</p> required <code>data_store</code> <code>DataStore</code> <p>The data store instance to use</p> required <code>logger</code> <code>Logger</code> <p>The logger instance to use</p> required <code>**kwargs</code> <p>Additional downloader parameters</p> <code>{}</code> <p>Returns:</p> Type Description <code>BaseHandlerDownloader</code> <p>Configured BaseHandlerDownloader instance</p> Source code in <code>gigaspatial/handlers/base.py</code> <pre><code>@abstractmethod\ndef create_downloader(\n    self,\n    config: BaseHandlerConfig,\n    data_store: DataStore,\n    logger: logging.Logger,\n    **kwargs,\n) -&gt; BaseHandlerDownloader:\n    \"\"\"\n    Create and return a downloader object for this handler.\n\n    Args:\n        config: The configuration object\n        data_store: The data store instance to use\n        logger: The logger instance to use\n        **kwargs: Additional downloader parameters\n\n    Returns:\n        Configured BaseHandlerDownloader instance\n    \"\"\"\n    pass\n</code></pre>"},{"location":"api/handlers/#gigaspatial.handlers.base.BaseHandler.create_reader","title":"<code>create_reader(config, data_store, logger, **kwargs)</code>  <code>abstractmethod</code>","text":"<p>Create and return a reader object for this handler.</p> <p>Parameters:</p> Name Type Description Default <code>config</code> <code>BaseHandlerConfig</code> <p>The configuration object</p> required <code>data_store</code> <code>DataStore</code> <p>The data store instance to use</p> required <code>logger</code> <code>Logger</code> <p>The logger instance to use</p> required <code>**kwargs</code> <p>Additional reader parameters</p> <code>{}</code> <p>Returns:</p> Type Description <code>BaseHandlerReader</code> <p>Configured BaseHandlerReader instance</p> Source code in <code>gigaspatial/handlers/base.py</code> <pre><code>@abstractmethod\ndef create_reader(\n    self,\n    config: BaseHandlerConfig,\n    data_store: DataStore,\n    logger: logging.Logger,\n    **kwargs,\n) -&gt; BaseHandlerReader:\n    \"\"\"\n    Create and return a reader object for this handler.\n\n    Args:\n        config: The configuration object\n        data_store: The data store instance to use\n        logger: The logger instance to use\n        **kwargs: Additional reader parameters\n\n    Returns:\n        Configured BaseHandlerReader instance\n    \"\"\"\n    pass\n</code></pre>"},{"location":"api/handlers/#gigaspatial.handlers.base.BaseHandler.download_and_load","title":"<code>download_and_load(source, force_download=False, **kwargs)</code>","text":"<p>Convenience method to download (if needed) and load data in one call.</p> <p>Parameters:</p> Name Type Description Default <code>source</code> <code>Union[str, List[Union[tuple, Point]], BaseGeometry, GeoDataFrame, Path, List[Union[str, Path]]]</code> <p>The data source specification</p> required <code>force_download</code> <code>bool</code> <p>If True, download even if data exists locally</p> <code>False</code> <code>**kwargs</code> <p>Additional parameters</p> <code>{}</code> <p>Returns:</p> Type Description <code>Any</code> <p>Loaded data</p> Source code in <code>gigaspatial/handlers/base.py</code> <pre><code>def download_and_load(\n    self,\n    source: Union[\n        str,  # country\n        List[Union[tuple, Point]],  # points\n        BaseGeometry,  # geometry\n        gpd.GeoDataFrame,  # geodataframe\n        Path,  # path\n        List[Union[str, Path]],  # list of paths\n    ],\n    force_download: bool = False,\n    **kwargs,\n) -&gt; Any:\n    \"\"\"\n    Convenience method to download (if needed) and load data in one call.\n\n    Args:\n        source: The data source specification\n        force_download: If True, download even if data exists locally\n        **kwargs: Additional parameters\n\n    Returns:\n        Loaded data\n    \"\"\"\n    self.ensure_data_available(source, force_download=force_download, **kwargs)\n    return self.reader.load(source, **kwargs)\n</code></pre>"},{"location":"api/handlers/#gigaspatial.handlers.base.BaseHandler.ensure_data_available","title":"<code>ensure_data_available(source, force_download=False, **kwargs)</code>","text":"<p>Ensure that data is available for the given source.</p> <p>This method checks if the required data exists locally, and if not (or if force_download is True), downloads it using the downloader.</p> <p>Parameters:</p> Name Type Description Default <code>source</code> <code>Union[str, List[Union[tuple, Point]], BaseGeometry, GeoDataFrame, Path, List[Union[str, Path]]]</code> <p>The data source specification</p> required <code>force_download</code> <code>bool</code> <p>If True, download even if data exists locally</p> <code>False</code> <code>**kwargs</code> <p>Additional parameters passed to download methods</p> <code>{}</code> <p>Returns:</p> Name Type Description <code>bool</code> <code>bool</code> <p>True if data is available after this operation</p> Source code in <code>gigaspatial/handlers/base.py</code> <pre><code>def ensure_data_available(\n    self,\n    source: Union[\n        str,  # country\n        List[Union[tuple, Point]],  # points\n        BaseGeometry,  # geometry\n        gpd.GeoDataFrame,  # geodataframe\n        Path,  # path\n        List[Union[str, Path]],  # list of paths\n    ],\n    force_download: bool = False,\n    **kwargs,\n) -&gt; bool:\n    \"\"\"\n    Ensure that data is available for the given source.\n\n    This method checks if the required data exists locally, and if not (or if\n    force_download is True), downloads it using the downloader.\n\n    Args:\n        source: The data source specification\n        force_download: If True, download even if data exists locally\n        **kwargs: Additional parameters passed to download methods\n\n    Returns:\n        bool: True if data is available after this operation\n    \"\"\"\n    try:\n        # Resolve what data units are needed\n        if hasattr(self.config, \"get_relevant_data_units\"):\n            data_units = self.config.get_relevant_data_units(source, **kwargs)\n            data_paths = self.config.get_data_unit_paths(data_units, **kwargs)\n        else:\n            # Fallback: try to resolve paths directly\n            if hasattr(self.reader, \"resolve_source_paths\"):\n                data_paths = self.reader.resolve_source_paths(source, **kwargs)\n            else:\n                self.logger.warning(\"Cannot determine required data paths\")\n                return False\n\n        # Check if data exists (unless force download)\n        if not force_download:\n            missing_paths = [\n                path\n                for path in data_paths\n                if not self.data_store.file_exists(str(path))\n            ]\n            if not missing_paths:\n                self.logger.info(\"All required data is already available\")\n                return True\n\n        # Download missing or all data\n        if hasattr(self.config, \"get_relevant_data_units\"):\n            data_units = self.config.get_relevant_data_units(source, **kwargs)\n            self.downloader.download_data_units(data_units, **kwargs)\n        else:\n            self.downloader.download(source, **kwargs)\n\n        return True\n\n    except Exception as e:\n        self.logger.error(f\"Failed to ensure data availability: {e}\")\n        return False\n</code></pre>"},{"location":"api/handlers/#gigaspatial.handlers.base.BaseHandler.get_available_data_info","title":"<code>get_available_data_info(source, **kwargs)</code>","text":"<p>Get information about available data for the given source.</p> <p>Parameters:</p> Name Type Description Default <code>source</code> <code>Union[str, List[Union[tuple, Point]], BaseGeometry, GeoDataFrame]</code> <p>The data source specification</p> required <code>**kwargs</code> <p>Additional parameters</p> <code>{}</code> <p>Returns:</p> Name Type Description <code>dict</code> <code>dict</code> <p>Information about data availability, paths, etc.</p> Source code in <code>gigaspatial/handlers/base.py</code> <pre><code>def get_available_data_info(\n    self,\n    source: Union[\n        str,  # country\n        List[Union[tuple, Point]],  # points\n        BaseGeometry,  # geometry\n        gpd.GeoDataFrame,  # geodataframe\n    ],\n    **kwargs,\n) -&gt; dict:\n    \"\"\"\n    Get information about available data for the given source.\n\n    Args:\n        source: The data source specification\n        **kwargs: Additional parameters\n\n    Returns:\n        dict: Information about data availability, paths, etc.\n    \"\"\"\n    try:\n        if hasattr(self.config, \"get_relevant_data_units\"):\n            data_units = self.config.get_relevant_data_units(source, **kwargs)\n            data_paths = self.config.get_data_unit_paths(data_units, **kwargs)\n        else:\n            data_paths = self.reader.resolve_source_paths(source, **kwargs)\n\n        existing_paths = [\n            path for path in data_paths if self.data_store.file_exists(str(path))\n        ]\n        missing_paths = [\n            path\n            for path in data_paths\n            if not self.data_store.file_exists(str(path))\n        ]\n\n        return {\n            \"total_data_units\": len(data_paths),\n            \"available_data_units\": len(existing_paths),\n            \"missing_data_units\": len(missing_paths),\n            \"available_paths\": existing_paths,\n            \"missing_paths\": missing_paths,\n            \"all_available\": len(missing_paths) == 0,\n        }\n\n    except Exception as e:\n        self.logger.error(f\"Failed to get data info: {e}\")\n        return {\n            \"error\": str(e),\n            \"total_data_units\": 0,\n            \"available_data_units\": 0,\n            \"missing_data_units\": 0,\n            \"available_paths\": [],\n            \"missing_paths\": [],\n            \"all_available\": False,\n        }\n</code></pre>"},{"location":"api/handlers/#gigaspatial.handlers.base.BaseHandler.load_data","title":"<code>load_data(source, ensure_available=True, **kwargs)</code>","text":"<p>Load data from the given source.</p> <p>Parameters:</p> Name Type Description Default <code>source</code> <code>Union[str, List[Union[tuple, Point]], BaseGeometry, GeoDataFrame, Path, List[Union[str, Path]]]</code> <p>The data source specification</p> required <code>ensure_available</code> <code>bool</code> <p>If True, ensure data is downloaded before loading</p> <code>True</code> <code>**kwargs</code> <p>Additional parameters passed to load methods</p> <code>{}</code> <p>Returns:</p> Type Description <code>Any</code> <p>Loaded data (type depends on specific handler implementation)</p> Source code in <code>gigaspatial/handlers/base.py</code> <pre><code>def load_data(\n    self,\n    source: Union[\n        str,  # country\n        List[Union[tuple, Point]],  # points\n        BaseGeometry,  # geometry\n        gpd.GeoDataFrame,  # geodataframe\n        Path,  # path\n        List[Union[str, Path]],  # list of paths\n    ],\n    ensure_available: bool = True,\n    **kwargs,\n) -&gt; Any:\n    \"\"\"\n    Load data from the given source.\n\n    Args:\n        source: The data source specification\n        ensure_available: If True, ensure data is downloaded before loading\n        **kwargs: Additional parameters passed to load methods\n\n    Returns:\n        Loaded data (type depends on specific handler implementation)\n    \"\"\"\n    if ensure_available:\n        if not self.ensure_data_available(source, **kwargs):\n            raise RuntimeError(\"Could not ensure data availability for loading\")\n\n    return self.reader.load(source, **kwargs)\n</code></pre>"},{"location":"api/handlers/#gigaspatial.handlers.base.BaseHandlerConfig","title":"<code>BaseHandlerConfig</code>  <code>dataclass</code>","text":"<p>               Bases: <code>ABC</code></p> <p>Abstract base class for handler configuration objects. Provides standard fields for path, parallelism, data store, and logger. Extend this class for dataset-specific configuration.</p> Source code in <code>gigaspatial/handlers/base.py</code> <pre><code>@dataclass\nclass BaseHandlerConfig(ABC):\n    \"\"\"\n    Abstract base class for handler configuration objects.\n    Provides standard fields for path, parallelism, data store, and logger.\n    Extend this class for dataset-specific configuration.\n    \"\"\"\n\n    base_path: Path = None\n    n_workers: int = multiprocessing.cpu_count()\n    data_store: DataStore = field(default_factory=LocalDataStore)\n    logger: logging.Logger = field(default=None, repr=False)\n\n    def __post_init__(self):\n        if self.logger is None:\n            self.logger = global_config.get_logger(self.__class__.__name__)\n\n    def get_relevant_data_units(\n        self,\n        source: Union[\n            str,  # country\n            List[Union[Tuple[float, float], Point]],  # points\n            BaseGeometry,  # geometry\n            gpd.GeoDataFrame,  # geodataframe\n        ],\n        **kwargs,\n    ):\n        if isinstance(source, str):\n            data_units = self.get_relevant_data_units_by_country(source, **kwargs)\n        elif isinstance(source, (BaseGeometry, gpd.GeoDataFrame)):\n            data_units = self.get_relevant_data_units_by_geometry(source, **kwargs)\n        elif isinstance(source, Iterable):\n            if all(isinstance(p, (Iterable, Point)) for p in source):\n                data_units = self.get_relevant_data_units_by_points(source, **kwargs)\n            else:\n                raise ValueError(\n                    \"List input to get_relevant_data_units must be all points.\"\n                )\n        else:\n            raise NotImplementedError(f\"Unsupported source type: {type(source)}\")\n\n        return data_units\n\n    @abstractmethod\n    def get_relevant_data_units_by_geometry(\n        self, geometry: Union[BaseGeometry, gpd.GeoDataFrame], **kwargs\n    ) -&gt; Any:\n        \"\"\"\n        Given a geometry, return a list of relevant data unit identifiers (e.g., tiles, files, resources).\n        \"\"\"\n        pass\n\n    @abstractmethod\n    def get_relevant_data_units_by_points(\n        self, points: Iterable[Union[Point, tuple]], **kwargs\n    ) -&gt; Any:\n        \"\"\"\n        Given a list of points, return a list of relevant data unit identifiers.\n        \"\"\"\n        pass\n\n    def get_relevant_data_units_by_country(self, country: str, **kwargs) -&gt; Any:\n        \"\"\"\n        Given a country code or name, return a list of relevant data unit identifiers.\n        \"\"\"\n        from gigaspatial.handlers.boundaries import AdminBoundaries\n\n        country_geometry = (\n            AdminBoundaries.create(country_code=country, **kwargs)\n            .boundaries[0]\n            .geometry\n        )\n        return self.get_relevant_data_units_by_geometry(\n            geometry=country_geometry, **kwargs\n        )\n\n    @abstractmethod\n    def get_data_unit_path(self, unit: Any, **kwargs) -&gt; list:\n        \"\"\"\n        Given a data unit identifier, return the corresponding file path.\n        \"\"\"\n        pass\n\n    def get_data_unit_paths(self, units: Union[Iterable[Any]], **kwargs) -&gt; list:\n        \"\"\"\n        Given data unit identifiers, return the corresponding file paths.\n        \"\"\"\n        if not isinstance(units, Iterable):\n            units = [units]\n\n        if not units:\n            return []\n\n        return [self.get_data_unit_path(unit=unit, **kwargs) for unit in units]\n</code></pre>"},{"location":"api/handlers/#gigaspatial.handlers.base.BaseHandlerConfig.get_data_unit_path","title":"<code>get_data_unit_path(unit, **kwargs)</code>  <code>abstractmethod</code>","text":"<p>Given a data unit identifier, return the corresponding file path.</p> Source code in <code>gigaspatial/handlers/base.py</code> <pre><code>@abstractmethod\ndef get_data_unit_path(self, unit: Any, **kwargs) -&gt; list:\n    \"\"\"\n    Given a data unit identifier, return the corresponding file path.\n    \"\"\"\n    pass\n</code></pre>"},{"location":"api/handlers/#gigaspatial.handlers.base.BaseHandlerConfig.get_data_unit_paths","title":"<code>get_data_unit_paths(units, **kwargs)</code>","text":"<p>Given data unit identifiers, return the corresponding file paths.</p> Source code in <code>gigaspatial/handlers/base.py</code> <pre><code>def get_data_unit_paths(self, units: Union[Iterable[Any]], **kwargs) -&gt; list:\n    \"\"\"\n    Given data unit identifiers, return the corresponding file paths.\n    \"\"\"\n    if not isinstance(units, Iterable):\n        units = [units]\n\n    if not units:\n        return []\n\n    return [self.get_data_unit_path(unit=unit, **kwargs) for unit in units]\n</code></pre>"},{"location":"api/handlers/#gigaspatial.handlers.base.BaseHandlerConfig.get_relevant_data_units_by_country","title":"<code>get_relevant_data_units_by_country(country, **kwargs)</code>","text":"<p>Given a country code or name, return a list of relevant data unit identifiers.</p> Source code in <code>gigaspatial/handlers/base.py</code> <pre><code>def get_relevant_data_units_by_country(self, country: str, **kwargs) -&gt; Any:\n    \"\"\"\n    Given a country code or name, return a list of relevant data unit identifiers.\n    \"\"\"\n    from gigaspatial.handlers.boundaries import AdminBoundaries\n\n    country_geometry = (\n        AdminBoundaries.create(country_code=country, **kwargs)\n        .boundaries[0]\n        .geometry\n    )\n    return self.get_relevant_data_units_by_geometry(\n        geometry=country_geometry, **kwargs\n    )\n</code></pre>"},{"location":"api/handlers/#gigaspatial.handlers.base.BaseHandlerConfig.get_relevant_data_units_by_geometry","title":"<code>get_relevant_data_units_by_geometry(geometry, **kwargs)</code>  <code>abstractmethod</code>","text":"<p>Given a geometry, return a list of relevant data unit identifiers (e.g., tiles, files, resources).</p> Source code in <code>gigaspatial/handlers/base.py</code> <pre><code>@abstractmethod\ndef get_relevant_data_units_by_geometry(\n    self, geometry: Union[BaseGeometry, gpd.GeoDataFrame], **kwargs\n) -&gt; Any:\n    \"\"\"\n    Given a geometry, return a list of relevant data unit identifiers (e.g., tiles, files, resources).\n    \"\"\"\n    pass\n</code></pre>"},{"location":"api/handlers/#gigaspatial.handlers.base.BaseHandlerConfig.get_relevant_data_units_by_points","title":"<code>get_relevant_data_units_by_points(points, **kwargs)</code>  <code>abstractmethod</code>","text":"<p>Given a list of points, return a list of relevant data unit identifiers.</p> Source code in <code>gigaspatial/handlers/base.py</code> <pre><code>@abstractmethod\ndef get_relevant_data_units_by_points(\n    self, points: Iterable[Union[Point, tuple]], **kwargs\n) -&gt; Any:\n    \"\"\"\n    Given a list of points, return a list of relevant data unit identifiers.\n    \"\"\"\n    pass\n</code></pre>"},{"location":"api/handlers/#gigaspatial.handlers.base.BaseHandlerDownloader","title":"<code>BaseHandlerDownloader</code>","text":"<p>               Bases: <code>ABC</code></p> <p>Abstract base class for handler downloader classes. Standardizes config, data_store, and logger initialization. Extend this class for dataset-specific downloaders.</p> Source code in <code>gigaspatial/handlers/base.py</code> <pre><code>class BaseHandlerDownloader(ABC):\n    \"\"\"\n    Abstract base class for handler downloader classes.\n    Standardizes config, data_store, and logger initialization.\n    Extend this class for dataset-specific downloaders.\n    \"\"\"\n\n    def __init__(\n        self,\n        config: Optional[BaseHandlerConfig] = None,\n        data_store: Optional[DataStore] = None,\n        logger: Optional[logging.Logger] = None,\n    ):\n        self.config = config\n        if data_store:\n            self.data_store = data_store\n        elif config and hasattr(config, \"data_store\"):\n            self.data_store = config.data_store\n        else:\n            self.data_store = LocalDataStore()\n\n        self.logger = (\n            logger\n            or (getattr(config, \"logger\", None) if config else None)\n            or global_config.get_logger(self.__class__.__name__)\n        )\n\n    @abstractmethod\n    def download_data_unit(self, *args, **kwargs):\n        \"\"\"\n        Abstract method to download data. Implement in subclasses.\n        \"\"\"\n        pass\n\n    @abstractmethod\n    def download_data_units(self, *args, **kwargs):\n        \"\"\"\n        Abstract method to download data. Implement in subclasses.\n        \"\"\"\n        pass\n\n    @abstractmethod\n    def download(self, *args, **kwargs):\n        \"\"\"\n        Abstract method to download data. Implement in subclasses.\n        \"\"\"\n        pass\n</code></pre>"},{"location":"api/handlers/#gigaspatial.handlers.base.BaseHandlerDownloader.download","title":"<code>download(*args, **kwargs)</code>  <code>abstractmethod</code>","text":"<p>Abstract method to download data. Implement in subclasses.</p> Source code in <code>gigaspatial/handlers/base.py</code> <pre><code>@abstractmethod\ndef download(self, *args, **kwargs):\n    \"\"\"\n    Abstract method to download data. Implement in subclasses.\n    \"\"\"\n    pass\n</code></pre>"},{"location":"api/handlers/#gigaspatial.handlers.base.BaseHandlerDownloader.download_data_unit","title":"<code>download_data_unit(*args, **kwargs)</code>  <code>abstractmethod</code>","text":"<p>Abstract method to download data. Implement in subclasses.</p> Source code in <code>gigaspatial/handlers/base.py</code> <pre><code>@abstractmethod\ndef download_data_unit(self, *args, **kwargs):\n    \"\"\"\n    Abstract method to download data. Implement in subclasses.\n    \"\"\"\n    pass\n</code></pre>"},{"location":"api/handlers/#gigaspatial.handlers.base.BaseHandlerDownloader.download_data_units","title":"<code>download_data_units(*args, **kwargs)</code>  <code>abstractmethod</code>","text":"<p>Abstract method to download data. Implement in subclasses.</p> Source code in <code>gigaspatial/handlers/base.py</code> <pre><code>@abstractmethod\ndef download_data_units(self, *args, **kwargs):\n    \"\"\"\n    Abstract method to download data. Implement in subclasses.\n    \"\"\"\n    pass\n</code></pre>"},{"location":"api/handlers/#gigaspatial.handlers.base.BaseHandlerReader","title":"<code>BaseHandlerReader</code>","text":"<p>               Bases: <code>ABC</code></p> <p>Abstract base class for handler reader classes. Provides common methods for resolving source paths and loading data. Supports resolving by country, points, geometry, GeoDataFrame, or explicit paths. Includes generic loader functions for raster and tabular data.</p> Source code in <code>gigaspatial/handlers/base.py</code> <pre><code>class BaseHandlerReader(ABC):\n    \"\"\"\n    Abstract base class for handler reader classes.\n    Provides common methods for resolving source paths and loading data.\n    Supports resolving by country, points, geometry, GeoDataFrame, or explicit paths.\n    Includes generic loader functions for raster and tabular data.\n    \"\"\"\n\n    def __init__(\n        self,\n        config: Optional[BaseHandlerConfig] = None,\n        data_store: Optional[DataStore] = None,\n        logger: Optional[logging.Logger] = None,\n    ):\n        self.config = config\n        if data_store:\n            self.data_store = data_store\n        elif config and hasattr(config, \"data_store\"):\n            self.data_store = config.data_store\n        else:\n            self.data_store = LocalDataStore()\n\n        self.logger = (\n            logger\n            or (getattr(config, \"logger\", None) if config else None)\n            or global_config.get_logger(self.__class__.__name__)\n        )\n\n    def resolve_source_paths(\n        self,\n        source: Union[\n            str,  # country code\n            List[Union[Tuple[float, float], Point]],  # points\n            BaseGeometry,\n            gpd.GeoDataFrame,\n            Path,  # path\n            str,  # path\n            List[Union[str, Path]],\n        ],\n        **kwargs,\n    ) -&gt; List[Union[str, Path]]:\n        \"\"\"\n        Resolve source data paths based on the type of source input.\n\n        Args:\n            source: Can be a country code or name (str), list of points, geometry, GeoDataFrame, or explicit path(s)\n            **kwargs: Additional parameters for path resolution\n\n        Returns:\n            List of resolved source paths\n        \"\"\"\n        if isinstance(source, (str, Path)):\n            # Could be a country code or a path\n            if self.data_store.file_exists(str(source)) or str(source).endswith(\n                (\".csv\", \".tif\", \".json\", \".parquet\", \".gz\", \".geojson\", \".zip\")\n            ):\n                source_data_paths = self.resolve_by_paths(source)\n            else:\n                source_data_paths = self.resolve_by_country(source, **kwargs)\n        elif isinstance(source, (BaseGeometry, gpd.GeoDataFrame)):\n            source_data_paths = self.resolve_by_geometry(source, **kwargs)\n        elif isinstance(source, Iterable):\n            # List of points or paths\n            if all(isinstance(p, (Iterable, Point)) for p in source):\n                source_data_paths = self.resolve_by_points(source, **kwargs)\n            elif all(isinstance(p, (str, Path)) for p in source):\n                source_data_paths = self.resolve_by_paths(source)\n            else:\n                raise ValueError(\n                    \"List input to resolve_source_paths must be all points or all paths.\"\n                )\n        else:\n            raise NotImplementedError(f\"Unsupported source type: {type(source)}\")\n\n        self.logger.info(f\"Resolved {len(source_data_paths)} paths!\")\n        return source_data_paths\n\n    def resolve_by_country(self, country: str, **kwargs) -&gt; List[Union[str, Path]]:\n        \"\"\"\n        Resolve source paths for a given country code/name.\n        Uses the config's get_relevant_data_units_by_country method.\n        \"\"\"\n        if not self.config:\n            raise ValueError(\"Config is required for resolving by country\")\n        data_units = self.config.get_relevant_data_units_by_country(country, **kwargs)\n        return self.config.get_data_unit_paths(data_units, **kwargs)\n\n    def resolve_by_points(\n        self, points: List[Union[Tuple[float, float], Point]], **kwargs\n    ) -&gt; List[Union[str, Path]]:\n        \"\"\"\n        Resolve source paths for a list of points.\n        Uses the config's get_relevant_data_units_by_points method.\n        \"\"\"\n        if not self.config:\n            raise ValueError(\"Config is required for resolving by points\")\n        data_units = self.config.get_relevant_data_units_by_points(points, **kwargs)\n        return self.config.get_data_unit_paths(data_units, **kwargs)\n\n    def resolve_by_geometry(\n        self, geometry: Union[BaseGeometry, gpd.GeoDataFrame], **kwargs\n    ) -&gt; List[Union[str, Path]]:\n        \"\"\"\n        Resolve source paths for a geometry or GeoDataFrame.\n        Uses the config's get_relevant_data_units_by_geometry method.\n        \"\"\"\n        if not self.config:\n            raise ValueError(\"Config is required for resolving by geometry\")\n        data_units = self.config.get_relevant_data_units_by_geometry(geometry, **kwargs)\n        return self.config.get_data_unit_paths(data_units, **kwargs)\n\n    def resolve_by_paths(\n        self, paths: Union[Path, str, List[Union[str, Path]]], **kwargs\n    ) -&gt; List[Union[str, Path]]:\n        \"\"\"\n        Return explicit paths as a list.\n        \"\"\"\n        if isinstance(paths, (str, Path)):\n            return [paths]\n        return list(paths)\n\n    def _pre_load_hook(self, source_data_path, **kwargs) -&gt; Any:\n        \"\"\"Hook called before loading data.\"\"\"\n        if isinstance(source_data_path, (Path, str)):\n            source_data_path = [source_data_path]\n\n        if not source_data_path:\n            self.logger.warning(\"No paths found!\")\n            return []\n\n        source_data_paths = [str(file_path) for file_path in source_data_path]\n\n        self.logger.info(\n            f\"Pre-loading validation complete for {len(source_data_path)} files\"\n        )\n        return source_data_paths\n\n    def _post_load_hook(self, data, **kwargs) -&gt; Any:\n        \"\"\"Hook called after loading data.\"\"\"\n        if isinstance(data, Iterable):\n            if len(data) == 0:\n                self.logger.warning(\"No data was loaded from the source files\")\n                return data\n\n            self.logger.info(f\"{len(data)} valid data records.\")\n\n        self.logger.info(f\"Post-load processing complete.\")\n\n        return data\n\n    def _check_file_exists(self, file_paths: List[Union[str, Path]]):\n        \"\"\"\n        Check that all specified files exist in the data store.\n\n        Args:\n            file_paths (List[Union[str, Path]]): List of file paths to check.\n\n        Raises:\n            RuntimeError: If any file does not exist in the data store.\n        \"\"\"\n        for file_path in file_paths:\n            if not self.data_store.file_exists(str(file_path)):\n                raise RuntimeError(\n                    f\"Source file does not exist in the data store: {file_path}\"\n                )\n\n    def _load_raster_data(\n        self, raster_paths: List[Union[str, Path]]\n    ) -&gt; List[TifProcessor]:\n        \"\"\"\n        Load raster data from file paths.\n\n        Args:\n            raster_paths (List[Union[str, Path]]): List of file paths to raster files.\n\n        Returns:\n            List[TifProcessor]: List of TifProcessor objects for accessing the raster data.\n        \"\"\"\n        return [\n            TifProcessor(data_path, self.data_store, mode=\"single\")\n            for data_path in raster_paths\n        ]\n\n    def _load_tabular_data(\n        self, file_paths: List[Union[str, Path]], read_function: Callable = read_dataset\n    ) -&gt; Union[pd.DataFrame, gpd.GeoDataFrame]:\n        \"\"\"\n        Load and concatenate tabular data from multiple files.\n\n        Args:\n            file_paths (List[Union[str, Path]]): List of file paths to load data from.\n            read_function (Callable): Function to use for reading individual files.\n                Defaults to read_dataset. Should accept (data_store, file_path) arguments.\n\n        Returns:\n            Union[pd.DataFrame, gpd.GeoDataFrame]: Concatenated data from all files.\n                Returns empty DataFrame if no data is loaded.\n        \"\"\"\n        all_data = []\n        for file_path in file_paths:\n            all_data.append(read_function(self.data_store, file_path))\n        if not all_data:\n            return pd.DataFrame()\n        result = pd.concat(all_data, ignore_index=True)\n        return result\n\n    @abstractmethod\n    def load_from_paths(\n        self, source_data_path: List[Union[str, Path]], **kwargs\n    ) -&gt; Any:\n        \"\"\"\n        Abstract method to load source data from paths.\n\n        Args:\n            source_data_path: List of source paths\n            **kwargs: Additional parameters for data loading\n\n        Returns:\n            Loaded data (DataFrame, GeoDataFrame, etc.)\n        \"\"\"\n        pass\n\n    def load(\n        self,\n        source: Union[\n            str,  # country\n            List[Union[Tuple[float, float], Point]],  # points\n            BaseGeometry,\n            gpd.GeoDataFrame,\n            Path,\n            str,\n            List[Union[str, Path]],\n        ],\n        **kwargs,\n    ) -&gt; Any:\n        \"\"\"\n        Load data from the given source.\n\n        Args:\n            source: The data source (country code/name, points, geometry, paths, etc.).\n            **kwargs: Additional parameters to pass to the loading process.\n\n        Returns:\n            The loaded data. The type depends on the subclass implementation.\n        \"\"\"\n        source_data_paths = self.resolve_source_paths(source, **kwargs)\n        if not source_data_paths:\n            self.logger.warning(\n                \"No source data paths resolved. There's no matching data to load!\"\n            )\n            return None\n        processed_paths = self._pre_load_hook(source_data_paths, **kwargs)\n        if not processed_paths:\n            self.logger.warning(\"No valid paths to load data from.\")\n            return None\n\n        loaded_data = self.load_from_paths(processed_paths, **kwargs)\n        return self._post_load_hook(loaded_data, **kwargs)\n</code></pre>"},{"location":"api/handlers/#gigaspatial.handlers.base.BaseHandlerReader.load","title":"<code>load(source, **kwargs)</code>","text":"<p>Load data from the given source.</p> <p>Parameters:</p> Name Type Description Default <code>source</code> <code>Union[str, List[Union[Tuple[float, float], Point]], BaseGeometry, GeoDataFrame, Path, str, List[Union[str, Path]]]</code> <p>The data source (country code/name, points, geometry, paths, etc.).</p> required <code>**kwargs</code> <p>Additional parameters to pass to the loading process.</p> <code>{}</code> <p>Returns:</p> Type Description <code>Any</code> <p>The loaded data. The type depends on the subclass implementation.</p> Source code in <code>gigaspatial/handlers/base.py</code> <pre><code>def load(\n    self,\n    source: Union[\n        str,  # country\n        List[Union[Tuple[float, float], Point]],  # points\n        BaseGeometry,\n        gpd.GeoDataFrame,\n        Path,\n        str,\n        List[Union[str, Path]],\n    ],\n    **kwargs,\n) -&gt; Any:\n    \"\"\"\n    Load data from the given source.\n\n    Args:\n        source: The data source (country code/name, points, geometry, paths, etc.).\n        **kwargs: Additional parameters to pass to the loading process.\n\n    Returns:\n        The loaded data. The type depends on the subclass implementation.\n    \"\"\"\n    source_data_paths = self.resolve_source_paths(source, **kwargs)\n    if not source_data_paths:\n        self.logger.warning(\n            \"No source data paths resolved. There's no matching data to load!\"\n        )\n        return None\n    processed_paths = self._pre_load_hook(source_data_paths, **kwargs)\n    if not processed_paths:\n        self.logger.warning(\"No valid paths to load data from.\")\n        return None\n\n    loaded_data = self.load_from_paths(processed_paths, **kwargs)\n    return self._post_load_hook(loaded_data, **kwargs)\n</code></pre>"},{"location":"api/handlers/#gigaspatial.handlers.base.BaseHandlerReader.load_from_paths","title":"<code>load_from_paths(source_data_path, **kwargs)</code>  <code>abstractmethod</code>","text":"<p>Abstract method to load source data from paths.</p> <p>Parameters:</p> Name Type Description Default <code>source_data_path</code> <code>List[Union[str, Path]]</code> <p>List of source paths</p> required <code>**kwargs</code> <p>Additional parameters for data loading</p> <code>{}</code> <p>Returns:</p> Type Description <code>Any</code> <p>Loaded data (DataFrame, GeoDataFrame, etc.)</p> Source code in <code>gigaspatial/handlers/base.py</code> <pre><code>@abstractmethod\ndef load_from_paths(\n    self, source_data_path: List[Union[str, Path]], **kwargs\n) -&gt; Any:\n    \"\"\"\n    Abstract method to load source data from paths.\n\n    Args:\n        source_data_path: List of source paths\n        **kwargs: Additional parameters for data loading\n\n    Returns:\n        Loaded data (DataFrame, GeoDataFrame, etc.)\n    \"\"\"\n    pass\n</code></pre>"},{"location":"api/handlers/#gigaspatial.handlers.base.BaseHandlerReader.resolve_by_country","title":"<code>resolve_by_country(country, **kwargs)</code>","text":"<p>Resolve source paths for a given country code/name. Uses the config's get_relevant_data_units_by_country method.</p> Source code in <code>gigaspatial/handlers/base.py</code> <pre><code>def resolve_by_country(self, country: str, **kwargs) -&gt; List[Union[str, Path]]:\n    \"\"\"\n    Resolve source paths for a given country code/name.\n    Uses the config's get_relevant_data_units_by_country method.\n    \"\"\"\n    if not self.config:\n        raise ValueError(\"Config is required for resolving by country\")\n    data_units = self.config.get_relevant_data_units_by_country(country, **kwargs)\n    return self.config.get_data_unit_paths(data_units, **kwargs)\n</code></pre>"},{"location":"api/handlers/#gigaspatial.handlers.base.BaseHandlerReader.resolve_by_geometry","title":"<code>resolve_by_geometry(geometry, **kwargs)</code>","text":"<p>Resolve source paths for a geometry or GeoDataFrame. Uses the config's get_relevant_data_units_by_geometry method.</p> Source code in <code>gigaspatial/handlers/base.py</code> <pre><code>def resolve_by_geometry(\n    self, geometry: Union[BaseGeometry, gpd.GeoDataFrame], **kwargs\n) -&gt; List[Union[str, Path]]:\n    \"\"\"\n    Resolve source paths for a geometry or GeoDataFrame.\n    Uses the config's get_relevant_data_units_by_geometry method.\n    \"\"\"\n    if not self.config:\n        raise ValueError(\"Config is required for resolving by geometry\")\n    data_units = self.config.get_relevant_data_units_by_geometry(geometry, **kwargs)\n    return self.config.get_data_unit_paths(data_units, **kwargs)\n</code></pre>"},{"location":"api/handlers/#gigaspatial.handlers.base.BaseHandlerReader.resolve_by_paths","title":"<code>resolve_by_paths(paths, **kwargs)</code>","text":"<p>Return explicit paths as a list.</p> Source code in <code>gigaspatial/handlers/base.py</code> <pre><code>def resolve_by_paths(\n    self, paths: Union[Path, str, List[Union[str, Path]]], **kwargs\n) -&gt; List[Union[str, Path]]:\n    \"\"\"\n    Return explicit paths as a list.\n    \"\"\"\n    if isinstance(paths, (str, Path)):\n        return [paths]\n    return list(paths)\n</code></pre>"},{"location":"api/handlers/#gigaspatial.handlers.base.BaseHandlerReader.resolve_by_points","title":"<code>resolve_by_points(points, **kwargs)</code>","text":"<p>Resolve source paths for a list of points. Uses the config's get_relevant_data_units_by_points method.</p> Source code in <code>gigaspatial/handlers/base.py</code> <pre><code>def resolve_by_points(\n    self, points: List[Union[Tuple[float, float], Point]], **kwargs\n) -&gt; List[Union[str, Path]]:\n    \"\"\"\n    Resolve source paths for a list of points.\n    Uses the config's get_relevant_data_units_by_points method.\n    \"\"\"\n    if not self.config:\n        raise ValueError(\"Config is required for resolving by points\")\n    data_units = self.config.get_relevant_data_units_by_points(points, **kwargs)\n    return self.config.get_data_unit_paths(data_units, **kwargs)\n</code></pre>"},{"location":"api/handlers/#gigaspatial.handlers.base.BaseHandlerReader.resolve_source_paths","title":"<code>resolve_source_paths(source, **kwargs)</code>","text":"<p>Resolve source data paths based on the type of source input.</p> <p>Parameters:</p> Name Type Description Default <code>source</code> <code>Union[str, List[Union[Tuple[float, float], Point]], BaseGeometry, GeoDataFrame, Path, str, List[Union[str, Path]]]</code> <p>Can be a country code or name (str), list of points, geometry, GeoDataFrame, or explicit path(s)</p> required <code>**kwargs</code> <p>Additional parameters for path resolution</p> <code>{}</code> <p>Returns:</p> Type Description <code>List[Union[str, Path]]</code> <p>List of resolved source paths</p> Source code in <code>gigaspatial/handlers/base.py</code> <pre><code>def resolve_source_paths(\n    self,\n    source: Union[\n        str,  # country code\n        List[Union[Tuple[float, float], Point]],  # points\n        BaseGeometry,\n        gpd.GeoDataFrame,\n        Path,  # path\n        str,  # path\n        List[Union[str, Path]],\n    ],\n    **kwargs,\n) -&gt; List[Union[str, Path]]:\n    \"\"\"\n    Resolve source data paths based on the type of source input.\n\n    Args:\n        source: Can be a country code or name (str), list of points, geometry, GeoDataFrame, or explicit path(s)\n        **kwargs: Additional parameters for path resolution\n\n    Returns:\n        List of resolved source paths\n    \"\"\"\n    if isinstance(source, (str, Path)):\n        # Could be a country code or a path\n        if self.data_store.file_exists(str(source)) or str(source).endswith(\n            (\".csv\", \".tif\", \".json\", \".parquet\", \".gz\", \".geojson\", \".zip\")\n        ):\n            source_data_paths = self.resolve_by_paths(source)\n        else:\n            source_data_paths = self.resolve_by_country(source, **kwargs)\n    elif isinstance(source, (BaseGeometry, gpd.GeoDataFrame)):\n        source_data_paths = self.resolve_by_geometry(source, **kwargs)\n    elif isinstance(source, Iterable):\n        # List of points or paths\n        if all(isinstance(p, (Iterable, Point)) for p in source):\n            source_data_paths = self.resolve_by_points(source, **kwargs)\n        elif all(isinstance(p, (str, Path)) for p in source):\n            source_data_paths = self.resolve_by_paths(source)\n        else:\n            raise ValueError(\n                \"List input to resolve_source_paths must be all points or all paths.\"\n            )\n    else:\n        raise NotImplementedError(f\"Unsupported source type: {type(source)}\")\n\n    self.logger.info(f\"Resolved {len(source_data_paths)} paths!\")\n    return source_data_paths\n</code></pre>"},{"location":"api/handlers/#gigaspatial.handlers.boundaries","title":"<code>boundaries</code>","text":""},{"location":"api/handlers/#gigaspatial.handlers.boundaries.AdminBoundaries","title":"<code>AdminBoundaries</code>","text":"<p>               Bases: <code>BaseModel</code></p> <p>Base class for administrative boundary data with flexible fields.</p> Source code in <code>gigaspatial/handlers/boundaries.py</code> <pre><code>class AdminBoundaries(BaseModel):\n    \"\"\"Base class for administrative boundary data with flexible fields.\"\"\"\n\n    boundaries: List[AdminBoundary] = Field(default_factory=list)\n    level: int = Field(\n        ...,\n        ge=0,\n        le=4,\n        description=\"Administrative level (e.g., 0=country, 1=state, etc.)\",\n    )\n\n    logger: ClassVar = config.get_logger(\"AdminBoundaries\")\n\n    _schema_config: ClassVar[Dict[str, Dict[str, str]]] = {\n        \"gadm\": {\n            \"country_code\": \"GID_0\",\n            \"id\": \"GID_{level}\",\n            \"name\": \"NAME_{level}\",\n            \"parent_id\": \"GID_{parent_level}\",\n        },\n        \"internal\": {\n            \"id\": \"admin{level}_id_giga\",\n            \"name\": \"name\",\n            \"name_en\": \"name_en\",\n            \"country_code\": \"iso_3166_1_alpha_3\",\n        },\n    }\n\n    @classmethod\n    def get_schema_config(cls) -&gt; Dict[str, Dict[str, str]]:\n        \"\"\"Return field mappings for different data sources\"\"\"\n        return cls._schema_config\n\n    @classmethod\n    def from_gadm(\n        cls, country_code: str, admin_level: int = 0, **kwargs\n    ) -&gt; \"AdminBoundaries\":\n        \"\"\"Load and create instance from GADM data.\"\"\"\n        url = f\"https://geodata.ucdavis.edu/gadm/gadm4.1/json/gadm41_{country_code}_{admin_level}.json\"\n        cls.logger.info(\n            f\"Loading GADM data for country: {country_code}, admin level: {admin_level} from URL: {url}\"\n        )\n        try:\n            gdf = gpd.read_file(url)\n\n            gdf = cls._map_fields(gdf, \"gadm\", admin_level)\n\n            if admin_level == 0:\n                gdf[\"country_code\"] = gdf[\"id\"]\n                gdf[\"name\"] = gdf[\"COUNTRY\"]\n            elif admin_level == 1:\n                gdf[\"country_code\"] = gdf[\"parent_id\"]\n\n            boundaries = [\n                AdminBoundary(**row_dict) for row_dict in gdf.to_dict(\"records\")\n            ]\n            cls.logger.info(f\"Created {len(boundaries)} AdminBoundary objects.\")\n            return cls(\n                boundaries=boundaries, level=admin_level, country_code=country_code\n            )\n\n        except (ValueError, HTTPError, FileNotFoundError) as e:\n            cls.logger.warning(\n                f\"Error loading GADM data for {country_code} at admin level {admin_level}: {str(e)}\"\n            )\n            return cls._create_empty_instance(country_code, admin_level, \"gadm\")\n\n    @classmethod\n    def from_data_store(\n        cls,\n        data_store: DataStore,\n        path: Union[str, \"Path\"],\n        admin_level: int = 0,\n        **kwargs,\n    ) -&gt; \"AdminBoundaries\":\n        \"\"\"Load and create instance from internal data store.\"\"\"\n        cls.logger.info(\n            f\"Loading data from data store at path: {path}, admin level: {admin_level}\"\n        )\n        try:\n            gdf = read_dataset(data_store, str(path), **kwargs)\n\n            if gdf.empty:\n                cls.logger.warning(f\"No data found at {path}.\")\n                return cls._create_empty_instance(None, admin_level, \"internal\")\n\n            gdf = cls._map_fields(gdf, \"internal\", admin_level)\n\n            if admin_level == 0:\n                gdf[\"id\"] = gdf[\"country_code\"]\n            else:\n                gdf[\"parent_id\"] = gdf[\"id\"].apply(lambda x: x[:-3])\n\n            boundaries = [\n                AdminBoundary(**row_dict) for row_dict in gdf.to_dict(\"records\")\n            ]\n            cls.logger.info(f\"Created {len(boundaries)} AdminBoundary objects.\")\n            return cls(boundaries=boundaries, level=admin_level)\n\n        except (FileNotFoundError, KeyError) as e:\n            cls.logger.warning(\n                f\"No data found at {path} for admin level {admin_level}: {str(e)}\"\n            )\n            return cls._create_empty_instance(None, admin_level, \"internal\")\n\n    @classmethod\n    def from_georepo(\n        cls,\n        country_code: str = None,\n        admin_level: int = 0,\n        **kwargs,\n    ) -&gt; \"AdminBoundaries\":\n        \"\"\"\n        Load and create instance from GeoRepo (UNICEF) API.\n\n        Args:\n            country: Country name (if using name-based lookup)\n            iso3: ISO3 code (if using code-based lookup)\n            admin_level: Administrative level (0=country, 1=state, etc.)\n            api_key: GeoRepo API key (optional)\n            email: GeoRepo user email (optional)\n            kwargs: Extra arguments (ignored)\n\n        Returns:\n            AdminBoundaries instance\n        \"\"\"\n        cls.logger.info(\n            f\"Loading data from UNICEF GeoRepo for country: {country_code}, admin level: {admin_level}\"\n        )\n        from gigaspatial.handlers.unicef_georepo import get_country_boundaries_by_iso3\n\n        # Fetch boundaries from GeoRepo\n        geojson = get_country_boundaries_by_iso3(country_code, admin_level=admin_level)\n\n        features = geojson.get(\"features\", [])\n        boundaries = []\n        parent_level = admin_level - 1\n\n        for feat in features:\n            props = feat.get(\"properties\", {})\n            geometry = feat.get(\"geometry\")\n            shapely_geom = shape(geometry) if geometry else None\n            # For admin_level 0, no parent_id\n            parent_id = None\n            if admin_level &gt; 0:\n                parent_id = props.get(f\"adm{parent_level}_ucode\")\n\n            boundary = AdminBoundary(\n                id=props.get(\"ucode\"),\n                name=props.get(\"name\"),\n                name_en=props.get(\"name_en\"),\n                geometry=shapely_geom,\n                parent_id=parent_id,\n                country_code=country_code,\n            )\n            boundaries.append(boundary)\n\n        cls.logger.info(\n            f\"Created {len(boundaries)} AdminBoundary objects from GeoRepo data.\"\n        )\n\n        # Try to infer country_code from first boundary if not set\n        if boundaries and not boundaries[0].country_code:\n            boundaries[0].country_code = boundaries[0].id[:3]\n\n        return cls(boundaries=boundaries, level=admin_level)\n\n    @classmethod\n    def create(\n        cls,\n        country_code: Optional[str] = None,\n        admin_level: int = 0,\n        data_store: Optional[DataStore] = None,\n        path: Optional[Union[str, \"Path\"]] = None,\n        **kwargs,\n    ) -&gt; \"AdminBoundaries\":\n        \"\"\"Factory method to create AdminBoundaries instance from either GADM or data store.\"\"\"\n        cls.logger.info(\n            f\"Creating AdminBoundaries instance. Country: {country_code}, admin level: {admin_level}, data_store provided: {data_store is not None}, path provided: {path is not None}\"\n        )\n        iso3_code = pycountry.countries.lookup(country_code).alpha_3\n        if data_store is not None:\n            if path is None:\n                if country_code is None:\n                    ValueError(\n                        \"If data_store is provided, path or country_code must also be specified.\"\n                    )\n                path = config.get_admin_path(\n                    country_code=iso3_code,\n                    admin_level=admin_level,\n                )\n            return cls.from_data_store(data_store, path, admin_level, **kwargs)\n        elif country_code is not None:\n            from gigaspatial.handlers.unicef_georepo import GeoRepoClient\n\n            try:\n                client = GeoRepoClient()\n                if client.check_connection():\n                    cls.logger.info(\"GeoRepo connection successful.\")\n                    return cls.from_georepo(\n                        iso3_code,\n                        admin_level=admin_level,\n                    )\n            except ValueError as e:\n                cls.logger.warning(\n                    f\"GeoRepo initialization failed: {str(e)}. Falling back to GADM.\"\n                )\n            except Exception as e:\n                cls.logger.warning(f\"GeoRepo error: {str(e)}. Falling back to GADM.\")\n\n            return cls.from_gadm(iso3_code, admin_level, **kwargs)\n        else:\n            raise ValueError(\n                \"Either country_code or (data_store, path) must be provided.\"\n            )\n\n    @classmethod\n    def _create_empty_instance(\n        cls, country_code: Optional[str], admin_level: int, source_type: str\n    ) -&gt; \"AdminBoundaries\":\n        \"\"\"Create an empty instance with the required schema structure.\"\"\"\n        # for to_geodataframe() to use later\n        instance = cls(boundaries=[], level=admin_level, country_code=country_code)\n\n        schema_fields = set(cls.get_schema_config()[source_type].keys())\n        schema_fields.update([\"geometry\", \"country_code\", \"id\", \"name\", \"name_en\"])\n        if admin_level &gt; 0:\n            schema_fields.add(\"parent_id\")\n\n        instance._empty_schema = list(schema_fields)\n        return instance\n\n    @classmethod\n    def _map_fields(\n        cls,\n        gdf: gpd.GeoDataFrame,\n        source: str,\n        current_level: int,\n    ) -&gt; gpd.GeoDataFrame:\n        \"\"\"Map source fields to schema fields\"\"\"\n        config = cls.get_schema_config().get(source, {})\n        parent_level = current_level - 1\n\n        field_mapping = {}\n        for k, v in config.items():\n            if \"{parent_level}\" in v:\n                field_mapping[v.format(parent_level=parent_level)] = k\n            elif \"{level}\" in v:\n                field_mapping[v.format(level=current_level)] = k\n            else:\n                field_mapping[v] = k\n\n        return gdf.rename(columns=field_mapping)\n\n    def to_geodataframe(self) -&gt; gpd.GeoDataFrame:\n        \"\"\"Convert the AdminBoundaries to a GeoDataFrame.\"\"\"\n        if not self.boundaries:\n            if hasattr(self, \"_empty_schema\"):\n                columns = self._empty_schema\n            else:\n                columns = [\"id\", \"name\", \"country_code\", \"geometry\"]\n                if self.level &gt; 0:\n                    columns.append(\"parent_id\")\n\n            return gpd.GeoDataFrame(columns=columns, geometry=\"geometry\", crs=4326)\n\n        return gpd.GeoDataFrame(\n            [boundary.model_dump() for boundary in self.boundaries],\n            geometry=\"geometry\",\n            crs=4326,\n        )\n</code></pre>"},{"location":"api/handlers/#gigaspatial.handlers.boundaries.AdminBoundaries.create","title":"<code>create(country_code=None, admin_level=0, data_store=None, path=None, **kwargs)</code>  <code>classmethod</code>","text":"<p>Factory method to create AdminBoundaries instance from either GADM or data store.</p> Source code in <code>gigaspatial/handlers/boundaries.py</code> <pre><code>@classmethod\ndef create(\n    cls,\n    country_code: Optional[str] = None,\n    admin_level: int = 0,\n    data_store: Optional[DataStore] = None,\n    path: Optional[Union[str, \"Path\"]] = None,\n    **kwargs,\n) -&gt; \"AdminBoundaries\":\n    \"\"\"Factory method to create AdminBoundaries instance from either GADM or data store.\"\"\"\n    cls.logger.info(\n        f\"Creating AdminBoundaries instance. Country: {country_code}, admin level: {admin_level}, data_store provided: {data_store is not None}, path provided: {path is not None}\"\n    )\n    iso3_code = pycountry.countries.lookup(country_code).alpha_3\n    if data_store is not None:\n        if path is None:\n            if country_code is None:\n                ValueError(\n                    \"If data_store is provided, path or country_code must also be specified.\"\n                )\n            path = config.get_admin_path(\n                country_code=iso3_code,\n                admin_level=admin_level,\n            )\n        return cls.from_data_store(data_store, path, admin_level, **kwargs)\n    elif country_code is not None:\n        from gigaspatial.handlers.unicef_georepo import GeoRepoClient\n\n        try:\n            client = GeoRepoClient()\n            if client.check_connection():\n                cls.logger.info(\"GeoRepo connection successful.\")\n                return cls.from_georepo(\n                    iso3_code,\n                    admin_level=admin_level,\n                )\n        except ValueError as e:\n            cls.logger.warning(\n                f\"GeoRepo initialization failed: {str(e)}. Falling back to GADM.\"\n            )\n        except Exception as e:\n            cls.logger.warning(f\"GeoRepo error: {str(e)}. Falling back to GADM.\")\n\n        return cls.from_gadm(iso3_code, admin_level, **kwargs)\n    else:\n        raise ValueError(\n            \"Either country_code or (data_store, path) must be provided.\"\n        )\n</code></pre>"},{"location":"api/handlers/#gigaspatial.handlers.boundaries.AdminBoundaries.from_data_store","title":"<code>from_data_store(data_store, path, admin_level=0, **kwargs)</code>  <code>classmethod</code>","text":"<p>Load and create instance from internal data store.</p> Source code in <code>gigaspatial/handlers/boundaries.py</code> <pre><code>@classmethod\ndef from_data_store(\n    cls,\n    data_store: DataStore,\n    path: Union[str, \"Path\"],\n    admin_level: int = 0,\n    **kwargs,\n) -&gt; \"AdminBoundaries\":\n    \"\"\"Load and create instance from internal data store.\"\"\"\n    cls.logger.info(\n        f\"Loading data from data store at path: {path}, admin level: {admin_level}\"\n    )\n    try:\n        gdf = read_dataset(data_store, str(path), **kwargs)\n\n        if gdf.empty:\n            cls.logger.warning(f\"No data found at {path}.\")\n            return cls._create_empty_instance(None, admin_level, \"internal\")\n\n        gdf = cls._map_fields(gdf, \"internal\", admin_level)\n\n        if admin_level == 0:\n            gdf[\"id\"] = gdf[\"country_code\"]\n        else:\n            gdf[\"parent_id\"] = gdf[\"id\"].apply(lambda x: x[:-3])\n\n        boundaries = [\n            AdminBoundary(**row_dict) for row_dict in gdf.to_dict(\"records\")\n        ]\n        cls.logger.info(f\"Created {len(boundaries)} AdminBoundary objects.\")\n        return cls(boundaries=boundaries, level=admin_level)\n\n    except (FileNotFoundError, KeyError) as e:\n        cls.logger.warning(\n            f\"No data found at {path} for admin level {admin_level}: {str(e)}\"\n        )\n        return cls._create_empty_instance(None, admin_level, \"internal\")\n</code></pre>"},{"location":"api/handlers/#gigaspatial.handlers.boundaries.AdminBoundaries.from_gadm","title":"<code>from_gadm(country_code, admin_level=0, **kwargs)</code>  <code>classmethod</code>","text":"<p>Load and create instance from GADM data.</p> Source code in <code>gigaspatial/handlers/boundaries.py</code> <pre><code>@classmethod\ndef from_gadm(\n    cls, country_code: str, admin_level: int = 0, **kwargs\n) -&gt; \"AdminBoundaries\":\n    \"\"\"Load and create instance from GADM data.\"\"\"\n    url = f\"https://geodata.ucdavis.edu/gadm/gadm4.1/json/gadm41_{country_code}_{admin_level}.json\"\n    cls.logger.info(\n        f\"Loading GADM data for country: {country_code}, admin level: {admin_level} from URL: {url}\"\n    )\n    try:\n        gdf = gpd.read_file(url)\n\n        gdf = cls._map_fields(gdf, \"gadm\", admin_level)\n\n        if admin_level == 0:\n            gdf[\"country_code\"] = gdf[\"id\"]\n            gdf[\"name\"] = gdf[\"COUNTRY\"]\n        elif admin_level == 1:\n            gdf[\"country_code\"] = gdf[\"parent_id\"]\n\n        boundaries = [\n            AdminBoundary(**row_dict) for row_dict in gdf.to_dict(\"records\")\n        ]\n        cls.logger.info(f\"Created {len(boundaries)} AdminBoundary objects.\")\n        return cls(\n            boundaries=boundaries, level=admin_level, country_code=country_code\n        )\n\n    except (ValueError, HTTPError, FileNotFoundError) as e:\n        cls.logger.warning(\n            f\"Error loading GADM data for {country_code} at admin level {admin_level}: {str(e)}\"\n        )\n        return cls._create_empty_instance(country_code, admin_level, \"gadm\")\n</code></pre>"},{"location":"api/handlers/#gigaspatial.handlers.boundaries.AdminBoundaries.from_georepo","title":"<code>from_georepo(country_code=None, admin_level=0, **kwargs)</code>  <code>classmethod</code>","text":"<p>Load and create instance from GeoRepo (UNICEF) API.</p> <p>Parameters:</p> Name Type Description Default <code>country</code> <p>Country name (if using name-based lookup)</p> required <code>iso3</code> <p>ISO3 code (if using code-based lookup)</p> required <code>admin_level</code> <code>int</code> <p>Administrative level (0=country, 1=state, etc.)</p> <code>0</code> <code>api_key</code> <p>GeoRepo API key (optional)</p> required <code>email</code> <p>GeoRepo user email (optional)</p> required <code>kwargs</code> <p>Extra arguments (ignored)</p> <code>{}</code> <p>Returns:</p> Type Description <code>AdminBoundaries</code> <p>AdminBoundaries instance</p> Source code in <code>gigaspatial/handlers/boundaries.py</code> <pre><code>@classmethod\ndef from_georepo(\n    cls,\n    country_code: str = None,\n    admin_level: int = 0,\n    **kwargs,\n) -&gt; \"AdminBoundaries\":\n    \"\"\"\n    Load and create instance from GeoRepo (UNICEF) API.\n\n    Args:\n        country: Country name (if using name-based lookup)\n        iso3: ISO3 code (if using code-based lookup)\n        admin_level: Administrative level (0=country, 1=state, etc.)\n        api_key: GeoRepo API key (optional)\n        email: GeoRepo user email (optional)\n        kwargs: Extra arguments (ignored)\n\n    Returns:\n        AdminBoundaries instance\n    \"\"\"\n    cls.logger.info(\n        f\"Loading data from UNICEF GeoRepo for country: {country_code}, admin level: {admin_level}\"\n    )\n    from gigaspatial.handlers.unicef_georepo import get_country_boundaries_by_iso3\n\n    # Fetch boundaries from GeoRepo\n    geojson = get_country_boundaries_by_iso3(country_code, admin_level=admin_level)\n\n    features = geojson.get(\"features\", [])\n    boundaries = []\n    parent_level = admin_level - 1\n\n    for feat in features:\n        props = feat.get(\"properties\", {})\n        geometry = feat.get(\"geometry\")\n        shapely_geom = shape(geometry) if geometry else None\n        # For admin_level 0, no parent_id\n        parent_id = None\n        if admin_level &gt; 0:\n            parent_id = props.get(f\"adm{parent_level}_ucode\")\n\n        boundary = AdminBoundary(\n            id=props.get(\"ucode\"),\n            name=props.get(\"name\"),\n            name_en=props.get(\"name_en\"),\n            geometry=shapely_geom,\n            parent_id=parent_id,\n            country_code=country_code,\n        )\n        boundaries.append(boundary)\n\n    cls.logger.info(\n        f\"Created {len(boundaries)} AdminBoundary objects from GeoRepo data.\"\n    )\n\n    # Try to infer country_code from first boundary if not set\n    if boundaries and not boundaries[0].country_code:\n        boundaries[0].country_code = boundaries[0].id[:3]\n\n    return cls(boundaries=boundaries, level=admin_level)\n</code></pre>"},{"location":"api/handlers/#gigaspatial.handlers.boundaries.AdminBoundaries.get_schema_config","title":"<code>get_schema_config()</code>  <code>classmethod</code>","text":"<p>Return field mappings for different data sources</p> Source code in <code>gigaspatial/handlers/boundaries.py</code> <pre><code>@classmethod\ndef get_schema_config(cls) -&gt; Dict[str, Dict[str, str]]:\n    \"\"\"Return field mappings for different data sources\"\"\"\n    return cls._schema_config\n</code></pre>"},{"location":"api/handlers/#gigaspatial.handlers.boundaries.AdminBoundaries.to_geodataframe","title":"<code>to_geodataframe()</code>","text":"<p>Convert the AdminBoundaries to a GeoDataFrame.</p> Source code in <code>gigaspatial/handlers/boundaries.py</code> <pre><code>def to_geodataframe(self) -&gt; gpd.GeoDataFrame:\n    \"\"\"Convert the AdminBoundaries to a GeoDataFrame.\"\"\"\n    if not self.boundaries:\n        if hasattr(self, \"_empty_schema\"):\n            columns = self._empty_schema\n        else:\n            columns = [\"id\", \"name\", \"country_code\", \"geometry\"]\n            if self.level &gt; 0:\n                columns.append(\"parent_id\")\n\n        return gpd.GeoDataFrame(columns=columns, geometry=\"geometry\", crs=4326)\n\n    return gpd.GeoDataFrame(\n        [boundary.model_dump() for boundary in self.boundaries],\n        geometry=\"geometry\",\n        crs=4326,\n    )\n</code></pre>"},{"location":"api/handlers/#gigaspatial.handlers.boundaries.AdminBoundary","title":"<code>AdminBoundary</code>","text":"<p>               Bases: <code>BaseModel</code></p> <p>Base class for administrative boundary data with flexible fields.</p> Source code in <code>gigaspatial/handlers/boundaries.py</code> <pre><code>class AdminBoundary(BaseModel):\n    \"\"\"Base class for administrative boundary data with flexible fields.\"\"\"\n\n    id: str = Field(..., description=\"Unique identifier for the administrative unit\")\n    name: str = Field(..., description=\"Primary local name\")\n    geometry: Union[Polygon, MultiPolygon] = Field(\n        ..., description=\"Geometry of the administrative boundary\"\n    )\n\n    name_en: Optional[str] = Field(\n        None, description=\"English name if different from local name\"\n    )\n    parent_id: Optional[str] = Field(\n        None, description=\"ID of parent administrative unit\"\n    )\n    country_code: Optional[str] = Field(\n        None, min_length=3, max_length=3, description=\"ISO 3166-1 alpha-3 country code\"\n    )\n\n    class Config:\n        # extra = \"allow\"\n        arbitrary_types_allowed = True\n</code></pre>"},{"location":"api/handlers/#gigaspatial.handlers.ghsl","title":"<code>ghsl</code>","text":""},{"location":"api/handlers/#gigaspatial.handlers.ghsl.CoordSystem","title":"<code>CoordSystem</code>","text":"<p>               Bases: <code>int</code>, <code>Enum</code></p> <p>Enum for coordinate systems used by GHSL datasets.</p> Source code in <code>gigaspatial/handlers/ghsl.py</code> <pre><code>class CoordSystem(int, Enum):\n    \"\"\"Enum for coordinate systems used by GHSL datasets.\"\"\"\n\n    WGS84 = 4326\n    Mollweide = 54009\n</code></pre>"},{"location":"api/handlers/#gigaspatial.handlers.ghsl.GHSLDataConfig","title":"<code>GHSLDataConfig</code>  <code>dataclass</code>","text":"<p>               Bases: <code>BaseHandlerConfig</code></p> Source code in <code>gigaspatial/handlers/ghsl.py</code> <pre><code>@dataclass(config=ConfigDict(arbitrary_types_allowed=True))\nclass GHSLDataConfig(BaseHandlerConfig):\n    # constants\n    AVAILABLE_YEARS: List = Field(default=np.append(np.arange(1975, 2031, 5), 2018))\n    AVAILABLE_RESOLUTIONS: List = Field(default=[10, 100, 1000])\n\n    # base config\n    GHSL_DB_BASE_URL: HttpUrl = Field(\n        default=\"https://jeodpp.jrc.ec.europa.eu/ftp/jrc-opendata/GHSL/\"\n    )\n    TILES_URL: str = \"https://ghsl.jrc.ec.europa.eu/download/GHSL_data_{}_shapefile.zip\"\n\n    # user config\n    base_path: Path = Field(default=global_config.get_path(\"ghsl\", \"bronze\"))\n    coord_system: CoordSystem = CoordSystem.WGS84\n    release: str = \"R2023A\"\n\n    product: Literal[\n        \"GHS_BUILT_S\",\n        \"GHS_BUILT_H_AGBH\",\n        \"GHS_BUILT_H_ANBH\",\n        \"GHS_BUILT_V\",\n        \"GHS_POP\",\n        \"GHS_SMOD\",\n    ] = Field(...)\n    year: int = 2020\n    resolution: int = 100\n\n    def __post_init__(self):\n        super().__post_init__()\n\n    def _load_tiles(self):\n        \"\"\"Load GHSL tiles from tiles shapefile.\"\"\"\n        try:\n            self.tiles_gdf = gpd.read_file(self.TILES_URL)\n        except Exception as e:\n            self.logger.error(f\"Failed to download tiles shapefile: {e}\")\n            raise ValueError(\n                f\"Could not download GHSL tiles from {self.TILES_URL}\"\n            ) from e\n\n    @field_validator(\"year\")\n    def validate_year(cls, value: str) -&gt; int:\n        if value in cls.AVAILABLE_YEARS:\n            return value\n        raise ValueError(\n            f\"No datasets found for the provided year: {value}\\nAvailable years are: {cls.AVAILABLE_YEARS}\"\n        )\n\n    @field_validator(\"resolution\")\n    def validate_resolution(cls, value: str) -&gt; int:\n        if value in cls.AVAILABLE_RESOLUTIONS:\n            return value\n        raise ValueError(\n            f\"No datasets found for the provided resolution: {value}\\nAvailable resolutions are: {cls.AVAILABLE_RESOLUTIONS}\"\n        )\n\n    @model_validator(mode=\"after\")\n    def validate_configuration(self):\n        \"\"\"\n        Validate that the configuration is valid based on dataset availability constraints.\n\n        Specific rules:\n        -\n        \"\"\"\n        if self.year == 2018 and self.product in [\"GHS_BUILT_V\", \"GHS_POP\", \"GHS_SMOD\"]:\n            raise ValueError(f\"{self.product} product is not available for 2018\")\n\n        if self.resolution == 10 and self.product != \"GHS_BUILT_H\":\n            raise ValueError(\n                f\"{self.product} product is not available at 10 (10m) resolution\"\n            )\n\n        if \"GHS_BUILT_H\" in self.product:\n            if self.year != 2018:\n                self.logger.warning(\n                    \"Building height product is only available for 2018, year is set as 2018\"\n                )\n                self.year = 2018\n\n        if self.product == \"GHS_BUILT_S\":\n            if self.year == 2018 and self.resolution != 10:\n                self.logger.warning(\n                    \"Built-up surface product for 2018 is only available at 10m resolution, resolution is set as 10m\"\n                )\n                self.resolution = 10\n\n            if self.resolution == 10 and self.year != 2018:\n                self.logger.warning(\n                    \"Built-up surface product at resolution 10 is only available for 2018, year is set as 2018\"\n                )\n                self.year = 2018\n\n            if self.resolution == 10 and self.coord_system != CoordSystem.Mollweide:\n                self.logger.warning(\n                    f\"Built-up surface product at resolution 10 is only available with Mollweide ({CoordSystem.Mollweide}) projection, coordinate system is set as Mollweide\"\n                )\n                self.coord_system = CoordSystem.Mollweide\n\n        if self.product == \"GHS_SMOD\":\n            if self.resolution != 1000:\n                self.logger.warning(\n                    f\"Settlement model (SMOD) product is only available at 1000 (1km) resolution, resolution is set as 1000\"\n                )\n                self.resolution = 1000\n\n            if self.coord_system != CoordSystem.Mollweide:\n                self.logger.warning(\n                    f\"Settlement model (SMOD) product is only available with Mollweide ({CoordSystem.Mollweide}) projection, coordinate system is set as Mollweide\"\n                )\n                self.coord_system = CoordSystem.Mollweide\n\n        self.TILES_URL = self.TILES_URL.format(self.coord_system.value)\n        self._load_tiles()\n\n        return self\n\n    @property\n    def crs(self) -&gt; str:\n        return \"EPSG:4326\" if self.coord_system == CoordSystem.WGS84 else \"ESRI:54009\"\n\n    def get_relevant_data_units_by_geometry(\n        self, geometry: Union[BaseGeometry, gpd.GeoDataFrame], **kwargs\n    ) -&gt; List[dict]:\n        \"\"\"\n        Return intersecting tiles for a given geometry or GeoDataFrame.\n        \"\"\"\n        return self._get_relevant_tiles(geometry)\n\n    def get_relevant_data_units_by_points(\n        self, points: Iterable[Union[Point, tuple]], **kwargs\n    ) -&gt; List[dict]:\n        \"\"\"\n        Return intersecting tiles f or a list of points.\n        \"\"\"\n        return self._get_relevant_tiles(points)\n\n    def get_data_unit_path(self, unit: str = None, file_ext=\".zip\", **kwargs) -&gt; Path:\n        \"\"\"Construct and return the path for the configured dataset or dataset tile.\"\"\"\n        info = self._get_product_info()\n\n        tile_path = (\n            self.base_path\n            / info[\"product_folder\"]\n            / (\n                f\"{info['product_name']}_V{info['product_version']}_0\"\n                + (f\"_{unit}\" if unit else \"\")\n                + file_ext\n            )\n        )\n\n        return tile_path\n\n    def compute_dataset_url(self, tile_id=None) -&gt; str:\n        \"\"\"Compute the download URL for a GHSL dataset.\"\"\"\n        info = self._get_product_info()\n\n        path_segments = [\n            str(self.GHSL_DB_BASE_URL),\n            info[\"product_folder\"],\n            info[\"product_name\"],\n            f\"V{info['product_version']}-0\",\n            \"tiles\" if tile_id else \"\",\n            f\"{info['product_name']}_V{info['product_version']}_0\"\n            + (f\"_{tile_id}\" if tile_id else \"\")\n            + \".zip\",\n        ]\n\n        return \"/\".join(path_segments)\n\n    def _get_relevant_tiles(\n        self,\n        source: Union[\n            BaseGeometry,\n            gpd.GeoDataFrame,\n            Iterable[Union[Point, tuple]],\n        ],\n        crs=\"EPSG:4326\",\n    ) -&gt; list:\n        \"\"\"\n        Identify and return the GHSL tiles that spatially intersect with the given geometry.\n\n        The input geometry can be a Shapely geometry object, a GeoDataFrame,\n        or a list of Point objects or (lon, lat) tuples. The method ensures\n        the input geometry is in GHSL tiles projection for the spatial intersection.\n\n        Args:\n            source: A Shapely geometry, a GeoDataFrame, or a list of Point\n                      objects or (lat, lon) tuples representing the area of interest.\n\n        Returns:\n            A list the tile ids for the intersecting tiles.\n\n        Raises:\n            ValueError: If the input `source` is not one of the supported types.\n        \"\"\"\n        if isinstance(source, gpd.GeoDataFrame):\n            # if source.crs != \"EPSG:4326\":\n            #    source = source.to_crs(\"EPSG:4326\")\n            search_geom = source.geometry.unary_union\n        elif isinstance(\n            source,\n            BaseGeometry,\n        ):\n            search_geom = source\n        elif isinstance(source, Iterable) and all(\n            len(pt) == 2 or isinstance(pt, Point) for pt in source\n        ):\n            points = [\n                pt if isinstance(pt, Point) else Point(pt[1], pt[0]) for pt in source\n            ]\n            search_geom = MultiPoint(points)\n        else:\n            raise ValueError(\n                f\"Expected Geometry, GeoDataFrame or iterable object of Points got {source.__class__}\"\n            )\n\n        if self.tiles_gdf.crs != crs:\n            search_geom = (\n                gpd.GeoDataFrame(geometry=[search_geom], crs=crs)\n                .to_crs(self.tiles_gdf.crs)\n                .geometry[0]\n            )\n\n        # Find intersecting tiles\n        mask = (\n            tile_geom.intersects(search_geom) for tile_geom in self.tiles_gdf.geometry\n        )\n\n        return self.tiles_gdf.loc[mask, \"tile_id\"].to_list()\n\n    def _get_product_info(self) -&gt; dict:\n        \"\"\"Generate and return common product information used in multiple methods.\"\"\"\n        resolution_str = (\n            str(self.resolution)\n            if self.coord_system == CoordSystem.Mollweide\n            else (\"3ss\" if self.resolution == 100 else \"30ss\")\n        )\n        product_folder = f\"{self.product}_GLOBE_{self.release}\"\n        product_name = f\"{self.product}_E{self.year}_GLOBE_{self.release}_{self.coord_system.value}_{resolution_str}\"\n        product_version = 2 if self.product == \"GHS_SMOD\" else 1\n\n        return {\n            \"resolution_str\": resolution_str,\n            \"product_folder\": product_folder,\n            \"product_name\": product_name,\n            \"product_version\": product_version,\n        }\n\n    def __repr__(self) -&gt; str:\n        \"\"\"Return a string representation of the GHSL dataset configuration.\"\"\"\n        return (\n            f\"GHSLDataConfig(\"\n            f\"product='{self.product}', \"\n            f\"year={self.year}, \"\n            f\"resolution={self.resolution}, \"\n            f\"coord_system={self.coord_system.name}, \"\n            f\"release='{self.release}'\"\n            f\")\"\n        )\n</code></pre>"},{"location":"api/handlers/#gigaspatial.handlers.ghsl.GHSLDataConfig.__repr__","title":"<code>__repr__()</code>","text":"<p>Return a string representation of the GHSL dataset configuration.</p> Source code in <code>gigaspatial/handlers/ghsl.py</code> <pre><code>def __repr__(self) -&gt; str:\n    \"\"\"Return a string representation of the GHSL dataset configuration.\"\"\"\n    return (\n        f\"GHSLDataConfig(\"\n        f\"product='{self.product}', \"\n        f\"year={self.year}, \"\n        f\"resolution={self.resolution}, \"\n        f\"coord_system={self.coord_system.name}, \"\n        f\"release='{self.release}'\"\n        f\")\"\n    )\n</code></pre>"},{"location":"api/handlers/#gigaspatial.handlers.ghsl.GHSLDataConfig.compute_dataset_url","title":"<code>compute_dataset_url(tile_id=None)</code>","text":"<p>Compute the download URL for a GHSL dataset.</p> Source code in <code>gigaspatial/handlers/ghsl.py</code> <pre><code>def compute_dataset_url(self, tile_id=None) -&gt; str:\n    \"\"\"Compute the download URL for a GHSL dataset.\"\"\"\n    info = self._get_product_info()\n\n    path_segments = [\n        str(self.GHSL_DB_BASE_URL),\n        info[\"product_folder\"],\n        info[\"product_name\"],\n        f\"V{info['product_version']}-0\",\n        \"tiles\" if tile_id else \"\",\n        f\"{info['product_name']}_V{info['product_version']}_0\"\n        + (f\"_{tile_id}\" if tile_id else \"\")\n        + \".zip\",\n    ]\n\n    return \"/\".join(path_segments)\n</code></pre>"},{"location":"api/handlers/#gigaspatial.handlers.ghsl.GHSLDataConfig.get_data_unit_path","title":"<code>get_data_unit_path(unit=None, file_ext='.zip', **kwargs)</code>","text":"<p>Construct and return the path for the configured dataset or dataset tile.</p> Source code in <code>gigaspatial/handlers/ghsl.py</code> <pre><code>def get_data_unit_path(self, unit: str = None, file_ext=\".zip\", **kwargs) -&gt; Path:\n    \"\"\"Construct and return the path for the configured dataset or dataset tile.\"\"\"\n    info = self._get_product_info()\n\n    tile_path = (\n        self.base_path\n        / info[\"product_folder\"]\n        / (\n            f\"{info['product_name']}_V{info['product_version']}_0\"\n            + (f\"_{unit}\" if unit else \"\")\n            + file_ext\n        )\n    )\n\n    return tile_path\n</code></pre>"},{"location":"api/handlers/#gigaspatial.handlers.ghsl.GHSLDataConfig.get_relevant_data_units_by_geometry","title":"<code>get_relevant_data_units_by_geometry(geometry, **kwargs)</code>","text":"<p>Return intersecting tiles for a given geometry or GeoDataFrame.</p> Source code in <code>gigaspatial/handlers/ghsl.py</code> <pre><code>def get_relevant_data_units_by_geometry(\n    self, geometry: Union[BaseGeometry, gpd.GeoDataFrame], **kwargs\n) -&gt; List[dict]:\n    \"\"\"\n    Return intersecting tiles for a given geometry or GeoDataFrame.\n    \"\"\"\n    return self._get_relevant_tiles(geometry)\n</code></pre>"},{"location":"api/handlers/#gigaspatial.handlers.ghsl.GHSLDataConfig.get_relevant_data_units_by_points","title":"<code>get_relevant_data_units_by_points(points, **kwargs)</code>","text":"<p>Return intersecting tiles f or a list of points.</p> Source code in <code>gigaspatial/handlers/ghsl.py</code> <pre><code>def get_relevant_data_units_by_points(\n    self, points: Iterable[Union[Point, tuple]], **kwargs\n) -&gt; List[dict]:\n    \"\"\"\n    Return intersecting tiles f or a list of points.\n    \"\"\"\n    return self._get_relevant_tiles(points)\n</code></pre>"},{"location":"api/handlers/#gigaspatial.handlers.ghsl.GHSLDataConfig.validate_configuration","title":"<code>validate_configuration()</code>","text":"<p>Validate that the configuration is valid based on dataset availability constraints.</p>"},{"location":"api/handlers/#gigaspatial.handlers.ghsl.GHSLDataConfig.validate_configuration--specific-rules","title":"Specific rules:","text":"Source code in <code>gigaspatial/handlers/ghsl.py</code> <pre><code>@model_validator(mode=\"after\")\ndef validate_configuration(self):\n    \"\"\"\n    Validate that the configuration is valid based on dataset availability constraints.\n\n    Specific rules:\n    -\n    \"\"\"\n    if self.year == 2018 and self.product in [\"GHS_BUILT_V\", \"GHS_POP\", \"GHS_SMOD\"]:\n        raise ValueError(f\"{self.product} product is not available for 2018\")\n\n    if self.resolution == 10 and self.product != \"GHS_BUILT_H\":\n        raise ValueError(\n            f\"{self.product} product is not available at 10 (10m) resolution\"\n        )\n\n    if \"GHS_BUILT_H\" in self.product:\n        if self.year != 2018:\n            self.logger.warning(\n                \"Building height product is only available for 2018, year is set as 2018\"\n            )\n            self.year = 2018\n\n    if self.product == \"GHS_BUILT_S\":\n        if self.year == 2018 and self.resolution != 10:\n            self.logger.warning(\n                \"Built-up surface product for 2018 is only available at 10m resolution, resolution is set as 10m\"\n            )\n            self.resolution = 10\n\n        if self.resolution == 10 and self.year != 2018:\n            self.logger.warning(\n                \"Built-up surface product at resolution 10 is only available for 2018, year is set as 2018\"\n            )\n            self.year = 2018\n\n        if self.resolution == 10 and self.coord_system != CoordSystem.Mollweide:\n            self.logger.warning(\n                f\"Built-up surface product at resolution 10 is only available with Mollweide ({CoordSystem.Mollweide}) projection, coordinate system is set as Mollweide\"\n            )\n            self.coord_system = CoordSystem.Mollweide\n\n    if self.product == \"GHS_SMOD\":\n        if self.resolution != 1000:\n            self.logger.warning(\n                f\"Settlement model (SMOD) product is only available at 1000 (1km) resolution, resolution is set as 1000\"\n            )\n            self.resolution = 1000\n\n        if self.coord_system != CoordSystem.Mollweide:\n            self.logger.warning(\n                f\"Settlement model (SMOD) product is only available with Mollweide ({CoordSystem.Mollweide}) projection, coordinate system is set as Mollweide\"\n            )\n            self.coord_system = CoordSystem.Mollweide\n\n    self.TILES_URL = self.TILES_URL.format(self.coord_system.value)\n    self._load_tiles()\n\n    return self\n</code></pre>"},{"location":"api/handlers/#gigaspatial.handlers.ghsl.GHSLDataDownloader","title":"<code>GHSLDataDownloader</code>","text":"<p>               Bases: <code>BaseHandlerDownloader</code></p> <p>A class to handle downloads of GHSL datasets.</p> Source code in <code>gigaspatial/handlers/ghsl.py</code> <pre><code>class GHSLDataDownloader(BaseHandlerDownloader):\n    \"\"\"A class to handle downloads of GHSL datasets.\"\"\"\n\n    def __init__(\n        self,\n        config: Union[GHSLDataConfig, dict[str, Union[str, int]]],\n        data_store: Optional[DataStore] = None,\n        logger: Optional[logging.Logger] = None,\n    ):\n        \"\"\"\n        Initialize the downloader.\n\n        Args:\n            config: Configuration for the GHSL dataset, either as a GHSLDataConfig object or a dictionary of parameters\n            data_store: Optional data storage interface. If not provided, uses LocalDataStore.\n            logger: Optional custom logger. If not provided, uses default logger.\n        \"\"\"\n        config = (\n            config if isinstance(config, GHSLDataConfig) else GHSLDataConfig(**config)\n        )\n        super().__init__(config=config, data_store=data_store, logger=logger)\n\n    def download_data_unit(\n        self,\n        tile_id: str,\n        extract: bool = True,\n        file_pattern: Optional[str] = r\".*\\.tif$\",\n        **kwargs,\n    ) -&gt; Optional[Union[Path, List[Path]]]:\n        \"\"\"\n        Downloads and optionally extracts files for a given tile.\n\n        Args:\n            tile_id: tile ID to process.\n            extract: If True and the downloaded file is a zip, extract its contents. Defaults to False.\n            file_pattern: Optional regex pattern to filter extracted files (if extract=True).\n            **kwargs: Additional parameters passed to download methods\n\n        Returns:\n            Path to the downloaded file if extract=False,\n            List of paths to the extracted files if extract=True,\n            None on failure.\n        \"\"\"\n        url = self.config.compute_dataset_url(tile_id=tile_id)\n        output_path = self.config.get_data_unit_path(tile_id)\n\n        if not extract:\n            return self._download_file(url, output_path)\n\n        extracted_files: List[Path] = []\n\n        try:\n            with tempfile.NamedTemporaryFile(delete=False, suffix=\".zip\") as temp_file:\n                downloaded_path = self._download_file(url, Path(temp_file.name))\n                if not downloaded_path:\n                    return None\n\n            with zipfile.ZipFile(str(downloaded_path), \"r\") as zip_ref:\n                if file_pattern:\n                    import re\n\n                    pattern = re.compile(file_pattern)\n                    files_to_extract = [\n                        f for f in zip_ref.namelist() if pattern.match(f)\n                    ]\n                else:\n                    files_to_extract = zip_ref.namelist()\n\n                for file in files_to_extract:\n                    extracted_path = output_path.parent / Path(file).name\n                    with zip_ref.open(file) as source:\n                        file_content = source.read()\n                        self.data_store.write_file(str(extracted_path), file_content)\n                    extracted_files.append(extracted_path)\n                    self.logger.info(f\"Extracted {file} to {extracted_path}\")\n\n            Path(temp_file.name).unlink()\n            return extracted_files\n\n        except Exception as e:\n            self.logger.error(f\"Error downloading/extracting tile {tile_id}: {e}\")\n            return None\n\n    def download_data_units(\n        self,\n        tile_ids: List[str],\n        extract: bool = True,\n        file_pattern: Optional[str] = r\".*\\.tif$\",\n        **kwargs,\n    ) -&gt; List[Optional[Union[Path, List[Path]]]]:\n        \"\"\"\n        Downloads multiple tiles in parallel, with an option to extract them.\n\n        Args:\n            tile_ids: A list of tile IDs to download.\n            extract: If True and the downloaded files are zips, extract their contents. Defaults to False.\n            file_pattern: Optional regex pattern to filter extracted files (if extract=True).\n            **kwargs: Additional parameters passed to download methods\n\n        Returns:\n            A list where each element corresponds to a tile ID and contains:\n            - Path to the downloaded file if extract=False.\n            - List of paths to extracted files if extract=True.\n            - None if the download or extraction failed for a tile.\n        \"\"\"\n        if not tile_ids:\n            self.logger.warning(\"No tiles to download\")\n            return []\n\n        with multiprocessing.Pool(processes=self.config.n_workers) as pool:\n            download_func = functools.partial(\n                self.download_data_unit, extract=extract, file_pattern=file_pattern\n            )\n            file_paths = list(\n                tqdm(\n                    pool.imap(download_func, tile_ids),\n                    total=len(tile_ids),\n                    desc=f\"Downloading data\",\n                )\n            )\n\n        return file_paths\n\n    def download(\n        self,\n        source: Union[\n            str,  # country\n            List[Union[Tuple[float, float], Point]],  # points\n            BaseGeometry,  # shapely geoms\n            gpd.GeoDataFrame,\n        ],\n        extract: bool = True,\n        file_pattern: Optional[str] = r\".*\\.tif$\",\n        **kwargs,\n    ) -&gt; List[Optional[Union[Path, List[Path]]]]:\n        \"\"\"\n        Download GHSL data for a specified geographic region.\n\n        The region can be defined by a country code/name, a list of points,\n        a Shapely geometry, or a GeoDataFrame. This method identifies the\n        relevant GHSL tiles intersecting the region and downloads the\n        specified type of data (polygons or points) for those tiles in parallel.\n\n        Args:\n            source: Defines the geographic area for which to download data.\n                    Can be:\n                      - A string representing a country code or name.\n                      - A list of (latitude, longitude) tuples or Shapely Point objects.\n                      - A Shapely BaseGeometry object (e.g., Polygon, MultiPolygon).\n                      - A GeoDataFrame with geometry column in EPSG:4326.\n            extract: If True and the downloaded files are zips, extract their contents. Defaults to False.\n            file_pattern: Optional regex pattern to filter extracted files (if extract=True).\n            **kwargs: Additional keyword arguments. These will be passed down to\n                      `AdminBoundaries.create()` (if `source` is a country)\n                      and to `self.download_data_units()`.\n\n        Returns:\n            A list of local file paths for the successfully downloaded tiles.\n            Returns an empty list if no data is found for the region or if\n            all downloads fail.\n        \"\"\"\n\n        tiles = self.config.get_relevant_data_units(source, **kwargs)\n        return self.download_data_units(\n            tiles, extract=extract, file_pattern=file_pattern, **kwargs\n        )\n\n    def download_by_country(\n        self,\n        country_code: str,\n        data_store: Optional[DataStore] = None,\n        country_geom_path: Optional[Union[str, Path]] = None,\n        extract: bool = True,\n        file_pattern: Optional[str] = r\".*\\.tif$\",\n        **kwargs,\n    ) -&gt; List[Optional[Union[Path, List[Path]]]]:\n        \"\"\"\n        Download GHSL data for a specific country.\n\n        This is a convenience method to download data for an entire country\n        using its code or name.\n\n        Args:\n            country_code: The country code (e.g., 'USA', 'GBR') or name.\n            data_store: Optional instance of a `DataStore` to be used by\n                        `AdminBoundaries` for loading country boundaries. If None,\n                        `AdminBoundaries` will use its default data loading.\n            country_geom_path: Optional path to a GeoJSON file containing the\n                               country boundary. If provided, this boundary is used\n                               instead of the default from `AdminBoundaries`.\n            extract: If True and the downloaded files are zips, extract their contents. Defaults to False.\n            file_pattern: Optional regex pattern to filter extracted files (if extract=True).\n            **kwargs: Additional keyword arguments that are passed to\n                      `download_data_units`. For example, `extract` to download and extract.\n\n        Returns:\n            A list of local file paths for the successfully downloaded tiles\n            for the specified country.\n        \"\"\"\n        return self.download(\n            source=country_code,\n            data_store=data_store,\n            path=country_geom_path,\n            extract=extract,\n            file_pattern=file_pattern,\n            **kwargs,\n        )\n\n    def _download_file(self, url: str, output_path: Path) -&gt; Optional[Path]:\n        \"\"\"\n        Downloads a file from a URL to a specified output path with a progress bar.\n\n        Args:\n            url: The URL to download from.\n            output_path: The local path to save the downloaded file.\n\n        Returns:\n            The path to the downloaded file on success, None on failure.\n        \"\"\"\n        try:\n            response = requests.get(url, stream=True)\n            response.raise_for_status()\n\n            total_size = int(response.headers.get(\"content-length\", 0))\n\n            with self.data_store.open(str(output_path), \"wb\") as file:\n                with tqdm(\n                    total=total_size,\n                    unit=\"B\",\n                    unit_scale=True,\n                    desc=f\"Downloading {output_path.name}\",\n                ) as pbar:\n                    for chunk in response.iter_content(chunk_size=8192):\n                        if chunk:\n                            file.write(chunk)\n                            pbar.update(len(chunk))\n\n            self.logger.debug(f\"Successfully downloaded: {url} to {output_path}\")\n            return output_path\n\n        except requests.exceptions.RequestException as e:\n            self.logger.error(f\"Failed to download {url}: {str(e)}\")\n            return None\n        except Exception as e:\n            self.logger.error(f\"Unexpected error downloading {url}: {str(e)}\")\n            return None\n</code></pre>"},{"location":"api/handlers/#gigaspatial.handlers.ghsl.GHSLDataDownloader.__init__","title":"<code>__init__(config, data_store=None, logger=None)</code>","text":"<p>Initialize the downloader.</p> <p>Parameters:</p> Name Type Description Default <code>config</code> <code>Union[GHSLDataConfig, dict[str, Union[str, int]]]</code> <p>Configuration for the GHSL dataset, either as a GHSLDataConfig object or a dictionary of parameters</p> required <code>data_store</code> <code>Optional[DataStore]</code> <p>Optional data storage interface. If not provided, uses LocalDataStore.</p> <code>None</code> <code>logger</code> <code>Optional[Logger]</code> <p>Optional custom logger. If not provided, uses default logger.</p> <code>None</code> Source code in <code>gigaspatial/handlers/ghsl.py</code> <pre><code>def __init__(\n    self,\n    config: Union[GHSLDataConfig, dict[str, Union[str, int]]],\n    data_store: Optional[DataStore] = None,\n    logger: Optional[logging.Logger] = None,\n):\n    \"\"\"\n    Initialize the downloader.\n\n    Args:\n        config: Configuration for the GHSL dataset, either as a GHSLDataConfig object or a dictionary of parameters\n        data_store: Optional data storage interface. If not provided, uses LocalDataStore.\n        logger: Optional custom logger. If not provided, uses default logger.\n    \"\"\"\n    config = (\n        config if isinstance(config, GHSLDataConfig) else GHSLDataConfig(**config)\n    )\n    super().__init__(config=config, data_store=data_store, logger=logger)\n</code></pre>"},{"location":"api/handlers/#gigaspatial.handlers.ghsl.GHSLDataDownloader.download","title":"<code>download(source, extract=True, file_pattern='.*\\\\.tif$', **kwargs)</code>","text":"<p>Download GHSL data for a specified geographic region.</p> <p>The region can be defined by a country code/name, a list of points, a Shapely geometry, or a GeoDataFrame. This method identifies the relevant GHSL tiles intersecting the region and downloads the specified type of data (polygons or points) for those tiles in parallel.</p> <p>Parameters:</p> Name Type Description Default <code>source</code> <code>Union[str, List[Union[Tuple[float, float], Point]], BaseGeometry, GeoDataFrame]</code> <p>Defines the geographic area for which to download data.     Can be:       - A string representing a country code or name.       - A list of (latitude, longitude) tuples or Shapely Point objects.       - A Shapely BaseGeometry object (e.g., Polygon, MultiPolygon).       - A GeoDataFrame with geometry column in EPSG:4326.</p> required <code>extract</code> <code>bool</code> <p>If True and the downloaded files are zips, extract their contents. Defaults to False.</p> <code>True</code> <code>file_pattern</code> <code>Optional[str]</code> <p>Optional regex pattern to filter extracted files (if extract=True).</p> <code>'.*\\\\.tif$'</code> <code>**kwargs</code> <p>Additional keyword arguments. These will be passed down to       <code>AdminBoundaries.create()</code> (if <code>source</code> is a country)       and to <code>self.download_data_units()</code>.</p> <code>{}</code> <p>Returns:</p> Type Description <code>List[Optional[Union[Path, List[Path]]]]</code> <p>A list of local file paths for the successfully downloaded tiles.</p> <code>List[Optional[Union[Path, List[Path]]]]</code> <p>Returns an empty list if no data is found for the region or if</p> <code>List[Optional[Union[Path, List[Path]]]]</code> <p>all downloads fail.</p> Source code in <code>gigaspatial/handlers/ghsl.py</code> <pre><code>def download(\n    self,\n    source: Union[\n        str,  # country\n        List[Union[Tuple[float, float], Point]],  # points\n        BaseGeometry,  # shapely geoms\n        gpd.GeoDataFrame,\n    ],\n    extract: bool = True,\n    file_pattern: Optional[str] = r\".*\\.tif$\",\n    **kwargs,\n) -&gt; List[Optional[Union[Path, List[Path]]]]:\n    \"\"\"\n    Download GHSL data for a specified geographic region.\n\n    The region can be defined by a country code/name, a list of points,\n    a Shapely geometry, or a GeoDataFrame. This method identifies the\n    relevant GHSL tiles intersecting the region and downloads the\n    specified type of data (polygons or points) for those tiles in parallel.\n\n    Args:\n        source: Defines the geographic area for which to download data.\n                Can be:\n                  - A string representing a country code or name.\n                  - A list of (latitude, longitude) tuples or Shapely Point objects.\n                  - A Shapely BaseGeometry object (e.g., Polygon, MultiPolygon).\n                  - A GeoDataFrame with geometry column in EPSG:4326.\n        extract: If True and the downloaded files are zips, extract their contents. Defaults to False.\n        file_pattern: Optional regex pattern to filter extracted files (if extract=True).\n        **kwargs: Additional keyword arguments. These will be passed down to\n                  `AdminBoundaries.create()` (if `source` is a country)\n                  and to `self.download_data_units()`.\n\n    Returns:\n        A list of local file paths for the successfully downloaded tiles.\n        Returns an empty list if no data is found for the region or if\n        all downloads fail.\n    \"\"\"\n\n    tiles = self.config.get_relevant_data_units(source, **kwargs)\n    return self.download_data_units(\n        tiles, extract=extract, file_pattern=file_pattern, **kwargs\n    )\n</code></pre>"},{"location":"api/handlers/#gigaspatial.handlers.ghsl.GHSLDataDownloader.download_by_country","title":"<code>download_by_country(country_code, data_store=None, country_geom_path=None, extract=True, file_pattern='.*\\\\.tif$', **kwargs)</code>","text":"<p>Download GHSL data for a specific country.</p> <p>This is a convenience method to download data for an entire country using its code or name.</p> <p>Parameters:</p> Name Type Description Default <code>country_code</code> <code>str</code> <p>The country code (e.g., 'USA', 'GBR') or name.</p> required <code>data_store</code> <code>Optional[DataStore]</code> <p>Optional instance of a <code>DataStore</code> to be used by         <code>AdminBoundaries</code> for loading country boundaries. If None,         <code>AdminBoundaries</code> will use its default data loading.</p> <code>None</code> <code>country_geom_path</code> <code>Optional[Union[str, Path]]</code> <p>Optional path to a GeoJSON file containing the                country boundary. If provided, this boundary is used                instead of the default from <code>AdminBoundaries</code>.</p> <code>None</code> <code>extract</code> <code>bool</code> <p>If True and the downloaded files are zips, extract their contents. Defaults to False.</p> <code>True</code> <code>file_pattern</code> <code>Optional[str]</code> <p>Optional regex pattern to filter extracted files (if extract=True).</p> <code>'.*\\\\.tif$'</code> <code>**kwargs</code> <p>Additional keyword arguments that are passed to       <code>download_data_units</code>. For example, <code>extract</code> to download and extract.</p> <code>{}</code> <p>Returns:</p> Type Description <code>List[Optional[Union[Path, List[Path]]]]</code> <p>A list of local file paths for the successfully downloaded tiles</p> <code>List[Optional[Union[Path, List[Path]]]]</code> <p>for the specified country.</p> Source code in <code>gigaspatial/handlers/ghsl.py</code> <pre><code>def download_by_country(\n    self,\n    country_code: str,\n    data_store: Optional[DataStore] = None,\n    country_geom_path: Optional[Union[str, Path]] = None,\n    extract: bool = True,\n    file_pattern: Optional[str] = r\".*\\.tif$\",\n    **kwargs,\n) -&gt; List[Optional[Union[Path, List[Path]]]]:\n    \"\"\"\n    Download GHSL data for a specific country.\n\n    This is a convenience method to download data for an entire country\n    using its code or name.\n\n    Args:\n        country_code: The country code (e.g., 'USA', 'GBR') or name.\n        data_store: Optional instance of a `DataStore` to be used by\n                    `AdminBoundaries` for loading country boundaries. If None,\n                    `AdminBoundaries` will use its default data loading.\n        country_geom_path: Optional path to a GeoJSON file containing the\n                           country boundary. If provided, this boundary is used\n                           instead of the default from `AdminBoundaries`.\n        extract: If True and the downloaded files are zips, extract their contents. Defaults to False.\n        file_pattern: Optional regex pattern to filter extracted files (if extract=True).\n        **kwargs: Additional keyword arguments that are passed to\n                  `download_data_units`. For example, `extract` to download and extract.\n\n    Returns:\n        A list of local file paths for the successfully downloaded tiles\n        for the specified country.\n    \"\"\"\n    return self.download(\n        source=country_code,\n        data_store=data_store,\n        path=country_geom_path,\n        extract=extract,\n        file_pattern=file_pattern,\n        **kwargs,\n    )\n</code></pre>"},{"location":"api/handlers/#gigaspatial.handlers.ghsl.GHSLDataDownloader.download_data_unit","title":"<code>download_data_unit(tile_id, extract=True, file_pattern='.*\\\\.tif$', **kwargs)</code>","text":"<p>Downloads and optionally extracts files for a given tile.</p> <p>Parameters:</p> Name Type Description Default <code>tile_id</code> <code>str</code> <p>tile ID to process.</p> required <code>extract</code> <code>bool</code> <p>If True and the downloaded file is a zip, extract its contents. Defaults to False.</p> <code>True</code> <code>file_pattern</code> <code>Optional[str]</code> <p>Optional regex pattern to filter extracted files (if extract=True).</p> <code>'.*\\\\.tif$'</code> <code>**kwargs</code> <p>Additional parameters passed to download methods</p> <code>{}</code> <p>Returns:</p> Type Description <code>Optional[Union[Path, List[Path]]]</code> <p>Path to the downloaded file if extract=False,</p> <code>Optional[Union[Path, List[Path]]]</code> <p>List of paths to the extracted files if extract=True,</p> <code>Optional[Union[Path, List[Path]]]</code> <p>None on failure.</p> Source code in <code>gigaspatial/handlers/ghsl.py</code> <pre><code>def download_data_unit(\n    self,\n    tile_id: str,\n    extract: bool = True,\n    file_pattern: Optional[str] = r\".*\\.tif$\",\n    **kwargs,\n) -&gt; Optional[Union[Path, List[Path]]]:\n    \"\"\"\n    Downloads and optionally extracts files for a given tile.\n\n    Args:\n        tile_id: tile ID to process.\n        extract: If True and the downloaded file is a zip, extract its contents. Defaults to False.\n        file_pattern: Optional regex pattern to filter extracted files (if extract=True).\n        **kwargs: Additional parameters passed to download methods\n\n    Returns:\n        Path to the downloaded file if extract=False,\n        List of paths to the extracted files if extract=True,\n        None on failure.\n    \"\"\"\n    url = self.config.compute_dataset_url(tile_id=tile_id)\n    output_path = self.config.get_data_unit_path(tile_id)\n\n    if not extract:\n        return self._download_file(url, output_path)\n\n    extracted_files: List[Path] = []\n\n    try:\n        with tempfile.NamedTemporaryFile(delete=False, suffix=\".zip\") as temp_file:\n            downloaded_path = self._download_file(url, Path(temp_file.name))\n            if not downloaded_path:\n                return None\n\n        with zipfile.ZipFile(str(downloaded_path), \"r\") as zip_ref:\n            if file_pattern:\n                import re\n\n                pattern = re.compile(file_pattern)\n                files_to_extract = [\n                    f for f in zip_ref.namelist() if pattern.match(f)\n                ]\n            else:\n                files_to_extract = zip_ref.namelist()\n\n            for file in files_to_extract:\n                extracted_path = output_path.parent / Path(file).name\n                with zip_ref.open(file) as source:\n                    file_content = source.read()\n                    self.data_store.write_file(str(extracted_path), file_content)\n                extracted_files.append(extracted_path)\n                self.logger.info(f\"Extracted {file} to {extracted_path}\")\n\n        Path(temp_file.name).unlink()\n        return extracted_files\n\n    except Exception as e:\n        self.logger.error(f\"Error downloading/extracting tile {tile_id}: {e}\")\n        return None\n</code></pre>"},{"location":"api/handlers/#gigaspatial.handlers.ghsl.GHSLDataDownloader.download_data_units","title":"<code>download_data_units(tile_ids, extract=True, file_pattern='.*\\\\.tif$', **kwargs)</code>","text":"<p>Downloads multiple tiles in parallel, with an option to extract them.</p> <p>Parameters:</p> Name Type Description Default <code>tile_ids</code> <code>List[str]</code> <p>A list of tile IDs to download.</p> required <code>extract</code> <code>bool</code> <p>If True and the downloaded files are zips, extract their contents. Defaults to False.</p> <code>True</code> <code>file_pattern</code> <code>Optional[str]</code> <p>Optional regex pattern to filter extracted files (if extract=True).</p> <code>'.*\\\\.tif$'</code> <code>**kwargs</code> <p>Additional parameters passed to download methods</p> <code>{}</code> <p>Returns:</p> Type Description <code>List[Optional[Union[Path, List[Path]]]]</code> <p>A list where each element corresponds to a tile ID and contains:</p> <code>List[Optional[Union[Path, List[Path]]]]</code> <ul> <li>Path to the downloaded file if extract=False.</li> </ul> <code>List[Optional[Union[Path, List[Path]]]]</code> <ul> <li>List of paths to extracted files if extract=True.</li> </ul> <code>List[Optional[Union[Path, List[Path]]]]</code> <ul> <li>None if the download or extraction failed for a tile.</li> </ul> Source code in <code>gigaspatial/handlers/ghsl.py</code> <pre><code>def download_data_units(\n    self,\n    tile_ids: List[str],\n    extract: bool = True,\n    file_pattern: Optional[str] = r\".*\\.tif$\",\n    **kwargs,\n) -&gt; List[Optional[Union[Path, List[Path]]]]:\n    \"\"\"\n    Downloads multiple tiles in parallel, with an option to extract them.\n\n    Args:\n        tile_ids: A list of tile IDs to download.\n        extract: If True and the downloaded files are zips, extract their contents. Defaults to False.\n        file_pattern: Optional regex pattern to filter extracted files (if extract=True).\n        **kwargs: Additional parameters passed to download methods\n\n    Returns:\n        A list where each element corresponds to a tile ID and contains:\n        - Path to the downloaded file if extract=False.\n        - List of paths to extracted files if extract=True.\n        - None if the download or extraction failed for a tile.\n    \"\"\"\n    if not tile_ids:\n        self.logger.warning(\"No tiles to download\")\n        return []\n\n    with multiprocessing.Pool(processes=self.config.n_workers) as pool:\n        download_func = functools.partial(\n            self.download_data_unit, extract=extract, file_pattern=file_pattern\n        )\n        file_paths = list(\n            tqdm(\n                pool.imap(download_func, tile_ids),\n                total=len(tile_ids),\n                desc=f\"Downloading data\",\n            )\n        )\n\n    return file_paths\n</code></pre>"},{"location":"api/handlers/#gigaspatial.handlers.ghsl.GHSLDataHandler","title":"<code>GHSLDataHandler</code>","text":"<p>               Bases: <code>BaseHandler</code></p> <p>Handler for GHSL (Global Human Settlement Layer) dataset.</p> <p>This class provides a unified interface for downloading and loading GHSL data. It manages the lifecycle of configuration, downloading, and reading components.</p> Source code in <code>gigaspatial/handlers/ghsl.py</code> <pre><code>class GHSLDataHandler(BaseHandler):\n    \"\"\"\n    Handler for GHSL (Global Human Settlement Layer) dataset.\n\n    This class provides a unified interface for downloading and loading GHSL data.\n    It manages the lifecycle of configuration, downloading, and reading components.\n    \"\"\"\n\n    def __init__(\n        self,\n        product: Literal[\n            \"GHS_BUILT_S\",\n            \"GHS_BUILT_H_AGBH\",\n            \"GHS_BUILT_H_ANBH\",\n            \"GHS_BUILT_V\",\n            \"GHS_POP\",\n            \"GHS_SMOD\",\n        ],\n        year: int = 2020,\n        resolution: int = 100,\n        config: Optional[GHSLDataConfig] = None,\n        downloader: Optional[GHSLDataDownloader] = None,\n        reader: Optional[GHSLDataReader] = None,\n        data_store: Optional[DataStore] = None,\n        logger: Optional[logging.Logger] = None,\n        **kwargs,\n    ):\n        \"\"\"\n        Initialize the GHSLDataHandler.\n\n        Args:\n            product: The GHSL product to use. Must be one of:\n                    - GHS_BUILT_S: Built-up surface\n                    - GHS_BUILT_H_AGBH: Average building height\n                    - GHS_BUILT_H_ANBH: Average number of building heights\n                    - GHS_BUILT_V: Building volume\n                    - GHS_POP: Population\n                    - GHS_SMOD: Settlement model\n            year: The year of the data (default: 2020)\n            resolution: The resolution in meters (default: 100)\n            config: Optional configuration object\n            downloader: Optional downloader instance\n            reader: Optional reader instance\n            data_store: Optional data store instance\n            logger: Optional logger instance\n            **kwargs: Additional configuration parameters\n        \"\"\"\n        self._product = product\n        self._year = year\n        self._resolution = resolution\n        super().__init__(\n            config=config,\n            downloader=downloader,\n            reader=reader,\n            data_store=data_store,\n            logger=logger,\n        )\n\n    def create_config(\n        self, data_store: DataStore, logger: logging.Logger, **kwargs\n    ) -&gt; GHSLDataConfig:\n        \"\"\"\n        Create and return a GHSLDataConfig instance.\n\n        Args:\n            data_store: The data store instance to use\n            logger: The logger instance to use\n            **kwargs: Additional configuration parameters\n\n        Returns:\n            Configured GHSLDataConfig instance\n        \"\"\"\n        return GHSLDataConfig(\n            product=self._product,\n            year=self._year,\n            resolution=self._resolution,\n            data_store=data_store,\n            logger=logger,\n            **kwargs,\n        )\n\n    def create_downloader(\n        self,\n        config: GHSLDataConfig,\n        data_store: DataStore,\n        logger: logging.Logger,\n        **kwargs,\n    ) -&gt; GHSLDataDownloader:\n        \"\"\"\n        Create and return a GHSLDataDownloader instance.\n\n        Args:\n            config: The configuration object\n            data_store: The data store instance to use\n            logger: The logger instance to use\n            **kwargs: Additional downloader parameters\n\n        Returns:\n            Configured GHSLDataDownloader instance\n        \"\"\"\n        return GHSLDataDownloader(\n            config=config, data_store=data_store, logger=logger, **kwargs\n        )\n\n    def create_reader(\n        self,\n        config: GHSLDataConfig,\n        data_store: DataStore,\n        logger: logging.Logger,\n        **kwargs,\n    ) -&gt; GHSLDataReader:\n        \"\"\"\n        Create and return a GHSLDataReader instance.\n\n        Args:\n            config: The configuration object\n            data_store: The data store instance to use\n            logger: The logger instance to use\n            **kwargs: Additional reader parameters\n\n        Returns:\n            Configured GHSLDataReader instance\n        \"\"\"\n        return GHSLDataReader(\n            config=config, data_store=data_store, logger=logger, **kwargs\n        )\n\n    def load_data(\n        self,\n        source: Union[\n            str,  # country\n            List[Union[tuple, Point]],  # points\n            BaseGeometry,  # geometry\n            gpd.GeoDataFrame,  # geodataframe\n            Path,  # path\n            List[Union[str, Path]],  # list of paths\n        ],\n        ensure_available: bool = True,\n        **kwargs,\n    ):\n        return super().load_data(\n            source=source,\n            ensure_available=ensure_available,\n            file_ext=\".tif\",\n            extract=True,\n            file_pattern=r\".*\\.tif$\",\n            **kwargs,\n        )\n\n    def load_into_dataframe(\n        self,\n        source: Union[\n            str,  # country\n            List[Union[tuple, Point]],  # points\n            BaseGeometry,  # geometry\n            gpd.GeoDataFrame,  # geodataframe\n            Path,  # path\n            List[Union[str, Path]],  # list of paths\n        ],\n        ensure_available: bool = True,\n        **kwargs,\n    ) -&gt; pd.DataFrame:\n        \"\"\"\n        Load GHSL data into a pandas DataFrame.\n\n        Args:\n            source: The data source specification\n            ensure_available: If True, ensure data is downloaded before loading\n            **kwargs: Additional parameters passed to load methods\n\n        Returns:\n            DataFrame containing the GHSL data\n        \"\"\"\n        tif_processors = self.load_data(\n            source=source, ensure_available=ensure_available, **kwargs\n        )\n        return pd.concat(\n            [tp.to_dataframe() for tp in tif_processors], ignore_index=True\n        )\n</code></pre>"},{"location":"api/handlers/#gigaspatial.handlers.ghsl.GHSLDataHandler.__init__","title":"<code>__init__(product, year=2020, resolution=100, config=None, downloader=None, reader=None, data_store=None, logger=None, **kwargs)</code>","text":"<p>Initialize the GHSLDataHandler.</p> <p>Parameters:</p> Name Type Description Default <code>product</code> <code>Literal['GHS_BUILT_S', 'GHS_BUILT_H_AGBH', 'GHS_BUILT_H_ANBH', 'GHS_BUILT_V', 'GHS_POP', 'GHS_SMOD']</code> <p>The GHSL product to use. Must be one of:     - GHS_BUILT_S: Built-up surface     - GHS_BUILT_H_AGBH: Average building height     - GHS_BUILT_H_ANBH: Average number of building heights     - GHS_BUILT_V: Building volume     - GHS_POP: Population     - GHS_SMOD: Settlement model</p> required <code>year</code> <code>int</code> <p>The year of the data (default: 2020)</p> <code>2020</code> <code>resolution</code> <code>int</code> <p>The resolution in meters (default: 100)</p> <code>100</code> <code>config</code> <code>Optional[GHSLDataConfig]</code> <p>Optional configuration object</p> <code>None</code> <code>downloader</code> <code>Optional[GHSLDataDownloader]</code> <p>Optional downloader instance</p> <code>None</code> <code>reader</code> <code>Optional[GHSLDataReader]</code> <p>Optional reader instance</p> <code>None</code> <code>data_store</code> <code>Optional[DataStore]</code> <p>Optional data store instance</p> <code>None</code> <code>logger</code> <code>Optional[Logger]</code> <p>Optional logger instance</p> <code>None</code> <code>**kwargs</code> <p>Additional configuration parameters</p> <code>{}</code> Source code in <code>gigaspatial/handlers/ghsl.py</code> <pre><code>def __init__(\n    self,\n    product: Literal[\n        \"GHS_BUILT_S\",\n        \"GHS_BUILT_H_AGBH\",\n        \"GHS_BUILT_H_ANBH\",\n        \"GHS_BUILT_V\",\n        \"GHS_POP\",\n        \"GHS_SMOD\",\n    ],\n    year: int = 2020,\n    resolution: int = 100,\n    config: Optional[GHSLDataConfig] = None,\n    downloader: Optional[GHSLDataDownloader] = None,\n    reader: Optional[GHSLDataReader] = None,\n    data_store: Optional[DataStore] = None,\n    logger: Optional[logging.Logger] = None,\n    **kwargs,\n):\n    \"\"\"\n    Initialize the GHSLDataHandler.\n\n    Args:\n        product: The GHSL product to use. Must be one of:\n                - GHS_BUILT_S: Built-up surface\n                - GHS_BUILT_H_AGBH: Average building height\n                - GHS_BUILT_H_ANBH: Average number of building heights\n                - GHS_BUILT_V: Building volume\n                - GHS_POP: Population\n                - GHS_SMOD: Settlement model\n        year: The year of the data (default: 2020)\n        resolution: The resolution in meters (default: 100)\n        config: Optional configuration object\n        downloader: Optional downloader instance\n        reader: Optional reader instance\n        data_store: Optional data store instance\n        logger: Optional logger instance\n        **kwargs: Additional configuration parameters\n    \"\"\"\n    self._product = product\n    self._year = year\n    self._resolution = resolution\n    super().__init__(\n        config=config,\n        downloader=downloader,\n        reader=reader,\n        data_store=data_store,\n        logger=logger,\n    )\n</code></pre>"},{"location":"api/handlers/#gigaspatial.handlers.ghsl.GHSLDataHandler.create_config","title":"<code>create_config(data_store, logger, **kwargs)</code>","text":"<p>Create and return a GHSLDataConfig instance.</p> <p>Parameters:</p> Name Type Description Default <code>data_store</code> <code>DataStore</code> <p>The data store instance to use</p> required <code>logger</code> <code>Logger</code> <p>The logger instance to use</p> required <code>**kwargs</code> <p>Additional configuration parameters</p> <code>{}</code> <p>Returns:</p> Type Description <code>GHSLDataConfig</code> <p>Configured GHSLDataConfig instance</p> Source code in <code>gigaspatial/handlers/ghsl.py</code> <pre><code>def create_config(\n    self, data_store: DataStore, logger: logging.Logger, **kwargs\n) -&gt; GHSLDataConfig:\n    \"\"\"\n    Create and return a GHSLDataConfig instance.\n\n    Args:\n        data_store: The data store instance to use\n        logger: The logger instance to use\n        **kwargs: Additional configuration parameters\n\n    Returns:\n        Configured GHSLDataConfig instance\n    \"\"\"\n    return GHSLDataConfig(\n        product=self._product,\n        year=self._year,\n        resolution=self._resolution,\n        data_store=data_store,\n        logger=logger,\n        **kwargs,\n    )\n</code></pre>"},{"location":"api/handlers/#gigaspatial.handlers.ghsl.GHSLDataHandler.create_downloader","title":"<code>create_downloader(config, data_store, logger, **kwargs)</code>","text":"<p>Create and return a GHSLDataDownloader instance.</p> <p>Parameters:</p> Name Type Description Default <code>config</code> <code>GHSLDataConfig</code> <p>The configuration object</p> required <code>data_store</code> <code>DataStore</code> <p>The data store instance to use</p> required <code>logger</code> <code>Logger</code> <p>The logger instance to use</p> required <code>**kwargs</code> <p>Additional downloader parameters</p> <code>{}</code> <p>Returns:</p> Type Description <code>GHSLDataDownloader</code> <p>Configured GHSLDataDownloader instance</p> Source code in <code>gigaspatial/handlers/ghsl.py</code> <pre><code>def create_downloader(\n    self,\n    config: GHSLDataConfig,\n    data_store: DataStore,\n    logger: logging.Logger,\n    **kwargs,\n) -&gt; GHSLDataDownloader:\n    \"\"\"\n    Create and return a GHSLDataDownloader instance.\n\n    Args:\n        config: The configuration object\n        data_store: The data store instance to use\n        logger: The logger instance to use\n        **kwargs: Additional downloader parameters\n\n    Returns:\n        Configured GHSLDataDownloader instance\n    \"\"\"\n    return GHSLDataDownloader(\n        config=config, data_store=data_store, logger=logger, **kwargs\n    )\n</code></pre>"},{"location":"api/handlers/#gigaspatial.handlers.ghsl.GHSLDataHandler.create_reader","title":"<code>create_reader(config, data_store, logger, **kwargs)</code>","text":"<p>Create and return a GHSLDataReader instance.</p> <p>Parameters:</p> Name Type Description Default <code>config</code> <code>GHSLDataConfig</code> <p>The configuration object</p> required <code>data_store</code> <code>DataStore</code> <p>The data store instance to use</p> required <code>logger</code> <code>Logger</code> <p>The logger instance to use</p> required <code>**kwargs</code> <p>Additional reader parameters</p> <code>{}</code> <p>Returns:</p> Type Description <code>GHSLDataReader</code> <p>Configured GHSLDataReader instance</p> Source code in <code>gigaspatial/handlers/ghsl.py</code> <pre><code>def create_reader(\n    self,\n    config: GHSLDataConfig,\n    data_store: DataStore,\n    logger: logging.Logger,\n    **kwargs,\n) -&gt; GHSLDataReader:\n    \"\"\"\n    Create and return a GHSLDataReader instance.\n\n    Args:\n        config: The configuration object\n        data_store: The data store instance to use\n        logger: The logger instance to use\n        **kwargs: Additional reader parameters\n\n    Returns:\n        Configured GHSLDataReader instance\n    \"\"\"\n    return GHSLDataReader(\n        config=config, data_store=data_store, logger=logger, **kwargs\n    )\n</code></pre>"},{"location":"api/handlers/#gigaspatial.handlers.ghsl.GHSLDataHandler.load_into_dataframe","title":"<code>load_into_dataframe(source, ensure_available=True, **kwargs)</code>","text":"<p>Load GHSL data into a pandas DataFrame.</p> <p>Parameters:</p> Name Type Description Default <code>source</code> <code>Union[str, List[Union[tuple, Point]], BaseGeometry, GeoDataFrame, Path, List[Union[str, Path]]]</code> <p>The data source specification</p> required <code>ensure_available</code> <code>bool</code> <p>If True, ensure data is downloaded before loading</p> <code>True</code> <code>**kwargs</code> <p>Additional parameters passed to load methods</p> <code>{}</code> <p>Returns:</p> Type Description <code>DataFrame</code> <p>DataFrame containing the GHSL data</p> Source code in <code>gigaspatial/handlers/ghsl.py</code> <pre><code>def load_into_dataframe(\n    self,\n    source: Union[\n        str,  # country\n        List[Union[tuple, Point]],  # points\n        BaseGeometry,  # geometry\n        gpd.GeoDataFrame,  # geodataframe\n        Path,  # path\n        List[Union[str, Path]],  # list of paths\n    ],\n    ensure_available: bool = True,\n    **kwargs,\n) -&gt; pd.DataFrame:\n    \"\"\"\n    Load GHSL data into a pandas DataFrame.\n\n    Args:\n        source: The data source specification\n        ensure_available: If True, ensure data is downloaded before loading\n        **kwargs: Additional parameters passed to load methods\n\n    Returns:\n        DataFrame containing the GHSL data\n    \"\"\"\n    tif_processors = self.load_data(\n        source=source, ensure_available=ensure_available, **kwargs\n    )\n    return pd.concat(\n        [tp.to_dataframe() for tp in tif_processors], ignore_index=True\n    )\n</code></pre>"},{"location":"api/handlers/#gigaspatial.handlers.ghsl.GHSLDataReader","title":"<code>GHSLDataReader</code>","text":"<p>               Bases: <code>BaseHandlerReader</code></p> Source code in <code>gigaspatial/handlers/ghsl.py</code> <pre><code>class GHSLDataReader(BaseHandlerReader):\n\n    def __init__(\n        self,\n        config: Union[GHSLDataConfig, dict[str, Union[str, int]]],\n        data_store: Optional[DataStore] = None,\n        logger: Optional[logging.Logger] = None,\n    ):\n        \"\"\"\n        Initialize the downloader.\n\n        Args:\n            config: Configuration for the GHSL dataset, either as a GHSLDataConfig object or a dictionary of parameters\n            data_store: Optional data storage interface. If not provided, uses LocalDataStore.\n            logger: Optional custom logger. If not provided, uses default logger.\n        \"\"\"\n        config = (\n            config if isinstance(config, GHSLDataConfig) else GHSLDataConfig(**config)\n        )\n        super().__init__(config=config, data_store=data_store, logger=logger)\n\n    def load_from_paths(\n        self, source_data_path: List[Union[str, Path]], **kwargs\n    ) -&gt; List[TifProcessor]:\n        \"\"\"\n        Load TifProcessors from GHSL dataset.\n        Args:\n            source_data_path: List of file paths to load\n        Returns:\n            List[TifProcessor]: List of TifProcessor objects for accessing the raster data.\n        \"\"\"\n        return self._load_raster_data(raster_paths=source_data_path)\n\n    def load(self, source, **kwargs):\n        return super().load(source=source, file_ext=\".tif\")\n</code></pre>"},{"location":"api/handlers/#gigaspatial.handlers.ghsl.GHSLDataReader.__init__","title":"<code>__init__(config, data_store=None, logger=None)</code>","text":"<p>Initialize the downloader.</p> <p>Parameters:</p> Name Type Description Default <code>config</code> <code>Union[GHSLDataConfig, dict[str, Union[str, int]]]</code> <p>Configuration for the GHSL dataset, either as a GHSLDataConfig object or a dictionary of parameters</p> required <code>data_store</code> <code>Optional[DataStore]</code> <p>Optional data storage interface. If not provided, uses LocalDataStore.</p> <code>None</code> <code>logger</code> <code>Optional[Logger]</code> <p>Optional custom logger. If not provided, uses default logger.</p> <code>None</code> Source code in <code>gigaspatial/handlers/ghsl.py</code> <pre><code>def __init__(\n    self,\n    config: Union[GHSLDataConfig, dict[str, Union[str, int]]],\n    data_store: Optional[DataStore] = None,\n    logger: Optional[logging.Logger] = None,\n):\n    \"\"\"\n    Initialize the downloader.\n\n    Args:\n        config: Configuration for the GHSL dataset, either as a GHSLDataConfig object or a dictionary of parameters\n        data_store: Optional data storage interface. If not provided, uses LocalDataStore.\n        logger: Optional custom logger. If not provided, uses default logger.\n    \"\"\"\n    config = (\n        config if isinstance(config, GHSLDataConfig) else GHSLDataConfig(**config)\n    )\n    super().__init__(config=config, data_store=data_store, logger=logger)\n</code></pre>"},{"location":"api/handlers/#gigaspatial.handlers.ghsl.GHSLDataReader.load_from_paths","title":"<code>load_from_paths(source_data_path, **kwargs)</code>","text":"<p>Load TifProcessors from GHSL dataset. Args:     source_data_path: List of file paths to load Returns:     List[TifProcessor]: List of TifProcessor objects for accessing the raster data.</p> Source code in <code>gigaspatial/handlers/ghsl.py</code> <pre><code>def load_from_paths(\n    self, source_data_path: List[Union[str, Path]], **kwargs\n) -&gt; List[TifProcessor]:\n    \"\"\"\n    Load TifProcessors from GHSL dataset.\n    Args:\n        source_data_path: List of file paths to load\n    Returns:\n        List[TifProcessor]: List of TifProcessor objects for accessing the raster data.\n    \"\"\"\n    return self._load_raster_data(raster_paths=source_data_path)\n</code></pre>"},{"location":"api/handlers/#gigaspatial.handlers.giga","title":"<code>giga</code>","text":""},{"location":"api/handlers/#gigaspatial.handlers.giga.GigaSchoolLocationFetcher","title":"<code>GigaSchoolLocationFetcher</code>","text":"<p>Fetch and process school location data from the Giga School Geolocation Data API.</p> Source code in <code>gigaspatial/handlers/giga.py</code> <pre><code>@dataclass(config=ConfigDict(arbitrary_types_allowed=True))\nclass GigaSchoolLocationFetcher:\n    \"\"\"\n    Fetch and process school location data from the Giga School Geolocation Data API.\n    \"\"\"\n\n    country: str = Field(...)\n    api_url: str = Field(\n        default=\"https://uni-ooi-giga-maps-service.azurewebsites.net/api/v1/schools_location/country/{isocode3}\",\n        description=\"Base URL for the Giga School API\",\n    )\n    api_key: str = global_config.GIGA_SCHOOL_LOCATION_API_KEY\n    page_size: int = Field(default=1000, description=\"Number of records per API page\")\n    sleep_time: float = Field(\n        default=0.2, description=\"Sleep time between API requests\"\n    )\n\n    logger: logging.Logger = Field(default=None, repr=False)\n\n    def __post_init__(self):\n        try:\n            self.country = pycountry.countries.lookup(self.country).alpha_3\n        except LookupError:\n            raise ValueError(f\"Invalid country code provided: {self.country}\")\n        self.api_url = self.api_url.format(isocode3=self.country)\n        if self.logger is None:\n            self.logger = global_config.get_logger(self.__class__.__name__)\n\n    def fetch_locations(self, **kwargs) -&gt; pd.DataFrame:\n        \"\"\"\n        Fetch and process school locations.\n\n        Args:\n            **kwargs: Additional parameters for customization\n                - page_size: Override default page size\n                - sleep_time: Override default sleep time between requests\n                - max_pages: Limit the number of pages to fetch\n\n        Returns:\n            pd.DataFrame: School locations with geospatial info.\n        \"\"\"\n        # Override defaults with kwargs if provided\n        page_size = kwargs.get(\"page_size\", self.page_size)\n        sleep_time = kwargs.get(\"sleep_time\", self.sleep_time)\n        max_pages = kwargs.get(\"max_pages\", None)\n\n        # Prepare headers\n        headers = {\n            \"Authorization\": f\"Bearer {self.api_key}\",\n            \"Accept\": \"application/json\",\n        }\n\n        all_data = []\n        page = 1\n\n        self.logger.info(\n            f\"Starting to fetch school locations for country: {self.country}\"\n        )\n\n        while True:\n            # Check if we've reached max_pages limit\n            if max_pages and page &gt; max_pages:\n                self.logger.info(f\"Reached maximum pages limit: {max_pages}\")\n                break\n\n            params = {\"page\": page, \"size\": page_size}\n\n            try:\n                self.logger.debug(f\"Fetching page {page} with params: {params}\")\n                response = requests.get(self.api_url, headers=headers, params=params)\n                response.raise_for_status()\n\n                parsed = response.json()\n                data = parsed.get(\"data\", [])\n\n            except requests.exceptions.RequestException as e:\n                self.logger.error(f\"Request failed on page {page}: {e}\")\n                break\n            except ValueError as e:\n                self.logger.error(f\"Failed to parse JSON response on page {page}: {e}\")\n                break\n\n            # Check if we got any data\n            if not data:\n                self.logger.info(f\"No data on page {page}. Stopping.\")\n                break\n\n            all_data.extend(data)\n            self.logger.info(f\"Fetched page {page} with {len(data)} records\")\n\n            # If we got fewer records than page_size, we've reached the end\n            if len(data) &lt; page_size:\n                self.logger.info(\"Reached end of data (partial page received)\")\n                break\n\n            page += 1\n\n            # Sleep to be respectful to the API\n            if sleep_time &gt; 0:\n                time.sleep(sleep_time)\n\n        self.logger.info(f\"Finished fetching. Total records: {len(all_data)}\")\n\n        # Convert to DataFrame and process\n        if not all_data:\n            self.logger.warning(\"No data fetched, returning empty DataFrame\")\n            return pd.DataFrame()\n\n        df = pd.DataFrame(all_data)\n\n        df = self._process_geospatial_data(df)\n\n        return df\n\n    def _process_geospatial_data(self, df: pd.DataFrame) -&gt; pd.DataFrame:\n        \"\"\"\n        Process and enhance the DataFrame with geospatial information.\n\n        Args:\n            df: Raw DataFrame from API\n\n        Returns:\n            pd.DataFrame: Enhanced DataFrame with geospatial data\n        \"\"\"\n        if df.empty:\n            return df\n\n        df[\"geometry\"] = df.apply(\n            lambda row: Point(row[\"longitude\"], row[\"latitude\"]), axis=1\n        )\n        self.logger.info(f\"Created geometry for all {len(df)} records\")\n\n        return df\n</code></pre>"},{"location":"api/handlers/#gigaspatial.handlers.giga.GigaSchoolLocationFetcher.fetch_locations","title":"<code>fetch_locations(**kwargs)</code>","text":"<p>Fetch and process school locations.</p> <p>Parameters:</p> Name Type Description Default <code>**kwargs</code> <p>Additional parameters for customization - page_size: Override default page size - sleep_time: Override default sleep time between requests - max_pages: Limit the number of pages to fetch</p> <code>{}</code> <p>Returns:</p> Type Description <code>DataFrame</code> <p>pd.DataFrame: School locations with geospatial info.</p> Source code in <code>gigaspatial/handlers/giga.py</code> <pre><code>def fetch_locations(self, **kwargs) -&gt; pd.DataFrame:\n    \"\"\"\n    Fetch and process school locations.\n\n    Args:\n        **kwargs: Additional parameters for customization\n            - page_size: Override default page size\n            - sleep_time: Override default sleep time between requests\n            - max_pages: Limit the number of pages to fetch\n\n    Returns:\n        pd.DataFrame: School locations with geospatial info.\n    \"\"\"\n    # Override defaults with kwargs if provided\n    page_size = kwargs.get(\"page_size\", self.page_size)\n    sleep_time = kwargs.get(\"sleep_time\", self.sleep_time)\n    max_pages = kwargs.get(\"max_pages\", None)\n\n    # Prepare headers\n    headers = {\n        \"Authorization\": f\"Bearer {self.api_key}\",\n        \"Accept\": \"application/json\",\n    }\n\n    all_data = []\n    page = 1\n\n    self.logger.info(\n        f\"Starting to fetch school locations for country: {self.country}\"\n    )\n\n    while True:\n        # Check if we've reached max_pages limit\n        if max_pages and page &gt; max_pages:\n            self.logger.info(f\"Reached maximum pages limit: {max_pages}\")\n            break\n\n        params = {\"page\": page, \"size\": page_size}\n\n        try:\n            self.logger.debug(f\"Fetching page {page} with params: {params}\")\n            response = requests.get(self.api_url, headers=headers, params=params)\n            response.raise_for_status()\n\n            parsed = response.json()\n            data = parsed.get(\"data\", [])\n\n        except requests.exceptions.RequestException as e:\n            self.logger.error(f\"Request failed on page {page}: {e}\")\n            break\n        except ValueError as e:\n            self.logger.error(f\"Failed to parse JSON response on page {page}: {e}\")\n            break\n\n        # Check if we got any data\n        if not data:\n            self.logger.info(f\"No data on page {page}. Stopping.\")\n            break\n\n        all_data.extend(data)\n        self.logger.info(f\"Fetched page {page} with {len(data)} records\")\n\n        # If we got fewer records than page_size, we've reached the end\n        if len(data) &lt; page_size:\n            self.logger.info(\"Reached end of data (partial page received)\")\n            break\n\n        page += 1\n\n        # Sleep to be respectful to the API\n        if sleep_time &gt; 0:\n            time.sleep(sleep_time)\n\n    self.logger.info(f\"Finished fetching. Total records: {len(all_data)}\")\n\n    # Convert to DataFrame and process\n    if not all_data:\n        self.logger.warning(\"No data fetched, returning empty DataFrame\")\n        return pd.DataFrame()\n\n    df = pd.DataFrame(all_data)\n\n    df = self._process_geospatial_data(df)\n\n    return df\n</code></pre>"},{"location":"api/handlers/#gigaspatial.handlers.google_open_buildings","title":"<code>google_open_buildings</code>","text":""},{"location":"api/handlers/#gigaspatial.handlers.google_open_buildings.GoogleOpenBuildingsConfig","title":"<code>GoogleOpenBuildingsConfig</code>  <code>dataclass</code>","text":"<p>               Bases: <code>BaseHandlerConfig</code></p> <p>Configuration for Google Open Buildings dataset files. Implements the BaseHandlerConfig interface for data unit resolution.</p> Source code in <code>gigaspatial/handlers/google_open_buildings.py</code> <pre><code>@dataclass\nclass GoogleOpenBuildingsConfig(BaseHandlerConfig):\n    \"\"\"\n    Configuration for Google Open Buildings dataset files.\n    Implements the BaseHandlerConfig interface for data unit resolution.\n    \"\"\"\n\n    TILES_URL: str = (\n        \"https://openbuildings-public-dot-gweb-research.uw.r.appspot.com/public/tiles.geojson\"\n    )\n    base_path: Path = global_config.get_path(\"google_open_buildings\", \"bronze\")\n    data_types: tuple = (\"polygons\", \"points\")\n\n    def __post_init__(self):\n        super().__post_init__()\n        self._load_s2_tiles()\n\n    def _load_s2_tiles(self):\n        \"\"\"Load S2 tiles from GeoJSON file.\"\"\"\n        response = requests.get(self.TILES_URL)\n        response.raise_for_status()\n        self.tiles_gdf = gpd.GeoDataFrame.from_features(\n            response.json()[\"features\"], crs=\"EPSG:4326\"\n        )\n\n    def get_relevant_data_units_by_geometry(\n        self, geometry: Union[BaseGeometry, gpd.GeoDataFrame], **kwargs\n    ) -&gt; List[dict]:\n        \"\"\"\n        Return intersecting tiles for a given geometry or GeoDataFrame.\n        \"\"\"\n        return self._get_relevant_tiles(geometry)\n\n    def get_relevant_data_units_by_points(\n        self, points: Iterable[Union[Point, tuple]], **kwargs\n    ) -&gt; List[dict]:\n        \"\"\"\n        Return intersecting tiles for a list of points.\n        \"\"\"\n        return self._get_relevant_tiles(points)\n\n    def get_data_unit_path(\n        self,\n        unit: Union[pd.Series, dict, str],\n        data_type: str = \"polygons\",\n        **kwargs,\n    ) -&gt; Path:\n        \"\"\"\n        Given a tile row or tile_id, return the corresponding file path.\n        \"\"\"\n        tile_id = (\n            unit[\"tile_id\"]\n            if isinstance(unit, pd.Series) or isinstance(unit, dict)\n            else unit\n        )\n        return self.base_path / f\"{data_type}_s2_level_4_{tile_id}_buildings.csv.gz\"\n\n    def get_data_unit_paths(\n        self,\n        units: Union[pd.DataFrame, Iterable[Union[dict, str]]],\n        data_type: str = \"polygons\",\n        **kwargs,\n    ) -&gt; list:\n        \"\"\"\n        Given data unit identifiers, return the corresponding file paths.\n        \"\"\"\n        if isinstance(units, pd.DataFrame):\n            return [\n                self.get_data_unit_path(row, data_type=data_type, **kwargs)\n                for _, row in units.iterrows()\n            ]\n        return super().get_data_unit_paths(units, data_type=data_type)\n\n    def _get_relevant_tiles(\n        self,\n        source: Union[\n            BaseGeometry,\n            gpd.GeoDataFrame,\n            Iterable[Union[Point, tuple]],\n        ],\n    ) -&gt; List[dict]:\n        \"\"\"\n        Identify and return the S2 tiles that spatially intersect with the given geometry.\n        \"\"\"\n        if isinstance(source, gpd.GeoDataFrame):\n            if source.crs != \"EPSG:4326\":\n                source = source.to_crs(\"EPSG:4326\")\n            search_geom = source.geometry.unary_union\n        elif isinstance(source, BaseGeometry):\n            search_geom = source\n        elif isinstance(source, Iterable) and all(\n            len(pt) == 2 or isinstance(pt, Point) for pt in source\n        ):\n            points = [\n                pt if isinstance(pt, Point) else Point(pt[1], pt[0]) for pt in source\n            ]\n            search_geom = MultiPoint(points)\n        else:\n            raise ValueError(\n                f\"Expected Geometry, GeoDataFrame or iterable object of Points got {source.__class__}\"\n            )\n        mask = (\n            tile_geom.intersects(search_geom) for tile_geom in self.tiles_gdf.geometry\n        )\n        return self.tiles_gdf.loc[mask, [\"tile_id\", \"tile_url\", \"size_mb\"]].to_dict(\n            \"records\"\n        )\n</code></pre>"},{"location":"api/handlers/#gigaspatial.handlers.google_open_buildings.GoogleOpenBuildingsConfig.get_data_unit_path","title":"<code>get_data_unit_path(unit, data_type='polygons', **kwargs)</code>","text":"<p>Given a tile row or tile_id, return the corresponding file path.</p> Source code in <code>gigaspatial/handlers/google_open_buildings.py</code> <pre><code>def get_data_unit_path(\n    self,\n    unit: Union[pd.Series, dict, str],\n    data_type: str = \"polygons\",\n    **kwargs,\n) -&gt; Path:\n    \"\"\"\n    Given a tile row or tile_id, return the corresponding file path.\n    \"\"\"\n    tile_id = (\n        unit[\"tile_id\"]\n        if isinstance(unit, pd.Series) or isinstance(unit, dict)\n        else unit\n    )\n    return self.base_path / f\"{data_type}_s2_level_4_{tile_id}_buildings.csv.gz\"\n</code></pre>"},{"location":"api/handlers/#gigaspatial.handlers.google_open_buildings.GoogleOpenBuildingsConfig.get_data_unit_paths","title":"<code>get_data_unit_paths(units, data_type='polygons', **kwargs)</code>","text":"<p>Given data unit identifiers, return the corresponding file paths.</p> Source code in <code>gigaspatial/handlers/google_open_buildings.py</code> <pre><code>def get_data_unit_paths(\n    self,\n    units: Union[pd.DataFrame, Iterable[Union[dict, str]]],\n    data_type: str = \"polygons\",\n    **kwargs,\n) -&gt; list:\n    \"\"\"\n    Given data unit identifiers, return the corresponding file paths.\n    \"\"\"\n    if isinstance(units, pd.DataFrame):\n        return [\n            self.get_data_unit_path(row, data_type=data_type, **kwargs)\n            for _, row in units.iterrows()\n        ]\n    return super().get_data_unit_paths(units, data_type=data_type)\n</code></pre>"},{"location":"api/handlers/#gigaspatial.handlers.google_open_buildings.GoogleOpenBuildingsConfig.get_relevant_data_units_by_geometry","title":"<code>get_relevant_data_units_by_geometry(geometry, **kwargs)</code>","text":"<p>Return intersecting tiles for a given geometry or GeoDataFrame.</p> Source code in <code>gigaspatial/handlers/google_open_buildings.py</code> <pre><code>def get_relevant_data_units_by_geometry(\n    self, geometry: Union[BaseGeometry, gpd.GeoDataFrame], **kwargs\n) -&gt; List[dict]:\n    \"\"\"\n    Return intersecting tiles for a given geometry or GeoDataFrame.\n    \"\"\"\n    return self._get_relevant_tiles(geometry)\n</code></pre>"},{"location":"api/handlers/#gigaspatial.handlers.google_open_buildings.GoogleOpenBuildingsConfig.get_relevant_data_units_by_points","title":"<code>get_relevant_data_units_by_points(points, **kwargs)</code>","text":"<p>Return intersecting tiles for a list of points.</p> Source code in <code>gigaspatial/handlers/google_open_buildings.py</code> <pre><code>def get_relevant_data_units_by_points(\n    self, points: Iterable[Union[Point, tuple]], **kwargs\n) -&gt; List[dict]:\n    \"\"\"\n    Return intersecting tiles for a list of points.\n    \"\"\"\n    return self._get_relevant_tiles(points)\n</code></pre>"},{"location":"api/handlers/#gigaspatial.handlers.google_open_buildings.GoogleOpenBuildingsDownloader","title":"<code>GoogleOpenBuildingsDownloader</code>","text":"<p>               Bases: <code>BaseHandlerDownloader</code></p> <p>A class to handle downloads of Google's Open Buildings dataset.</p> Source code in <code>gigaspatial/handlers/google_open_buildings.py</code> <pre><code>class GoogleOpenBuildingsDownloader(BaseHandlerDownloader):\n    \"\"\"A class to handle downloads of Google's Open Buildings dataset.\"\"\"\n\n    def __init__(\n        self,\n        config: Optional[GoogleOpenBuildingsConfig] = None,\n        data_store: Optional[DataStore] = None,\n        logger: Optional[logging.Logger] = None,\n    ):\n        \"\"\"\n        Initialize the downloader.\n\n        Args:\n            config: Optional configuration for file paths and download settings.\n                    If None, a default `GoogleOpenBuildingsConfig` is used.\n            data_store: Optional instance of a `DataStore` for managing data\n                        storage. If None, a `LocalDataStore` is used.\n            logger: Optional custom logger instance. If None, a default logger\n                    named after the module is created and used.\n        \"\"\"\n        config = config or GoogleOpenBuildingsConfig()\n        super().__init__(config=config, data_store=data_store, logger=logger)\n\n    def download_data_unit(\n        self,\n        tile_info: Union[pd.Series, dict],\n        data_type: Literal[\"polygons\", \"points\"] = \"polygons\",\n    ) -&gt; Optional[str]:\n        \"\"\"Download data file for a single tile.\"\"\"\n\n        tile_url = tile_info[\"tile_url\"]\n        if data_type == \"points\":\n            tile_url = tile_url.replace(\"polygons\", \"points\")\n\n        try:\n            response = requests.get(tile_url, stream=True)\n            response.raise_for_status()\n\n            file_path = str(\n                self.config.get_data_unit_path(\n                    tile_info[\"tile_id\"], data_type=data_type\n                )\n            )\n\n            with self.data_store.open(file_path, \"wb\") as file:\n                for chunk in response.iter_content(chunk_size=8192):\n                    file.write(chunk)\n\n                self.logger.debug(\n                    f\"Successfully downloaded tile: {tile_info['tile_id']}\"\n                )\n                return file_path\n\n        except requests.exceptions.RequestException as e:\n            self.logger.error(\n                f\"Failed to download tile {tile_info['tile_id']}: {str(e)}\"\n            )\n            return None\n        except Exception as e:\n            self.logger.error(f\"Unexpected error downloading dataset: {str(e)}\")\n            return None\n\n    def download_data_units(\n        self,\n        tiles: Union[pd.DataFrame, List[dict]],\n        data_type: Literal[\"polygons\", \"points\"] = \"polygons\",\n    ) -&gt; List[str]:\n        \"\"\"Download data files for multiple tiles.\"\"\"\n\n        if len(tiles) == 0:\n            self.logger.warning(f\"There is no matching data\")\n            return []\n\n        with multiprocessing.Pool(self.config.n_workers) as pool:\n            download_func = functools.partial(\n                self.download_data_unit, data_type=data_type\n            )\n            file_paths = list(\n                tqdm(\n                    pool.imap(\n                        download_func,\n                        (\n                            [row for _, row in tiles.iterrows()]\n                            if isinstance(tiles, pd.DataFrame)\n                            else tiles\n                        ),\n                    ),\n                    total=len(tiles),\n                    desc=f\"Downloading {data_type} data\",\n                )\n            )\n\n        return [path for path in file_paths if path is not None]\n\n    def download(\n        self,\n        source: Union[\n            str,  # country\n            List[Union[Tuple[float, float], Point]],  # points\n            BaseGeometry,  # shapely geoms\n            gpd.GeoDataFrame,\n        ],\n        data_type: Literal[\"polygons\", \"points\"] = \"polygons\",\n        **kwargs,\n    ) -&gt; List[str]:\n        \"\"\"Download Google Open Buildings data for a specified geographic region.\n\n        The region can be defined by a country code/name, a list of points,\n        a Shapely geometry, or a GeoDataFrame. This method identifies the\n        relevant S2 tiles intersecting the region and downloads the\n        specified type of data (polygons or points) for those tiles in parallel.\n\n        Args:\n            source: Defines the geographic area for which to download data.\n                    Can be:\n                      - A string representing a country code or name.\n                      - A list of (latitude, longitude) tuples or Shapely Point objects.\n                      - A Shapely BaseGeometry object (e.g., Polygon, MultiPolygon).\n                      - A GeoDataFrame with geometry column in EPSG:4326.\n            data_type: The type of building data to download ('polygons' or 'points').\n                       Defaults to 'polygons'.\n            **kwargs: Additional keyword arguments that are passed to\n                      `AdminBoundaries.create()` if `source` is a country code.\n                      For example, `path` to a custom boundaries file.\n\n        Returns:\n            A list of local file paths for the successfully downloaded tiles.\n            Returns an empty list if no data is found for the region or if\n            all downloads fail.\n        \"\"\"\n\n        tiles = self.config.get_relevant_data_units(source, **kwargs)\n        return self.download_data_units(tiles, data_type)\n\n    def download_by_country(\n        self,\n        country: str,\n        data_type: Literal[\"polygons\", \"points\"] = \"polygons\",\n        data_store: Optional[DataStore] = None,\n        country_geom_path: Optional[Union[str, Path]] = None,\n    ) -&gt; List[str]:\n        \"\"\"\n        Download Google Open Buildings data for a specific country.\n\n        This is a convenience method to download data for an entire country\n        using its code or name.\n\n        Args:\n            country: The country code (e.g., 'USA', 'GBR') or name.\n            data_type: The type of building data to download ('polygons' or 'points').\n                       Defaults to 'polygons'.\n            data_store: Optional instance of a `DataStore` to be used by\n                        `AdminBoundaries` for loading country boundaries. If None,\n                        `AdminBoundaries` will use its default data loading.\n            country_geom_path: Optional path to a GeoJSON file containing the\n                               country boundary. If provided, this boundary is used\n                               instead of the default from `AdminBoundaries`.\n\n        Returns:\n            A list of local file paths for the successfully downloaded tiles\n            for the specified country.\n        \"\"\"\n        return self.download(\n            source=country,\n            data_type=data_type,\n            data_store=data_store,\n            path=country_geom_path,\n        )\n</code></pre>"},{"location":"api/handlers/#gigaspatial.handlers.google_open_buildings.GoogleOpenBuildingsDownloader.__init__","title":"<code>__init__(config=None, data_store=None, logger=None)</code>","text":"<p>Initialize the downloader.</p> <p>Parameters:</p> Name Type Description Default <code>config</code> <code>Optional[GoogleOpenBuildingsConfig]</code> <p>Optional configuration for file paths and download settings.     If None, a default <code>GoogleOpenBuildingsConfig</code> is used.</p> <code>None</code> <code>data_store</code> <code>Optional[DataStore]</code> <p>Optional instance of a <code>DataStore</code> for managing data         storage. If None, a <code>LocalDataStore</code> is used.</p> <code>None</code> <code>logger</code> <code>Optional[Logger]</code> <p>Optional custom logger instance. If None, a default logger     named after the module is created and used.</p> <code>None</code> Source code in <code>gigaspatial/handlers/google_open_buildings.py</code> <pre><code>def __init__(\n    self,\n    config: Optional[GoogleOpenBuildingsConfig] = None,\n    data_store: Optional[DataStore] = None,\n    logger: Optional[logging.Logger] = None,\n):\n    \"\"\"\n    Initialize the downloader.\n\n    Args:\n        config: Optional configuration for file paths and download settings.\n                If None, a default `GoogleOpenBuildingsConfig` is used.\n        data_store: Optional instance of a `DataStore` for managing data\n                    storage. If None, a `LocalDataStore` is used.\n        logger: Optional custom logger instance. If None, a default logger\n                named after the module is created and used.\n    \"\"\"\n    config = config or GoogleOpenBuildingsConfig()\n    super().__init__(config=config, data_store=data_store, logger=logger)\n</code></pre>"},{"location":"api/handlers/#gigaspatial.handlers.google_open_buildings.GoogleOpenBuildingsDownloader.download","title":"<code>download(source, data_type='polygons', **kwargs)</code>","text":"<p>Download Google Open Buildings data for a specified geographic region.</p> <p>The region can be defined by a country code/name, a list of points, a Shapely geometry, or a GeoDataFrame. This method identifies the relevant S2 tiles intersecting the region and downloads the specified type of data (polygons or points) for those tiles in parallel.</p> <p>Parameters:</p> Name Type Description Default <code>source</code> <code>Union[str, List[Union[Tuple[float, float], Point]], BaseGeometry, GeoDataFrame]</code> <p>Defines the geographic area for which to download data.     Can be:       - A string representing a country code or name.       - A list of (latitude, longitude) tuples or Shapely Point objects.       - A Shapely BaseGeometry object (e.g., Polygon, MultiPolygon).       - A GeoDataFrame with geometry column in EPSG:4326.</p> required <code>data_type</code> <code>Literal['polygons', 'points']</code> <p>The type of building data to download ('polygons' or 'points').        Defaults to 'polygons'.</p> <code>'polygons'</code> <code>**kwargs</code> <p>Additional keyword arguments that are passed to       <code>AdminBoundaries.create()</code> if <code>source</code> is a country code.       For example, <code>path</code> to a custom boundaries file.</p> <code>{}</code> <p>Returns:</p> Type Description <code>List[str]</code> <p>A list of local file paths for the successfully downloaded tiles.</p> <code>List[str]</code> <p>Returns an empty list if no data is found for the region or if</p> <code>List[str]</code> <p>all downloads fail.</p> Source code in <code>gigaspatial/handlers/google_open_buildings.py</code> <pre><code>def download(\n    self,\n    source: Union[\n        str,  # country\n        List[Union[Tuple[float, float], Point]],  # points\n        BaseGeometry,  # shapely geoms\n        gpd.GeoDataFrame,\n    ],\n    data_type: Literal[\"polygons\", \"points\"] = \"polygons\",\n    **kwargs,\n) -&gt; List[str]:\n    \"\"\"Download Google Open Buildings data for a specified geographic region.\n\n    The region can be defined by a country code/name, a list of points,\n    a Shapely geometry, or a GeoDataFrame. This method identifies the\n    relevant S2 tiles intersecting the region and downloads the\n    specified type of data (polygons or points) for those tiles in parallel.\n\n    Args:\n        source: Defines the geographic area for which to download data.\n                Can be:\n                  - A string representing a country code or name.\n                  - A list of (latitude, longitude) tuples or Shapely Point objects.\n                  - A Shapely BaseGeometry object (e.g., Polygon, MultiPolygon).\n                  - A GeoDataFrame with geometry column in EPSG:4326.\n        data_type: The type of building data to download ('polygons' or 'points').\n                   Defaults to 'polygons'.\n        **kwargs: Additional keyword arguments that are passed to\n                  `AdminBoundaries.create()` if `source` is a country code.\n                  For example, `path` to a custom boundaries file.\n\n    Returns:\n        A list of local file paths for the successfully downloaded tiles.\n        Returns an empty list if no data is found for the region or if\n        all downloads fail.\n    \"\"\"\n\n    tiles = self.config.get_relevant_data_units(source, **kwargs)\n    return self.download_data_units(tiles, data_type)\n</code></pre>"},{"location":"api/handlers/#gigaspatial.handlers.google_open_buildings.GoogleOpenBuildingsDownloader.download_by_country","title":"<code>download_by_country(country, data_type='polygons', data_store=None, country_geom_path=None)</code>","text":"<p>Download Google Open Buildings data for a specific country.</p> <p>This is a convenience method to download data for an entire country using its code or name.</p> <p>Parameters:</p> Name Type Description Default <code>country</code> <code>str</code> <p>The country code (e.g., 'USA', 'GBR') or name.</p> required <code>data_type</code> <code>Literal['polygons', 'points']</code> <p>The type of building data to download ('polygons' or 'points').        Defaults to 'polygons'.</p> <code>'polygons'</code> <code>data_store</code> <code>Optional[DataStore]</code> <p>Optional instance of a <code>DataStore</code> to be used by         <code>AdminBoundaries</code> for loading country boundaries. If None,         <code>AdminBoundaries</code> will use its default data loading.</p> <code>None</code> <code>country_geom_path</code> <code>Optional[Union[str, Path]]</code> <p>Optional path to a GeoJSON file containing the                country boundary. If provided, this boundary is used                instead of the default from <code>AdminBoundaries</code>.</p> <code>None</code> <p>Returns:</p> Type Description <code>List[str]</code> <p>A list of local file paths for the successfully downloaded tiles</p> <code>List[str]</code> <p>for the specified country.</p> Source code in <code>gigaspatial/handlers/google_open_buildings.py</code> <pre><code>def download_by_country(\n    self,\n    country: str,\n    data_type: Literal[\"polygons\", \"points\"] = \"polygons\",\n    data_store: Optional[DataStore] = None,\n    country_geom_path: Optional[Union[str, Path]] = None,\n) -&gt; List[str]:\n    \"\"\"\n    Download Google Open Buildings data for a specific country.\n\n    This is a convenience method to download data for an entire country\n    using its code or name.\n\n    Args:\n        country: The country code (e.g., 'USA', 'GBR') or name.\n        data_type: The type of building data to download ('polygons' or 'points').\n                   Defaults to 'polygons'.\n        data_store: Optional instance of a `DataStore` to be used by\n                    `AdminBoundaries` for loading country boundaries. If None,\n                    `AdminBoundaries` will use its default data loading.\n        country_geom_path: Optional path to a GeoJSON file containing the\n                           country boundary. If provided, this boundary is used\n                           instead of the default from `AdminBoundaries`.\n\n    Returns:\n        A list of local file paths for the successfully downloaded tiles\n        for the specified country.\n    \"\"\"\n    return self.download(\n        source=country,\n        data_type=data_type,\n        data_store=data_store,\n        path=country_geom_path,\n    )\n</code></pre>"},{"location":"api/handlers/#gigaspatial.handlers.google_open_buildings.GoogleOpenBuildingsDownloader.download_data_unit","title":"<code>download_data_unit(tile_info, data_type='polygons')</code>","text":"<p>Download data file for a single tile.</p> Source code in <code>gigaspatial/handlers/google_open_buildings.py</code> <pre><code>def download_data_unit(\n    self,\n    tile_info: Union[pd.Series, dict],\n    data_type: Literal[\"polygons\", \"points\"] = \"polygons\",\n) -&gt; Optional[str]:\n    \"\"\"Download data file for a single tile.\"\"\"\n\n    tile_url = tile_info[\"tile_url\"]\n    if data_type == \"points\":\n        tile_url = tile_url.replace(\"polygons\", \"points\")\n\n    try:\n        response = requests.get(tile_url, stream=True)\n        response.raise_for_status()\n\n        file_path = str(\n            self.config.get_data_unit_path(\n                tile_info[\"tile_id\"], data_type=data_type\n            )\n        )\n\n        with self.data_store.open(file_path, \"wb\") as file:\n            for chunk in response.iter_content(chunk_size=8192):\n                file.write(chunk)\n\n            self.logger.debug(\n                f\"Successfully downloaded tile: {tile_info['tile_id']}\"\n            )\n            return file_path\n\n    except requests.exceptions.RequestException as e:\n        self.logger.error(\n            f\"Failed to download tile {tile_info['tile_id']}: {str(e)}\"\n        )\n        return None\n    except Exception as e:\n        self.logger.error(f\"Unexpected error downloading dataset: {str(e)}\")\n        return None\n</code></pre>"},{"location":"api/handlers/#gigaspatial.handlers.google_open_buildings.GoogleOpenBuildingsDownloader.download_data_units","title":"<code>download_data_units(tiles, data_type='polygons')</code>","text":"<p>Download data files for multiple tiles.</p> Source code in <code>gigaspatial/handlers/google_open_buildings.py</code> <pre><code>def download_data_units(\n    self,\n    tiles: Union[pd.DataFrame, List[dict]],\n    data_type: Literal[\"polygons\", \"points\"] = \"polygons\",\n) -&gt; List[str]:\n    \"\"\"Download data files for multiple tiles.\"\"\"\n\n    if len(tiles) == 0:\n        self.logger.warning(f\"There is no matching data\")\n        return []\n\n    with multiprocessing.Pool(self.config.n_workers) as pool:\n        download_func = functools.partial(\n            self.download_data_unit, data_type=data_type\n        )\n        file_paths = list(\n            tqdm(\n                pool.imap(\n                    download_func,\n                    (\n                        [row for _, row in tiles.iterrows()]\n                        if isinstance(tiles, pd.DataFrame)\n                        else tiles\n                    ),\n                ),\n                total=len(tiles),\n                desc=f\"Downloading {data_type} data\",\n            )\n        )\n\n    return [path for path in file_paths if path is not None]\n</code></pre>"},{"location":"api/handlers/#gigaspatial.handlers.google_open_buildings.GoogleOpenBuildingsHandler","title":"<code>GoogleOpenBuildingsHandler</code>","text":"<p>               Bases: <code>BaseHandler</code></p> <p>Handler for Google Open Buildings dataset.</p> <p>This class provides a unified interface for downloading and loading Google Open Buildings data. It manages the lifecycle of configuration, downloading, and reading components.</p> Source code in <code>gigaspatial/handlers/google_open_buildings.py</code> <pre><code>class GoogleOpenBuildingsHandler(BaseHandler):\n    \"\"\"\n    Handler for Google Open Buildings dataset.\n\n    This class provides a unified interface for downloading and loading Google Open Buildings data.\n    It manages the lifecycle of configuration, downloading, and reading components.\n    \"\"\"\n\n    def create_config(\n        self, data_store: DataStore, logger: logging.Logger, **kwargs\n    ) -&gt; GoogleOpenBuildingsConfig:\n        \"\"\"\n        Create and return a GoogleOpenBuildingsConfig instance.\n\n        Args:\n            data_store: The data store instance to use\n            logger: The logger instance to use\n            **kwargs: Additional configuration parameters\n\n        Returns:\n            Configured GoogleOpenBuildingsConfig instance\n        \"\"\"\n        return GoogleOpenBuildingsConfig(data_store=data_store, logger=logger, **kwargs)\n\n    def create_downloader(\n        self,\n        config: GoogleOpenBuildingsConfig,\n        data_store: DataStore,\n        logger: logging.Logger,\n        **kwargs,\n    ) -&gt; GoogleOpenBuildingsDownloader:\n        \"\"\"\n        Create and return a GoogleOpenBuildingsDownloader instance.\n\n        Args:\n            config: The configuration object\n            data_store: The data store instance to use\n            logger: The logger instance to use\n            **kwargs: Additional downloader parameters\n\n        Returns:\n            Configured GoogleOpenBuildingsDownloader instance\n        \"\"\"\n        return GoogleOpenBuildingsDownloader(\n            config=config, data_store=data_store, logger=logger, **kwargs\n        )\n\n    def create_reader(\n        self,\n        config: GoogleOpenBuildingsConfig,\n        data_store: DataStore,\n        logger: logging.Logger,\n        **kwargs,\n    ) -&gt; GoogleOpenBuildingsReader:\n        \"\"\"\n        Create and return a GoogleOpenBuildingsReader instance.\n\n        Args:\n            config: The configuration object\n            data_store: The data store instance to use\n            logger: The logger instance to use\n            **kwargs: Additional reader parameters\n\n        Returns:\n            Configured GoogleOpenBuildingsReader instance\n        \"\"\"\n        return GoogleOpenBuildingsReader(\n            config=config, data_store=data_store, logger=logger, **kwargs\n        )\n\n    def load_points(\n        self,\n        source: Union[\n            str,  # country\n            List[Union[tuple, Point]],  # points\n            BaseGeometry,  # geometry\n            gpd.GeoDataFrame,  # geodataframe\n            Path,  # path\n            List[Union[str, Path]],  # list of paths\n        ],\n        ensure_available: bool = True,\n        **kwargs,\n    ) -&gt; gpd.GeoDataFrame:\n        \"\"\"\n        Load point data from Google Open Buildings dataset.\n\n        Args:\n            source: The data source specification\n            ensure_available: If True, ensure data is downloaded before loading\n            **kwargs: Additional parameters passed to load methods\n\n        Returns:\n            GeoDataFrame containing building point data\n        \"\"\"\n        return self.load_data(\n            source=source,\n            ensure_available=ensure_available,\n            data_type=\"points\",\n            **kwargs,\n        )\n\n    def load_polygons(\n        self,\n        source: Union[\n            str,  # country\n            List[Union[tuple, Point]],  # points\n            BaseGeometry,  # geometry\n            gpd.GeoDataFrame,  # geodataframe\n            Path,  # path\n            List[Union[str, Path]],  # list of paths\n        ],\n        ensure_available: bool = True,\n        **kwargs,\n    ) -&gt; gpd.GeoDataFrame:\n        \"\"\"\n        Load polygon data from Google Open Buildings dataset.\n\n        Args:\n            source: The data source specification\n            ensure_available: If True, ensure data is downloaded before loading\n            **kwargs: Additional parameters passed to load methods\n\n        Returns:\n            GeoDataFrame containing building polygon data\n        \"\"\"\n        return self.load_data(\n            source=source,\n            ensure_available=ensure_available,\n            data_type=\"polygons\",\n            **kwargs,\n        )\n</code></pre>"},{"location":"api/handlers/#gigaspatial.handlers.google_open_buildings.GoogleOpenBuildingsHandler.create_config","title":"<code>create_config(data_store, logger, **kwargs)</code>","text":"<p>Create and return a GoogleOpenBuildingsConfig instance.</p> <p>Parameters:</p> Name Type Description Default <code>data_store</code> <code>DataStore</code> <p>The data store instance to use</p> required <code>logger</code> <code>Logger</code> <p>The logger instance to use</p> required <code>**kwargs</code> <p>Additional configuration parameters</p> <code>{}</code> <p>Returns:</p> Type Description <code>GoogleOpenBuildingsConfig</code> <p>Configured GoogleOpenBuildingsConfig instance</p> Source code in <code>gigaspatial/handlers/google_open_buildings.py</code> <pre><code>def create_config(\n    self, data_store: DataStore, logger: logging.Logger, **kwargs\n) -&gt; GoogleOpenBuildingsConfig:\n    \"\"\"\n    Create and return a GoogleOpenBuildingsConfig instance.\n\n    Args:\n        data_store: The data store instance to use\n        logger: The logger instance to use\n        **kwargs: Additional configuration parameters\n\n    Returns:\n        Configured GoogleOpenBuildingsConfig instance\n    \"\"\"\n    return GoogleOpenBuildingsConfig(data_store=data_store, logger=logger, **kwargs)\n</code></pre>"},{"location":"api/handlers/#gigaspatial.handlers.google_open_buildings.GoogleOpenBuildingsHandler.create_downloader","title":"<code>create_downloader(config, data_store, logger, **kwargs)</code>","text":"<p>Create and return a GoogleOpenBuildingsDownloader instance.</p> <p>Parameters:</p> Name Type Description Default <code>config</code> <code>GoogleOpenBuildingsConfig</code> <p>The configuration object</p> required <code>data_store</code> <code>DataStore</code> <p>The data store instance to use</p> required <code>logger</code> <code>Logger</code> <p>The logger instance to use</p> required <code>**kwargs</code> <p>Additional downloader parameters</p> <code>{}</code> <p>Returns:</p> Type Description <code>GoogleOpenBuildingsDownloader</code> <p>Configured GoogleOpenBuildingsDownloader instance</p> Source code in <code>gigaspatial/handlers/google_open_buildings.py</code> <pre><code>def create_downloader(\n    self,\n    config: GoogleOpenBuildingsConfig,\n    data_store: DataStore,\n    logger: logging.Logger,\n    **kwargs,\n) -&gt; GoogleOpenBuildingsDownloader:\n    \"\"\"\n    Create and return a GoogleOpenBuildingsDownloader instance.\n\n    Args:\n        config: The configuration object\n        data_store: The data store instance to use\n        logger: The logger instance to use\n        **kwargs: Additional downloader parameters\n\n    Returns:\n        Configured GoogleOpenBuildingsDownloader instance\n    \"\"\"\n    return GoogleOpenBuildingsDownloader(\n        config=config, data_store=data_store, logger=logger, **kwargs\n    )\n</code></pre>"},{"location":"api/handlers/#gigaspatial.handlers.google_open_buildings.GoogleOpenBuildingsHandler.create_reader","title":"<code>create_reader(config, data_store, logger, **kwargs)</code>","text":"<p>Create and return a GoogleOpenBuildingsReader instance.</p> <p>Parameters:</p> Name Type Description Default <code>config</code> <code>GoogleOpenBuildingsConfig</code> <p>The configuration object</p> required <code>data_store</code> <code>DataStore</code> <p>The data store instance to use</p> required <code>logger</code> <code>Logger</code> <p>The logger instance to use</p> required <code>**kwargs</code> <p>Additional reader parameters</p> <code>{}</code> <p>Returns:</p> Type Description <code>GoogleOpenBuildingsReader</code> <p>Configured GoogleOpenBuildingsReader instance</p> Source code in <code>gigaspatial/handlers/google_open_buildings.py</code> <pre><code>def create_reader(\n    self,\n    config: GoogleOpenBuildingsConfig,\n    data_store: DataStore,\n    logger: logging.Logger,\n    **kwargs,\n) -&gt; GoogleOpenBuildingsReader:\n    \"\"\"\n    Create and return a GoogleOpenBuildingsReader instance.\n\n    Args:\n        config: The configuration object\n        data_store: The data store instance to use\n        logger: The logger instance to use\n        **kwargs: Additional reader parameters\n\n    Returns:\n        Configured GoogleOpenBuildingsReader instance\n    \"\"\"\n    return GoogleOpenBuildingsReader(\n        config=config, data_store=data_store, logger=logger, **kwargs\n    )\n</code></pre>"},{"location":"api/handlers/#gigaspatial.handlers.google_open_buildings.GoogleOpenBuildingsHandler.load_points","title":"<code>load_points(source, ensure_available=True, **kwargs)</code>","text":"<p>Load point data from Google Open Buildings dataset.</p> <p>Parameters:</p> Name Type Description Default <code>source</code> <code>Union[str, List[Union[tuple, Point]], BaseGeometry, GeoDataFrame, Path, List[Union[str, Path]]]</code> <p>The data source specification</p> required <code>ensure_available</code> <code>bool</code> <p>If True, ensure data is downloaded before loading</p> <code>True</code> <code>**kwargs</code> <p>Additional parameters passed to load methods</p> <code>{}</code> <p>Returns:</p> Type Description <code>GeoDataFrame</code> <p>GeoDataFrame containing building point data</p> Source code in <code>gigaspatial/handlers/google_open_buildings.py</code> <pre><code>def load_points(\n    self,\n    source: Union[\n        str,  # country\n        List[Union[tuple, Point]],  # points\n        BaseGeometry,  # geometry\n        gpd.GeoDataFrame,  # geodataframe\n        Path,  # path\n        List[Union[str, Path]],  # list of paths\n    ],\n    ensure_available: bool = True,\n    **kwargs,\n) -&gt; gpd.GeoDataFrame:\n    \"\"\"\n    Load point data from Google Open Buildings dataset.\n\n    Args:\n        source: The data source specification\n        ensure_available: If True, ensure data is downloaded before loading\n        **kwargs: Additional parameters passed to load methods\n\n    Returns:\n        GeoDataFrame containing building point data\n    \"\"\"\n    return self.load_data(\n        source=source,\n        ensure_available=ensure_available,\n        data_type=\"points\",\n        **kwargs,\n    )\n</code></pre>"},{"location":"api/handlers/#gigaspatial.handlers.google_open_buildings.GoogleOpenBuildingsHandler.load_polygons","title":"<code>load_polygons(source, ensure_available=True, **kwargs)</code>","text":"<p>Load polygon data from Google Open Buildings dataset.</p> <p>Parameters:</p> Name Type Description Default <code>source</code> <code>Union[str, List[Union[tuple, Point]], BaseGeometry, GeoDataFrame, Path, List[Union[str, Path]]]</code> <p>The data source specification</p> required <code>ensure_available</code> <code>bool</code> <p>If True, ensure data is downloaded before loading</p> <code>True</code> <code>**kwargs</code> <p>Additional parameters passed to load methods</p> <code>{}</code> <p>Returns:</p> Type Description <code>GeoDataFrame</code> <p>GeoDataFrame containing building polygon data</p> Source code in <code>gigaspatial/handlers/google_open_buildings.py</code> <pre><code>def load_polygons(\n    self,\n    source: Union[\n        str,  # country\n        List[Union[tuple, Point]],  # points\n        BaseGeometry,  # geometry\n        gpd.GeoDataFrame,  # geodataframe\n        Path,  # path\n        List[Union[str, Path]],  # list of paths\n    ],\n    ensure_available: bool = True,\n    **kwargs,\n) -&gt; gpd.GeoDataFrame:\n    \"\"\"\n    Load polygon data from Google Open Buildings dataset.\n\n    Args:\n        source: The data source specification\n        ensure_available: If True, ensure data is downloaded before loading\n        **kwargs: Additional parameters passed to load methods\n\n    Returns:\n        GeoDataFrame containing building polygon data\n    \"\"\"\n    return self.load_data(\n        source=source,\n        ensure_available=ensure_available,\n        data_type=\"polygons\",\n        **kwargs,\n    )\n</code></pre>"},{"location":"api/handlers/#gigaspatial.handlers.google_open_buildings.GoogleOpenBuildingsReader","title":"<code>GoogleOpenBuildingsReader</code>","text":"<p>               Bases: <code>BaseHandlerReader</code></p> <p>Reader for Google Open Buildings data, supporting country, points, and geometry-based resolution.</p> Source code in <code>gigaspatial/handlers/google_open_buildings.py</code> <pre><code>class GoogleOpenBuildingsReader(BaseHandlerReader):\n    \"\"\"\n    Reader for Google Open Buildings data, supporting country, points, and geometry-based resolution.\n    \"\"\"\n\n    def __init__(\n        self,\n        config: Optional[GoogleOpenBuildingsConfig] = None,\n        data_store: Optional[DataStore] = None,\n        logger: Optional[logging.Logger] = None,\n    ):\n        config = config or GoogleOpenBuildingsConfig()\n        super().__init__(config=config, data_store=data_store, logger=logger)\n\n    def load_from_paths(\n        self, source_data_path: List[Union[str, Path]], **kwargs\n    ) -&gt; gpd.GeoDataFrame:\n        \"\"\"\n        Load building data from Google Open Buildings dataset.\n        Args:\n            source_data_path: List of file paths to load\n        Returns:\n            GeoDataFrame containing building data\n        \"\"\"\n        result = self._load_tabular_data(file_paths=source_data_path)\n        return result\n\n    def load(self, source, data_type=\"polygons\", **kwargs):\n        return super().load(source=source, data_type=data_type, **kwargs)\n\n    def load_points(self, source, **kwargs):\n        \"\"\"This is a convenience method to load points data\"\"\"\n        return self.load(source=source, data_type=\"points\", **kwargs)\n\n    def load_polygons(self, source, **kwargs):\n        \"\"\"This is a convenience method to load polygons data\"\"\"\n        return self.load(source=source, data_type=\"polygons\", **kwargs)\n</code></pre>"},{"location":"api/handlers/#gigaspatial.handlers.google_open_buildings.GoogleOpenBuildingsReader.load_from_paths","title":"<code>load_from_paths(source_data_path, **kwargs)</code>","text":"<p>Load building data from Google Open Buildings dataset. Args:     source_data_path: List of file paths to load Returns:     GeoDataFrame containing building data</p> Source code in <code>gigaspatial/handlers/google_open_buildings.py</code> <pre><code>def load_from_paths(\n    self, source_data_path: List[Union[str, Path]], **kwargs\n) -&gt; gpd.GeoDataFrame:\n    \"\"\"\n    Load building data from Google Open Buildings dataset.\n    Args:\n        source_data_path: List of file paths to load\n    Returns:\n        GeoDataFrame containing building data\n    \"\"\"\n    result = self._load_tabular_data(file_paths=source_data_path)\n    return result\n</code></pre>"},{"location":"api/handlers/#gigaspatial.handlers.google_open_buildings.GoogleOpenBuildingsReader.load_points","title":"<code>load_points(source, **kwargs)</code>","text":"<p>This is a convenience method to load points data</p> Source code in <code>gigaspatial/handlers/google_open_buildings.py</code> <pre><code>def load_points(self, source, **kwargs):\n    \"\"\"This is a convenience method to load points data\"\"\"\n    return self.load(source=source, data_type=\"points\", **kwargs)\n</code></pre>"},{"location":"api/handlers/#gigaspatial.handlers.google_open_buildings.GoogleOpenBuildingsReader.load_polygons","title":"<code>load_polygons(source, **kwargs)</code>","text":"<p>This is a convenience method to load polygons data</p> Source code in <code>gigaspatial/handlers/google_open_buildings.py</code> <pre><code>def load_polygons(self, source, **kwargs):\n    \"\"\"This is a convenience method to load polygons data\"\"\"\n    return self.load(source=source, data_type=\"polygons\", **kwargs)\n</code></pre>"},{"location":"api/handlers/#gigaspatial.handlers.hdx","title":"<code>hdx</code>","text":""},{"location":"api/handlers/#gigaspatial.handlers.hdx.HDXConfig","title":"<code>HDXConfig</code>  <code>dataclass</code>","text":"<p>               Bases: <code>BaseHandlerConfig</code></p> <p>Configuration for HDX data access</p> Source code in <code>gigaspatial/handlers/hdx.py</code> <pre><code>@dataclass(config=ConfigDict(arbitrary_types_allowed=True))\nclass HDXConfig(BaseHandlerConfig):\n    \"\"\"Configuration for HDX data access\"\"\"\n\n    # User configuration\n    dataset_name: str = Field(\n        default=..., description=\"Name of the HDX dataset to download\"\n    )\n\n    # Optional configuration with defaults\n    base_path: Path = Field(default=global_config.get_path(\"hdx\", \"bronze\"))\n    user_agent: str = Field(\n        default=\"gigaspatial\", description=\"User agent for HDX API requests\"\n    )\n    hdx_site: str = Field(default=\"prod\", description=\"HDX site to use (prod or test)\")\n\n    # Internal state\n    _hdx_configured: bool = Field(default=False, init=False)\n    dataset: Optional[Dataset] = Field(default=None, init=False)\n\n    def __post_init__(self):\n        super().__post_init__()\n        try:\n            Configuration.read()\n            self._hdx_configured = True\n        except Exception:\n            self._hdx_configured = False\n        self.configure_hdx()\n        self.dataset = self.fetch_dataset()\n\n    @property\n    def output_dir_path(self) -&gt; Path:\n        \"\"\"Path to save the downloaded HDX dataset\"\"\"\n        return self.base_path / self.dataset_name\n\n    def configure_hdx(self):\n        \"\"\"Configure HDX API if not already configured\"\"\"\n        if not self._hdx_configured:\n            try:\n                Configuration.create(\n                    hdx_site=self.hdx_site,\n                    user_agent=self.user_agent,\n                    hdx_read_only=True,\n                )\n                self._hdx_configured = True\n            except Exception as e:\n                self.logger.error(f\"Error configuring HDX API: {str(e)}\")\n                raise\n\n    def fetch_dataset(self) -&gt; Dataset:\n        \"\"\"Get the HDX dataset\"\"\"\n        try:\n            self.logger.info(f\"Fetching HDX dataset: {self.dataset_name}\")\n            dataset = Dataset.read_from_hdx(self.dataset_name)\n            if not dataset:\n                raise ValueError(f\"Dataset '{self.dataset_name}' not found on HDX\")\n            return dataset\n        except Exception as e:\n            self.logger.error(f\"Error fetching HDX dataset: {str(e)}\")\n            raise\n\n    def _match_pattern(self, value: str, pattern: str) -&gt; bool:\n        \"\"\"Check if a value matches a pattern\"\"\"\n        if isinstance(pattern, str):\n            return pattern.lower() in value.lower()\n        return value == pattern\n\n    def _get_patterns_for_value(self, value: Any) -&gt; List[str]:\n        \"\"\"Generate patterns for a given value or list of values\"\"\"\n        if isinstance(value, list):\n            patterns = []\n            for v in value:\n                patterns.extend(self._get_patterns_for_value(v))\n            return patterns\n\n        if not isinstance(value, str):\n            return [value]\n\n        patterns = []\n        value = value.lower()\n\n        # Add exact match\n        patterns.append(value)\n\n        # Add common variations\n        patterns.extend(\n            [\n                f\"/{value}_\",  # URL path with prefix\n                f\"/{value}.\",  # URL path with extension\n                f\"_{value}_\",  # Filename with value in middle\n                f\"_{value}.\",  # Filename with value at end\n            ]\n        )\n\n        # If value contains spaces, generate additional patterns\n        if \" \" in value:\n            # Generate patterns for space-less version\n            no_space = value.replace(\" \", \"\")\n            patterns.extend(self._get_patterns_for_value(no_space))\n\n            # Generate patterns for hyphenated version\n            hyphenated = value.replace(\" \", \"-\")\n            patterns.extend(self._get_patterns_for_value(hyphenated))\n\n        return patterns\n\n    def get_dataset_resources(\n        self, filter: Optional[Dict[str, Any]] = None, exact_match: bool = False\n    ) -&gt; List[Resource]:\n        \"\"\"Get resources from the HDX dataset\n\n        Args:\n            filter: Dictionary of key-value pairs to filter resources\n            exact_match: If True, perform exact matching. If False, use pattern matching\n        \"\"\"\n        try:\n            resources = self.dataset.get_resources()\n\n            # Apply resource filter if specified\n            if filter:\n                filtered_resources = []\n                for res in resources:\n                    match = True\n                    for key, value in filter.items():\n                        if key not in res.data:\n                            match = False\n                            break\n\n                        if exact_match:\n                            # For exact matching, check if value matches or is in list of values\n                            if isinstance(value, list):\n                                if res.data[key] not in value:\n                                    match = False\n                                    break\n                            elif res.data[key] != value:\n                                match = False\n                                break\n                        else:\n                            # For pattern matching, generate patterns for value(s)\n                            patterns = self._get_patterns_for_value(value)\n                            if not any(\n                                self._match_pattern(str(res.data[key]), pattern)\n                                for pattern in patterns\n                            ):\n                                match = False\n                                break\n\n                    if match:\n                        filtered_resources.append(res)\n                resources = filtered_resources\n\n            return resources\n        except Exception as e:\n            self.logger.error(f\"Error getting dataset resources: {str(e)}\")\n            raise\n\n    def get_relevant_data_units(\n        self, source: Union[str, Dict], **kwargs\n    ) -&gt; List[Resource]:\n        \"\"\"Get relevant data units based on the source type\n\n        Args:\n            source: Either a country name/code (str) or a filter dictionary\n            **kwargs: Additional keyword arguments passed to the specific method\n\n        Returns:\n            List of matching resources\n        \"\"\"\n        if isinstance(source, str):\n            # If source is a string, assume it's a country and use country-based filtering\n            return self.get_relevant_data_units_by_country(source, **kwargs)\n        elif isinstance(source, dict):\n            # If source is a dict, use it directly as a filter\n            return self.get_dataset_resources(filter=source, **kwargs)\n        else:\n            raise ValueError(f\"Unsupported source type: {type(source)}\")\n\n    def get_relevant_data_units_by_geometry(\n        self, geometry: Union[BaseGeometry, gpd.GeoDataFrame], **kwargs\n    ) -&gt; List[Resource]:\n        raise NotImplementedError(\n            \"HDX does not support geometry-based filtering. \"\n            \"Please use country-based filtering or direct resource filtering instead.\"\n        )\n\n    def get_relevant_data_units_by_points(\n        self, points: List[Union[Point, tuple]], **kwargs\n    ) -&gt; List[Resource]:\n        raise NotImplementedError(\n            \"HDX does not support point-based filtering. \"\n            \"Please use country-based filtering or direct resource filtering instead.\"\n        )\n\n    def get_relevant_data_units_by_country(\n        self,\n        country: str,\n        key: str = \"url\",\n        **kwargs,\n    ) -&gt; Any:\n        \"\"\"Get relevant data units for a country\n\n        Args:\n            country: Country name or code\n            key: The key to filter on in the resource data\n            patterns: List of patterns to match against the resource data\n            **kwargs: Additional keyword arguments\n        \"\"\"\n        country = pycountry.countries.lookup(country)\n        values = [country.alpha_3, country.alpha_2, country.name]\n        return self.get_dataset_resources(\n            filter={key: values},\n        )\n\n    def get_data_unit_path(self, unit: str, **kwargs) -&gt; str:\n        \"\"\"Get the path for a data unit\"\"\"\n        try:\n            filename = unit.data[\"name\"]\n        except:\n            filename = unit.get(\"download_url\").split(\"/\")[-1]\n\n        return self.output_dir_path / filename\n\n    def list_resources(self) -&gt; List[str]:\n        \"\"\"List all resources in the dataset directory using the data_store.\"\"\"\n        dataset_folder = str(self.output_dir_path)\n        # Check if the dataset directory exists in the data_store\n        if not (\n            self.data_store.is_dir(dataset_folder)\n            or self.data_store.file_exists(dataset_folder)\n        ):\n            raise FileNotFoundError(\n                f\"HDX dataset not found at {dataset_folder}. \"\n                \"Download the data first using HDXDownloader.\"\n            )\n        return self.data_store.list_files(dataset_folder)\n\n    def __repr__(self) -&gt; str:\n        return (\n            f\"HDXConfig(\\n\"\n            f\"  dataset_name='{self.dataset_name}'\\n\"\n            f\"  base_path='{self.base_path}'\\n\"\n            f\"  hdx_site='{self.hdx_site}'\\n\"\n            f\"  user_agent='{self.user_agent}'\\n\"\n            f\")\"\n        )\n</code></pre>"},{"location":"api/handlers/#gigaspatial.handlers.hdx.HDXConfig.output_dir_path","title":"<code>output_dir_path: Path</code>  <code>property</code>","text":"<p>Path to save the downloaded HDX dataset</p>"},{"location":"api/handlers/#gigaspatial.handlers.hdx.HDXConfig.configure_hdx","title":"<code>configure_hdx()</code>","text":"<p>Configure HDX API if not already configured</p> Source code in <code>gigaspatial/handlers/hdx.py</code> <pre><code>def configure_hdx(self):\n    \"\"\"Configure HDX API if not already configured\"\"\"\n    if not self._hdx_configured:\n        try:\n            Configuration.create(\n                hdx_site=self.hdx_site,\n                user_agent=self.user_agent,\n                hdx_read_only=True,\n            )\n            self._hdx_configured = True\n        except Exception as e:\n            self.logger.error(f\"Error configuring HDX API: {str(e)}\")\n            raise\n</code></pre>"},{"location":"api/handlers/#gigaspatial.handlers.hdx.HDXConfig.fetch_dataset","title":"<code>fetch_dataset()</code>","text":"<p>Get the HDX dataset</p> Source code in <code>gigaspatial/handlers/hdx.py</code> <pre><code>def fetch_dataset(self) -&gt; Dataset:\n    \"\"\"Get the HDX dataset\"\"\"\n    try:\n        self.logger.info(f\"Fetching HDX dataset: {self.dataset_name}\")\n        dataset = Dataset.read_from_hdx(self.dataset_name)\n        if not dataset:\n            raise ValueError(f\"Dataset '{self.dataset_name}' not found on HDX\")\n        return dataset\n    except Exception as e:\n        self.logger.error(f\"Error fetching HDX dataset: {str(e)}\")\n        raise\n</code></pre>"},{"location":"api/handlers/#gigaspatial.handlers.hdx.HDXConfig.get_data_unit_path","title":"<code>get_data_unit_path(unit, **kwargs)</code>","text":"<p>Get the path for a data unit</p> Source code in <code>gigaspatial/handlers/hdx.py</code> <pre><code>def get_data_unit_path(self, unit: str, **kwargs) -&gt; str:\n    \"\"\"Get the path for a data unit\"\"\"\n    try:\n        filename = unit.data[\"name\"]\n    except:\n        filename = unit.get(\"download_url\").split(\"/\")[-1]\n\n    return self.output_dir_path / filename\n</code></pre>"},{"location":"api/handlers/#gigaspatial.handlers.hdx.HDXConfig.get_dataset_resources","title":"<code>get_dataset_resources(filter=None, exact_match=False)</code>","text":"<p>Get resources from the HDX dataset</p> <p>Parameters:</p> Name Type Description Default <code>filter</code> <code>Optional[Dict[str, Any]]</code> <p>Dictionary of key-value pairs to filter resources</p> <code>None</code> <code>exact_match</code> <code>bool</code> <p>If True, perform exact matching. If False, use pattern matching</p> <code>False</code> Source code in <code>gigaspatial/handlers/hdx.py</code> <pre><code>def get_dataset_resources(\n    self, filter: Optional[Dict[str, Any]] = None, exact_match: bool = False\n) -&gt; List[Resource]:\n    \"\"\"Get resources from the HDX dataset\n\n    Args:\n        filter: Dictionary of key-value pairs to filter resources\n        exact_match: If True, perform exact matching. If False, use pattern matching\n    \"\"\"\n    try:\n        resources = self.dataset.get_resources()\n\n        # Apply resource filter if specified\n        if filter:\n            filtered_resources = []\n            for res in resources:\n                match = True\n                for key, value in filter.items():\n                    if key not in res.data:\n                        match = False\n                        break\n\n                    if exact_match:\n                        # For exact matching, check if value matches or is in list of values\n                        if isinstance(value, list):\n                            if res.data[key] not in value:\n                                match = False\n                                break\n                        elif res.data[key] != value:\n                            match = False\n                            break\n                    else:\n                        # For pattern matching, generate patterns for value(s)\n                        patterns = self._get_patterns_for_value(value)\n                        if not any(\n                            self._match_pattern(str(res.data[key]), pattern)\n                            for pattern in patterns\n                        ):\n                            match = False\n                            break\n\n                if match:\n                    filtered_resources.append(res)\n            resources = filtered_resources\n\n        return resources\n    except Exception as e:\n        self.logger.error(f\"Error getting dataset resources: {str(e)}\")\n        raise\n</code></pre>"},{"location":"api/handlers/#gigaspatial.handlers.hdx.HDXConfig.get_relevant_data_units","title":"<code>get_relevant_data_units(source, **kwargs)</code>","text":"<p>Get relevant data units based on the source type</p> <p>Parameters:</p> Name Type Description Default <code>source</code> <code>Union[str, Dict]</code> <p>Either a country name/code (str) or a filter dictionary</p> required <code>**kwargs</code> <p>Additional keyword arguments passed to the specific method</p> <code>{}</code> <p>Returns:</p> Type Description <code>List[Resource]</code> <p>List of matching resources</p> Source code in <code>gigaspatial/handlers/hdx.py</code> <pre><code>def get_relevant_data_units(\n    self, source: Union[str, Dict], **kwargs\n) -&gt; List[Resource]:\n    \"\"\"Get relevant data units based on the source type\n\n    Args:\n        source: Either a country name/code (str) or a filter dictionary\n        **kwargs: Additional keyword arguments passed to the specific method\n\n    Returns:\n        List of matching resources\n    \"\"\"\n    if isinstance(source, str):\n        # If source is a string, assume it's a country and use country-based filtering\n        return self.get_relevant_data_units_by_country(source, **kwargs)\n    elif isinstance(source, dict):\n        # If source is a dict, use it directly as a filter\n        return self.get_dataset_resources(filter=source, **kwargs)\n    else:\n        raise ValueError(f\"Unsupported source type: {type(source)}\")\n</code></pre>"},{"location":"api/handlers/#gigaspatial.handlers.hdx.HDXConfig.get_relevant_data_units_by_country","title":"<code>get_relevant_data_units_by_country(country, key='url', **kwargs)</code>","text":"<p>Get relevant data units for a country</p> <p>Parameters:</p> Name Type Description Default <code>country</code> <code>str</code> <p>Country name or code</p> required <code>key</code> <code>str</code> <p>The key to filter on in the resource data</p> <code>'url'</code> <code>patterns</code> <p>List of patterns to match against the resource data</p> required <code>**kwargs</code> <p>Additional keyword arguments</p> <code>{}</code> Source code in <code>gigaspatial/handlers/hdx.py</code> <pre><code>def get_relevant_data_units_by_country(\n    self,\n    country: str,\n    key: str = \"url\",\n    **kwargs,\n) -&gt; Any:\n    \"\"\"Get relevant data units for a country\n\n    Args:\n        country: Country name or code\n        key: The key to filter on in the resource data\n        patterns: List of patterns to match against the resource data\n        **kwargs: Additional keyword arguments\n    \"\"\"\n    country = pycountry.countries.lookup(country)\n    values = [country.alpha_3, country.alpha_2, country.name]\n    return self.get_dataset_resources(\n        filter={key: values},\n    )\n</code></pre>"},{"location":"api/handlers/#gigaspatial.handlers.hdx.HDXConfig.list_resources","title":"<code>list_resources()</code>","text":"<p>List all resources in the dataset directory using the data_store.</p> Source code in <code>gigaspatial/handlers/hdx.py</code> <pre><code>def list_resources(self) -&gt; List[str]:\n    \"\"\"List all resources in the dataset directory using the data_store.\"\"\"\n    dataset_folder = str(self.output_dir_path)\n    # Check if the dataset directory exists in the data_store\n    if not (\n        self.data_store.is_dir(dataset_folder)\n        or self.data_store.file_exists(dataset_folder)\n    ):\n        raise FileNotFoundError(\n            f\"HDX dataset not found at {dataset_folder}. \"\n            \"Download the data first using HDXDownloader.\"\n        )\n    return self.data_store.list_files(dataset_folder)\n</code></pre>"},{"location":"api/handlers/#gigaspatial.handlers.hdx.HDXDownloader","title":"<code>HDXDownloader</code>","text":"<p>               Bases: <code>BaseHandlerDownloader</code></p> <p>Downloader for HDX datasets</p> Source code in <code>gigaspatial/handlers/hdx.py</code> <pre><code>class HDXDownloader(BaseHandlerDownloader):\n    \"\"\"Downloader for HDX datasets\"\"\"\n\n    def __init__(\n        self,\n        config: Union[HDXConfig, dict],\n        data_store: Optional[DataStore] = None,\n        logger: Optional[logging.Logger] = None,\n    ):\n        config = config if isinstance(config, HDXConfig) else HDXConfig(**config)\n        super().__init__(config=config, data_store=data_store, logger=logger)\n\n    def download_data_unit(self, resource: str, **kwargs) -&gt; str:\n        \"\"\"Download a single resource\"\"\"\n        try:\n            resource_name = resource.get(\"name\", \"Unknown\")\n            self.logger.info(f\"Downloading resource: {resource_name}\")\n\n            with tempfile.TemporaryDirectory() as tmpdir:\n                url, local_path = resource.download(folder=tmpdir)\n                with open(local_path, \"rb\") as f:\n                    data = f.read()\n                # Compose the target path in the DataStore\n                target_path = str(self.config.get_data_unit_path(resource))\n                self.data_store.write_file(target_path, data)\n                self.logger.info(\n                    f\"Downloaded resource: {resource_name} to {target_path}\"\n                )\n                return target_path\n        except Exception as e:\n            self.logger.error(f\"Error downloading resource {resource_name}: {str(e)}\")\n            return None\n\n    def download_data_units(self, resources: List[Resource], **kwargs) -&gt; List[str]:\n        \"\"\"Download multiple resources sequentially\n\n        Args:\n            resources: List of HDX Resource objects\n            **kwargs: Additional keyword arguments\n\n        Returns:\n            List of paths to downloaded files\n        \"\"\"\n        if len(resources) == 0:\n            self.logger.warning(\"There is no resource to download\")\n            return []\n\n        downloaded_paths = []\n        for resource in tqdm(resources, desc=\"Downloading resources\"):\n            path = self.download_data_unit(resource)\n            if path:\n                downloaded_paths.append(path)\n\n        return downloaded_paths\n\n    def download(self, source: Union[Dict, str], **kwargs) -&gt; List[str]:\n        \"\"\"Download data for a source\"\"\"\n        resources = self.config.get_relevant_data_units(source, **kwargs)\n        return self.download_data_units(resources)\n</code></pre>"},{"location":"api/handlers/#gigaspatial.handlers.hdx.HDXDownloader.download","title":"<code>download(source, **kwargs)</code>","text":"<p>Download data for a source</p> Source code in <code>gigaspatial/handlers/hdx.py</code> <pre><code>def download(self, source: Union[Dict, str], **kwargs) -&gt; List[str]:\n    \"\"\"Download data for a source\"\"\"\n    resources = self.config.get_relevant_data_units(source, **kwargs)\n    return self.download_data_units(resources)\n</code></pre>"},{"location":"api/handlers/#gigaspatial.handlers.hdx.HDXDownloader.download_data_unit","title":"<code>download_data_unit(resource, **kwargs)</code>","text":"<p>Download a single resource</p> Source code in <code>gigaspatial/handlers/hdx.py</code> <pre><code>def download_data_unit(self, resource: str, **kwargs) -&gt; str:\n    \"\"\"Download a single resource\"\"\"\n    try:\n        resource_name = resource.get(\"name\", \"Unknown\")\n        self.logger.info(f\"Downloading resource: {resource_name}\")\n\n        with tempfile.TemporaryDirectory() as tmpdir:\n            url, local_path = resource.download(folder=tmpdir)\n            with open(local_path, \"rb\") as f:\n                data = f.read()\n            # Compose the target path in the DataStore\n            target_path = str(self.config.get_data_unit_path(resource))\n            self.data_store.write_file(target_path, data)\n            self.logger.info(\n                f\"Downloaded resource: {resource_name} to {target_path}\"\n            )\n            return target_path\n    except Exception as e:\n        self.logger.error(f\"Error downloading resource {resource_name}: {str(e)}\")\n        return None\n</code></pre>"},{"location":"api/handlers/#gigaspatial.handlers.hdx.HDXDownloader.download_data_units","title":"<code>download_data_units(resources, **kwargs)</code>","text":"<p>Download multiple resources sequentially</p> <p>Parameters:</p> Name Type Description Default <code>resources</code> <code>List[Resource]</code> <p>List of HDX Resource objects</p> required <code>**kwargs</code> <p>Additional keyword arguments</p> <code>{}</code> <p>Returns:</p> Type Description <code>List[str]</code> <p>List of paths to downloaded files</p> Source code in <code>gigaspatial/handlers/hdx.py</code> <pre><code>def download_data_units(self, resources: List[Resource], **kwargs) -&gt; List[str]:\n    \"\"\"Download multiple resources sequentially\n\n    Args:\n        resources: List of HDX Resource objects\n        **kwargs: Additional keyword arguments\n\n    Returns:\n        List of paths to downloaded files\n    \"\"\"\n    if len(resources) == 0:\n        self.logger.warning(\"There is no resource to download\")\n        return []\n\n    downloaded_paths = []\n    for resource in tqdm(resources, desc=\"Downloading resources\"):\n        path = self.download_data_unit(resource)\n        if path:\n            downloaded_paths.append(path)\n\n    return downloaded_paths\n</code></pre>"},{"location":"api/handlers/#gigaspatial.handlers.hdx.HDXHandler","title":"<code>HDXHandler</code>","text":"<p>               Bases: <code>BaseHandler</code></p> <p>Handler for HDX datasets</p> Source code in <code>gigaspatial/handlers/hdx.py</code> <pre><code>class HDXHandler(BaseHandler):\n    \"\"\"Handler for HDX datasets\"\"\"\n\n    def __init__(\n        self,\n        dataset_name: str,\n        config: Optional[HDXConfig] = None,\n        downloader: Optional[HDXDownloader] = None,\n        reader: Optional[HDXReader] = None,\n        data_store: Optional[DataStore] = None,\n        logger: Optional[logging.Logger] = None,\n        **kwargs,\n    ):\n        self._dataset_name = dataset_name\n        super().__init__(\n            config=config,\n            downloader=downloader,\n            reader=reader,\n            data_store=data_store,\n            logger=logger,\n            **kwargs,\n        )\n\n    def create_config(\n        self, data_store: DataStore, logger: logging.Logger, **kwargs\n    ) -&gt; HDXConfig:\n        \"\"\"Create and return a HDXConfig instance\"\"\"\n        return HDXConfig(\n            dataset_name=self._dataset_name,\n            data_store=data_store,\n            logger=logger,\n            **kwargs,\n        )\n\n    def create_downloader(\n        self,\n        config: HDXConfig,\n        data_store: DataStore,\n        logger: logging.Logger,\n        **kwargs,\n    ) -&gt; HDXDownloader:\n        \"\"\"Create and return a HDXDownloader instance\"\"\"\n        return HDXDownloader(\n            config=config,\n            data_store=data_store,\n            logger=logger,\n            **kwargs,\n        )\n\n    def create_reader(\n        self,\n        config: HDXConfig,\n        data_store: DataStore,\n        logger: logging.Logger,\n        **kwargs,\n    ) -&gt; HDXReader:\n        \"\"\"Create and return a HDXReader instance\"\"\"\n        return HDXReader(\n            config=config,\n            data_store=data_store,\n            logger=logger,\n            **kwargs,\n        )\n</code></pre>"},{"location":"api/handlers/#gigaspatial.handlers.hdx.HDXHandler.create_config","title":"<code>create_config(data_store, logger, **kwargs)</code>","text":"<p>Create and return a HDXConfig instance</p> Source code in <code>gigaspatial/handlers/hdx.py</code> <pre><code>def create_config(\n    self, data_store: DataStore, logger: logging.Logger, **kwargs\n) -&gt; HDXConfig:\n    \"\"\"Create and return a HDXConfig instance\"\"\"\n    return HDXConfig(\n        dataset_name=self._dataset_name,\n        data_store=data_store,\n        logger=logger,\n        **kwargs,\n    )\n</code></pre>"},{"location":"api/handlers/#gigaspatial.handlers.hdx.HDXHandler.create_downloader","title":"<code>create_downloader(config, data_store, logger, **kwargs)</code>","text":"<p>Create and return a HDXDownloader instance</p> Source code in <code>gigaspatial/handlers/hdx.py</code> <pre><code>def create_downloader(\n    self,\n    config: HDXConfig,\n    data_store: DataStore,\n    logger: logging.Logger,\n    **kwargs,\n) -&gt; HDXDownloader:\n    \"\"\"Create and return a HDXDownloader instance\"\"\"\n    return HDXDownloader(\n        config=config,\n        data_store=data_store,\n        logger=logger,\n        **kwargs,\n    )\n</code></pre>"},{"location":"api/handlers/#gigaspatial.handlers.hdx.HDXHandler.create_reader","title":"<code>create_reader(config, data_store, logger, **kwargs)</code>","text":"<p>Create and return a HDXReader instance</p> Source code in <code>gigaspatial/handlers/hdx.py</code> <pre><code>def create_reader(\n    self,\n    config: HDXConfig,\n    data_store: DataStore,\n    logger: logging.Logger,\n    **kwargs,\n) -&gt; HDXReader:\n    \"\"\"Create and return a HDXReader instance\"\"\"\n    return HDXReader(\n        config=config,\n        data_store=data_store,\n        logger=logger,\n        **kwargs,\n    )\n</code></pre>"},{"location":"api/handlers/#gigaspatial.handlers.hdx.HDXReader","title":"<code>HDXReader</code>","text":"<p>               Bases: <code>BaseHandlerReader</code></p> <p>Reader for HDX datasets</p> Source code in <code>gigaspatial/handlers/hdx.py</code> <pre><code>class HDXReader(BaseHandlerReader):\n    \"\"\"Reader for HDX datasets\"\"\"\n\n    def __init__(\n        self,\n        config: Optional[HDXConfig] = None,\n        data_store: Optional[DataStore] = None,\n        logger: Optional[logging.Logger] = None,\n    ):\n        config = config if isinstance(config, HDXConfig) else HDXConfig(**config)\n        super().__init__(config=config, data_store=data_store, logger=logger)\n\n    def resolve_source_paths(\n        self,\n        source: Union[\n            str,  # country code\n            Dict,  # filter\n            Path,  # path\n            str,  # path\n            List[Union[str, Path]],\n        ],\n        **kwargs,\n    ) -&gt; List[Union[str, Path]]:\n        if isinstance(source, (str, Path)):\n            # Could be a country code or a path\n            if self.data_store.file_exists(str(source)) or str(source).endswith(\n                (\".csv\", \".tif\", \".json\", \".parquet\", \".gz\", \".geojson\", \".zip\")\n            ):\n                source_data_paths = self.resolve_by_paths(source)\n            else:\n                source_data_paths = self.resolve_by_country(source, **kwargs)\n        elif isinstance(source, Dict):\n            resources = self.config.get_relevant_data_units(source=source, **kwargs)\n            source_data_paths = self.config.get_data_unit_paths(resources, **kwargs)\n        elif isinstance(source, Iterable) and all(\n            isinstance(p, (str, Path)) for p in source\n        ):\n            source_data_paths = self.resolve_by_paths(source)\n        else:\n            raise NotImplementedError(f\"Unsupported source type: {type(source)}\")\n\n        self.logger.info(f\"Resolved {len(source_data_paths)} paths!\")\n        return source_data_paths\n\n    def load_from_paths(\n        self, source_data_path: List[Union[str, Path]], **kwargs\n    ) -&gt; Any:\n        \"\"\"Load data from paths\"\"\"\n        if len(source_data_path)==1:\n            return read_dataset(self.data_store, source_data_path[0])\n\n        all_data = {}\n        for file_path in source_data_path:\n            try:\n                all_data[file_path] = read_dataset(self.data_store, file_path)\n            except Exception as e:\n                raise ValueError(f\"Could not read file {file_path}: {str(e)}\")\n        return all_data\n\n    def load_all_resources(self):\n        resources = self.config.list_resources()\n        return self.load_from_paths(resources)\n</code></pre>"},{"location":"api/handlers/#gigaspatial.handlers.hdx.HDXReader.load_from_paths","title":"<code>load_from_paths(source_data_path, **kwargs)</code>","text":"<p>Load data from paths</p> Source code in <code>gigaspatial/handlers/hdx.py</code> <pre><code>def load_from_paths(\n    self, source_data_path: List[Union[str, Path]], **kwargs\n) -&gt; Any:\n    \"\"\"Load data from paths\"\"\"\n    if len(source_data_path)==1:\n        return read_dataset(self.data_store, source_data_path[0])\n\n    all_data = {}\n    for file_path in source_data_path:\n        try:\n            all_data[file_path] = read_dataset(self.data_store, file_path)\n        except Exception as e:\n            raise ValueError(f\"Could not read file {file_path}: {str(e)}\")\n    return all_data\n</code></pre>"},{"location":"api/handlers/#gigaspatial.handlers.mapbox_image","title":"<code>mapbox_image</code>","text":""},{"location":"api/handlers/#gigaspatial.handlers.mapbox_image.MapboxImageDownloader","title":"<code>MapboxImageDownloader</code>","text":"<p>Class to download images from Mapbox Static Images API using a specific style</p> Source code in <code>gigaspatial/handlers/mapbox_image.py</code> <pre><code>class MapboxImageDownloader:\n    \"\"\"Class to download images from Mapbox Static Images API using a specific style\"\"\"\n\n    BASE_URL = \"https://api.mapbox.com/styles/v1\"\n\n    def __init__(\n        self,\n        access_token: str = config.MAPBOX_ACCESS_TOKEN,\n        style_id: Optional[str] = None,\n        data_store: Optional[DataStore] = None,\n    ):\n        \"\"\"\n        Initialize the downloader with Mapbox credentials\n\n        Args:\n            access_token: Mapbox access token\n            style_id: Mapbox style ID to use for image download\n            data_store: Instance of DataStore for accessing data storage\n        \"\"\"\n        self.access_token = access_token\n        self.style_id = style_id if style_id else \"mapbox/satellite-v9\"\n        self.data_store = data_store or LocalDataStore()\n        self.logger = config.get_logger(self.__class__.__name__)\n\n    def _construct_url(self, bounds: Iterable[float], image_size: str) -&gt; str:\n        \"\"\"Construct the Mapbox Static Images API URL\"\"\"\n        bounds_str = f\"[{','.join(map(str, bounds))}]\"\n\n        return (\n            f\"{self.BASE_URL}/{self.style_id}/static/{bounds_str}/{image_size}\"\n            f\"?access_token={self.access_token}&amp;attribution=false&amp;logo=false\"\n        )\n\n    def _download_single_image(self, url: str, output_path: Path) -&gt; bool:\n        \"\"\"Download a single image from URL\"\"\"\n        try:\n            response = requests.get(url)\n            response.raise_for_status()\n\n            with self.data_store.open(str(output_path), \"wb\") as f:\n                f.write(response.content)\n            return True\n        except Exception as e:\n            self.logger.warning(f\"Error downloading {output_path.name}: {str(e)}\")\n            return False\n\n    def download_images_by_tiles(\n        self,\n        mercator_tiles: \"MercatorTiles\",\n        output_dir: Union[str, Path],\n        image_size: Tuple[int, int] = (512, 512),\n        max_workers: int = 4,\n        image_prefix: str = \"image_\",\n    ) -&gt; None:\n        \"\"\"\n        Download images for given mercator tiles using the specified style\n\n        Args:\n            mercator_tiles: MercatorTiles instance containing quadkeys\n            output_dir: Directory to save images\n            image_size: Tuple of (width, height) for output images\n            max_workers: Maximum number of concurrent downloads\n            image_prefix: Prefix for output image names\n        \"\"\"\n        output_dir = Path(output_dir)\n        # self.data_store.makedirs(str(output_dir), exist_ok=True)\n\n        image_size_str = f\"{image_size[0]}x{image_size[1]}\"\n        total_tiles = len(mercator_tiles.quadkeys)\n\n        self.logger.info(\n            f\"Downloading {total_tiles} tiles with size {image_size_str}...\"\n        )\n\n        def _get_tile_bounds(quadkey: str) -&gt; List[float]:\n            \"\"\"Get tile bounds from quadkey\"\"\"\n            tile = mercantile.quadkey_to_tile(quadkey)\n            bounds = mercantile.bounds(tile)\n            return [bounds.west, bounds.south, bounds.east, bounds.north]\n\n        def download_image(quadkey: str) -&gt; bool:\n            bounds = _get_tile_bounds(quadkey)\n            file_name = f\"{image_prefix}{quadkey}.png\"\n\n            url = self._construct_url(bounds, image_size_str)\n            success = self._download_single_image(url, output_dir / file_name)\n\n            return success\n\n        with ThreadPoolExecutor(max_workers=max_workers) as executor:\n            futures = [\n                executor.submit(download_image, quadkey)\n                for quadkey in mercator_tiles.quadkeys\n            ]\n\n            successful_downloads = 0\n            with tqdm(total=total_tiles) as pbar:\n                for future in as_completed(futures):\n                    if future.result():\n                        successful_downloads += 1\n                    pbar.update(1)\n\n        self.logger.info(\n            f\"Successfully downloaded {successful_downloads}/{total_tiles} images!\"\n        )\n\n    def download_images_by_bounds(\n        self,\n        gdf: gpd.GeoDataFrame,\n        output_dir: Union[str, Path],\n        image_size: Tuple[int, int] = (512, 512),\n        max_workers: int = 4,\n        image_prefix: str = \"image_\",\n    ) -&gt; None:\n        \"\"\"\n        Download images for given points using the specified style\n\n        Args:\n            gdf_points: GeoDataFrame containing bounding box polygons\n            output_dir: Directory to save images\n            image_size: Tuple of (width, height) for output images\n            max_workers: Maximum number of concurrent downloads\n            image_prefix: Prefix for output image names\n        \"\"\"\n        output_dir = Path(output_dir)\n        # self.data_store.makedirs(str(output_dir), exist_ok=True)\n\n        image_size_str = f\"{image_size[0]}x{image_size[1]}\"\n        total_images = len(gdf)\n\n        self.logger.info(\n            f\"Downloading {total_images} images with size {image_size_str}...\"\n        )\n\n        def download_image(idx: Any, bounds: Tuple[float, float, float, float]) -&gt; bool:\n            file_name = f\"{image_prefix}{idx}.png\"\n            url = self._construct_url(bounds, image_size_str)\n            success = self._download_single_image(url, output_dir / file_name)\n            return success\n\n        with ThreadPoolExecutor(max_workers=max_workers) as executor:\n            futures = [\n                executor.submit(download_image, row.Index, row.geometry.bounds)\n                for row in gdf.itertuples()\n            ]\n\n            successful_downloads = 0\n            with tqdm(total=total_images) as pbar:\n                for future in as_completed(futures):\n                    if future.result():\n                        successful_downloads += 1\n                    pbar.update(1)\n\n        self.logger.info(\n            f\"Successfully downloaded {successful_downloads}/{total_images} images!\"\n        )\n\n    def download_images_by_coordinates(\n        self,\n        data: Union[pd.DataFrame, List[Tuple[float, float]]],\n        res_meters_pixel: float,\n        output_dir: Union[str, Path],\n        image_size: Tuple[int, int] = (512, 512),\n        max_workers: int = 4,\n        image_prefix: str = \"image_\",\n    ) -&gt; None:\n        \"\"\"\n        Download images for given coordinates by creating bounded boxes around points\n\n        Args:\n            data: Either a DataFrame with either latitude/longitude columns or a geometry column or a list of (lat, lon) tuples\n            res_meters_pixel: Size of the bounding box in meters (creates a square)\n            output_dir: Directory to save images\n            image_size: Tuple of (width, height) for output images\n            max_workers: Maximum number of concurrent downloads\n            image_prefix: Prefix for output image names\n        \"\"\"\n\n        if isinstance(data, pd.DataFrame):\n            coordinates_df = data\n        else:\n            coordinates_df = pd.DataFrame(data, columns=[\"latitude\", \"longitude\"])\n\n        gdf = convert_to_geodataframe(coordinates_df)\n\n        buffered_gdf = buffer_geodataframe(\n            gdf, res_meters_pixel / 2, cap_style=\"square\"\n        )\n\n        self.download_images_by_bounds(\n            buffered_gdf, output_dir, image_size, max_workers, image_prefix\n        )\n</code></pre>"},{"location":"api/handlers/#gigaspatial.handlers.mapbox_image.MapboxImageDownloader.__init__","title":"<code>__init__(access_token=config.MAPBOX_ACCESS_TOKEN, style_id=None, data_store=None)</code>","text":"<p>Initialize the downloader with Mapbox credentials</p> <p>Parameters:</p> Name Type Description Default <code>access_token</code> <code>str</code> <p>Mapbox access token</p> <code>MAPBOX_ACCESS_TOKEN</code> <code>style_id</code> <code>Optional[str]</code> <p>Mapbox style ID to use for image download</p> <code>None</code> <code>data_store</code> <code>Optional[DataStore]</code> <p>Instance of DataStore for accessing data storage</p> <code>None</code> Source code in <code>gigaspatial/handlers/mapbox_image.py</code> <pre><code>def __init__(\n    self,\n    access_token: str = config.MAPBOX_ACCESS_TOKEN,\n    style_id: Optional[str] = None,\n    data_store: Optional[DataStore] = None,\n):\n    \"\"\"\n    Initialize the downloader with Mapbox credentials\n\n    Args:\n        access_token: Mapbox access token\n        style_id: Mapbox style ID to use for image download\n        data_store: Instance of DataStore for accessing data storage\n    \"\"\"\n    self.access_token = access_token\n    self.style_id = style_id if style_id else \"mapbox/satellite-v9\"\n    self.data_store = data_store or LocalDataStore()\n    self.logger = config.get_logger(self.__class__.__name__)\n</code></pre>"},{"location":"api/handlers/#gigaspatial.handlers.mapbox_image.MapboxImageDownloader.download_images_by_bounds","title":"<code>download_images_by_bounds(gdf, output_dir, image_size=(512, 512), max_workers=4, image_prefix='image_')</code>","text":"<p>Download images for given points using the specified style</p> <p>Parameters:</p> Name Type Description Default <code>gdf_points</code> <p>GeoDataFrame containing bounding box polygons</p> required <code>output_dir</code> <code>Union[str, Path]</code> <p>Directory to save images</p> required <code>image_size</code> <code>Tuple[int, int]</code> <p>Tuple of (width, height) for output images</p> <code>(512, 512)</code> <code>max_workers</code> <code>int</code> <p>Maximum number of concurrent downloads</p> <code>4</code> <code>image_prefix</code> <code>str</code> <p>Prefix for output image names</p> <code>'image_'</code> Source code in <code>gigaspatial/handlers/mapbox_image.py</code> <pre><code>def download_images_by_bounds(\n    self,\n    gdf: gpd.GeoDataFrame,\n    output_dir: Union[str, Path],\n    image_size: Tuple[int, int] = (512, 512),\n    max_workers: int = 4,\n    image_prefix: str = \"image_\",\n) -&gt; None:\n    \"\"\"\n    Download images for given points using the specified style\n\n    Args:\n        gdf_points: GeoDataFrame containing bounding box polygons\n        output_dir: Directory to save images\n        image_size: Tuple of (width, height) for output images\n        max_workers: Maximum number of concurrent downloads\n        image_prefix: Prefix for output image names\n    \"\"\"\n    output_dir = Path(output_dir)\n    # self.data_store.makedirs(str(output_dir), exist_ok=True)\n\n    image_size_str = f\"{image_size[0]}x{image_size[1]}\"\n    total_images = len(gdf)\n\n    self.logger.info(\n        f\"Downloading {total_images} images with size {image_size_str}...\"\n    )\n\n    def download_image(idx: Any, bounds: Tuple[float, float, float, float]) -&gt; bool:\n        file_name = f\"{image_prefix}{idx}.png\"\n        url = self._construct_url(bounds, image_size_str)\n        success = self._download_single_image(url, output_dir / file_name)\n        return success\n\n    with ThreadPoolExecutor(max_workers=max_workers) as executor:\n        futures = [\n            executor.submit(download_image, row.Index, row.geometry.bounds)\n            for row in gdf.itertuples()\n        ]\n\n        successful_downloads = 0\n        with tqdm(total=total_images) as pbar:\n            for future in as_completed(futures):\n                if future.result():\n                    successful_downloads += 1\n                pbar.update(1)\n\n    self.logger.info(\n        f\"Successfully downloaded {successful_downloads}/{total_images} images!\"\n    )\n</code></pre>"},{"location":"api/handlers/#gigaspatial.handlers.mapbox_image.MapboxImageDownloader.download_images_by_coordinates","title":"<code>download_images_by_coordinates(data, res_meters_pixel, output_dir, image_size=(512, 512), max_workers=4, image_prefix='image_')</code>","text":"<p>Download images for given coordinates by creating bounded boxes around points</p> <p>Parameters:</p> Name Type Description Default <code>data</code> <code>Union[DataFrame, List[Tuple[float, float]]]</code> <p>Either a DataFrame with either latitude/longitude columns or a geometry column or a list of (lat, lon) tuples</p> required <code>res_meters_pixel</code> <code>float</code> <p>Size of the bounding box in meters (creates a square)</p> required <code>output_dir</code> <code>Union[str, Path]</code> <p>Directory to save images</p> required <code>image_size</code> <code>Tuple[int, int]</code> <p>Tuple of (width, height) for output images</p> <code>(512, 512)</code> <code>max_workers</code> <code>int</code> <p>Maximum number of concurrent downloads</p> <code>4</code> <code>image_prefix</code> <code>str</code> <p>Prefix for output image names</p> <code>'image_'</code> Source code in <code>gigaspatial/handlers/mapbox_image.py</code> <pre><code>def download_images_by_coordinates(\n    self,\n    data: Union[pd.DataFrame, List[Tuple[float, float]]],\n    res_meters_pixel: float,\n    output_dir: Union[str, Path],\n    image_size: Tuple[int, int] = (512, 512),\n    max_workers: int = 4,\n    image_prefix: str = \"image_\",\n) -&gt; None:\n    \"\"\"\n    Download images for given coordinates by creating bounded boxes around points\n\n    Args:\n        data: Either a DataFrame with either latitude/longitude columns or a geometry column or a list of (lat, lon) tuples\n        res_meters_pixel: Size of the bounding box in meters (creates a square)\n        output_dir: Directory to save images\n        image_size: Tuple of (width, height) for output images\n        max_workers: Maximum number of concurrent downloads\n        image_prefix: Prefix for output image names\n    \"\"\"\n\n    if isinstance(data, pd.DataFrame):\n        coordinates_df = data\n    else:\n        coordinates_df = pd.DataFrame(data, columns=[\"latitude\", \"longitude\"])\n\n    gdf = convert_to_geodataframe(coordinates_df)\n\n    buffered_gdf = buffer_geodataframe(\n        gdf, res_meters_pixel / 2, cap_style=\"square\"\n    )\n\n    self.download_images_by_bounds(\n        buffered_gdf, output_dir, image_size, max_workers, image_prefix\n    )\n</code></pre>"},{"location":"api/handlers/#gigaspatial.handlers.mapbox_image.MapboxImageDownloader.download_images_by_tiles","title":"<code>download_images_by_tiles(mercator_tiles, output_dir, image_size=(512, 512), max_workers=4, image_prefix='image_')</code>","text":"<p>Download images for given mercator tiles using the specified style</p> <p>Parameters:</p> Name Type Description Default <code>mercator_tiles</code> <code>MercatorTiles</code> <p>MercatorTiles instance containing quadkeys</p> required <code>output_dir</code> <code>Union[str, Path]</code> <p>Directory to save images</p> required <code>image_size</code> <code>Tuple[int, int]</code> <p>Tuple of (width, height) for output images</p> <code>(512, 512)</code> <code>max_workers</code> <code>int</code> <p>Maximum number of concurrent downloads</p> <code>4</code> <code>image_prefix</code> <code>str</code> <p>Prefix for output image names</p> <code>'image_'</code> Source code in <code>gigaspatial/handlers/mapbox_image.py</code> <pre><code>def download_images_by_tiles(\n    self,\n    mercator_tiles: \"MercatorTiles\",\n    output_dir: Union[str, Path],\n    image_size: Tuple[int, int] = (512, 512),\n    max_workers: int = 4,\n    image_prefix: str = \"image_\",\n) -&gt; None:\n    \"\"\"\n    Download images for given mercator tiles using the specified style\n\n    Args:\n        mercator_tiles: MercatorTiles instance containing quadkeys\n        output_dir: Directory to save images\n        image_size: Tuple of (width, height) for output images\n        max_workers: Maximum number of concurrent downloads\n        image_prefix: Prefix for output image names\n    \"\"\"\n    output_dir = Path(output_dir)\n    # self.data_store.makedirs(str(output_dir), exist_ok=True)\n\n    image_size_str = f\"{image_size[0]}x{image_size[1]}\"\n    total_tiles = len(mercator_tiles.quadkeys)\n\n    self.logger.info(\n        f\"Downloading {total_tiles} tiles with size {image_size_str}...\"\n    )\n\n    def _get_tile_bounds(quadkey: str) -&gt; List[float]:\n        \"\"\"Get tile bounds from quadkey\"\"\"\n        tile = mercantile.quadkey_to_tile(quadkey)\n        bounds = mercantile.bounds(tile)\n        return [bounds.west, bounds.south, bounds.east, bounds.north]\n\n    def download_image(quadkey: str) -&gt; bool:\n        bounds = _get_tile_bounds(quadkey)\n        file_name = f\"{image_prefix}{quadkey}.png\"\n\n        url = self._construct_url(bounds, image_size_str)\n        success = self._download_single_image(url, output_dir / file_name)\n\n        return success\n\n    with ThreadPoolExecutor(max_workers=max_workers) as executor:\n        futures = [\n            executor.submit(download_image, quadkey)\n            for quadkey in mercator_tiles.quadkeys\n        ]\n\n        successful_downloads = 0\n        with tqdm(total=total_tiles) as pbar:\n            for future in as_completed(futures):\n                if future.result():\n                    successful_downloads += 1\n                pbar.update(1)\n\n    self.logger.info(\n        f\"Successfully downloaded {successful_downloads}/{total_tiles} images!\"\n    )\n</code></pre>"},{"location":"api/handlers/#gigaspatial.handlers.maxar_image","title":"<code>maxar_image</code>","text":""},{"location":"api/handlers/#gigaspatial.handlers.maxar_image.MaxarConfig","title":"<code>MaxarConfig</code>","text":"<p>               Bases: <code>BaseModel</code></p> <p>Configuration for Maxar Image Downloader using Pydantic</p> Source code in <code>gigaspatial/handlers/maxar_image.py</code> <pre><code>class MaxarConfig(BaseModel):\n    \"\"\"Configuration for Maxar Image Downloader using Pydantic\"\"\"\n\n    username: str = Field(\n        default=global_config.MAXAR_USERNAME, description=\"Maxar API username\"\n    )\n    password: str = Field(\n        default=global_config.MAXAR_PASSWORD, description=\"Maxar API password\"\n    )\n    connection_string: str = Field(\n        default=global_config.MAXAR_CONNECTION_STRING,\n        description=\"Maxar WMS connection string\",\n    )\n\n    base_url: HttpUrl = Field(\n        default=\"https://evwhs.digitalglobe.com/mapservice/wmsaccess?\",\n        description=\"Base URL for Maxar WMS service\",\n    )\n\n    layers: List[Literal[\"DigitalGlobe:ImageryFootprint\", \"DigitalGlobe:Imagery\"]] = (\n        Field(\n            default=[\"DigitalGlobe:Imagery\"],\n            description=\"List of layers to request from WMS\",\n        )\n    )\n\n    feature_profile: str = Field(\n        default=\"Most_Aesthetic_Mosaic_Profile\",\n        description=\"Feature profile to use for WMS requests\",\n    )\n\n    coverage_cql_filter: str = Field(\n        default=\"\", description=\"CQL filter for coverage selection\"\n    )\n\n    exceptions: str = Field(\n        default=\"application/vnd.ogc.se_xml\",\n        description=\"Exception handling format for WMS\",\n    )\n\n    transparent: bool = Field(\n        default=True,\n        description=\"Whether the requested images should have transparency\",\n    )\n\n    image_format: Literal[\"image/png\", \"image/jpeg\", \"image/geotiff\"] = Field(\n        default=\"image/png\",\n    )\n\n    data_crs: Literal[\"EPSG:4326\", \"EPSG:3395\", \"EPSG:3857\", \"CAR:42004\"] = Field(\n        default=\"EPSG:4326\"\n    )\n\n    max_retries: int = Field(\n        default=3, description=\"Number of retries for failed image downloads\"\n    )\n\n    retry_delay: int = Field(default=5, description=\"Delay in seconds between retries\")\n\n    @field_validator(\"username\", \"password\", \"connection_string\")\n    @classmethod\n    def validate_non_empty(cls, value: str, field) -&gt; str:\n        \"\"\"Ensure required credentials are provided\"\"\"\n        if not value or value.strip() == \"\":\n            raise ValueError(\n                f\"{field.name} cannot be empty. Please provide a valid {field.name}.\"\n            )\n        return value\n\n    @property\n    def wms_url(self) -&gt; str:\n        \"\"\"Generate the full WMS URL with connection string\"\"\"\n        return f\"{self.base_url}connectid={self.connection_string}\"\n\n    @property\n    def suffix(self) -&gt; str:\n        return f\".{self.image_format.split('/')[1]}\"\n</code></pre>"},{"location":"api/handlers/#gigaspatial.handlers.maxar_image.MaxarConfig.wms_url","title":"<code>wms_url: str</code>  <code>property</code>","text":"<p>Generate the full WMS URL with connection string</p>"},{"location":"api/handlers/#gigaspatial.handlers.maxar_image.MaxarConfig.validate_non_empty","title":"<code>validate_non_empty(value, field)</code>  <code>classmethod</code>","text":"<p>Ensure required credentials are provided</p> Source code in <code>gigaspatial/handlers/maxar_image.py</code> <pre><code>@field_validator(\"username\", \"password\", \"connection_string\")\n@classmethod\ndef validate_non_empty(cls, value: str, field) -&gt; str:\n    \"\"\"Ensure required credentials are provided\"\"\"\n    if not value or value.strip() == \"\":\n        raise ValueError(\n            f\"{field.name} cannot be empty. Please provide a valid {field.name}.\"\n        )\n    return value\n</code></pre>"},{"location":"api/handlers/#gigaspatial.handlers.maxar_image.MaxarImageDownloader","title":"<code>MaxarImageDownloader</code>","text":"<p>Class to download images from Maxar</p> Source code in <code>gigaspatial/handlers/maxar_image.py</code> <pre><code>class MaxarImageDownloader:\n    \"\"\"Class to download images from Maxar\"\"\"\n\n    def __init__(\n        self,\n        config: Optional[MaxarConfig] = None,\n        data_store: Optional[DataStore] = None,\n    ):\n        \"\"\"\n        Initialize the downloader with Maxar config.\n\n        Args:\n            config: MaxarConfig instance containing credentials and settings\n            data_store: Instance of DataStore for accessing data storage\n        \"\"\"\n        self.config = config or MaxarConfig()\n        self.wms = WebMapService(\n            self.config.wms_url,\n            username=self.config.username,\n            password=self.config.password,\n        )\n        self.data_store = data_store or LocalDataStore()\n        self.logger = global_config.get_logger(self.__class__.__name__)\n\n    def _download_single_image(self, bbox, output_path: Union[Path, str], size) -&gt; bool:\n        \"\"\"Download a single image from bbox and pixel size\"\"\"\n        for attempt in range(self.config.max_retries):\n            try:\n                img_data = self.wms.getmap(\n                    bbox=bbox,\n                    layers=self.config.layers,\n                    srs=self.config.data_crs,\n                    size=size,\n                    featureProfile=self.config.feature_profile,\n                    coverage_cql_filter=self.config.coverage_cql_filter,\n                    exceptions=self.config.exceptions,\n                    transparent=self.config.transparent,\n                    format=self.config.image_format,\n                )\n                self.data_store.write_file(str(output_path), img_data.read())\n                return True\n            except Exception as e:\n                self.logger.warning(\n                    f\"Attempt {attempt + 1} of downloading {output_path.name} failed: {str(e)}\"\n                )\n                if attempt &lt; self.max_retries - 1:\n                    sleep(self.config.retry_delay)\n                else:\n                    self.logger.warning(\n                        f\"Failed to download {output_path.name} after {self.config.max_retries} attemps: {str(e)}\"\n                    )\n                    return False\n\n    def download_images_by_tiles(\n        self,\n        mercator_tiles: \"MercatorTiles\",\n        output_dir: Union[str, Path],\n        image_size: Tuple[int, int] = (512, 512),\n        image_prefix: str = \"maxar_image_\",\n    ) -&gt; None:\n        \"\"\"\n        Download images for given mercator tiles using the specified style\n\n        Args:\n            mercator_tiles: MercatorTiles instance containing quadkeys\n            output_dir: Directory to save images\n            image_size: Tuple of (width, height) for output images\n            image_prefix: Prefix for output image names\n        \"\"\"\n        output_dir = Path(output_dir)\n\n        image_size_str = f\"{image_size[0]}x{image_size[1]}\"\n        total_tiles = len(mercator_tiles.quadkeys)\n\n        self.logger.info(\n            f\"Downloading {total_tiles} tiles with size {image_size_str}...\"\n        )\n\n        def _get_tile_bounds(quadkey: str) -&gt; Tuple[float]:\n            \"\"\"Get tile bounds from quadkey\"\"\"\n            tile = mercantile.quadkey_to_tile(quadkey)\n            bounds = mercantile.bounds(tile)\n            return (bounds.west, bounds.south, bounds.east, bounds.north)\n\n        def download_image(\n            quadkey: str, image_size: Tuple[int, int], suffix: str = self.config.suffix\n        ) -&gt; bool:\n            bounds = _get_tile_bounds(quadkey)\n            file_name = f\"{image_prefix}{quadkey}{suffix}\"\n\n            success = self._download_single_image(\n                bounds, output_dir / file_name, image_size\n            )\n\n            return success\n\n        successful_downloads = 0\n        with tqdm(total=total_tiles) as pbar:\n            for quadkey in mercator_tiles.quadkeys:\n                if download_image(quadkey, image_size):\n                    successful_downloads += 1\n                pbar.update(1)\n\n        self.logger.info(\n            f\"Successfully downloaded {successful_downloads}/{total_tiles} images!\"\n        )\n\n    def download_images_by_bounds(\n        self,\n        gdf: gpd.GeoDataFrame,\n        output_dir: Union[str, Path],\n        image_size: Tuple[int, int] = (512, 512),\n        image_prefix: str = \"maxar_image_\",\n    ) -&gt; None:\n        \"\"\"\n        Download images for given points using the specified style\n\n        Args:\n            gdf_points: GeoDataFrame containing bounding box polygons\n            output_dir: Directory to save images\n            image_size: Tuple of (width, height) for output images\n            image_prefix: Prefix for output image names\n        \"\"\"\n        output_dir = Path(output_dir)\n\n        image_size_str = f\"{image_size[0]}x{image_size[1]}\"\n        total_images = len(gdf)\n\n        self.logger.info(\n            f\"Downloading {total_images} images with size {image_size_str}...\"\n        )\n\n        def download_image(\n            idx: Any,\n            bounds: Tuple[float, float, float, float],\n            image_size,\n            suffix: str = self.config.suffix,\n        ) -&gt; bool:\n            file_name = f\"{image_prefix}{idx}{suffix}\"\n            success = self._download_single_image(\n                bounds, output_dir / file_name, image_size\n            )\n            return success\n\n        gdf = gdf.to_crs(self.config.data_crs)\n\n        successful_downloads = 0\n        with tqdm(total=total_images) as pbar:\n            for row in gdf.itertuples():\n                if download_image(row.Index, tuple(row.geometry.bounds), image_size):\n                    successful_downloads += 1\n                pbar.update(1)\n\n        self.logger.info(\n            f\"Successfully downloaded {successful_downloads}/{total_images} images!\"\n        )\n\n    def download_images_by_coordinates(\n        self,\n        data: Union[pd.DataFrame, List[Tuple[float, float]]],\n        res_meters_pixel: float,\n        output_dir: Union[str, Path],\n        image_size: Tuple[int, int] = (512, 512),\n        image_prefix: str = \"maxar_image_\",\n    ) -&gt; None:\n        \"\"\"\n        Download images for given coordinates by creating bounded boxes around points\n\n        Args:\n            data: Either a DataFrame with either latitude/longitude columns or a geometry column or a list of (lat, lon) tuples\n            res_meters_pixel: resolution in meters per pixel\n            output_dir: Directory to save images\n            image_size: Tuple of (width, height) for output images\n            image_prefix: Prefix for output image names\n        \"\"\"\n\n        if isinstance(data, pd.DataFrame):\n            coordinates_df = data\n        else:\n            coordinates_df = pd.DataFrame(data, columns=[\"latitude\", \"longitude\"])\n\n        gdf = convert_to_geodataframe(coordinates_df)\n\n        buffered_gdf = buffer_geodataframe(\n            gdf, res_meters_pixel / 2, cap_style=\"square\"\n        )\n\n        buffered_gdf = buffered_gdf.to_crs(self.config.data_crs)\n\n        self.download_images_by_bounds(\n            buffered_gdf, output_dir, image_size, image_prefix\n        )\n</code></pre>"},{"location":"api/handlers/#gigaspatial.handlers.maxar_image.MaxarImageDownloader.__init__","title":"<code>__init__(config=None, data_store=None)</code>","text":"<p>Initialize the downloader with Maxar config.</p> <p>Parameters:</p> Name Type Description Default <code>config</code> <code>Optional[MaxarConfig]</code> <p>MaxarConfig instance containing credentials and settings</p> <code>None</code> <code>data_store</code> <code>Optional[DataStore]</code> <p>Instance of DataStore for accessing data storage</p> <code>None</code> Source code in <code>gigaspatial/handlers/maxar_image.py</code> <pre><code>def __init__(\n    self,\n    config: Optional[MaxarConfig] = None,\n    data_store: Optional[DataStore] = None,\n):\n    \"\"\"\n    Initialize the downloader with Maxar config.\n\n    Args:\n        config: MaxarConfig instance containing credentials and settings\n        data_store: Instance of DataStore for accessing data storage\n    \"\"\"\n    self.config = config or MaxarConfig()\n    self.wms = WebMapService(\n        self.config.wms_url,\n        username=self.config.username,\n        password=self.config.password,\n    )\n    self.data_store = data_store or LocalDataStore()\n    self.logger = global_config.get_logger(self.__class__.__name__)\n</code></pre>"},{"location":"api/handlers/#gigaspatial.handlers.maxar_image.MaxarImageDownloader.download_images_by_bounds","title":"<code>download_images_by_bounds(gdf, output_dir, image_size=(512, 512), image_prefix='maxar_image_')</code>","text":"<p>Download images for given points using the specified style</p> <p>Parameters:</p> Name Type Description Default <code>gdf_points</code> <p>GeoDataFrame containing bounding box polygons</p> required <code>output_dir</code> <code>Union[str, Path]</code> <p>Directory to save images</p> required <code>image_size</code> <code>Tuple[int, int]</code> <p>Tuple of (width, height) for output images</p> <code>(512, 512)</code> <code>image_prefix</code> <code>str</code> <p>Prefix for output image names</p> <code>'maxar_image_'</code> Source code in <code>gigaspatial/handlers/maxar_image.py</code> <pre><code>def download_images_by_bounds(\n    self,\n    gdf: gpd.GeoDataFrame,\n    output_dir: Union[str, Path],\n    image_size: Tuple[int, int] = (512, 512),\n    image_prefix: str = \"maxar_image_\",\n) -&gt; None:\n    \"\"\"\n    Download images for given points using the specified style\n\n    Args:\n        gdf_points: GeoDataFrame containing bounding box polygons\n        output_dir: Directory to save images\n        image_size: Tuple of (width, height) for output images\n        image_prefix: Prefix for output image names\n    \"\"\"\n    output_dir = Path(output_dir)\n\n    image_size_str = f\"{image_size[0]}x{image_size[1]}\"\n    total_images = len(gdf)\n\n    self.logger.info(\n        f\"Downloading {total_images} images with size {image_size_str}...\"\n    )\n\n    def download_image(\n        idx: Any,\n        bounds: Tuple[float, float, float, float],\n        image_size,\n        suffix: str = self.config.suffix,\n    ) -&gt; bool:\n        file_name = f\"{image_prefix}{idx}{suffix}\"\n        success = self._download_single_image(\n            bounds, output_dir / file_name, image_size\n        )\n        return success\n\n    gdf = gdf.to_crs(self.config.data_crs)\n\n    successful_downloads = 0\n    with tqdm(total=total_images) as pbar:\n        for row in gdf.itertuples():\n            if download_image(row.Index, tuple(row.geometry.bounds), image_size):\n                successful_downloads += 1\n            pbar.update(1)\n\n    self.logger.info(\n        f\"Successfully downloaded {successful_downloads}/{total_images} images!\"\n    )\n</code></pre>"},{"location":"api/handlers/#gigaspatial.handlers.maxar_image.MaxarImageDownloader.download_images_by_coordinates","title":"<code>download_images_by_coordinates(data, res_meters_pixel, output_dir, image_size=(512, 512), image_prefix='maxar_image_')</code>","text":"<p>Download images for given coordinates by creating bounded boxes around points</p> <p>Parameters:</p> Name Type Description Default <code>data</code> <code>Union[DataFrame, List[Tuple[float, float]]]</code> <p>Either a DataFrame with either latitude/longitude columns or a geometry column or a list of (lat, lon) tuples</p> required <code>res_meters_pixel</code> <code>float</code> <p>resolution in meters per pixel</p> required <code>output_dir</code> <code>Union[str, Path]</code> <p>Directory to save images</p> required <code>image_size</code> <code>Tuple[int, int]</code> <p>Tuple of (width, height) for output images</p> <code>(512, 512)</code> <code>image_prefix</code> <code>str</code> <p>Prefix for output image names</p> <code>'maxar_image_'</code> Source code in <code>gigaspatial/handlers/maxar_image.py</code> <pre><code>def download_images_by_coordinates(\n    self,\n    data: Union[pd.DataFrame, List[Tuple[float, float]]],\n    res_meters_pixel: float,\n    output_dir: Union[str, Path],\n    image_size: Tuple[int, int] = (512, 512),\n    image_prefix: str = \"maxar_image_\",\n) -&gt; None:\n    \"\"\"\n    Download images for given coordinates by creating bounded boxes around points\n\n    Args:\n        data: Either a DataFrame with either latitude/longitude columns or a geometry column or a list of (lat, lon) tuples\n        res_meters_pixel: resolution in meters per pixel\n        output_dir: Directory to save images\n        image_size: Tuple of (width, height) for output images\n        image_prefix: Prefix for output image names\n    \"\"\"\n\n    if isinstance(data, pd.DataFrame):\n        coordinates_df = data\n    else:\n        coordinates_df = pd.DataFrame(data, columns=[\"latitude\", \"longitude\"])\n\n    gdf = convert_to_geodataframe(coordinates_df)\n\n    buffered_gdf = buffer_geodataframe(\n        gdf, res_meters_pixel / 2, cap_style=\"square\"\n    )\n\n    buffered_gdf = buffered_gdf.to_crs(self.config.data_crs)\n\n    self.download_images_by_bounds(\n        buffered_gdf, output_dir, image_size, image_prefix\n    )\n</code></pre>"},{"location":"api/handlers/#gigaspatial.handlers.maxar_image.MaxarImageDownloader.download_images_by_tiles","title":"<code>download_images_by_tiles(mercator_tiles, output_dir, image_size=(512, 512), image_prefix='maxar_image_')</code>","text":"<p>Download images for given mercator tiles using the specified style</p> <p>Parameters:</p> Name Type Description Default <code>mercator_tiles</code> <code>MercatorTiles</code> <p>MercatorTiles instance containing quadkeys</p> required <code>output_dir</code> <code>Union[str, Path]</code> <p>Directory to save images</p> required <code>image_size</code> <code>Tuple[int, int]</code> <p>Tuple of (width, height) for output images</p> <code>(512, 512)</code> <code>image_prefix</code> <code>str</code> <p>Prefix for output image names</p> <code>'maxar_image_'</code> Source code in <code>gigaspatial/handlers/maxar_image.py</code> <pre><code>def download_images_by_tiles(\n    self,\n    mercator_tiles: \"MercatorTiles\",\n    output_dir: Union[str, Path],\n    image_size: Tuple[int, int] = (512, 512),\n    image_prefix: str = \"maxar_image_\",\n) -&gt; None:\n    \"\"\"\n    Download images for given mercator tiles using the specified style\n\n    Args:\n        mercator_tiles: MercatorTiles instance containing quadkeys\n        output_dir: Directory to save images\n        image_size: Tuple of (width, height) for output images\n        image_prefix: Prefix for output image names\n    \"\"\"\n    output_dir = Path(output_dir)\n\n    image_size_str = f\"{image_size[0]}x{image_size[1]}\"\n    total_tiles = len(mercator_tiles.quadkeys)\n\n    self.logger.info(\n        f\"Downloading {total_tiles} tiles with size {image_size_str}...\"\n    )\n\n    def _get_tile_bounds(quadkey: str) -&gt; Tuple[float]:\n        \"\"\"Get tile bounds from quadkey\"\"\"\n        tile = mercantile.quadkey_to_tile(quadkey)\n        bounds = mercantile.bounds(tile)\n        return (bounds.west, bounds.south, bounds.east, bounds.north)\n\n    def download_image(\n        quadkey: str, image_size: Tuple[int, int], suffix: str = self.config.suffix\n    ) -&gt; bool:\n        bounds = _get_tile_bounds(quadkey)\n        file_name = f\"{image_prefix}{quadkey}{suffix}\"\n\n        success = self._download_single_image(\n            bounds, output_dir / file_name, image_size\n        )\n\n        return success\n\n    successful_downloads = 0\n    with tqdm(total=total_tiles) as pbar:\n        for quadkey in mercator_tiles.quadkeys:\n            if download_image(quadkey, image_size):\n                successful_downloads += 1\n            pbar.update(1)\n\n    self.logger.info(\n        f\"Successfully downloaded {successful_downloads}/{total_tiles} images!\"\n    )\n</code></pre>"},{"location":"api/handlers/#gigaspatial.handlers.microsoft_global_buildings","title":"<code>microsoft_global_buildings</code>","text":""},{"location":"api/handlers/#gigaspatial.handlers.microsoft_global_buildings.MSBuildingsConfig","title":"<code>MSBuildingsConfig</code>  <code>dataclass</code>","text":"<p>               Bases: <code>BaseHandlerConfig</code></p> <p>Configuration for Microsoft Global Buildings dataset files.</p> Source code in <code>gigaspatial/handlers/microsoft_global_buildings.py</code> <pre><code>@dataclass(config=ConfigDict(arbitrary_types_allowed=True))\nclass MSBuildingsConfig(BaseHandlerConfig):\n    \"\"\"Configuration for Microsoft Global Buildings dataset files.\"\"\"\n\n    TILE_URLS: str = (\n        \"https://minedbuildings.z5.web.core.windows.net/global-buildings/dataset-links.csv\"\n    )\n    MERCATOR_ZOOM_LEVEL: int = 9\n    base_path: Path = global_config.get_path(\"microsoft_global_buildings\", \"bronze\")\n\n    LOCATION_MAPPING_FILE: Path = base_path / \"location_mapping.json\"\n    SIMILARITY_SCORE: float = 0.8\n    DEFAULT_MAPPING: Dict[str, str] = field(\n        default_factory=lambda: {\n            \"Bonaire\": \"BES\",\n            \"Brunei\": \"BRN\",\n            \"IvoryCoast\": \"CIV\",\n            \"CongoDRC\": \"COD\",\n            \"DemocraticRepublicoftheCongo\": \"COD\",\n            \"RepublicoftheCongo\": \"COG\",\n            \"TheGambia\": \"GMB\",\n            \"FYROMakedonija\": \"MKD\",\n            \"SultanateofOman\": \"OMN\",\n            \"StateofQatar\": \"QAT\",\n            \"Russia\": \"RUS\",\n            \"KingdomofSaudiArabia\": \"SAU\",\n            \"Svalbard\": \"SJM\",\n            \"Swaziland\": \"SWZ\",\n            \"StMartin\": \"SXM\",\n            \"leSaint-Martin\": \"MAF\",\n            \"Turkey\": \"TUR\",\n            \"VaticanCity\": \"VAT\",\n            \"BritishVirginIslands\": \"VGB\",\n            \"USVirginIslands\": \"VIR\",\n            \"RepublicofYemen\": \"YEM\",\n            \"CzechRepublic\": \"CZE\",\n            \"French-Martinique\": \"MTQ\",\n            \"French-Guadeloupe\": \"GLP\",\n            \"UnitedStates\": \"USA\",\n        }\n    )\n    CUSTOM_MAPPING: Optional[Dict[str, str]] = None\n\n    def __post_init__(self):\n        \"\"\"Initialize the configuration, load tile URLs, and set up location mapping.\"\"\"\n        super().__post_init__()\n        self._load_tile_urls()\n        self.upload_date = self.df_tiles.upload_date[0]\n        self._setup_location_mapping()\n\n    def _load_tile_urls(self):\n        \"\"\"Load dataset links from csv file.\"\"\"\n        self.df_tiles = pd.read_csv(\n            self.TILE_URLS,\n            names=[\"location\", \"quadkey\", \"url\", \"size\", \"upload_date\"],\n            dtype={\"quadkey\": str},\n            header=0,\n        )\n\n    def _setup_location_mapping(self):\n        \"\"\"Load or create the mapping between dataset locations and ISO country codes.\"\"\"\n        from gigaspatial.core.io.readers import read_json\n        from gigaspatial.core.io.writers import write_json\n\n        if self.data_store.file_exists(str(self.LOCATION_MAPPING_FILE)):\n            self.location_mapping = read_json(\n                self.data_store, str(self.LOCATION_MAPPING_FILE)\n            )\n        else:\n            self.location_mapping = self.create_location_mapping(\n                similarity_score_threshold=self.SIMILARITY_SCORE\n            )\n            self.location_mapping.update(self.DEFAULT_MAPPING)\n            write_json(\n                self.location_mapping, self.data_store, str(self.LOCATION_MAPPING_FILE)\n            )\n\n        self.location_mapping.update(self.CUSTOM_MAPPING or {})\n        self._map_locations()\n        self.df_tiles.loc[self.df_tiles.country.isnull(), \"country\"] = None\n\n    def _map_locations(self):\n        \"\"\"Map the 'location' column in the tiles DataFrame to ISO country codes.\"\"\"\n        self.df_tiles[\"country\"] = self.df_tiles.location.map(self.location_mapping)\n\n    def create_location_mapping(self, similarity_score_threshold: float = 0.8):\n        \"\"\"\n        Create a mapping between the dataset's location names and ISO 3166-1 alpha-3 country codes.\n\n        This function iterates through known countries and attempts to find matching\n        locations in the dataset based on string similarity.\n\n        Args:\n            similarity_score_threshold: The minimum similarity score (between 0 and 1)\n                                        for a dataset location to be considered a match\n                                        for a country. Defaults to 0.8.\n\n        Returns:\n            A dictionary where keys are dataset location names and values are\n            the corresponding ISO 3166-1 alpha-3 country codes.\n        \"\"\"\n\n        def similar(a, b):\n            return SequenceMatcher(None, a, b).ratio()\n\n        location_mapping = dict()\n\n        for country in pycountry.countries:\n            if country.name not in self.df_tiles.location.unique():\n                try:\n                    country_quadkey = CountryMercatorTiles.create(\n                        country.alpha_3, self.MERCATOR_ZOOM_LEVEL\n                    )\n                except:\n                    self.logger.warning(f\"{country.name} is not mapped.\")\n                    continue\n                country_datasets = country_quadkey.filter_quadkeys(\n                    self.df_tiles.quadkey\n                )\n                matching_locations = self.df_tiles[\n                    self.df_tiles.quadkey.isin(country_datasets.quadkeys)\n                ].location.unique()\n                scores = np.array(\n                    [\n                        (\n                            similar(c, country.common_name)\n                            if hasattr(country, \"common_name\")\n                            else similar(c, country.name)\n                        )\n                        for c in matching_locations\n                    ]\n                )\n                if any(scores &gt; similarity_score_threshold):\n                    matched = matching_locations[scores &gt; similarity_score_threshold]\n                    if len(matched) &gt; 2:\n                        self.logger.warning(\n                            f\"Multiple matches exist for {country.name}. {country.name} is not mapped.\"\n                        )\n                    location_mapping[matched[0]] = country.alpha_3\n                    self.logger.debug(f\"{country.name} matched with {matched[0]}!\")\n                else:\n                    self.logger.warning(\n                        f\"No direct matches for {country.name}. {country.name} is not mapped.\"\n                    )\n                    self.logger.debug(\"Possible matches are: \")\n                    for c, score in zip(matching_locations, scores):\n                        self.logger.debug(c, score)\n            else:\n                location_mapping[country.name] = country.alpha_3\n\n        return location_mapping\n\n    def get_relevant_data_units_by_geometry(\n        self, geometry: Union[BaseGeometry, gpd.GeoDataFrame], **kwargs\n    ) -&gt; pd.DataFrame:\n        \"\"\"\n        Return intersecting tiles for a given geometry or GeoDataFrame.\n        \"\"\"\n        return self._get_relevant_tiles(geometry)\n\n    def get_relevant_data_units_by_points(\n        self, points: Iterable[Union[Point, tuple]], **kwargs\n    ) -&gt; pd.DataFrame:\n        \"\"\"\n        Return intersecting tiles for a list of points.\n        \"\"\"\n        return self._get_relevant_tiles(points)\n\n    def get_relevant_data_units_by_country(\n        self, country: str, **kwargs\n    ) -&gt; pd.DataFrame:\n        \"\"\"\n        Return intersecting tiles for a given country.\n        \"\"\"\n        return self._get_relevant_tiles(country)\n\n    def get_data_unit_path(self, unit: Union[pd.Series, dict], **kwargs) -&gt; Path:\n\n        tile_location = unit[\"country\"] if unit[\"country\"] else unit[\"location\"]\n\n        return (\n            self.base_path\n            / tile_location\n            / self.upload_date\n            / f'{unit[\"quadkey\"]}.csv.gz'\n        )\n\n    def get_data_unit_paths(\n        self, units: Union[pd.DataFrame, Iterable[dict]], **kwargs\n    ) -&gt; List:\n        if isinstance(units, pd.DataFrame):\n            return [self.get_data_unit_path(row) for _, row in units.iterrows()]\n        return super().get_data_unit_paths(units)\n\n    def _get_relevant_tiles(\n        self,\n        source: Union[\n            str,  # country\n            BaseGeometry,  # shapely geoms\n            gpd.GeoDataFrame,\n            Iterable[Union[Point, Tuple[float, float]]],  # points\n        ],\n    ) -&gt; pd.DataFrame:\n        \"\"\"\n        Get the DataFrame of Microsoft Buildings tiles that intersect with a given source spatial geometry.\n\n        In case country given, this method first tries to find tiles directly mapped to the given country.\n        If no directly mapped tiles are found and the country is not in the location\n        mapping, it attempts to find overlapping tiles by creating Mercator tiles\n        for the country and filtering the dataset's tiles.\n\n        Args:\n            source: A country code/name, a Shapely geometry, a GeoDataFrame, or a list of Point\n                    objects or (lat, lon) tuples representing the area of interest.\n                    The coordinates are assumed to be in EPSG:4326.\n\n        Returns:\n            A pandas DataFrame containing the rows from the tiles list that\n            spatially intersect with the `source`. Returns an empty DataFrame\n            if no intersecting tiles are found.\n        \"\"\"\n        if isinstance(source, str):\n            try:\n                country_code = pycountry.countries.lookup(source).alpha_3\n            except:\n                raise ValueError(\"Invalid`country` value!\")\n\n            mask = self.df_tiles[\"country\"] == country_code\n\n            if any(mask):\n                return self.df_tiles.loc[\n                    mask, [\"quadkey\", \"url\", \"country\", \"location\"]\n                ].to_dict(\"records\")\n\n            self.logger.warning(\n                f\"The country code '{country_code}' is not directly in the location mapping. \"\n                \"Manually checking for overlapping locations with the country boundary.\"\n            )\n\n            source_tiles = CountryMercatorTiles.create(\n                country_code, self.MERCATOR_ZOOM_LEVEL\n            )\n        else:\n            source_tiles = MercatorTiles.from_spatial(\n                source=source, zoom_level=self.MERCATOR_ZOOM_LEVEL\n            )\n\n        filtered_tiles = source_tiles.filter_quadkeys(self.df_tiles.quadkey)\n\n        mask = self.df_tiles.quadkey.isin(filtered_tiles.quadkeys)\n\n        return self.df_tiles.loc[\n            mask, [\"quadkey\", \"url\", \"country\", \"location\"]\n        ].to_dict(\"records\")\n</code></pre>"},{"location":"api/handlers/#gigaspatial.handlers.microsoft_global_buildings.MSBuildingsConfig.__post_init__","title":"<code>__post_init__()</code>","text":"<p>Initialize the configuration, load tile URLs, and set up location mapping.</p> Source code in <code>gigaspatial/handlers/microsoft_global_buildings.py</code> <pre><code>def __post_init__(self):\n    \"\"\"Initialize the configuration, load tile URLs, and set up location mapping.\"\"\"\n    super().__post_init__()\n    self._load_tile_urls()\n    self.upload_date = self.df_tiles.upload_date[0]\n    self._setup_location_mapping()\n</code></pre>"},{"location":"api/handlers/#gigaspatial.handlers.microsoft_global_buildings.MSBuildingsConfig.create_location_mapping","title":"<code>create_location_mapping(similarity_score_threshold=0.8)</code>","text":"<p>Create a mapping between the dataset's location names and ISO 3166-1 alpha-3 country codes.</p> <p>This function iterates through known countries and attempts to find matching locations in the dataset based on string similarity.</p> <p>Parameters:</p> Name Type Description Default <code>similarity_score_threshold</code> <code>float</code> <p>The minimum similarity score (between 0 and 1)                         for a dataset location to be considered a match                         for a country. Defaults to 0.8.</p> <code>0.8</code> <p>Returns:</p> Type Description <p>A dictionary where keys are dataset location names and values are</p> <p>the corresponding ISO 3166-1 alpha-3 country codes.</p> Source code in <code>gigaspatial/handlers/microsoft_global_buildings.py</code> <pre><code>def create_location_mapping(self, similarity_score_threshold: float = 0.8):\n    \"\"\"\n    Create a mapping between the dataset's location names and ISO 3166-1 alpha-3 country codes.\n\n    This function iterates through known countries and attempts to find matching\n    locations in the dataset based on string similarity.\n\n    Args:\n        similarity_score_threshold: The minimum similarity score (between 0 and 1)\n                                    for a dataset location to be considered a match\n                                    for a country. Defaults to 0.8.\n\n    Returns:\n        A dictionary where keys are dataset location names and values are\n        the corresponding ISO 3166-1 alpha-3 country codes.\n    \"\"\"\n\n    def similar(a, b):\n        return SequenceMatcher(None, a, b).ratio()\n\n    location_mapping = dict()\n\n    for country in pycountry.countries:\n        if country.name not in self.df_tiles.location.unique():\n            try:\n                country_quadkey = CountryMercatorTiles.create(\n                    country.alpha_3, self.MERCATOR_ZOOM_LEVEL\n                )\n            except:\n                self.logger.warning(f\"{country.name} is not mapped.\")\n                continue\n            country_datasets = country_quadkey.filter_quadkeys(\n                self.df_tiles.quadkey\n            )\n            matching_locations = self.df_tiles[\n                self.df_tiles.quadkey.isin(country_datasets.quadkeys)\n            ].location.unique()\n            scores = np.array(\n                [\n                    (\n                        similar(c, country.common_name)\n                        if hasattr(country, \"common_name\")\n                        else similar(c, country.name)\n                    )\n                    for c in matching_locations\n                ]\n            )\n            if any(scores &gt; similarity_score_threshold):\n                matched = matching_locations[scores &gt; similarity_score_threshold]\n                if len(matched) &gt; 2:\n                    self.logger.warning(\n                        f\"Multiple matches exist for {country.name}. {country.name} is not mapped.\"\n                    )\n                location_mapping[matched[0]] = country.alpha_3\n                self.logger.debug(f\"{country.name} matched with {matched[0]}!\")\n            else:\n                self.logger.warning(\n                    f\"No direct matches for {country.name}. {country.name} is not mapped.\"\n                )\n                self.logger.debug(\"Possible matches are: \")\n                for c, score in zip(matching_locations, scores):\n                    self.logger.debug(c, score)\n        else:\n            location_mapping[country.name] = country.alpha_3\n\n    return location_mapping\n</code></pre>"},{"location":"api/handlers/#gigaspatial.handlers.microsoft_global_buildings.MSBuildingsConfig.get_relevant_data_units_by_country","title":"<code>get_relevant_data_units_by_country(country, **kwargs)</code>","text":"<p>Return intersecting tiles for a given country.</p> Source code in <code>gigaspatial/handlers/microsoft_global_buildings.py</code> <pre><code>def get_relevant_data_units_by_country(\n    self, country: str, **kwargs\n) -&gt; pd.DataFrame:\n    \"\"\"\n    Return intersecting tiles for a given country.\n    \"\"\"\n    return self._get_relevant_tiles(country)\n</code></pre>"},{"location":"api/handlers/#gigaspatial.handlers.microsoft_global_buildings.MSBuildingsConfig.get_relevant_data_units_by_geometry","title":"<code>get_relevant_data_units_by_geometry(geometry, **kwargs)</code>","text":"<p>Return intersecting tiles for a given geometry or GeoDataFrame.</p> Source code in <code>gigaspatial/handlers/microsoft_global_buildings.py</code> <pre><code>def get_relevant_data_units_by_geometry(\n    self, geometry: Union[BaseGeometry, gpd.GeoDataFrame], **kwargs\n) -&gt; pd.DataFrame:\n    \"\"\"\n    Return intersecting tiles for a given geometry or GeoDataFrame.\n    \"\"\"\n    return self._get_relevant_tiles(geometry)\n</code></pre>"},{"location":"api/handlers/#gigaspatial.handlers.microsoft_global_buildings.MSBuildingsConfig.get_relevant_data_units_by_points","title":"<code>get_relevant_data_units_by_points(points, **kwargs)</code>","text":"<p>Return intersecting tiles for a list of points.</p> Source code in <code>gigaspatial/handlers/microsoft_global_buildings.py</code> <pre><code>def get_relevant_data_units_by_points(\n    self, points: Iterable[Union[Point, tuple]], **kwargs\n) -&gt; pd.DataFrame:\n    \"\"\"\n    Return intersecting tiles for a list of points.\n    \"\"\"\n    return self._get_relevant_tiles(points)\n</code></pre>"},{"location":"api/handlers/#gigaspatial.handlers.microsoft_global_buildings.MSBuildingsDownloader","title":"<code>MSBuildingsDownloader</code>","text":"<p>               Bases: <code>BaseHandlerDownloader</code></p> <p>A class to handle downloads of Microsoft's Global ML Building Footprints dataset.</p> Source code in <code>gigaspatial/handlers/microsoft_global_buildings.py</code> <pre><code>class MSBuildingsDownloader(BaseHandlerDownloader):\n    \"\"\"A class to handle downloads of Microsoft's Global ML Building Footprints dataset.\"\"\"\n\n    def __init__(\n        self,\n        config: Optional[MSBuildingsConfig] = None,\n        data_store: Optional[DataStore] = None,\n        logger: Optional[logging.Logger] = None,\n    ):\n        \"\"\"\n        Initialize the downloader.\n\n        Args:\n            config: Optional configuration for customizing download behavior and file paths.\n                    If None, a default `MSBuildingsConfig` is used.\n            data_store: Optional instance of a `DataStore` for managing data storage.\n                        If provided, it overrides the `data_store` in the `config`.\n                        If None, the `data_store` from the `config` is used.\n            logger: Optional custom logger instance. If None, a default logger\n                    named after the module is created and used.\n        \"\"\"\n        config = config or MSBuildingsConfig()\n        super().__init__(config=config, data_store=data_store, logger=logger)\n\n    def download_data_unit(\n        self,\n        tile_info: Union[pd.Series, dict],\n        **kwargs,\n    ) -&gt; Optional[str]:\n        \"\"\"Download data file for a single tile.\"\"\"\n\n        tile_url = tile_info[\"url\"]\n\n        try:\n            response = requests.get(tile_url, stream=True)\n            response.raise_for_status()\n\n            file_path = str(self.config.get_data_unit_path(tile_info))\n\n            with self.data_store.open(file_path, \"wb\") as file:\n                for chunk in response.iter_content(chunk_size=8192):\n                    file.write(chunk)\n\n                self.logger.debug(\n                    f\"Successfully downloaded tile: {tile_info['quadkey']}\"\n                )\n                return file_path\n\n        except requests.exceptions.RequestException as e:\n            self.logger.error(\n                f\"Failed to download tile {tile_info['quadkey']}: {str(e)}\"\n            )\n            return None\n        except Exception as e:\n            self.logger.error(f\"Unexpected error downloading dataset: {str(e)}\")\n            return None\n\n    def download_data_units(\n        self,\n        tiles: Union[pd.DataFrame, List[dict]],\n        **kwargs,\n    ) -&gt; List[str]:\n        \"\"\"Download data files for multiple tiles.\"\"\"\n\n        if len(tiles) == 0:\n            self.logger.warning(f\"There is no matching data\")\n            return []\n\n        with multiprocessing.Pool(self.config.n_workers) as pool:\n            download_func = functools.partial(self.download_data_unit)\n            file_paths = list(\n                tqdm(\n                    pool.imap(\n                        download_func,\n                        (\n                            [row for _, row in tiles.iterrows()]\n                            if isinstance(tiles, pd.DataFrame)\n                            else tiles\n                        ),\n                    ),\n                    total=len(tiles),\n                    desc=f\"Downloading polygons data\",\n                )\n            )\n\n        return [path for path in file_paths if path is not None]\n\n    def download(\n        self,\n        source: Union[\n            str,  # country\n            List[Union[Tuple[float, float], Point]],  # points\n            BaseGeometry,  # shapely geoms\n            gpd.GeoDataFrame,\n        ],\n        **kwargs,\n    ) -&gt; List[str]:\n        \"\"\"\n        Download Microsoft Global ML Building Footprints data for a specified geographic region.\n\n        The region can be defined by a country, a list of points,\n        a Shapely geometry, or a GeoDataFrame. This method identifies the\n        relevant data tiles intersecting the region and downloads them in parallel.\n\n        Args:\n            source: Defines the geographic area for which to download data.\n                    Can be:\n                      - A string representing a country code or name.\n                      - A list of (latitude, longitude) tuples or Shapely Point objects.\n                      - A Shapely BaseGeometry object (e.g., Polygon, MultiPolygon).\n                      - A GeoDataFrame with a geometry column in EPSG:4326.\n            **kwargs: Additional parameters passed to data unit resolution methods\n\n        Returns:\n            A list of local file paths for the successfully downloaded tiles.\n            Returns an empty list if no data is found for the region or if\n            all downloads fail.\n        \"\"\"\n\n        tiles = self.config.get_relevant_data_units(source, **kwargs)\n        return self.download_data_units(tiles, **kwargs)\n\n    def download_by_country(\n        self,\n        country: str,\n        data_store: Optional[DataStore] = None,\n        country_geom_path: Optional[Union[str, Path]] = None,\n    ) -&gt; List[str]:\n        \"\"\"\n        Download Microsoft Global ML Building Footprints data for a specific country.\n\n        This is a convenience method to download data for an entire country\n        using its code or name.\n\n        Args:\n            country: The country code (e.g., 'USA', 'GBR') or name.\n            data_store: Optional instance of a `DataStore` to be used by\n                `AdminBoundaries` for loading country boundaries. If None,\n                `AdminBoundaries` will use its default data loading.\n            country_geom_path: Optional path to a GeoJSON file containing the\n                country boundary. If provided, this boundary is used\n                instead of the default from `AdminBoundaries`.\n\n        Returns:\n            A list of local file paths for the successfully downloaded tiles.\n            Returns an empty list if no data is found for the country or if\n            all downloads fail.\n        \"\"\"\n        return self.download(\n            source=country, data_store=data_store, path=country_geom_path\n        )\n</code></pre>"},{"location":"api/handlers/#gigaspatial.handlers.microsoft_global_buildings.MSBuildingsDownloader.__init__","title":"<code>__init__(config=None, data_store=None, logger=None)</code>","text":"<p>Initialize the downloader.</p> <p>Parameters:</p> Name Type Description Default <code>config</code> <code>Optional[MSBuildingsConfig]</code> <p>Optional configuration for customizing download behavior and file paths.     If None, a default <code>MSBuildingsConfig</code> is used.</p> <code>None</code> <code>data_store</code> <code>Optional[DataStore]</code> <p>Optional instance of a <code>DataStore</code> for managing data storage.         If provided, it overrides the <code>data_store</code> in the <code>config</code>.         If None, the <code>data_store</code> from the <code>config</code> is used.</p> <code>None</code> <code>logger</code> <code>Optional[Logger]</code> <p>Optional custom logger instance. If None, a default logger     named after the module is created and used.</p> <code>None</code> Source code in <code>gigaspatial/handlers/microsoft_global_buildings.py</code> <pre><code>def __init__(\n    self,\n    config: Optional[MSBuildingsConfig] = None,\n    data_store: Optional[DataStore] = None,\n    logger: Optional[logging.Logger] = None,\n):\n    \"\"\"\n    Initialize the downloader.\n\n    Args:\n        config: Optional configuration for customizing download behavior and file paths.\n                If None, a default `MSBuildingsConfig` is used.\n        data_store: Optional instance of a `DataStore` for managing data storage.\n                    If provided, it overrides the `data_store` in the `config`.\n                    If None, the `data_store` from the `config` is used.\n        logger: Optional custom logger instance. If None, a default logger\n                named after the module is created and used.\n    \"\"\"\n    config = config or MSBuildingsConfig()\n    super().__init__(config=config, data_store=data_store, logger=logger)\n</code></pre>"},{"location":"api/handlers/#gigaspatial.handlers.microsoft_global_buildings.MSBuildingsDownloader.download","title":"<code>download(source, **kwargs)</code>","text":"<p>Download Microsoft Global ML Building Footprints data for a specified geographic region.</p> <p>The region can be defined by a country, a list of points, a Shapely geometry, or a GeoDataFrame. This method identifies the relevant data tiles intersecting the region and downloads them in parallel.</p> <p>Parameters:</p> Name Type Description Default <code>source</code> <code>Union[str, List[Union[Tuple[float, float], Point]], BaseGeometry, GeoDataFrame]</code> <p>Defines the geographic area for which to download data.     Can be:       - A string representing a country code or name.       - A list of (latitude, longitude) tuples or Shapely Point objects.       - A Shapely BaseGeometry object (e.g., Polygon, MultiPolygon).       - A GeoDataFrame with a geometry column in EPSG:4326.</p> required <code>**kwargs</code> <p>Additional parameters passed to data unit resolution methods</p> <code>{}</code> <p>Returns:</p> Type Description <code>List[str]</code> <p>A list of local file paths for the successfully downloaded tiles.</p> <code>List[str]</code> <p>Returns an empty list if no data is found for the region or if</p> <code>List[str]</code> <p>all downloads fail.</p> Source code in <code>gigaspatial/handlers/microsoft_global_buildings.py</code> <pre><code>def download(\n    self,\n    source: Union[\n        str,  # country\n        List[Union[Tuple[float, float], Point]],  # points\n        BaseGeometry,  # shapely geoms\n        gpd.GeoDataFrame,\n    ],\n    **kwargs,\n) -&gt; List[str]:\n    \"\"\"\n    Download Microsoft Global ML Building Footprints data for a specified geographic region.\n\n    The region can be defined by a country, a list of points,\n    a Shapely geometry, or a GeoDataFrame. This method identifies the\n    relevant data tiles intersecting the region and downloads them in parallel.\n\n    Args:\n        source: Defines the geographic area for which to download data.\n                Can be:\n                  - A string representing a country code or name.\n                  - A list of (latitude, longitude) tuples or Shapely Point objects.\n                  - A Shapely BaseGeometry object (e.g., Polygon, MultiPolygon).\n                  - A GeoDataFrame with a geometry column in EPSG:4326.\n        **kwargs: Additional parameters passed to data unit resolution methods\n\n    Returns:\n        A list of local file paths for the successfully downloaded tiles.\n        Returns an empty list if no data is found for the region or if\n        all downloads fail.\n    \"\"\"\n\n    tiles = self.config.get_relevant_data_units(source, **kwargs)\n    return self.download_data_units(tiles, **kwargs)\n</code></pre>"},{"location":"api/handlers/#gigaspatial.handlers.microsoft_global_buildings.MSBuildingsDownloader.download_by_country","title":"<code>download_by_country(country, data_store=None, country_geom_path=None)</code>","text":"<p>Download Microsoft Global ML Building Footprints data for a specific country.</p> <p>This is a convenience method to download data for an entire country using its code or name.</p> <p>Parameters:</p> Name Type Description Default <code>country</code> <code>str</code> <p>The country code (e.g., 'USA', 'GBR') or name.</p> required <code>data_store</code> <code>Optional[DataStore]</code> <p>Optional instance of a <code>DataStore</code> to be used by <code>AdminBoundaries</code> for loading country boundaries. If None, <code>AdminBoundaries</code> will use its default data loading.</p> <code>None</code> <code>country_geom_path</code> <code>Optional[Union[str, Path]]</code> <p>Optional path to a GeoJSON file containing the country boundary. If provided, this boundary is used instead of the default from <code>AdminBoundaries</code>.</p> <code>None</code> <p>Returns:</p> Type Description <code>List[str]</code> <p>A list of local file paths for the successfully downloaded tiles.</p> <code>List[str]</code> <p>Returns an empty list if no data is found for the country or if</p> <code>List[str]</code> <p>all downloads fail.</p> Source code in <code>gigaspatial/handlers/microsoft_global_buildings.py</code> <pre><code>def download_by_country(\n    self,\n    country: str,\n    data_store: Optional[DataStore] = None,\n    country_geom_path: Optional[Union[str, Path]] = None,\n) -&gt; List[str]:\n    \"\"\"\n    Download Microsoft Global ML Building Footprints data for a specific country.\n\n    This is a convenience method to download data for an entire country\n    using its code or name.\n\n    Args:\n        country: The country code (e.g., 'USA', 'GBR') or name.\n        data_store: Optional instance of a `DataStore` to be used by\n            `AdminBoundaries` for loading country boundaries. If None,\n            `AdminBoundaries` will use its default data loading.\n        country_geom_path: Optional path to a GeoJSON file containing the\n            country boundary. If provided, this boundary is used\n            instead of the default from `AdminBoundaries`.\n\n    Returns:\n        A list of local file paths for the successfully downloaded tiles.\n        Returns an empty list if no data is found for the country or if\n        all downloads fail.\n    \"\"\"\n    return self.download(\n        source=country, data_store=data_store, path=country_geom_path\n    )\n</code></pre>"},{"location":"api/handlers/#gigaspatial.handlers.microsoft_global_buildings.MSBuildingsDownloader.download_data_unit","title":"<code>download_data_unit(tile_info, **kwargs)</code>","text":"<p>Download data file for a single tile.</p> Source code in <code>gigaspatial/handlers/microsoft_global_buildings.py</code> <pre><code>def download_data_unit(\n    self,\n    tile_info: Union[pd.Series, dict],\n    **kwargs,\n) -&gt; Optional[str]:\n    \"\"\"Download data file for a single tile.\"\"\"\n\n    tile_url = tile_info[\"url\"]\n\n    try:\n        response = requests.get(tile_url, stream=True)\n        response.raise_for_status()\n\n        file_path = str(self.config.get_data_unit_path(tile_info))\n\n        with self.data_store.open(file_path, \"wb\") as file:\n            for chunk in response.iter_content(chunk_size=8192):\n                file.write(chunk)\n\n            self.logger.debug(\n                f\"Successfully downloaded tile: {tile_info['quadkey']}\"\n            )\n            return file_path\n\n    except requests.exceptions.RequestException as e:\n        self.logger.error(\n            f\"Failed to download tile {tile_info['quadkey']}: {str(e)}\"\n        )\n        return None\n    except Exception as e:\n        self.logger.error(f\"Unexpected error downloading dataset: {str(e)}\")\n        return None\n</code></pre>"},{"location":"api/handlers/#gigaspatial.handlers.microsoft_global_buildings.MSBuildingsDownloader.download_data_units","title":"<code>download_data_units(tiles, **kwargs)</code>","text":"<p>Download data files for multiple tiles.</p> Source code in <code>gigaspatial/handlers/microsoft_global_buildings.py</code> <pre><code>def download_data_units(\n    self,\n    tiles: Union[pd.DataFrame, List[dict]],\n    **kwargs,\n) -&gt; List[str]:\n    \"\"\"Download data files for multiple tiles.\"\"\"\n\n    if len(tiles) == 0:\n        self.logger.warning(f\"There is no matching data\")\n        return []\n\n    with multiprocessing.Pool(self.config.n_workers) as pool:\n        download_func = functools.partial(self.download_data_unit)\n        file_paths = list(\n            tqdm(\n                pool.imap(\n                    download_func,\n                    (\n                        [row for _, row in tiles.iterrows()]\n                        if isinstance(tiles, pd.DataFrame)\n                        else tiles\n                    ),\n                ),\n                total=len(tiles),\n                desc=f\"Downloading polygons data\",\n            )\n        )\n\n    return [path for path in file_paths if path is not None]\n</code></pre>"},{"location":"api/handlers/#gigaspatial.handlers.microsoft_global_buildings.MSBuildingsHandler","title":"<code>MSBuildingsHandler</code>","text":"<p>               Bases: <code>BaseHandler</code></p> <p>Handler for Microsoft Global Buildings dataset.</p> <p>This class provides a unified interface for downloading and loading Microsoft Global Buildings data. It manages the lifecycle of configuration, downloading, and reading components.</p> Source code in <code>gigaspatial/handlers/microsoft_global_buildings.py</code> <pre><code>class MSBuildingsHandler(BaseHandler):\n    \"\"\"\n    Handler for Microsoft Global Buildings dataset.\n\n    This class provides a unified interface for downloading and loading Microsoft Global Buildings data.\n    It manages the lifecycle of configuration, downloading, and reading components.\n    \"\"\"\n\n    def create_config(\n        self, data_store: DataStore, logger: logging.Logger, **kwargs\n    ) -&gt; MSBuildingsConfig:\n        \"\"\"\n        Create and return a MSBuildingsConfig instance.\n\n        Args:\n            data_store: The data store instance to use\n            logger: The logger instance to use\n            **kwargs: Additional configuration parameters\n\n        Returns:\n            Configured MSBuildingsConfig instance\n        \"\"\"\n        return MSBuildingsConfig(data_store=data_store, logger=logger, **kwargs)\n\n    def create_downloader(\n        self,\n        config: MSBuildingsConfig,\n        data_store: DataStore,\n        logger: logging.Logger,\n        **kwargs,\n    ) -&gt; MSBuildingsDownloader:\n        \"\"\"\n        Create and return a MSBuildingsDownloader instance.\n\n        Args:\n            config: The configuration object\n            data_store: The data store instance to use\n            logger: The logger instance to use\n            **kwargs: Additional downloader parameters\n\n        Returns:\n            Configured MSBuildingsDownloader instance\n        \"\"\"\n        return MSBuildingsDownloader(\n            config=config, data_store=data_store, logger=logger, **kwargs\n        )\n\n    def create_reader(\n        self,\n        config: MSBuildingsConfig,\n        data_store: DataStore,\n        logger: logging.Logger,\n        **kwargs,\n    ) -&gt; MSBuildingsReader:\n        \"\"\"\n        Create and return a MSBuildingsReader instance.\n\n        Args:\n            config: The configuration object\n            data_store: The data store instance to use\n            logger: The logger instance to use\n            **kwargs: Additional reader parameters\n\n        Returns:\n            Configured MSBuildingsReader instance\n        \"\"\"\n        return MSBuildingsReader(\n            config=config, data_store=data_store, logger=logger, **kwargs\n        )\n</code></pre>"},{"location":"api/handlers/#gigaspatial.handlers.microsoft_global_buildings.MSBuildingsHandler.create_config","title":"<code>create_config(data_store, logger, **kwargs)</code>","text":"<p>Create and return a MSBuildingsConfig instance.</p> <p>Parameters:</p> Name Type Description Default <code>data_store</code> <code>DataStore</code> <p>The data store instance to use</p> required <code>logger</code> <code>Logger</code> <p>The logger instance to use</p> required <code>**kwargs</code> <p>Additional configuration parameters</p> <code>{}</code> <p>Returns:</p> Type Description <code>MSBuildingsConfig</code> <p>Configured MSBuildingsConfig instance</p> Source code in <code>gigaspatial/handlers/microsoft_global_buildings.py</code> <pre><code>def create_config(\n    self, data_store: DataStore, logger: logging.Logger, **kwargs\n) -&gt; MSBuildingsConfig:\n    \"\"\"\n    Create and return a MSBuildingsConfig instance.\n\n    Args:\n        data_store: The data store instance to use\n        logger: The logger instance to use\n        **kwargs: Additional configuration parameters\n\n    Returns:\n        Configured MSBuildingsConfig instance\n    \"\"\"\n    return MSBuildingsConfig(data_store=data_store, logger=logger, **kwargs)\n</code></pre>"},{"location":"api/handlers/#gigaspatial.handlers.microsoft_global_buildings.MSBuildingsHandler.create_downloader","title":"<code>create_downloader(config, data_store, logger, **kwargs)</code>","text":"<p>Create and return a MSBuildingsDownloader instance.</p> <p>Parameters:</p> Name Type Description Default <code>config</code> <code>MSBuildingsConfig</code> <p>The configuration object</p> required <code>data_store</code> <code>DataStore</code> <p>The data store instance to use</p> required <code>logger</code> <code>Logger</code> <p>The logger instance to use</p> required <code>**kwargs</code> <p>Additional downloader parameters</p> <code>{}</code> <p>Returns:</p> Type Description <code>MSBuildingsDownloader</code> <p>Configured MSBuildingsDownloader instance</p> Source code in <code>gigaspatial/handlers/microsoft_global_buildings.py</code> <pre><code>def create_downloader(\n    self,\n    config: MSBuildingsConfig,\n    data_store: DataStore,\n    logger: logging.Logger,\n    **kwargs,\n) -&gt; MSBuildingsDownloader:\n    \"\"\"\n    Create and return a MSBuildingsDownloader instance.\n\n    Args:\n        config: The configuration object\n        data_store: The data store instance to use\n        logger: The logger instance to use\n        **kwargs: Additional downloader parameters\n\n    Returns:\n        Configured MSBuildingsDownloader instance\n    \"\"\"\n    return MSBuildingsDownloader(\n        config=config, data_store=data_store, logger=logger, **kwargs\n    )\n</code></pre>"},{"location":"api/handlers/#gigaspatial.handlers.microsoft_global_buildings.MSBuildingsHandler.create_reader","title":"<code>create_reader(config, data_store, logger, **kwargs)</code>","text":"<p>Create and return a MSBuildingsReader instance.</p> <p>Parameters:</p> Name Type Description Default <code>config</code> <code>MSBuildingsConfig</code> <p>The configuration object</p> required <code>data_store</code> <code>DataStore</code> <p>The data store instance to use</p> required <code>logger</code> <code>Logger</code> <p>The logger instance to use</p> required <code>**kwargs</code> <p>Additional reader parameters</p> <code>{}</code> <p>Returns:</p> Type Description <code>MSBuildingsReader</code> <p>Configured MSBuildingsReader instance</p> Source code in <code>gigaspatial/handlers/microsoft_global_buildings.py</code> <pre><code>def create_reader(\n    self,\n    config: MSBuildingsConfig,\n    data_store: DataStore,\n    logger: logging.Logger,\n    **kwargs,\n) -&gt; MSBuildingsReader:\n    \"\"\"\n    Create and return a MSBuildingsReader instance.\n\n    Args:\n        config: The configuration object\n        data_store: The data store instance to use\n        logger: The logger instance to use\n        **kwargs: Additional reader parameters\n\n    Returns:\n        Configured MSBuildingsReader instance\n    \"\"\"\n    return MSBuildingsReader(\n        config=config, data_store=data_store, logger=logger, **kwargs\n    )\n</code></pre>"},{"location":"api/handlers/#gigaspatial.handlers.microsoft_global_buildings.MSBuildingsReader","title":"<code>MSBuildingsReader</code>","text":"<p>               Bases: <code>BaseHandlerReader</code></p> <p>Reader for Microsoft Global Buildings data, supporting country, points, and geometry-based resolution.</p> Source code in <code>gigaspatial/handlers/microsoft_global_buildings.py</code> <pre><code>class MSBuildingsReader(BaseHandlerReader):\n    \"\"\"\n    Reader for Microsoft Global Buildings data, supporting country, points, and geometry-based resolution.\n    \"\"\"\n\n    def __init__(\n        self,\n        config: Optional[MSBuildingsConfig] = None,\n        data_store: Optional[DataStore] = None,\n        logger: Optional[logging.Logger] = None,\n    ):\n        config = config or MSBuildingsConfig()\n        super().__init__(config=config, data_store=data_store, logger=logger)\n\n    def load_from_paths(\n        self, source_data_path: List[Union[str, Path]], **kwargs\n    ) -&gt; gpd.GeoDataFrame:\n        \"\"\"\n        Load building data from Microsoft Buildings dataset.\n        Args:\n            source_data_path: List of file paths to load\n        Returns:\n            GeoDataFrame containing building data\n        \"\"\"\n        from gigaspatial.core.io.readers import read_gzipped_json_or_csv\n        from shapely.geometry import shape\n\n        def read_ms_dataset(data_store: DataStore, file_path: str):\n            df = read_gzipped_json_or_csv(file_path=file_path, data_store=data_store)\n            df[\"geometry\"] = df[\"geometry\"].apply(shape)\n            return gpd.GeoDataFrame(df, crs=4326)\n\n        result = self._load_tabular_data(\n            file_paths=source_data_path, read_function=read_ms_dataset\n        )\n        return result\n</code></pre>"},{"location":"api/handlers/#gigaspatial.handlers.microsoft_global_buildings.MSBuildingsReader.load_from_paths","title":"<code>load_from_paths(source_data_path, **kwargs)</code>","text":"<p>Load building data from Microsoft Buildings dataset. Args:     source_data_path: List of file paths to load Returns:     GeoDataFrame containing building data</p> Source code in <code>gigaspatial/handlers/microsoft_global_buildings.py</code> <pre><code>def load_from_paths(\n    self, source_data_path: List[Union[str, Path]], **kwargs\n) -&gt; gpd.GeoDataFrame:\n    \"\"\"\n    Load building data from Microsoft Buildings dataset.\n    Args:\n        source_data_path: List of file paths to load\n    Returns:\n        GeoDataFrame containing building data\n    \"\"\"\n    from gigaspatial.core.io.readers import read_gzipped_json_or_csv\n    from shapely.geometry import shape\n\n    def read_ms_dataset(data_store: DataStore, file_path: str):\n        df = read_gzipped_json_or_csv(file_path=file_path, data_store=data_store)\n        df[\"geometry\"] = df[\"geometry\"].apply(shape)\n        return gpd.GeoDataFrame(df, crs=4326)\n\n    result = self._load_tabular_data(\n        file_paths=source_data_path, read_function=read_ms_dataset\n    )\n    return result\n</code></pre>"},{"location":"api/handlers/#gigaspatial.handlers.opencellid","title":"<code>opencellid</code>","text":""},{"location":"api/handlers/#gigaspatial.handlers.opencellid.OpenCellIDConfig","title":"<code>OpenCellIDConfig</code>","text":"<p>               Bases: <code>BaseModel</code></p> <p>Configuration for OpenCellID data access</p> Source code in <code>gigaspatial/handlers/opencellid.py</code> <pre><code>class OpenCellIDConfig(BaseModel):\n    \"\"\"Configuration for OpenCellID data access\"\"\"\n\n    # Base URLs\n    BASE_URL: HttpUrl = Field(default=\"https://opencellid.org/\")\n    DOWNLOAD_URL: HttpUrl = Field(default=\"https://opencellid.org/downloads.php?token=\")\n\n    # User configuration\n    country: str = Field(...)\n    api_token: str = Field(\n        default=global_config.OPENCELLID_ACCESS_TOKEN,\n        description=\"OpenCellID API Access Token\",\n    )\n    base_path: Path = Field(default=global_config.get_path(\"opencellid\", \"bronze\"))\n    created_newer: int = Field(\n        default=2003, description=\"Filter out cell towers added before this year\"\n    )\n    created_before: int = Field(\n        default=datetime.now().year,\n        description=\"Filter out cell towers added after this year\",\n    )\n    drop_duplicates: bool = Field(\n        default=True,\n        description=\"Drop cells that are in the exact same location and radio technology\",\n    )\n\n    @field_validator(\"country\")\n    def validate_country(cls, value: str) -&gt; str:\n        try:\n            return pycountry.countries.lookup(value).alpha_3\n        except LookupError:\n            raise ValueError(f\"Invalid country code provided: {value}\")\n\n    @property\n    def output_file_path(self) -&gt; Path:\n        \"\"\"Path to save the downloaded OpenCellID data\"\"\"\n        return self.base_path / f\"opencellid_{self.country.lower()}.csv.gz\"\n\n    def __repr__(self) -&gt; str:\n        return (\n            f\"OpenCellIDConfig(\\n\"\n            f\"  country='{self.country}'\\n\"\n            f\"  created_newer={self.created_newer}\\n\"\n            f\"  created_before={self.created_before}\\n\"\n            f\"  drop_duplicates={self.drop_duplicates}\\n\"\n            f\")\"\n        )\n</code></pre>"},{"location":"api/handlers/#gigaspatial.handlers.opencellid.OpenCellIDConfig.output_file_path","title":"<code>output_file_path: Path</code>  <code>property</code>","text":"<p>Path to save the downloaded OpenCellID data</p>"},{"location":"api/handlers/#gigaspatial.handlers.opencellid.OpenCellIDDownloader","title":"<code>OpenCellIDDownloader</code>","text":"<p>Downloader for OpenCellID data</p> Source code in <code>gigaspatial/handlers/opencellid.py</code> <pre><code>class OpenCellIDDownloader:\n    \"\"\"Downloader for OpenCellID data\"\"\"\n\n    def __init__(\n        self,\n        config: Union[OpenCellIDConfig, dict],\n        data_store: Optional[DataStore] = None,\n        logger: Optional[logging.Logger] = None,\n    ):\n        if isinstance(config, dict):\n            self.config = OpenCellIDConfig(**config)\n        else:\n            self.config = config\n\n        self.data_store = data_store or LocalDataStore()\n        self.logger = logger or global_config.get_logger(self.__class__.__name__)\n\n    @classmethod\n    def from_country(\n        cls,\n        country: str,\n        api_token: str = global_config.OPENCELLID_ACCESS_TOKEN,\n        **kwargs,\n    ):\n        \"\"\"Create a downloader for a specific country\"\"\"\n        config = OpenCellIDConfig(country=country, api_token=api_token, **kwargs)\n        return cls(config=config)\n\n    def get_download_links(self) -&gt; List[str]:\n        \"\"\"Get download links for the country from OpenCellID website\"\"\"\n        url = f\"{self.config.DOWNLOAD_URL}{self.config.api_token}\"\n        country_alpha2 = pycountry.countries.get(\n            alpha_3=self.config.country.upper()\n        ).alpha_2\n\n        try:\n            # Find table with cell tower data links\n            self.logger.info(f\"Fetching download links for {self.config.country}\")\n            html_content = requests.get(url).text\n            soup = BeautifulSoup(html_content, \"lxml\")\n            table = soup.find(\"table\", {\"id\": \"regions\"})\n\n            if not table:\n                raise ValueError(\n                    \"Could not find cell tower data table on OpenCellID website\"\n                )\n\n            # Parse table headers\n            t_headers = []\n            for th in table.find_all(\"th\"):\n                t_headers.append(th.text.replace(\"\\n\", \" \").strip())\n\n            # Parse table data\n            table_data = []\n            for tr in table.tbody.find_all(\"tr\"):\n                t_row = {}\n\n                for td, th in zip(tr.find_all(\"td\"), t_headers):\n                    if \"Files\" in th:\n                        t_row[th] = []\n                        for a in td.find_all(\"a\"):\n                            t_row[th].append(a.get(\"href\"))\n                    else:\n                        t_row[th] = td.text.replace(\"\\n\", \"\").strip()\n\n                table_data.append(t_row)\n\n            cell_dict = pd.DataFrame(table_data)\n\n            # Get links for the country code\n            if country_alpha2 not in cell_dict[\"Country Code\"].values:\n                raise ValueError(\n                    f\"Country code {country_alpha2} not found in OpenCellID database\"\n                )\n            else:\n                links = cell_dict[cell_dict[\"Country Code\"] == country_alpha2][\n                    \"Files (grouped by MCC)\"\n                ].values[0]\n\n            return links\n\n        except Exception as e:\n            self.logger.error(f\"Error fetching download links: {str(e)}\")\n            raise\n\n    def download_and_process(self) -&gt; str:\n        \"\"\"Download and process OpenCellID data for the configured country\"\"\"\n\n        try:\n            links = self.get_download_links()\n            self.logger.info(f\"Found {len(links)} data files for {self.config.country}\")\n\n            dfs = []\n\n            for link in links:\n                self.logger.info(f\"Downloading data from {link}\")\n                response = requests.get(link, stream=True)\n                response.raise_for_status()\n\n                # Use a temporary file for download\n                with tempfile.NamedTemporaryFile(delete=False, suffix=\".gz\") as tmpfile:\n                    for chunk in response.iter_content(chunk_size=1024):\n                        if chunk:\n                            tmpfile.write(chunk)\n                    temp_file = tmpfile.name\n\n                try:\n                    # Read the downloaded gzipped CSV data\n                    with gzip.open(temp_file, \"rt\") as feed_data:\n                        dfs.append(\n                            pd.read_csv(\n                                feed_data,\n                                names=[\n                                    \"radio\",\n                                    \"mcc\",\n                                    \"net\",\n                                    \"area\",\n                                    \"cell\",\n                                    \"unit\",\n                                    \"lon\",\n                                    \"lat\",\n                                    \"range\",\n                                    \"samples\",\n                                    \"changeable\",\n                                    \"created\",\n                                    \"updated\",\n                                    \"average_signal\",\n                                ],\n                            )\n                        )\n                except IOError as e:\n                    with open(temp_file, \"r\") as error_file:\n                        contents = error_file.readline()\n\n                    if \"RATE_LIMITED\" in contents:\n                        raise RuntimeError(\n                            \"API rate limit exceeded. You're rate-limited!\"\n                        )\n                    elif \"INVALID_TOKEN\" in contents:\n                        raise RuntimeError(\"API token rejected by OpenCellID!\")\n                    else:\n                        raise RuntimeError(\n                            f\"Error processing downloaded data: {str(e)}\"\n                        )\n                finally:\n                    # Clean up temporary file\n                    if os.path.exists(temp_file):\n                        os.remove(temp_file)\n\n            df_cell = pd.concat(dfs, ignore_index=True)\n\n            # Process the data\n            if not df_cell.empty:\n                # Convert timestamps to datetime\n                df_cell[\"created\"] = pd.to_datetime(\n                    df_cell[\"created\"], unit=\"s\", origin=\"unix\"\n                )\n                df_cell[\"updated\"] = pd.to_datetime(\n                    df_cell[\"updated\"], unit=\"s\", origin=\"unix\"\n                )\n\n                # Filter by year\n                df_cell = df_cell[\n                    (df_cell.created.dt.year &gt;= self.config.created_newer)\n                    &amp; (df_cell.created.dt.year &lt; self.config.created_before)\n                ]\n\n                # Drop duplicates if configured\n                if self.config.drop_duplicates:\n                    df_cell = (\n                        df_cell.groupby([\"radio\", \"lon\", \"lat\"]).first().reset_index()\n                    )\n\n                # Save processed data using data_store\n                output_path = str(self.config.output_file_path)\n                self.logger.info(f\"Saving processed data to {output_path}\")\n                with self.data_store.open(output_path, \"wb\") as f:\n                    df_cell.to_csv(f, compression=\"gzip\", index=False)\n\n                return output_path\n            else:\n                raise ValueError(f\"No data found for {self.config.country}\")\n\n        except Exception as e:\n            self.logger.error(f\"Error downloading and processing data: {str(e)}\")\n            raise\n</code></pre>"},{"location":"api/handlers/#gigaspatial.handlers.opencellid.OpenCellIDDownloader.download_and_process","title":"<code>download_and_process()</code>","text":"<p>Download and process OpenCellID data for the configured country</p> Source code in <code>gigaspatial/handlers/opencellid.py</code> <pre><code>def download_and_process(self) -&gt; str:\n    \"\"\"Download and process OpenCellID data for the configured country\"\"\"\n\n    try:\n        links = self.get_download_links()\n        self.logger.info(f\"Found {len(links)} data files for {self.config.country}\")\n\n        dfs = []\n\n        for link in links:\n            self.logger.info(f\"Downloading data from {link}\")\n            response = requests.get(link, stream=True)\n            response.raise_for_status()\n\n            # Use a temporary file for download\n            with tempfile.NamedTemporaryFile(delete=False, suffix=\".gz\") as tmpfile:\n                for chunk in response.iter_content(chunk_size=1024):\n                    if chunk:\n                        tmpfile.write(chunk)\n                temp_file = tmpfile.name\n\n            try:\n                # Read the downloaded gzipped CSV data\n                with gzip.open(temp_file, \"rt\") as feed_data:\n                    dfs.append(\n                        pd.read_csv(\n                            feed_data,\n                            names=[\n                                \"radio\",\n                                \"mcc\",\n                                \"net\",\n                                \"area\",\n                                \"cell\",\n                                \"unit\",\n                                \"lon\",\n                                \"lat\",\n                                \"range\",\n                                \"samples\",\n                                \"changeable\",\n                                \"created\",\n                                \"updated\",\n                                \"average_signal\",\n                            ],\n                        )\n                    )\n            except IOError as e:\n                with open(temp_file, \"r\") as error_file:\n                    contents = error_file.readline()\n\n                if \"RATE_LIMITED\" in contents:\n                    raise RuntimeError(\n                        \"API rate limit exceeded. You're rate-limited!\"\n                    )\n                elif \"INVALID_TOKEN\" in contents:\n                    raise RuntimeError(\"API token rejected by OpenCellID!\")\n                else:\n                    raise RuntimeError(\n                        f\"Error processing downloaded data: {str(e)}\"\n                    )\n            finally:\n                # Clean up temporary file\n                if os.path.exists(temp_file):\n                    os.remove(temp_file)\n\n        df_cell = pd.concat(dfs, ignore_index=True)\n\n        # Process the data\n        if not df_cell.empty:\n            # Convert timestamps to datetime\n            df_cell[\"created\"] = pd.to_datetime(\n                df_cell[\"created\"], unit=\"s\", origin=\"unix\"\n            )\n            df_cell[\"updated\"] = pd.to_datetime(\n                df_cell[\"updated\"], unit=\"s\", origin=\"unix\"\n            )\n\n            # Filter by year\n            df_cell = df_cell[\n                (df_cell.created.dt.year &gt;= self.config.created_newer)\n                &amp; (df_cell.created.dt.year &lt; self.config.created_before)\n            ]\n\n            # Drop duplicates if configured\n            if self.config.drop_duplicates:\n                df_cell = (\n                    df_cell.groupby([\"radio\", \"lon\", \"lat\"]).first().reset_index()\n                )\n\n            # Save processed data using data_store\n            output_path = str(self.config.output_file_path)\n            self.logger.info(f\"Saving processed data to {output_path}\")\n            with self.data_store.open(output_path, \"wb\") as f:\n                df_cell.to_csv(f, compression=\"gzip\", index=False)\n\n            return output_path\n        else:\n            raise ValueError(f\"No data found for {self.config.country}\")\n\n    except Exception as e:\n        self.logger.error(f\"Error downloading and processing data: {str(e)}\")\n        raise\n</code></pre>"},{"location":"api/handlers/#gigaspatial.handlers.opencellid.OpenCellIDDownloader.from_country","title":"<code>from_country(country, api_token=global_config.OPENCELLID_ACCESS_TOKEN, **kwargs)</code>  <code>classmethod</code>","text":"<p>Create a downloader for a specific country</p> Source code in <code>gigaspatial/handlers/opencellid.py</code> <pre><code>@classmethod\ndef from_country(\n    cls,\n    country: str,\n    api_token: str = global_config.OPENCELLID_ACCESS_TOKEN,\n    **kwargs,\n):\n    \"\"\"Create a downloader for a specific country\"\"\"\n    config = OpenCellIDConfig(country=country, api_token=api_token, **kwargs)\n    return cls(config=config)\n</code></pre>"},{"location":"api/handlers/#gigaspatial.handlers.opencellid.OpenCellIDDownloader.get_download_links","title":"<code>get_download_links()</code>","text":"<p>Get download links for the country from OpenCellID website</p> Source code in <code>gigaspatial/handlers/opencellid.py</code> <pre><code>def get_download_links(self) -&gt; List[str]:\n    \"\"\"Get download links for the country from OpenCellID website\"\"\"\n    url = f\"{self.config.DOWNLOAD_URL}{self.config.api_token}\"\n    country_alpha2 = pycountry.countries.get(\n        alpha_3=self.config.country.upper()\n    ).alpha_2\n\n    try:\n        # Find table with cell tower data links\n        self.logger.info(f\"Fetching download links for {self.config.country}\")\n        html_content = requests.get(url).text\n        soup = BeautifulSoup(html_content, \"lxml\")\n        table = soup.find(\"table\", {\"id\": \"regions\"})\n\n        if not table:\n            raise ValueError(\n                \"Could not find cell tower data table on OpenCellID website\"\n            )\n\n        # Parse table headers\n        t_headers = []\n        for th in table.find_all(\"th\"):\n            t_headers.append(th.text.replace(\"\\n\", \" \").strip())\n\n        # Parse table data\n        table_data = []\n        for tr in table.tbody.find_all(\"tr\"):\n            t_row = {}\n\n            for td, th in zip(tr.find_all(\"td\"), t_headers):\n                if \"Files\" in th:\n                    t_row[th] = []\n                    for a in td.find_all(\"a\"):\n                        t_row[th].append(a.get(\"href\"))\n                else:\n                    t_row[th] = td.text.replace(\"\\n\", \"\").strip()\n\n            table_data.append(t_row)\n\n        cell_dict = pd.DataFrame(table_data)\n\n        # Get links for the country code\n        if country_alpha2 not in cell_dict[\"Country Code\"].values:\n            raise ValueError(\n                f\"Country code {country_alpha2} not found in OpenCellID database\"\n            )\n        else:\n            links = cell_dict[cell_dict[\"Country Code\"] == country_alpha2][\n                \"Files (grouped by MCC)\"\n            ].values[0]\n\n        return links\n\n    except Exception as e:\n        self.logger.error(f\"Error fetching download links: {str(e)}\")\n        raise\n</code></pre>"},{"location":"api/handlers/#gigaspatial.handlers.opencellid.OpenCellIDReader","title":"<code>OpenCellIDReader</code>","text":"<p>Reader for OpenCellID data</p> Source code in <code>gigaspatial/handlers/opencellid.py</code> <pre><code>class OpenCellIDReader:\n    \"\"\"Reader for OpenCellID data\"\"\"\n\n    def __init__(\n        self,\n        country: str,\n        data_store: Optional[DataStore] = None,\n        base_path: Optional[Path] = None,\n    ):\n        self.country = pycountry.countries.lookup(country).alpha_3\n        self.data_store = data_store or LocalDataStore()\n        self.base_path = base_path or global_config.get_path(\"opencellid\", \"bronze\")\n\n    def read_data(self) -&gt; pd.DataFrame:\n        \"\"\"Read OpenCellID data for the specified country\"\"\"\n        file_path = str(self.base_path / f\"opencellid_{self.country.lower()}.csv.gz\")\n\n        if not self.data_store.file_exists(file_path):\n            raise FileNotFoundError(\n                f\"OpenCellID data for {self.country} not found at {file_path}. \"\n                \"Download the data first using OpenCellIDDownloader.\"\n            )\n\n        return read_dataset(self.data_store, file_path)\n\n    def to_geodataframe(self) -&gt; gpd.GeoDataFrame:\n        \"\"\"Convert OpenCellID data to a GeoDataFrame\"\"\"\n        df = self.read_data()\n        gdf = gpd.GeoDataFrame(\n            df, geometry=gpd.points_from_xy(df.lon, df.lat), crs=\"EPSG:4326\"\n        )\n        return gdf\n</code></pre>"},{"location":"api/handlers/#gigaspatial.handlers.opencellid.OpenCellIDReader.read_data","title":"<code>read_data()</code>","text":"<p>Read OpenCellID data for the specified country</p> Source code in <code>gigaspatial/handlers/opencellid.py</code> <pre><code>def read_data(self) -&gt; pd.DataFrame:\n    \"\"\"Read OpenCellID data for the specified country\"\"\"\n    file_path = str(self.base_path / f\"opencellid_{self.country.lower()}.csv.gz\")\n\n    if not self.data_store.file_exists(file_path):\n        raise FileNotFoundError(\n            f\"OpenCellID data for {self.country} not found at {file_path}. \"\n            \"Download the data first using OpenCellIDDownloader.\"\n        )\n\n    return read_dataset(self.data_store, file_path)\n</code></pre>"},{"location":"api/handlers/#gigaspatial.handlers.opencellid.OpenCellIDReader.to_geodataframe","title":"<code>to_geodataframe()</code>","text":"<p>Convert OpenCellID data to a GeoDataFrame</p> Source code in <code>gigaspatial/handlers/opencellid.py</code> <pre><code>def to_geodataframe(self) -&gt; gpd.GeoDataFrame:\n    \"\"\"Convert OpenCellID data to a GeoDataFrame\"\"\"\n    df = self.read_data()\n    gdf = gpd.GeoDataFrame(\n        df, geometry=gpd.points_from_xy(df.lon, df.lat), crs=\"EPSG:4326\"\n    )\n    return gdf\n</code></pre>"},{"location":"api/handlers/#gigaspatial.handlers.osm","title":"<code>osm</code>","text":""},{"location":"api/handlers/#gigaspatial.handlers.osm.OSMLocationFetcher","title":"<code>OSMLocationFetcher</code>  <code>dataclass</code>","text":"<p>A class to fetch and process location data from OpenStreetMap using the Overpass API.</p> <p>This class supports fetching various OSM location types including amenities, buildings, shops, and other POI categories.</p> Source code in <code>gigaspatial/handlers/osm.py</code> <pre><code>@dataclass\nclass OSMLocationFetcher:\n    \"\"\"\n    A class to fetch and process location data from OpenStreetMap using the Overpass API.\n\n    This class supports fetching various OSM location types including amenities, buildings,\n    shops, and other POI categories.\n    \"\"\"\n\n    country: str\n    location_types: Union[List[str], Dict[str, List[str]]]\n    base_url: str = \"http://overpass-api.de/api/interpreter\"\n    timeout: int = 600\n    max_retries: int = 3\n    retry_delay: int = 5\n\n    def __post_init__(self):\n        \"\"\"Validate inputs, normalize location_types, and set up logging.\"\"\"\n        try:\n            self.country = pycountry.countries.lookup(self.country).alpha_2\n        except LookupError:\n            raise ValueError(f\"Invalid country code provided: {self.country}\")\n\n        # Normalize location_types to always be a dictionary\n        if isinstance(self.location_types, list):\n            self.location_types = {\"amenity\": self.location_types}\n        elif not isinstance(self.location_types, dict):\n            raise TypeError(\n                \"location_types must be a list of strings or a dictionary mapping categories to type lists\"\n            )\n\n        self.logger = config.get_logger(self.__class__.__name__)\n\n    def _build_queries(self, since_year: Optional[int] = None) -&gt; List[str]:\n        \"\"\"\n        Construct separate Overpass QL queries for different element types and categories.\n        Returns list of [nodes_relations_query, ways_query]\n        \"\"\"\n        if since_year:\n            date_filter = f'(newer:\"{since_year}-01-01T00:00:00Z\")'\n        else:\n            date_filter = \"\"\n\n        # Query for nodes and relations (with center output)\n        nodes_relations_queries = []\n        for category, types in self.location_types.items():\n            nodes_relations_queries.extend(\n                [\n                    f\"\"\"node[\"{category}\"~\"^({\"|\".join(types)})\"]{date_filter}(area.searchArea);\"\"\",\n                    f\"\"\"relation[\"{category}\"~\"^({\"|\".join(types)})\"]{date_filter}(area.searchArea);\"\"\",\n                ]\n            )\n\n        nodes_relations_queries = \"\\n\".join(nodes_relations_queries)\n\n        nodes_relations_query = f\"\"\"\n        [out:json][timeout:{self.timeout}];\n        area[\"ISO3166-1\"={self.country}]-&gt;.searchArea;\n        (\n            {nodes_relations_queries}\n        );\n        out center;\n        \"\"\"\n\n        # Query for ways (with geometry output)\n        ways_queries = []\n        for category, types in self.location_types.items():\n            ways_queries.append(\n                f\"\"\"way[\"{category}\"~\"^({\"|\".join(types)})\"]{date_filter}(area.searchArea);\"\"\"\n            )\n\n        ways_queries = \"\\n\".join(ways_queries)\n\n        ways_query = f\"\"\"\n        [out:json][timeout:{self.timeout}];\n        area[\"ISO3166-1\"={self.country}]-&gt;.searchArea;\n        (\n            {ways_queries}\n        );\n        out geom;\n        \"\"\"\n\n        return [nodes_relations_query, ways_query]\n\n    def _make_request(self, query: str) -&gt; Dict:\n        \"\"\"Make HTTP request to Overpass API with retry mechanism.\"\"\"\n        for attempt in range(self.max_retries):\n            try:\n                self.logger.debug(f\"Executing query:\\n{query}\")\n                response = requests.get(\n                    self.base_url, params={\"data\": query}, timeout=self.timeout\n                )\n                response.raise_for_status()\n                return response.json()\n            except RequestException as e:\n                self.logger.warning(f\"Attempt {attempt + 1} failed: {str(e)}\")\n                if attempt &lt; self.max_retries - 1:\n                    sleep(self.retry_delay)\n                else:\n                    raise RuntimeError(\n                        f\"Failed to fetch data after {self.max_retries} attempts\"\n                    ) from e\n\n    def _extract_matching_categories(self, tags: Dict[str, str]) -&gt; Dict[str, str]:\n        \"\"\"\n        Extract all matching categories and their values from the tags.\n        Returns:\n            Dict mapping each matching category to its value\n        \"\"\"\n        matches = {}\n        for category, types in self.location_types.items():\n            if category in tags and tags[category] in types:\n                matches[category] = tags[category]\n        return matches\n\n    def _process_node_relation(self, element: Dict) -&gt; List[Dict[str, any]]:\n        \"\"\"\n        Process a node or relation element.\n        May return multiple processed elements if the element matches multiple categories.\n        \"\"\"\n        try:\n            tags = element.get(\"tags\", {})\n            matching_categories = self._extract_matching_categories(tags)\n\n            if not matching_categories:\n                self.logger.warning(\n                    f\"Element {element['id']} missing or not matching specified category tags\"\n                )\n                return []\n\n            _lat = element.get(\"lat\") or element[\"center\"][\"lat\"]\n            _lon = element.get(\"lon\") or element[\"center\"][\"lon\"]\n            point_geom = Point(_lon, _lat)\n\n            # for each matching category, create a separate element\n            results = []\n            for category, value in matching_categories.items():\n                results.append(\n                    {\n                        \"source_id\": element[\"id\"],\n                        \"category\": category,\n                        \"category_value\": value,\n                        \"name\": tags.get(\"name\", \"\"),\n                        \"name_en\": tags.get(\"name:en\", \"\"),\n                        \"type\": element[\"type\"],\n                        \"geometry\": point_geom,\n                        \"latitude\": _lat,\n                        \"longitude\": _lon,\n                        \"matching_categories\": list(matching_categories.keys()),\n                    }\n                )\n\n            return results\n\n        except KeyError as e:\n            self.logger.error(f\"Corrupt data received for node/relation: {str(e)}\")\n            return []\n\n    def _process_way(self, element: Dict) -&gt; List[Dict[str, any]]:\n        \"\"\"\n        Process a way element with geometry.\n        May return multiple processed elements if the element matches multiple categories.\n        \"\"\"\n        try:\n            tags = element.get(\"tags\", {})\n            matching_categories = self._extract_matching_categories(tags)\n\n            if not matching_categories:\n                self.logger.warning(\n                    f\"Element {element['id']} missing or not matching specified category tags\"\n                )\n                return []\n\n            # Create polygon from geometry points\n            polygon = Polygon([(p[\"lon\"], p[\"lat\"]) for p in element[\"geometry\"]])\n            centroid = polygon.centroid\n\n            # For each matching category, create a separate element\n            results = []\n            for category, value in matching_categories.items():\n                results.append(\n                    {\n                        \"source_id\": element[\"id\"],\n                        \"category\": category,\n                        \"category_value\": value,\n                        \"name\": tags.get(\"name\", \"\"),\n                        \"name_en\": tags.get(\"name:en\", \"\"),\n                        \"type\": element[\"type\"],\n                        \"geometry\": polygon,\n                        \"latitude\": centroid.y,\n                        \"longitude\": centroid.x,\n                        \"matching_categories\": list(matching_categories.keys()),\n                    }\n                )\n\n            return results\n        except (KeyError, ValueError) as e:\n            self.logger.error(f\"Error processing way geometry: {str(e)}\")\n            return []\n\n    def fetch_locations(\n        self,\n        since_year: Optional[int] = None,\n        handle_duplicates: Literal[\"separate\", \"combine\", \"primary\"] = \"separate\",\n    ) -&gt; pd.DataFrame:\n        \"\"\"\n        Fetch and process OSM locations.\n\n        Args:\n            since_year (int, optional): Filter for locations added/modified since this year.\n            handle_duplicates (str): How to handle objects matching multiple categories:\n                - 'separate': Create separate entries for each category (default)\n                - 'combine': Use a single entry with a list of matching categories\n                - 'primary': Keep only the first matching category\n\n        Returns:\n            pd.DataFrame: Processed OSM locations\n        \"\"\"\n        if handle_duplicates not in (\"separate\", \"combine\", \"primary\"):\n            raise ValueError(\n                \"handle_duplicates must be one of: 'separate', 'combine', 'primary'\"\n            )\n\n        self.logger.info(\n            f\"Fetching OSM locations from Overpass API for country: {self.country}\"\n        )\n        self.logger.info(f\"Location types: {self.location_types}\")\n        self.logger.info(f\"Handling duplicate category matches as: {handle_duplicates}\")\n\n        # Get queries for different element types\n        nodes_relations_query, ways_query = self._build_queries(since_year)\n\n        # Fetch nodes and relations\n        nodes_relations_response = self._make_request(nodes_relations_query)\n        nodes_relations = nodes_relations_response.get(\"elements\", [])\n\n        # Fetch ways\n        ways_response = self._make_request(ways_query)\n        ways = ways_response.get(\"elements\", [])\n\n        if not nodes_relations and not ways:\n            self.logger.warning(\"No locations found for the specified criteria\")\n            return pd.DataFrame()\n\n        self.logger.info(\n            f\"Processing {len(nodes_relations)} nodes/relations and {len(ways)} ways...\"\n        )\n\n        # Process nodes and relations\n        with ThreadPoolExecutor() as executor:\n            processed_nodes_relations = [\n                item\n                for sublist in executor.map(\n                    self._process_node_relation, nodes_relations\n                )\n                for item in sublist\n            ]\n\n        # Process ways\n        with ThreadPoolExecutor() as executor:\n            processed_ways = [\n                item\n                for sublist in executor.map(self._process_way, ways)\n                for item in sublist\n            ]\n\n        # Combine all processed elements\n        all_elements = processed_nodes_relations + processed_ways\n\n        if not all_elements:\n            self.logger.warning(\"No matching elements found after processing\")\n            return pd.DataFrame()\n\n        # Handle duplicates based on the specified strategy\n        if handle_duplicates != \"separate\":\n            # Group by source_id\n            grouped_elements = {}\n            for elem in all_elements:\n                source_id = elem[\"source_id\"]\n                if source_id not in grouped_elements:\n                    grouped_elements[source_id] = elem\n                elif handle_duplicates == \"combine\":\n                    # Combine matching categories\n                    if grouped_elements[source_id][\"category\"] != elem[\"category\"]:\n                        if isinstance(grouped_elements[source_id][\"category\"], str):\n                            grouped_elements[source_id][\"category\"] = [\n                                grouped_elements[source_id][\"category\"]\n                            ]\n                            grouped_elements[source_id][\"category_value\"] = [\n                                grouped_elements[source_id][\"category_value\"]\n                            ]\n\n                        if (\n                            elem[\"category\"]\n                            not in grouped_elements[source_id][\"category\"]\n                        ):\n                            grouped_elements[source_id][\"category\"].append(\n                                elem[\"category\"]\n                            )\n                            grouped_elements[source_id][\"category_value\"].append(\n                                elem[\"category_value\"]\n                            )\n                # For 'primary', just keep the first one we encountered\n\n            all_elements = list(grouped_elements.values())\n\n        locations = pd.DataFrame(all_elements)\n\n        # Log element type distribution\n        type_counts = locations[\"type\"].value_counts()\n        self.logger.info(\"\\nElement type distribution:\")\n        for element_type, count in type_counts.items():\n            self.logger.info(f\"{element_type}: {count}\")\n\n        # Log category distribution\n        if handle_duplicates == \"combine\":\n            # Count each category separately when they're in lists\n            category_counts = {}\n            for cats in locations[\"category\"]:\n                if isinstance(cats, list):\n                    for cat in cats:\n                        category_counts[cat] = category_counts.get(cat, 0) + 1\n                else:\n                    category_counts[cats] = category_counts.get(cats, 0) + 1\n\n            self.logger.info(\"\\nCategory distribution:\")\n            for category, count in category_counts.items():\n                self.logger.info(f\"{category}: {count}\")\n        else:\n            category_counts = locations[\"category\"].value_counts()\n            self.logger.info(\"\\nCategory distribution:\")\n            for category, count in category_counts.items():\n                self.logger.info(f\"{category}: {count}\")\n\n        # Log elements with multiple matching categories\n        multi_category = [e for e in all_elements if len(e[\"matching_categories\"]) &gt; 1]\n        if multi_category:\n            self.logger.info(\n                f\"\\n{len(multi_category)} elements matched multiple categories\"\n            )\n\n        self.logger.info(f\"Successfully processed {len(locations)} locations\")\n        return locations\n</code></pre>"},{"location":"api/handlers/#gigaspatial.handlers.osm.OSMLocationFetcher.__post_init__","title":"<code>__post_init__()</code>","text":"<p>Validate inputs, normalize location_types, and set up logging.</p> Source code in <code>gigaspatial/handlers/osm.py</code> <pre><code>def __post_init__(self):\n    \"\"\"Validate inputs, normalize location_types, and set up logging.\"\"\"\n    try:\n        self.country = pycountry.countries.lookup(self.country).alpha_2\n    except LookupError:\n        raise ValueError(f\"Invalid country code provided: {self.country}\")\n\n    # Normalize location_types to always be a dictionary\n    if isinstance(self.location_types, list):\n        self.location_types = {\"amenity\": self.location_types}\n    elif not isinstance(self.location_types, dict):\n        raise TypeError(\n            \"location_types must be a list of strings or a dictionary mapping categories to type lists\"\n        )\n\n    self.logger = config.get_logger(self.__class__.__name__)\n</code></pre>"},{"location":"api/handlers/#gigaspatial.handlers.osm.OSMLocationFetcher.fetch_locations","title":"<code>fetch_locations(since_year=None, handle_duplicates='separate')</code>","text":"<p>Fetch and process OSM locations.</p> <p>Parameters:</p> Name Type Description Default <code>since_year</code> <code>int</code> <p>Filter for locations added/modified since this year.</p> <code>None</code> <code>handle_duplicates</code> <code>str</code> <p>How to handle objects matching multiple categories: - 'separate': Create separate entries for each category (default) - 'combine': Use a single entry with a list of matching categories - 'primary': Keep only the first matching category</p> <code>'separate'</code> <p>Returns:</p> Type Description <code>DataFrame</code> <p>pd.DataFrame: Processed OSM locations</p> Source code in <code>gigaspatial/handlers/osm.py</code> <pre><code>def fetch_locations(\n    self,\n    since_year: Optional[int] = None,\n    handle_duplicates: Literal[\"separate\", \"combine\", \"primary\"] = \"separate\",\n) -&gt; pd.DataFrame:\n    \"\"\"\n    Fetch and process OSM locations.\n\n    Args:\n        since_year (int, optional): Filter for locations added/modified since this year.\n        handle_duplicates (str): How to handle objects matching multiple categories:\n            - 'separate': Create separate entries for each category (default)\n            - 'combine': Use a single entry with a list of matching categories\n            - 'primary': Keep only the first matching category\n\n    Returns:\n        pd.DataFrame: Processed OSM locations\n    \"\"\"\n    if handle_duplicates not in (\"separate\", \"combine\", \"primary\"):\n        raise ValueError(\n            \"handle_duplicates must be one of: 'separate', 'combine', 'primary'\"\n        )\n\n    self.logger.info(\n        f\"Fetching OSM locations from Overpass API for country: {self.country}\"\n    )\n    self.logger.info(f\"Location types: {self.location_types}\")\n    self.logger.info(f\"Handling duplicate category matches as: {handle_duplicates}\")\n\n    # Get queries for different element types\n    nodes_relations_query, ways_query = self._build_queries(since_year)\n\n    # Fetch nodes and relations\n    nodes_relations_response = self._make_request(nodes_relations_query)\n    nodes_relations = nodes_relations_response.get(\"elements\", [])\n\n    # Fetch ways\n    ways_response = self._make_request(ways_query)\n    ways = ways_response.get(\"elements\", [])\n\n    if not nodes_relations and not ways:\n        self.logger.warning(\"No locations found for the specified criteria\")\n        return pd.DataFrame()\n\n    self.logger.info(\n        f\"Processing {len(nodes_relations)} nodes/relations and {len(ways)} ways...\"\n    )\n\n    # Process nodes and relations\n    with ThreadPoolExecutor() as executor:\n        processed_nodes_relations = [\n            item\n            for sublist in executor.map(\n                self._process_node_relation, nodes_relations\n            )\n            for item in sublist\n        ]\n\n    # Process ways\n    with ThreadPoolExecutor() as executor:\n        processed_ways = [\n            item\n            for sublist in executor.map(self._process_way, ways)\n            for item in sublist\n        ]\n\n    # Combine all processed elements\n    all_elements = processed_nodes_relations + processed_ways\n\n    if not all_elements:\n        self.logger.warning(\"No matching elements found after processing\")\n        return pd.DataFrame()\n\n    # Handle duplicates based on the specified strategy\n    if handle_duplicates != \"separate\":\n        # Group by source_id\n        grouped_elements = {}\n        for elem in all_elements:\n            source_id = elem[\"source_id\"]\n            if source_id not in grouped_elements:\n                grouped_elements[source_id] = elem\n            elif handle_duplicates == \"combine\":\n                # Combine matching categories\n                if grouped_elements[source_id][\"category\"] != elem[\"category\"]:\n                    if isinstance(grouped_elements[source_id][\"category\"], str):\n                        grouped_elements[source_id][\"category\"] = [\n                            grouped_elements[source_id][\"category\"]\n                        ]\n                        grouped_elements[source_id][\"category_value\"] = [\n                            grouped_elements[source_id][\"category_value\"]\n                        ]\n\n                    if (\n                        elem[\"category\"]\n                        not in grouped_elements[source_id][\"category\"]\n                    ):\n                        grouped_elements[source_id][\"category\"].append(\n                            elem[\"category\"]\n                        )\n                        grouped_elements[source_id][\"category_value\"].append(\n                            elem[\"category_value\"]\n                        )\n            # For 'primary', just keep the first one we encountered\n\n        all_elements = list(grouped_elements.values())\n\n    locations = pd.DataFrame(all_elements)\n\n    # Log element type distribution\n    type_counts = locations[\"type\"].value_counts()\n    self.logger.info(\"\\nElement type distribution:\")\n    for element_type, count in type_counts.items():\n        self.logger.info(f\"{element_type}: {count}\")\n\n    # Log category distribution\n    if handle_duplicates == \"combine\":\n        # Count each category separately when they're in lists\n        category_counts = {}\n        for cats in locations[\"category\"]:\n            if isinstance(cats, list):\n                for cat in cats:\n                    category_counts[cat] = category_counts.get(cat, 0) + 1\n            else:\n                category_counts[cats] = category_counts.get(cats, 0) + 1\n\n        self.logger.info(\"\\nCategory distribution:\")\n        for category, count in category_counts.items():\n            self.logger.info(f\"{category}: {count}\")\n    else:\n        category_counts = locations[\"category\"].value_counts()\n        self.logger.info(\"\\nCategory distribution:\")\n        for category, count in category_counts.items():\n            self.logger.info(f\"{category}: {count}\")\n\n    # Log elements with multiple matching categories\n    multi_category = [e for e in all_elements if len(e[\"matching_categories\"]) &gt; 1]\n    if multi_category:\n        self.logger.info(\n            f\"\\n{len(multi_category)} elements matched multiple categories\"\n        )\n\n    self.logger.info(f\"Successfully processed {len(locations)} locations\")\n    return locations\n</code></pre>"},{"location":"api/handlers/#gigaspatial.handlers.overture","title":"<code>overture</code>","text":""},{"location":"api/handlers/#gigaspatial.handlers.overture.OvertureAmenityFetcher","title":"<code>OvertureAmenityFetcher</code>","text":"<p>A class to fetch and process amenity locations from Overture.</p> Source code in <code>gigaspatial/handlers/overture.py</code> <pre><code>@dataclass(config=ConfigDict(arbitrary_types_allowed=True))\nclass OvertureAmenityFetcher:\n    \"\"\"\n    A class to fetch and process amenity locations from Overture.\n    \"\"\"\n\n    # constants\n    release: Optional[str] = \"2024-12-18.0\"\n    base_url: Optional[str] = (\n        \"s3://overturemaps-us-west-2/release/{release}/theme=places/*/*\"\n    )\n\n    # user config\n    country: str = Field(...)\n    amenity_types: List[str] = Field(..., description=\"List of amenity types to fetch\")\n    geom: Union[Polygon, MultiPolygon] = None\n\n    # config for country boundary access from data storage\n    # if None GADM boundaries will be used\n    data_store: DataStore = None\n    country_geom_path: Optional[Union[str, Path]] = None\n\n    def __post_init__(self):\n        \"\"\"Validate inputs and set up logging.\"\"\"\n        try:\n            self.country = pycountry.countries.lookup(self.country).alpha_2\n        except LookupError:\n            raise ValueError(f\"Invalid country code provided: {self.country}\")\n\n        self.base_url = self.base_url.format(release=self.release)\n        self.logger = config.get_logger(self.__class__.__name__)\n\n        self.connection = self._set_connection()\n\n    def _set_connection(self):\n        \"\"\"Set the connection to the DB\"\"\"\n        db = duckdb.connect()\n        db.install_extension(\"spatial\")\n        db.load_extension(\"spatial\")\n        return db\n\n    def _load_country_geometry(\n        self,\n    ) -&gt; Union[Polygon, MultiPolygon]:\n        \"\"\"Load country boundary geometry from DataStore or GADM.\"\"\"\n\n        gdf_admin0 = AdminBoundaries.create(\n            country_code=pycountry.countries.lookup(self.country).alpha_3,\n            admin_level=0,\n            data_store=self.data_store,\n            path=self.country_geom_path,\n        ).to_geodataframe()\n\n        return gdf_admin0.geometry.iloc[0]\n\n    def _build_query(self, match_pattern: bool = False, **kwargs) -&gt; str:\n        \"\"\"Constructs and returns the query\"\"\"\n\n        if match_pattern:\n            amenity_query = \" OR \".join(\n                [f\"category ilike '%{amenity}%'\" for amenity in self.amenity_types]\n            )\n        else:\n            amenity_query = \" OR \".join(\n                [f\"category == '{amenity}'\" for amenity in self.amenity_types]\n            )\n\n        query = \"\"\"\n        SELECT id,\n            names.primary AS name,\n            ROUND(confidence,2) as confidence,\n            categories.primary AS category,\n            ST_AsText(geometry) as geometry,\n        FROM read_parquet('s3://overturemaps-us-west-2/release/2024-12-18.0/theme=places/type=place/*',\n            hive_partitioning=1)\n        WHERE bbox.xmin &gt; {}\n            AND bbox.ymin &gt; {} \n            AND bbox.xmax &lt;  {}\n            AND bbox.ymax &lt; {}\n            AND ({})\n        \"\"\"\n\n        if not self.geom:\n            self.geom = self._load_country_geometry()\n\n        return query.format(*self.geom.bounds, amenity_query)\n\n    def fetch_locations(\n        self, match_pattern: bool = False, **kwargs\n    ) -&gt; gpd.GeoDataFrame:\n        \"\"\"Fetch and process amenity locations.\"\"\"\n        self.logger.info(\"Fetching amenity locations from Overture DB...\")\n\n        query = self._build_query(match_pattern=match_pattern, **kwargs)\n\n        df = self.connection.execute(query).df()\n\n        self.logger.info(\"Processing geometries\")\n        gdf = gpd.GeoDataFrame(\n            df, geometry=gpd.GeoSeries.from_wkt(df[\"geometry\"]), crs=\"EPSG:4326\"\n        )\n\n        # filter by geometry boundary\n        s = STRtree(gdf.geometry)\n        result = s.query(self.geom, predicate=\"intersects\")\n\n        locations = gdf.iloc[result].reset_index(drop=True)\n\n        self.logger.info(f\"Successfully processed {len(locations)} amenity locations\")\n        return locations\n</code></pre>"},{"location":"api/handlers/#gigaspatial.handlers.overture.OvertureAmenityFetcher.__post_init__","title":"<code>__post_init__()</code>","text":"<p>Validate inputs and set up logging.</p> Source code in <code>gigaspatial/handlers/overture.py</code> <pre><code>def __post_init__(self):\n    \"\"\"Validate inputs and set up logging.\"\"\"\n    try:\n        self.country = pycountry.countries.lookup(self.country).alpha_2\n    except LookupError:\n        raise ValueError(f\"Invalid country code provided: {self.country}\")\n\n    self.base_url = self.base_url.format(release=self.release)\n    self.logger = config.get_logger(self.__class__.__name__)\n\n    self.connection = self._set_connection()\n</code></pre>"},{"location":"api/handlers/#gigaspatial.handlers.overture.OvertureAmenityFetcher.fetch_locations","title":"<code>fetch_locations(match_pattern=False, **kwargs)</code>","text":"<p>Fetch and process amenity locations.</p> Source code in <code>gigaspatial/handlers/overture.py</code> <pre><code>def fetch_locations(\n    self, match_pattern: bool = False, **kwargs\n) -&gt; gpd.GeoDataFrame:\n    \"\"\"Fetch and process amenity locations.\"\"\"\n    self.logger.info(\"Fetching amenity locations from Overture DB...\")\n\n    query = self._build_query(match_pattern=match_pattern, **kwargs)\n\n    df = self.connection.execute(query).df()\n\n    self.logger.info(\"Processing geometries\")\n    gdf = gpd.GeoDataFrame(\n        df, geometry=gpd.GeoSeries.from_wkt(df[\"geometry\"]), crs=\"EPSG:4326\"\n    )\n\n    # filter by geometry boundary\n    s = STRtree(gdf.geometry)\n    result = s.query(self.geom, predicate=\"intersects\")\n\n    locations = gdf.iloc[result].reset_index(drop=True)\n\n    self.logger.info(f\"Successfully processed {len(locations)} amenity locations\")\n    return locations\n</code></pre>"},{"location":"api/handlers/#gigaspatial.handlers.rwi","title":"<code>rwi</code>","text":""},{"location":"api/handlers/#gigaspatial.handlers.rwi.RWIConfig","title":"<code>RWIConfig</code>  <code>dataclass</code>","text":"<p>               Bases: <code>HDXConfig</code></p> <p>Configuration for Relative Wealth Index data access</p> Source code in <code>gigaspatial/handlers/rwi.py</code> <pre><code>@dataclass(config=ConfigDict(arbitrary_types_allowed=True))\nclass RWIConfig(HDXConfig):\n    \"\"\"Configuration for Relative Wealth Index data access\"\"\"\n\n    # Override dataset_name to be fixed for RWI\n    dataset_name: Literal[\"relative-wealth-index\"] = Field(\n        default=\"relative-wealth-index\"\n    )\n\n    # Additional RWI-specific configurations\n    country: Optional[str] = Field(\n        default=None, description=\"Country ISO code to filter data for\"\n    )\n    latest_only: bool = Field(\n        default=True,\n        description=\"If True, only get the latest resource for each country\",\n    )\n\n    def __post_init__(self):\n        super().__post_init__()\n\n    def get_relevant_data_units_by_country(\n        self, country: str, **kwargs\n    ) -&gt; List[Resource]:\n        \"\"\"Get relevant data units for a country, optionally filtering for latest version\"\"\"\n        resources = super().get_relevant_data_units_by_country(\n            country=country, key=\"url\"\n        )\n\n        if self.latest_only and len(resources) &gt; 1:\n            # Find the resource with the latest creation date\n            latest_resource = None\n            latest_date = None\n\n            for resource in resources:\n                created = resource.get(\"created\")\n                if created:\n                    try:\n                        created_dt = datetime.fromisoformat(\n                            created.replace(\"Z\", \"+00:00\")\n                        )\n                        if latest_date is None or created_dt &gt; latest_date:\n                            latest_date = created_dt\n                            latest_resource = resource\n                    except ValueError:\n                        self.logger.warning(\n                            f\"Could not parse creation date for resource: {created}\"\n                        )\n\n            if latest_resource:\n                resources = [latest_resource]\n\n        return resources\n</code></pre>"},{"location":"api/handlers/#gigaspatial.handlers.rwi.RWIConfig.get_relevant_data_units_by_country","title":"<code>get_relevant_data_units_by_country(country, **kwargs)</code>","text":"<p>Get relevant data units for a country, optionally filtering for latest version</p> Source code in <code>gigaspatial/handlers/rwi.py</code> <pre><code>def get_relevant_data_units_by_country(\n    self, country: str, **kwargs\n) -&gt; List[Resource]:\n    \"\"\"Get relevant data units for a country, optionally filtering for latest version\"\"\"\n    resources = super().get_relevant_data_units_by_country(\n        country=country, key=\"url\"\n    )\n\n    if self.latest_only and len(resources) &gt; 1:\n        # Find the resource with the latest creation date\n        latest_resource = None\n        latest_date = None\n\n        for resource in resources:\n            created = resource.get(\"created\")\n            if created:\n                try:\n                    created_dt = datetime.fromisoformat(\n                        created.replace(\"Z\", \"+00:00\")\n                    )\n                    if latest_date is None or created_dt &gt; latest_date:\n                        latest_date = created_dt\n                        latest_resource = resource\n                except ValueError:\n                    self.logger.warning(\n                        f\"Could not parse creation date for resource: {created}\"\n                    )\n\n        if latest_resource:\n            resources = [latest_resource]\n\n    return resources\n</code></pre>"},{"location":"api/handlers/#gigaspatial.handlers.rwi.RWIDownloader","title":"<code>RWIDownloader</code>","text":"<p>               Bases: <code>HDXDownloader</code></p> <p>Specialized downloader for the Relative Wealth Index dataset from HDX</p> Source code in <code>gigaspatial/handlers/rwi.py</code> <pre><code>class RWIDownloader(HDXDownloader):\n    \"\"\"Specialized downloader for the Relative Wealth Index dataset from HDX\"\"\"\n\n    def __init__(\n        self,\n        config: Union[RWIConfig, dict] = None,\n        data_store: Optional[DataStore] = None,\n        logger: Optional[logging.Logger] = None,\n    ):\n        config = config if isinstance(config, RWIConfig) else RWIConfig(**config)\n        super().__init__(config=config, data_store=data_store, logger=logger)\n</code></pre>"},{"location":"api/handlers/#gigaspatial.handlers.rwi.RWIHandler","title":"<code>RWIHandler</code>","text":"<p>               Bases: <code>HDXHandler</code></p> <p>Handler for Relative Wealth Index dataset</p> Source code in <code>gigaspatial/handlers/rwi.py</code> <pre><code>class RWIHandler(HDXHandler):\n    \"\"\"Handler for Relative Wealth Index dataset\"\"\"\n\n    def __init__(\n        self,\n        config: Optional[RWIConfig] = None,\n        downloader: Optional[RWIDownloader] = None,\n        reader: Optional[RWIReader] = None,\n        data_store: Optional[DataStore] = None,\n        logger: Optional[logging.Logger] = None,\n        **kwargs,\n    ):\n        super().__init__(\n            dataset_name=\"relative-wealth-index\",\n            config=config,\n            downloader=downloader,\n            reader=reader,\n            data_store=data_store,\n            logger=logger,\n            **kwargs,\n        )\n\n    def create_config(\n        self, data_store: DataStore, logger: logging.Logger, **kwargs\n    ) -&gt; RWIConfig:\n        \"\"\"Create and return a RWIConfig instance\"\"\"\n        return RWIConfig(\n            data_store=data_store,\n            logger=logger,\n            **kwargs,\n        )\n\n    def create_downloader(\n        self,\n        config: RWIConfig,\n        data_store: DataStore,\n        logger: logging.Logger,\n        **kwargs,\n    ) -&gt; RWIDownloader:\n        \"\"\"Create and return a RWIDownloader instance\"\"\"\n        return RWIDownloader(\n            config=config,\n            data_store=data_store,\n            logger=logger,\n            **kwargs,\n        )\n\n    def create_reader(\n        self,\n        config: RWIConfig,\n        data_store: DataStore,\n        logger: logging.Logger,\n        **kwargs,\n    ) -&gt; RWIReader:\n        \"\"\"Create and return a RWIReader instance\"\"\"\n        return RWIReader(\n            config=config,\n            data_store=data_store,\n            logger=logger,\n            **kwargs,\n        )\n</code></pre>"},{"location":"api/handlers/#gigaspatial.handlers.rwi.RWIHandler.create_config","title":"<code>create_config(data_store, logger, **kwargs)</code>","text":"<p>Create and return a RWIConfig instance</p> Source code in <code>gigaspatial/handlers/rwi.py</code> <pre><code>def create_config(\n    self, data_store: DataStore, logger: logging.Logger, **kwargs\n) -&gt; RWIConfig:\n    \"\"\"Create and return a RWIConfig instance\"\"\"\n    return RWIConfig(\n        data_store=data_store,\n        logger=logger,\n        **kwargs,\n    )\n</code></pre>"},{"location":"api/handlers/#gigaspatial.handlers.rwi.RWIHandler.create_downloader","title":"<code>create_downloader(config, data_store, logger, **kwargs)</code>","text":"<p>Create and return a RWIDownloader instance</p> Source code in <code>gigaspatial/handlers/rwi.py</code> <pre><code>def create_downloader(\n    self,\n    config: RWIConfig,\n    data_store: DataStore,\n    logger: logging.Logger,\n    **kwargs,\n) -&gt; RWIDownloader:\n    \"\"\"Create and return a RWIDownloader instance\"\"\"\n    return RWIDownloader(\n        config=config,\n        data_store=data_store,\n        logger=logger,\n        **kwargs,\n    )\n</code></pre>"},{"location":"api/handlers/#gigaspatial.handlers.rwi.RWIHandler.create_reader","title":"<code>create_reader(config, data_store, logger, **kwargs)</code>","text":"<p>Create and return a RWIReader instance</p> Source code in <code>gigaspatial/handlers/rwi.py</code> <pre><code>def create_reader(\n    self,\n    config: RWIConfig,\n    data_store: DataStore,\n    logger: logging.Logger,\n    **kwargs,\n) -&gt; RWIReader:\n    \"\"\"Create and return a RWIReader instance\"\"\"\n    return RWIReader(\n        config=config,\n        data_store=data_store,\n        logger=logger,\n        **kwargs,\n    )\n</code></pre>"},{"location":"api/handlers/#gigaspatial.handlers.rwi.RWIReader","title":"<code>RWIReader</code>","text":"<p>               Bases: <code>HDXReader</code></p> <p>Specialized reader for the Relative Wealth Index dataset from HDX</p> Source code in <code>gigaspatial/handlers/rwi.py</code> <pre><code>class RWIReader(HDXReader):\n    \"\"\"Specialized reader for the Relative Wealth Index dataset from HDX\"\"\"\n\n    def __init__(\n        self,\n        config: Union[RWIConfig, dict] = None,\n        data_store: Optional[DataStore] = None,\n        logger: Optional[logging.Logger] = None,\n    ):\n        config = config if isinstance(config, RWIConfig) else RWIConfig(**config)\n        super().__init__(config=config, data_store=data_store, logger=logger)\n</code></pre>"},{"location":"api/handlers/#gigaspatial.handlers.unicef_georepo","title":"<code>unicef_georepo</code>","text":""},{"location":"api/handlers/#gigaspatial.handlers.unicef_georepo.GeoRepoClient","title":"<code>GeoRepoClient</code>","text":"<p>A client for interacting with the GeoRepo API.</p> <p>GeoRepo is a platform for managing and accessing geospatial administrative boundary data. This client provides methods to search, retrieve, and work with modules, datasets, views, and administrative entities.</p> <p>Attributes:</p> Name Type Description <code>base_url</code> <code>str</code> <p>The base URL for the GeoRepo API</p> <code>api_key</code> <code>str</code> <p>The API key for authentication</p> <code>email</code> <code>str</code> <p>The email address associated with the API key</p> <code>headers</code> <code>dict</code> <p>HTTP headers used for API requests</p> Source code in <code>gigaspatial/handlers/unicef_georepo.py</code> <pre><code>class GeoRepoClient:\n    \"\"\"\n    A client for interacting with the GeoRepo API.\n\n    GeoRepo is a platform for managing and accessing geospatial administrative\n    boundary data. This client provides methods to search, retrieve, and work\n    with modules, datasets, views, and administrative entities.\n\n    Attributes:\n        base_url (str): The base URL for the GeoRepo API\n        api_key (str): The API key for authentication\n        email (str): The email address associated with the API key\n        headers (dict): HTTP headers used for API requests\n    \"\"\"\n\n    def __init__(self, api_key=None, email=None):\n        \"\"\"\n        Initialize the GeoRepo client.\n\n        Args:\n            api_key (str, optional): GeoRepo API key. If not provided, will use\n                the GEOREPO_API_KEY environment variable from config.\n            email (str, optional): Email address associated with the API key.\n                If not provided, will use the GEOREPO_USER_EMAIL environment\n                variable from config.\n\n        Raises:\n            ValueError: If api_key or email is not provided and cannot be found\n                in environment variables.\n        \"\"\"\n        self.base_url = \"https://georepo.unicef.org/api/v1\"\n        self.api_key = api_key or config.GEOREPO_API_KEY\n        self.email = email or config.GEOREPO_USER_EMAIL\n        self.logger = config.get_logger(self.__class__.__name__)\n\n        if not self.api_key:\n            raise ValueError(\n                \"API Key is required. Provide it as a parameter or set GEOREPO_API_KEY environment variable.\"\n            )\n\n        if not self.email:\n            raise ValueError(\n                \"Email is required. Provide it as a parameter or set GEOREPO_USER_EMAIL environment variable.\"\n            )\n\n        self.headers = {\n            \"Accept\": \"application/json\",\n            \"Authorization\": f\"Token {self.api_key}\",\n            \"GeoRepo-User-Key\": self.email,\n        }\n\n    def _make_request(self, method, endpoint, params=None, data=None):\n        \"\"\"Internal method to handle making HTTP requests.\"\"\"\n        try:\n            response = requests.request(\n                method, endpoint, headers=self.headers, params=params, json=data\n            )\n            response.raise_for_status()\n            return response\n        except requests.exceptions.RequestException as e:\n            raise requests.exceptions.HTTPError(f\"API request failed: {e}\")\n\n    def check_connection(self):\n        \"\"\"\n        Checks if the API connection is valid by making a simple request.\n\n        Returns:\n            bool: True if the connection is valid, False otherwise.\n        \"\"\"\n        endpoint = f\"{self.base_url}/search/module/list/\"\n        try:\n            self._make_request(\"GET\", endpoint)\n            return True\n        except requests.exceptions.HTTPError as e:\n            return False\n        except requests.exceptions.RequestException as e:\n            raise requests.exceptions.RequestException(\n                f\"Connection check encountered a network error: {e}\"\n            )\n\n    def list_modules(self):\n        \"\"\"\n        List all available modules in GeoRepo.\n\n        A module is a top-level organizational unit that contains datasets.\n        Examples include \"Admin Boundaries\", \"Health Facilities\", etc.\n\n        Returns:\n            dict: JSON response containing a list of modules with their metadata.\n                Each module includes 'uuid', 'name', 'description', and other properties.\n\n        Raises:\n            requests.HTTPError: If the API request fails.\n        \"\"\"\n        endpoint = f\"{self.base_url}/search/module/list/\"\n        response = self._make_request(\"GET\", endpoint)\n        return response.json()\n\n    def list_datasets_by_module(self, module_uuid):\n        \"\"\"\n        List all datasets within a specific module.\n\n        A dataset represents a collection of related geographic entities,\n        such as administrative boundaries for a specific country or region.\n\n        Args:\n            module_uuid (str): The UUID of the module to query.\n\n        Returns:\n            dict: JSON response containing a list of datasets with their metadata.\n                Each dataset includes 'uuid', 'name', 'description', creation date, etc.\n\n        Raises:\n            requests.HTTPError: If the API request fails or module_uuid is invalid.\n        \"\"\"\n        endpoint = f\"{self.base_url}/search/module/{module_uuid}/dataset/list/\"\n        response = self._make_request(\"GET\", endpoint)\n        return response.json()\n\n    def get_dataset_details(self, dataset_uuid):\n        \"\"\"\n        Get detailed information about a specific dataset.\n\n        This includes metadata about the dataset and information about\n        available administrative levels (e.g., country, province, district).\n\n        Args:\n            dataset_uuid (str): The UUID of the dataset to query.\n\n        Returns:\n            dict: JSON response containing dataset details including:\n                - Basic metadata (name, description, etc.)\n                - Available administrative levels and their properties\n                - Temporal information and data sources\n\n        Raises:\n            requests.HTTPError: If the API request fails or dataset_uuid is invalid.\n        \"\"\"\n        endpoint = f\"{self.base_url}/search/dataset/{dataset_uuid}/\"\n        response = self._make_request(\"GET\", endpoint)\n        return response.json()\n\n    def list_views_by_dataset(self, dataset_uuid, page=1, page_size=50):\n        \"\"\"\n        List views for a dataset with pagination support.\n\n        A view represents a specific version or subset of a dataset.\n        Views may be tagged as 'latest' or represent different time periods.\n\n        Args:\n            dataset_uuid (str): The UUID of the dataset to query.\n            page (int, optional): Page number for pagination. Defaults to 1.\n            page_size (int, optional): Number of results per page. Defaults to 50.\n\n        Returns:\n            dict: JSON response containing paginated list of views with metadata.\n                Includes 'results', 'total_page', 'current_page', and 'count' fields.\n                Each view includes 'uuid', 'name', 'tags', and other properties.\n\n        Raises:\n            requests.HTTPError: If the API request fails or dataset_uuid is invalid.\n        \"\"\"\n        endpoint = f\"{self.base_url}/search/dataset/{dataset_uuid}/view/list/\"\n        params = {\"page\": page, \"page_size\": page_size}\n        response = self._make_request(\"GET\", endpoint, params=params)\n        return response.json()\n\n    def list_entities_by_admin_level(\n        self,\n        view_uuid,\n        admin_level,\n        geom=\"no_geom\",\n        format=\"json\",\n        page=1,\n        page_size=50,\n    ):\n        \"\"\"\n        List entities at a specific administrative level within a view.\n\n        Administrative levels typically follow a hierarchy:\n        - Level 0: Countries\n        - Level 1: States/Provinces/Regions\n        - Level 2: Districts/Counties\n        - Level 3: Sub-districts/Municipalities\n        - And so on...\n\n        Args:\n            view_uuid (str): The UUID of the view to query.\n            admin_level (int): The administrative level to retrieve (0, 1, 2, etc.).\n            geom (str, optional): Geometry inclusion level. Options:\n                - \"no_geom\": No geometry data\n                - \"centroid\": Only centroid points\n                - \"full_geom\": Complete boundary geometries\n                Defaults to \"no_geom\".\n            format (str, optional): Response format (\"json\" or \"geojson\").\n                Defaults to \"json\".\n            page (int, optional): Page number for pagination. Defaults to 1.\n            page_size (int, optional): Number of results per page. Defaults to 50.\n\n        Returns:\n            tuple: A tuple containing:\n                - dict: JSON/GeoJSON response with entity data\n                - dict: Metadata with pagination info (page, total_page, total_count)\n\n        Raises:\n            requests.HTTPError: If the API request fails or parameters are invalid.\n        \"\"\"\n        endpoint = (\n            f\"{self.base_url}/search/view/{view_uuid}/entity/level/{admin_level}/\"\n        )\n        params = {\"page\": page, \"page_size\": page_size, \"geom\": geom, \"format\": format}\n        response = self._make_request(\"GET\", endpoint, params=params)\n\n        metadata = {\n            \"page\": int(response.headers.get(\"page\", 1)),\n            \"total_page\": int(response.headers.get(\"total_page\", 1)),\n            \"total_count\": int(response.headers.get(\"count\", 0)),\n        }\n\n        return response.json(), metadata\n\n    def get_entity_by_ucode(self, ucode, geom=\"full_geom\", format=\"geojson\"):\n        \"\"\"\n        Get detailed information about a specific entity using its Ucode.\n\n        A Ucode (Universal Code) is a unique identifier for geographic entities\n        within the GeoRepo system, typically in the format \"ISO3_LEVEL_NAME\".\n\n        Args:\n            ucode (str): The unique code identifier for the entity.\n            geom (str, optional): Geometry inclusion level. Options:\n                - \"no_geom\": No geometry data\n                - \"centroid\": Only centroid points\n                - \"full_geom\": Complete boundary geometries\n                Defaults to \"full_geom\".\n            format (str, optional): Response format (\"json\" or \"geojson\").\n                Defaults to \"geojson\".\n\n        Returns:\n            dict: JSON/GeoJSON response containing entity details including\n                geometry, properties, administrative level, and metadata.\n\n        Raises:\n            requests.HTTPError: If the API request fails or ucode is invalid.\n        \"\"\"\n        endpoint = f\"{self.base_url}/search/entity/ucode/{ucode}/\"\n        params = {\"geom\": geom, \"format\": format}\n        response = self._make_request(\"GET\", endpoint, params=params)\n        return response.json()\n\n    def list_entity_children(\n        self, view_uuid, entity_ucode, geom=\"no_geom\", format=\"json\"\n    ):\n        \"\"\"\n        List direct children of an entity in the administrative hierarchy.\n\n        For example, if given a country entity, this will return its states/provinces.\n        If given a state entity, this will return its districts/counties.\n\n        Args:\n            view_uuid (str): The UUID of the view containing the entity.\n            entity_ucode (str): The Ucode of the parent entity.\n            geom (str, optional): Geometry inclusion level. Options:\n                - \"no_geom\": No geometry data\n                - \"centroid\": Only centroid points\n                - \"full_geom\": Complete boundary geometries\n                Defaults to \"no_geom\".\n            format (str, optional): Response format (\"json\" or \"geojson\").\n                Defaults to \"json\".\n\n        Returns:\n            dict: JSON/GeoJSON response containing list of child entities\n                with their properties and optional geometry data.\n\n        Raises:\n            requests.HTTPError: If the API request fails or parameters are invalid.\n        \"\"\"\n        endpoint = (\n            f\"{self.base_url}/search/view/{view_uuid}/entity/{entity_ucode}/children/\"\n        )\n        params = {\"geom\": geom, \"format\": format}\n        response = self._make_request(\"GET\", endpoint, params=params)\n        return response.json()\n\n    def search_entities_by_name(self, view_uuid, name, page=1, page_size=50):\n        \"\"\"\n        Search for entities by name using fuzzy matching.\n\n        This performs a similarity-based search to find entities whose names\n        match or are similar to the provided search term.\n\n        Args:\n            view_uuid (str): The UUID of the view to search within.\n            name (str): The name or partial name to search for.\n            page (int, optional): Page number for pagination. Defaults to 1.\n            page_size (int, optional): Number of results per page. Defaults to 50.\n\n        Returns:\n            dict: JSON response containing paginated search results with\n                matching entities and their similarity scores.\n\n        Raises:\n            requests.HTTPError: If the API request fails or parameters are invalid.\n        \"\"\"\n        endpoint = f\"{self.base_url}/search/view/{view_uuid}/entity/{name}/\"\n        params = {\"page\": page, \"page_size\": page_size}\n        response = self._make_request(\"GET\", endpoint, params=params)\n        return response.json()\n\n    def get_admin_boundaries(\n        self, view_uuid, admin_level=None, geom=\"full_geom\", format=\"geojson\"\n    ):\n        \"\"\"\n        Get administrative boundaries for a specific level or all levels.\n\n        This is a convenience method that can retrieve boundaries for a single\n        administrative level or attempt to fetch all available levels.\n\n        Args:\n            view_uuid (str): The UUID of the view to query.\n            admin_level (int, optional): Administrative level to retrieve\n                (0=country, 1=region, etc.). If None, attempts to fetch all levels.\n            geom (str, optional): Geometry inclusion level. Options:\n                - \"no_geom\": No geometry data\n                - \"centroid\": Only centroid points\n                - \"full_geom\": Complete boundary geometries\n                Defaults to \"full_geom\".\n            format (str, optional): Response format (\"json\" or \"geojson\").\n                Defaults to \"geojson\".\n\n        Returns:\n            dict: JSON/GeoJSON response containing administrative boundaries\n                in the specified format. For GeoJSON, returns a FeatureCollection.\n\n        Raises:\n            requests.HTTPError: If the API request fails or parameters are invalid.\n        \"\"\"\n        # Construct the endpoint based on whether admin_level is provided\n        if admin_level is not None:\n            endpoint = (\n                f\"{self.base_url}/search/view/{view_uuid}/entity/level/{admin_level}/\"\n            )\n        else:\n            # For all levels, we need to fetch level 0 and then get children for each entity\n            endpoint = f\"{self.base_url}/search/view/{view_uuid}/entity/list/\"\n\n        params = {\n            \"geom\": geom,\n            \"format\": format,\n            \"page_size\": 100,\n        }\n\n        response = self._make_request(\"GET\", endpoint, params=params)\n        return response.json()\n\n    def get_vector_tiles_url(self, view_info):\n        \"\"\"\n        Generate an authenticated URL for accessing vector tiles.\n\n        Vector tiles are used for efficient map rendering and can be consumed\n        by mapping libraries like Mapbox GL JS or OpenLayers.\n\n        Args:\n            view_info (dict): Dictionary containing view information that must\n                include a 'vector_tiles' key with the base vector tiles URL.\n\n        Returns:\n            str: Fully authenticated vector tiles URL with API key and user email\n                parameters appended for access control.\n\n        Raises:\n            ValueError: If 'vector_tiles' key is not found in view_info.\n        \"\"\"\n        if \"vector_tiles\" not in view_info:\n            raise ValueError(\"Vector tiles URL not found in view information\")\n\n        vector_tiles_url = view_info[\"vector_tiles\"]\n\n        # Parse out the timestamp parameter if it exists\n        if \"?t=\" in vector_tiles_url:\n            base_url, timestamp = vector_tiles_url.split(\"?t=\")\n            return f\"{base_url}?t={timestamp}&amp;token={self.api_key}&amp;georepo_user_key={self.email}\"\n        else:\n            return (\n                f\"{vector_tiles_url}?token={self.api_key}&amp;georepo_user_key={self.email}\"\n            )\n\n    def find_country_by_iso3(self, view_uuid, iso3_code):\n        \"\"\"\n        Find a country entity using its ISO3 country code.\n\n        This method searches through all level-0 (country) entities to find\n        one that matches the provided ISO3 code. It checks both the entity's\n        Ucode and any external codes stored in the ext_codes field.\n\n        Args:\n            view_uuid (str): The UUID of the view to search within.\n            iso3_code (str): The ISO3 country code to search for (e.g., 'USA', 'KEN', 'BRA').\n\n        Returns:\n            dict or None: Entity information dictionary for the matching country\n                if found, including properties like name, ucode, admin_level, etc.\n                Returns None if no matching country is found.\n\n        Note:\n            This method handles pagination automatically to search through all\n            available countries in the dataset, which may involve multiple API calls.\n\n        Raises:\n            requests.HTTPError: If the API request fails or view_uuid is invalid.\n        \"\"\"\n        # Admin level 0 represents countries\n        endpoint = f\"{self.base_url}/search/view/{view_uuid}/entity/level/0/\"\n        params = {\n            \"page_size\": 100,\n            \"geom\": \"no_geom\",\n        }\n\n        # need to paginate since it can be a large dataset\n        all_countries = []\n        page = 1\n\n        while True:\n            params[\"page\"] = page\n            response = self._make_request(\"GET\", endpoint, params=params)\n            data = response.json()\n\n            countries = data.get(\"results\", [])\n            all_countries.extend(countries)\n\n            # check if there are more pages\n            if page &gt;= data.get(\"total_page\", 1):\n                break\n\n            page += 1\n\n        # Search by ISO3 code\n        for country in all_countries:\n            # Check if ISO3 code is in the ucode (typically at the beginning)\n            if country[\"ucode\"].startswith(iso3_code + \"_\"):\n                return country\n\n            # Also check in ext_codes which may contain the ISO3 code\n            ext_codes = country.get(\"ext_codes\", {})\n            if ext_codes:\n                # Check if ISO3 is directly in ext_codes\n                if (\n                    ext_codes.get(\"PCode\", \"\") == iso3_code\n                    or ext_codes.get(\"default\", \"\") == iso3_code\n                ):\n                    return country\n\n        return None\n</code></pre>"},{"location":"api/handlers/#gigaspatial.handlers.unicef_georepo.GeoRepoClient.__init__","title":"<code>__init__(api_key=None, email=None)</code>","text":"<p>Initialize the GeoRepo client.</p> <p>Parameters:</p> Name Type Description Default <code>api_key</code> <code>str</code> <p>GeoRepo API key. If not provided, will use the GEOREPO_API_KEY environment variable from config.</p> <code>None</code> <code>email</code> <code>str</code> <p>Email address associated with the API key. If not provided, will use the GEOREPO_USER_EMAIL environment variable from config.</p> <code>None</code> <p>Raises:</p> Type Description <code>ValueError</code> <p>If api_key or email is not provided and cannot be found in environment variables.</p> Source code in <code>gigaspatial/handlers/unicef_georepo.py</code> <pre><code>def __init__(self, api_key=None, email=None):\n    \"\"\"\n    Initialize the GeoRepo client.\n\n    Args:\n        api_key (str, optional): GeoRepo API key. If not provided, will use\n            the GEOREPO_API_KEY environment variable from config.\n        email (str, optional): Email address associated with the API key.\n            If not provided, will use the GEOREPO_USER_EMAIL environment\n            variable from config.\n\n    Raises:\n        ValueError: If api_key or email is not provided and cannot be found\n            in environment variables.\n    \"\"\"\n    self.base_url = \"https://georepo.unicef.org/api/v1\"\n    self.api_key = api_key or config.GEOREPO_API_KEY\n    self.email = email or config.GEOREPO_USER_EMAIL\n    self.logger = config.get_logger(self.__class__.__name__)\n\n    if not self.api_key:\n        raise ValueError(\n            \"API Key is required. Provide it as a parameter or set GEOREPO_API_KEY environment variable.\"\n        )\n\n    if not self.email:\n        raise ValueError(\n            \"Email is required. Provide it as a parameter or set GEOREPO_USER_EMAIL environment variable.\"\n        )\n\n    self.headers = {\n        \"Accept\": \"application/json\",\n        \"Authorization\": f\"Token {self.api_key}\",\n        \"GeoRepo-User-Key\": self.email,\n    }\n</code></pre>"},{"location":"api/handlers/#gigaspatial.handlers.unicef_georepo.GeoRepoClient.check_connection","title":"<code>check_connection()</code>","text":"<p>Checks if the API connection is valid by making a simple request.</p> <p>Returns:</p> Name Type Description <code>bool</code> <p>True if the connection is valid, False otherwise.</p> Source code in <code>gigaspatial/handlers/unicef_georepo.py</code> <pre><code>def check_connection(self):\n    \"\"\"\n    Checks if the API connection is valid by making a simple request.\n\n    Returns:\n        bool: True if the connection is valid, False otherwise.\n    \"\"\"\n    endpoint = f\"{self.base_url}/search/module/list/\"\n    try:\n        self._make_request(\"GET\", endpoint)\n        return True\n    except requests.exceptions.HTTPError as e:\n        return False\n    except requests.exceptions.RequestException as e:\n        raise requests.exceptions.RequestException(\n            f\"Connection check encountered a network error: {e}\"\n        )\n</code></pre>"},{"location":"api/handlers/#gigaspatial.handlers.unicef_georepo.GeoRepoClient.find_country_by_iso3","title":"<code>find_country_by_iso3(view_uuid, iso3_code)</code>","text":"<p>Find a country entity using its ISO3 country code.</p> <p>This method searches through all level-0 (country) entities to find one that matches the provided ISO3 code. It checks both the entity's Ucode and any external codes stored in the ext_codes field.</p> <p>Parameters:</p> Name Type Description Default <code>view_uuid</code> <code>str</code> <p>The UUID of the view to search within.</p> required <code>iso3_code</code> <code>str</code> <p>The ISO3 country code to search for (e.g., 'USA', 'KEN', 'BRA').</p> required <p>Returns:</p> Type Description <p>dict or None: Entity information dictionary for the matching country if found, including properties like name, ucode, admin_level, etc. Returns None if no matching country is found.</p> Note <p>This method handles pagination automatically to search through all available countries in the dataset, which may involve multiple API calls.</p> <p>Raises:</p> Type Description <code>HTTPError</code> <p>If the API request fails or view_uuid is invalid.</p> Source code in <code>gigaspatial/handlers/unicef_georepo.py</code> <pre><code>def find_country_by_iso3(self, view_uuid, iso3_code):\n    \"\"\"\n    Find a country entity using its ISO3 country code.\n\n    This method searches through all level-0 (country) entities to find\n    one that matches the provided ISO3 code. It checks both the entity's\n    Ucode and any external codes stored in the ext_codes field.\n\n    Args:\n        view_uuid (str): The UUID of the view to search within.\n        iso3_code (str): The ISO3 country code to search for (e.g., 'USA', 'KEN', 'BRA').\n\n    Returns:\n        dict or None: Entity information dictionary for the matching country\n            if found, including properties like name, ucode, admin_level, etc.\n            Returns None if no matching country is found.\n\n    Note:\n        This method handles pagination automatically to search through all\n        available countries in the dataset, which may involve multiple API calls.\n\n    Raises:\n        requests.HTTPError: If the API request fails or view_uuid is invalid.\n    \"\"\"\n    # Admin level 0 represents countries\n    endpoint = f\"{self.base_url}/search/view/{view_uuid}/entity/level/0/\"\n    params = {\n        \"page_size\": 100,\n        \"geom\": \"no_geom\",\n    }\n\n    # need to paginate since it can be a large dataset\n    all_countries = []\n    page = 1\n\n    while True:\n        params[\"page\"] = page\n        response = self._make_request(\"GET\", endpoint, params=params)\n        data = response.json()\n\n        countries = data.get(\"results\", [])\n        all_countries.extend(countries)\n\n        # check if there are more pages\n        if page &gt;= data.get(\"total_page\", 1):\n            break\n\n        page += 1\n\n    # Search by ISO3 code\n    for country in all_countries:\n        # Check if ISO3 code is in the ucode (typically at the beginning)\n        if country[\"ucode\"].startswith(iso3_code + \"_\"):\n            return country\n\n        # Also check in ext_codes which may contain the ISO3 code\n        ext_codes = country.get(\"ext_codes\", {})\n        if ext_codes:\n            # Check if ISO3 is directly in ext_codes\n            if (\n                ext_codes.get(\"PCode\", \"\") == iso3_code\n                or ext_codes.get(\"default\", \"\") == iso3_code\n            ):\n                return country\n\n    return None\n</code></pre>"},{"location":"api/handlers/#gigaspatial.handlers.unicef_georepo.GeoRepoClient.get_admin_boundaries","title":"<code>get_admin_boundaries(view_uuid, admin_level=None, geom='full_geom', format='geojson')</code>","text":"<p>Get administrative boundaries for a specific level or all levels.</p> <p>This is a convenience method that can retrieve boundaries for a single administrative level or attempt to fetch all available levels.</p> <p>Parameters:</p> Name Type Description Default <code>view_uuid</code> <code>str</code> <p>The UUID of the view to query.</p> required <code>admin_level</code> <code>int</code> <p>Administrative level to retrieve (0=country, 1=region, etc.). If None, attempts to fetch all levels.</p> <code>None</code> <code>geom</code> <code>str</code> <p>Geometry inclusion level. Options: - \"no_geom\": No geometry data - \"centroid\": Only centroid points - \"full_geom\": Complete boundary geometries Defaults to \"full_geom\".</p> <code>'full_geom'</code> <code>format</code> <code>str</code> <p>Response format (\"json\" or \"geojson\"). Defaults to \"geojson\".</p> <code>'geojson'</code> <p>Returns:</p> Name Type Description <code>dict</code> <p>JSON/GeoJSON response containing administrative boundaries in the specified format. For GeoJSON, returns a FeatureCollection.</p> <p>Raises:</p> Type Description <code>HTTPError</code> <p>If the API request fails or parameters are invalid.</p> Source code in <code>gigaspatial/handlers/unicef_georepo.py</code> <pre><code>def get_admin_boundaries(\n    self, view_uuid, admin_level=None, geom=\"full_geom\", format=\"geojson\"\n):\n    \"\"\"\n    Get administrative boundaries for a specific level or all levels.\n\n    This is a convenience method that can retrieve boundaries for a single\n    administrative level or attempt to fetch all available levels.\n\n    Args:\n        view_uuid (str): The UUID of the view to query.\n        admin_level (int, optional): Administrative level to retrieve\n            (0=country, 1=region, etc.). If None, attempts to fetch all levels.\n        geom (str, optional): Geometry inclusion level. Options:\n            - \"no_geom\": No geometry data\n            - \"centroid\": Only centroid points\n            - \"full_geom\": Complete boundary geometries\n            Defaults to \"full_geom\".\n        format (str, optional): Response format (\"json\" or \"geojson\").\n            Defaults to \"geojson\".\n\n    Returns:\n        dict: JSON/GeoJSON response containing administrative boundaries\n            in the specified format. For GeoJSON, returns a FeatureCollection.\n\n    Raises:\n        requests.HTTPError: If the API request fails or parameters are invalid.\n    \"\"\"\n    # Construct the endpoint based on whether admin_level is provided\n    if admin_level is not None:\n        endpoint = (\n            f\"{self.base_url}/search/view/{view_uuid}/entity/level/{admin_level}/\"\n        )\n    else:\n        # For all levels, we need to fetch level 0 and then get children for each entity\n        endpoint = f\"{self.base_url}/search/view/{view_uuid}/entity/list/\"\n\n    params = {\n        \"geom\": geom,\n        \"format\": format,\n        \"page_size\": 100,\n    }\n\n    response = self._make_request(\"GET\", endpoint, params=params)\n    return response.json()\n</code></pre>"},{"location":"api/handlers/#gigaspatial.handlers.unicef_georepo.GeoRepoClient.get_dataset_details","title":"<code>get_dataset_details(dataset_uuid)</code>","text":"<p>Get detailed information about a specific dataset.</p> <p>This includes metadata about the dataset and information about available administrative levels (e.g., country, province, district).</p> <p>Parameters:</p> Name Type Description Default <code>dataset_uuid</code> <code>str</code> <p>The UUID of the dataset to query.</p> required <p>Returns:</p> Name Type Description <code>dict</code> <p>JSON response containing dataset details including: - Basic metadata (name, description, etc.) - Available administrative levels and their properties - Temporal information and data sources</p> <p>Raises:</p> Type Description <code>HTTPError</code> <p>If the API request fails or dataset_uuid is invalid.</p> Source code in <code>gigaspatial/handlers/unicef_georepo.py</code> <pre><code>def get_dataset_details(self, dataset_uuid):\n    \"\"\"\n    Get detailed information about a specific dataset.\n\n    This includes metadata about the dataset and information about\n    available administrative levels (e.g., country, province, district).\n\n    Args:\n        dataset_uuid (str): The UUID of the dataset to query.\n\n    Returns:\n        dict: JSON response containing dataset details including:\n            - Basic metadata (name, description, etc.)\n            - Available administrative levels and their properties\n            - Temporal information and data sources\n\n    Raises:\n        requests.HTTPError: If the API request fails or dataset_uuid is invalid.\n    \"\"\"\n    endpoint = f\"{self.base_url}/search/dataset/{dataset_uuid}/\"\n    response = self._make_request(\"GET\", endpoint)\n    return response.json()\n</code></pre>"},{"location":"api/handlers/#gigaspatial.handlers.unicef_georepo.GeoRepoClient.get_entity_by_ucode","title":"<code>get_entity_by_ucode(ucode, geom='full_geom', format='geojson')</code>","text":"<p>Get detailed information about a specific entity using its Ucode.</p> <p>A Ucode (Universal Code) is a unique identifier for geographic entities within the GeoRepo system, typically in the format \"ISO3_LEVEL_NAME\".</p> <p>Parameters:</p> Name Type Description Default <code>ucode</code> <code>str</code> <p>The unique code identifier for the entity.</p> required <code>geom</code> <code>str</code> <p>Geometry inclusion level. Options: - \"no_geom\": No geometry data - \"centroid\": Only centroid points - \"full_geom\": Complete boundary geometries Defaults to \"full_geom\".</p> <code>'full_geom'</code> <code>format</code> <code>str</code> <p>Response format (\"json\" or \"geojson\"). Defaults to \"geojson\".</p> <code>'geojson'</code> <p>Returns:</p> Name Type Description <code>dict</code> <p>JSON/GeoJSON response containing entity details including geometry, properties, administrative level, and metadata.</p> <p>Raises:</p> Type Description <code>HTTPError</code> <p>If the API request fails or ucode is invalid.</p> Source code in <code>gigaspatial/handlers/unicef_georepo.py</code> <pre><code>def get_entity_by_ucode(self, ucode, geom=\"full_geom\", format=\"geojson\"):\n    \"\"\"\n    Get detailed information about a specific entity using its Ucode.\n\n    A Ucode (Universal Code) is a unique identifier for geographic entities\n    within the GeoRepo system, typically in the format \"ISO3_LEVEL_NAME\".\n\n    Args:\n        ucode (str): The unique code identifier for the entity.\n        geom (str, optional): Geometry inclusion level. Options:\n            - \"no_geom\": No geometry data\n            - \"centroid\": Only centroid points\n            - \"full_geom\": Complete boundary geometries\n            Defaults to \"full_geom\".\n        format (str, optional): Response format (\"json\" or \"geojson\").\n            Defaults to \"geojson\".\n\n    Returns:\n        dict: JSON/GeoJSON response containing entity details including\n            geometry, properties, administrative level, and metadata.\n\n    Raises:\n        requests.HTTPError: If the API request fails or ucode is invalid.\n    \"\"\"\n    endpoint = f\"{self.base_url}/search/entity/ucode/{ucode}/\"\n    params = {\"geom\": geom, \"format\": format}\n    response = self._make_request(\"GET\", endpoint, params=params)\n    return response.json()\n</code></pre>"},{"location":"api/handlers/#gigaspatial.handlers.unicef_georepo.GeoRepoClient.get_vector_tiles_url","title":"<code>get_vector_tiles_url(view_info)</code>","text":"<p>Generate an authenticated URL for accessing vector tiles.</p> <p>Vector tiles are used for efficient map rendering and can be consumed by mapping libraries like Mapbox GL JS or OpenLayers.</p> <p>Parameters:</p> Name Type Description Default <code>view_info</code> <code>dict</code> <p>Dictionary containing view information that must include a 'vector_tiles' key with the base vector tiles URL.</p> required <p>Returns:</p> Name Type Description <code>str</code> <p>Fully authenticated vector tiles URL with API key and user email parameters appended for access control.</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If 'vector_tiles' key is not found in view_info.</p> Source code in <code>gigaspatial/handlers/unicef_georepo.py</code> <pre><code>def get_vector_tiles_url(self, view_info):\n    \"\"\"\n    Generate an authenticated URL for accessing vector tiles.\n\n    Vector tiles are used for efficient map rendering and can be consumed\n    by mapping libraries like Mapbox GL JS or OpenLayers.\n\n    Args:\n        view_info (dict): Dictionary containing view information that must\n            include a 'vector_tiles' key with the base vector tiles URL.\n\n    Returns:\n        str: Fully authenticated vector tiles URL with API key and user email\n            parameters appended for access control.\n\n    Raises:\n        ValueError: If 'vector_tiles' key is not found in view_info.\n    \"\"\"\n    if \"vector_tiles\" not in view_info:\n        raise ValueError(\"Vector tiles URL not found in view information\")\n\n    vector_tiles_url = view_info[\"vector_tiles\"]\n\n    # Parse out the timestamp parameter if it exists\n    if \"?t=\" in vector_tiles_url:\n        base_url, timestamp = vector_tiles_url.split(\"?t=\")\n        return f\"{base_url}?t={timestamp}&amp;token={self.api_key}&amp;georepo_user_key={self.email}\"\n    else:\n        return (\n            f\"{vector_tiles_url}?token={self.api_key}&amp;georepo_user_key={self.email}\"\n        )\n</code></pre>"},{"location":"api/handlers/#gigaspatial.handlers.unicef_georepo.GeoRepoClient.list_datasets_by_module","title":"<code>list_datasets_by_module(module_uuid)</code>","text":"<p>List all datasets within a specific module.</p> <p>A dataset represents a collection of related geographic entities, such as administrative boundaries for a specific country or region.</p> <p>Parameters:</p> Name Type Description Default <code>module_uuid</code> <code>str</code> <p>The UUID of the module to query.</p> required <p>Returns:</p> Name Type Description <code>dict</code> <p>JSON response containing a list of datasets with their metadata. Each dataset includes 'uuid', 'name', 'description', creation date, etc.</p> <p>Raises:</p> Type Description <code>HTTPError</code> <p>If the API request fails or module_uuid is invalid.</p> Source code in <code>gigaspatial/handlers/unicef_georepo.py</code> <pre><code>def list_datasets_by_module(self, module_uuid):\n    \"\"\"\n    List all datasets within a specific module.\n\n    A dataset represents a collection of related geographic entities,\n    such as administrative boundaries for a specific country or region.\n\n    Args:\n        module_uuid (str): The UUID of the module to query.\n\n    Returns:\n        dict: JSON response containing a list of datasets with their metadata.\n            Each dataset includes 'uuid', 'name', 'description', creation date, etc.\n\n    Raises:\n        requests.HTTPError: If the API request fails or module_uuid is invalid.\n    \"\"\"\n    endpoint = f\"{self.base_url}/search/module/{module_uuid}/dataset/list/\"\n    response = self._make_request(\"GET\", endpoint)\n    return response.json()\n</code></pre>"},{"location":"api/handlers/#gigaspatial.handlers.unicef_georepo.GeoRepoClient.list_entities_by_admin_level","title":"<code>list_entities_by_admin_level(view_uuid, admin_level, geom='no_geom', format='json', page=1, page_size=50)</code>","text":"<p>List entities at a specific administrative level within a view.</p> <p>Administrative levels typically follow a hierarchy: - Level 0: Countries - Level 1: States/Provinces/Regions - Level 2: Districts/Counties - Level 3: Sub-districts/Municipalities - And so on...</p> <p>Parameters:</p> Name Type Description Default <code>view_uuid</code> <code>str</code> <p>The UUID of the view to query.</p> required <code>admin_level</code> <code>int</code> <p>The administrative level to retrieve (0, 1, 2, etc.).</p> required <code>geom</code> <code>str</code> <p>Geometry inclusion level. Options: - \"no_geom\": No geometry data - \"centroid\": Only centroid points - \"full_geom\": Complete boundary geometries Defaults to \"no_geom\".</p> <code>'no_geom'</code> <code>format</code> <code>str</code> <p>Response format (\"json\" or \"geojson\"). Defaults to \"json\".</p> <code>'json'</code> <code>page</code> <code>int</code> <p>Page number for pagination. Defaults to 1.</p> <code>1</code> <code>page_size</code> <code>int</code> <p>Number of results per page. Defaults to 50.</p> <code>50</code> <p>Returns:</p> Name Type Description <code>tuple</code> <p>A tuple containing: - dict: JSON/GeoJSON response with entity data - dict: Metadata with pagination info (page, total_page, total_count)</p> <p>Raises:</p> Type Description <code>HTTPError</code> <p>If the API request fails or parameters are invalid.</p> Source code in <code>gigaspatial/handlers/unicef_georepo.py</code> <pre><code>def list_entities_by_admin_level(\n    self,\n    view_uuid,\n    admin_level,\n    geom=\"no_geom\",\n    format=\"json\",\n    page=1,\n    page_size=50,\n):\n    \"\"\"\n    List entities at a specific administrative level within a view.\n\n    Administrative levels typically follow a hierarchy:\n    - Level 0: Countries\n    - Level 1: States/Provinces/Regions\n    - Level 2: Districts/Counties\n    - Level 3: Sub-districts/Municipalities\n    - And so on...\n\n    Args:\n        view_uuid (str): The UUID of the view to query.\n        admin_level (int): The administrative level to retrieve (0, 1, 2, etc.).\n        geom (str, optional): Geometry inclusion level. Options:\n            - \"no_geom\": No geometry data\n            - \"centroid\": Only centroid points\n            - \"full_geom\": Complete boundary geometries\n            Defaults to \"no_geom\".\n        format (str, optional): Response format (\"json\" or \"geojson\").\n            Defaults to \"json\".\n        page (int, optional): Page number for pagination. Defaults to 1.\n        page_size (int, optional): Number of results per page. Defaults to 50.\n\n    Returns:\n        tuple: A tuple containing:\n            - dict: JSON/GeoJSON response with entity data\n            - dict: Metadata with pagination info (page, total_page, total_count)\n\n    Raises:\n        requests.HTTPError: If the API request fails or parameters are invalid.\n    \"\"\"\n    endpoint = (\n        f\"{self.base_url}/search/view/{view_uuid}/entity/level/{admin_level}/\"\n    )\n    params = {\"page\": page, \"page_size\": page_size, \"geom\": geom, \"format\": format}\n    response = self._make_request(\"GET\", endpoint, params=params)\n\n    metadata = {\n        \"page\": int(response.headers.get(\"page\", 1)),\n        \"total_page\": int(response.headers.get(\"total_page\", 1)),\n        \"total_count\": int(response.headers.get(\"count\", 0)),\n    }\n\n    return response.json(), metadata\n</code></pre>"},{"location":"api/handlers/#gigaspatial.handlers.unicef_georepo.GeoRepoClient.list_entity_children","title":"<code>list_entity_children(view_uuid, entity_ucode, geom='no_geom', format='json')</code>","text":"<p>List direct children of an entity in the administrative hierarchy.</p> <p>For example, if given a country entity, this will return its states/provinces. If given a state entity, this will return its districts/counties.</p> <p>Parameters:</p> Name Type Description Default <code>view_uuid</code> <code>str</code> <p>The UUID of the view containing the entity.</p> required <code>entity_ucode</code> <code>str</code> <p>The Ucode of the parent entity.</p> required <code>geom</code> <code>str</code> <p>Geometry inclusion level. Options: - \"no_geom\": No geometry data - \"centroid\": Only centroid points - \"full_geom\": Complete boundary geometries Defaults to \"no_geom\".</p> <code>'no_geom'</code> <code>format</code> <code>str</code> <p>Response format (\"json\" or \"geojson\"). Defaults to \"json\".</p> <code>'json'</code> <p>Returns:</p> Name Type Description <code>dict</code> <p>JSON/GeoJSON response containing list of child entities with their properties and optional geometry data.</p> <p>Raises:</p> Type Description <code>HTTPError</code> <p>If the API request fails or parameters are invalid.</p> Source code in <code>gigaspatial/handlers/unicef_georepo.py</code> <pre><code>def list_entity_children(\n    self, view_uuid, entity_ucode, geom=\"no_geom\", format=\"json\"\n):\n    \"\"\"\n    List direct children of an entity in the administrative hierarchy.\n\n    For example, if given a country entity, this will return its states/provinces.\n    If given a state entity, this will return its districts/counties.\n\n    Args:\n        view_uuid (str): The UUID of the view containing the entity.\n        entity_ucode (str): The Ucode of the parent entity.\n        geom (str, optional): Geometry inclusion level. Options:\n            - \"no_geom\": No geometry data\n            - \"centroid\": Only centroid points\n            - \"full_geom\": Complete boundary geometries\n            Defaults to \"no_geom\".\n        format (str, optional): Response format (\"json\" or \"geojson\").\n            Defaults to \"json\".\n\n    Returns:\n        dict: JSON/GeoJSON response containing list of child entities\n            with their properties and optional geometry data.\n\n    Raises:\n        requests.HTTPError: If the API request fails or parameters are invalid.\n    \"\"\"\n    endpoint = (\n        f\"{self.base_url}/search/view/{view_uuid}/entity/{entity_ucode}/children/\"\n    )\n    params = {\"geom\": geom, \"format\": format}\n    response = self._make_request(\"GET\", endpoint, params=params)\n    return response.json()\n</code></pre>"},{"location":"api/handlers/#gigaspatial.handlers.unicef_georepo.GeoRepoClient.list_modules","title":"<code>list_modules()</code>","text":"<p>List all available modules in GeoRepo.</p> <p>A module is a top-level organizational unit that contains datasets. Examples include \"Admin Boundaries\", \"Health Facilities\", etc.</p> <p>Returns:</p> Name Type Description <code>dict</code> <p>JSON response containing a list of modules with their metadata. Each module includes 'uuid', 'name', 'description', and other properties.</p> <p>Raises:</p> Type Description <code>HTTPError</code> <p>If the API request fails.</p> Source code in <code>gigaspatial/handlers/unicef_georepo.py</code> <pre><code>def list_modules(self):\n    \"\"\"\n    List all available modules in GeoRepo.\n\n    A module is a top-level organizational unit that contains datasets.\n    Examples include \"Admin Boundaries\", \"Health Facilities\", etc.\n\n    Returns:\n        dict: JSON response containing a list of modules with their metadata.\n            Each module includes 'uuid', 'name', 'description', and other properties.\n\n    Raises:\n        requests.HTTPError: If the API request fails.\n    \"\"\"\n    endpoint = f\"{self.base_url}/search/module/list/\"\n    response = self._make_request(\"GET\", endpoint)\n    return response.json()\n</code></pre>"},{"location":"api/handlers/#gigaspatial.handlers.unicef_georepo.GeoRepoClient.list_views_by_dataset","title":"<code>list_views_by_dataset(dataset_uuid, page=1, page_size=50)</code>","text":"<p>List views for a dataset with pagination support.</p> <p>A view represents a specific version or subset of a dataset. Views may be tagged as 'latest' or represent different time periods.</p> <p>Parameters:</p> Name Type Description Default <code>dataset_uuid</code> <code>str</code> <p>The UUID of the dataset to query.</p> required <code>page</code> <code>int</code> <p>Page number for pagination. Defaults to 1.</p> <code>1</code> <code>page_size</code> <code>int</code> <p>Number of results per page. Defaults to 50.</p> <code>50</code> <p>Returns:</p> Name Type Description <code>dict</code> <p>JSON response containing paginated list of views with metadata. Includes 'results', 'total_page', 'current_page', and 'count' fields. Each view includes 'uuid', 'name', 'tags', and other properties.</p> <p>Raises:</p> Type Description <code>HTTPError</code> <p>If the API request fails or dataset_uuid is invalid.</p> Source code in <code>gigaspatial/handlers/unicef_georepo.py</code> <pre><code>def list_views_by_dataset(self, dataset_uuid, page=1, page_size=50):\n    \"\"\"\n    List views for a dataset with pagination support.\n\n    A view represents a specific version or subset of a dataset.\n    Views may be tagged as 'latest' or represent different time periods.\n\n    Args:\n        dataset_uuid (str): The UUID of the dataset to query.\n        page (int, optional): Page number for pagination. Defaults to 1.\n        page_size (int, optional): Number of results per page. Defaults to 50.\n\n    Returns:\n        dict: JSON response containing paginated list of views with metadata.\n            Includes 'results', 'total_page', 'current_page', and 'count' fields.\n            Each view includes 'uuid', 'name', 'tags', and other properties.\n\n    Raises:\n        requests.HTTPError: If the API request fails or dataset_uuid is invalid.\n    \"\"\"\n    endpoint = f\"{self.base_url}/search/dataset/{dataset_uuid}/view/list/\"\n    params = {\"page\": page, \"page_size\": page_size}\n    response = self._make_request(\"GET\", endpoint, params=params)\n    return response.json()\n</code></pre>"},{"location":"api/handlers/#gigaspatial.handlers.unicef_georepo.GeoRepoClient.search_entities_by_name","title":"<code>search_entities_by_name(view_uuid, name, page=1, page_size=50)</code>","text":"<p>Search for entities by name using fuzzy matching.</p> <p>This performs a similarity-based search to find entities whose names match or are similar to the provided search term.</p> <p>Parameters:</p> Name Type Description Default <code>view_uuid</code> <code>str</code> <p>The UUID of the view to search within.</p> required <code>name</code> <code>str</code> <p>The name or partial name to search for.</p> required <code>page</code> <code>int</code> <p>Page number for pagination. Defaults to 1.</p> <code>1</code> <code>page_size</code> <code>int</code> <p>Number of results per page. Defaults to 50.</p> <code>50</code> <p>Returns:</p> Name Type Description <code>dict</code> <p>JSON response containing paginated search results with matching entities and their similarity scores.</p> <p>Raises:</p> Type Description <code>HTTPError</code> <p>If the API request fails or parameters are invalid.</p> Source code in <code>gigaspatial/handlers/unicef_georepo.py</code> <pre><code>def search_entities_by_name(self, view_uuid, name, page=1, page_size=50):\n    \"\"\"\n    Search for entities by name using fuzzy matching.\n\n    This performs a similarity-based search to find entities whose names\n    match or are similar to the provided search term.\n\n    Args:\n        view_uuid (str): The UUID of the view to search within.\n        name (str): The name or partial name to search for.\n        page (int, optional): Page number for pagination. Defaults to 1.\n        page_size (int, optional): Number of results per page. Defaults to 50.\n\n    Returns:\n        dict: JSON response containing paginated search results with\n            matching entities and their similarity scores.\n\n    Raises:\n        requests.HTTPError: If the API request fails or parameters are invalid.\n    \"\"\"\n    endpoint = f\"{self.base_url}/search/view/{view_uuid}/entity/{name}/\"\n    params = {\"page\": page, \"page_size\": page_size}\n    response = self._make_request(\"GET\", endpoint, params=params)\n    return response.json()\n</code></pre>"},{"location":"api/handlers/#gigaspatial.handlers.unicef_georepo.find_admin_boundaries_module","title":"<code>find_admin_boundaries_module()</code>","text":"<p>Find and return the UUID of the Admin Boundaries module.</p> <p>This is a convenience function that searches through all available modules to locate the one named \"Admin Boundaries\", which typically contains administrative boundary datasets.</p> <p>Returns:</p> Name Type Description <code>str</code> <p>The UUID of the Admin Boundaries module.</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If the Admin Boundaries module is not found.</p> Source code in <code>gigaspatial/handlers/unicef_georepo.py</code> <pre><code>def find_admin_boundaries_module():\n    \"\"\"\n    Find and return the UUID of the Admin Boundaries module.\n\n    This is a convenience function that searches through all available modules\n    to locate the one named \"Admin Boundaries\", which typically contains\n    administrative boundary datasets.\n\n    Returns:\n        str: The UUID of the Admin Boundaries module.\n\n    Raises:\n        ValueError: If the Admin Boundaries module is not found.\n    \"\"\"\n    client = GeoRepoClient()\n    modules = client.list_modules()\n\n    for module in modules.get(\"results\", []):\n        if module[\"name\"] == \"Admin Boundaries\":\n            return module[\"uuid\"]\n\n    raise ValueError(\"Admin Boundaries module not found\")\n</code></pre>"},{"location":"api/handlers/#gigaspatial.handlers.unicef_georepo.get_country_boundaries_by_iso3","title":"<code>get_country_boundaries_by_iso3(iso3_code, client=None, admin_level=None)</code>","text":"<p>Get administrative boundaries for a specific country using its ISO3 code.</p> <p>This function provides a high-level interface to retrieve country boundaries by automatically finding the appropriate module, dataset, and view, then fetching the requested administrative boundaries.</p> <p>The function will: 1. Find the Admin Boundaries module 2. Locate a global dataset within that module 3. Find the latest view of that dataset 4. Search for the country using the ISO3 code 5. Look for a country-specific view if available 6. Retrieve boundaries at the specified admin level or all levels</p> <p>Parameters:</p> Name Type Description Default <code>iso3_code</code> <code>str</code> <p>The ISO3 country code (e.g., 'USA', 'KEN', 'BRA').</p> required <code>admin_level</code> <code>int</code> <p>The administrative level to retrieve: - 0: Country level - 1: State/Province/Region level - 2: District/County level - 3: Sub-district/Municipality level - etc. If None, retrieves all available administrative levels.</p> <code>None</code> <p>Returns:</p> Name Type Description <code>dict</code> <p>A GeoJSON FeatureCollection containing the requested boundaries. Each feature includes geometry and properties for the administrative unit.</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If the Admin Boundaries module, datasets, views, or country cannot be found.</p> <code>HTTPError</code> <p>If any API requests fail.</p> Note <p>This function may make multiple API calls and can take some time for countries with many administrative units. It handles pagination automatically and attempts to use country-specific views when available for better performance.</p> Example Source code in <code>gigaspatial/handlers/unicef_georepo.py</code> <pre><code>def get_country_boundaries_by_iso3(\n    iso3_code, client: GeoRepoClient = None, admin_level=None\n):\n    \"\"\"\n    Get administrative boundaries for a specific country using its ISO3 code.\n\n    This function provides a high-level interface to retrieve country boundaries\n    by automatically finding the appropriate module, dataset, and view, then\n    fetching the requested administrative boundaries.\n\n    The function will:\n    1. Find the Admin Boundaries module\n    2. Locate a global dataset within that module\n    3. Find the latest view of that dataset\n    4. Search for the country using the ISO3 code\n    5. Look for a country-specific view if available\n    6. Retrieve boundaries at the specified admin level or all levels\n\n    Args:\n        iso3_code (str): The ISO3 country code (e.g., 'USA', 'KEN', 'BRA').\n        admin_level (int, optional): The administrative level to retrieve:\n            - 0: Country level\n            - 1: State/Province/Region level\n            - 2: District/County level\n            - 3: Sub-district/Municipality level\n            - etc.\n            If None, retrieves all available administrative levels.\n\n    Returns:\n        dict: A GeoJSON FeatureCollection containing the requested boundaries.\n            Each feature includes geometry and properties for the administrative unit.\n\n    Raises:\n        ValueError: If the Admin Boundaries module, datasets, views, or country\n            cannot be found.\n        requests.HTTPError: If any API requests fail.\n\n    Note:\n        This function may make multiple API calls and can take some time for\n        countries with many administrative units. It handles pagination\n        automatically and attempts to use country-specific views when available\n        for better performance.\n\n    Example:\n        &gt;&gt;&gt; # Get all administrative levels for Kenya\n        &gt;&gt;&gt; boundaries = get_country_boundaries_by_iso3('KEN')\n        &gt;&gt;&gt;\n        &gt;&gt;&gt; # Get only province-level boundaries for Kenya\n        &gt;&gt;&gt; provinces = get_country_boundaries_by_iso3('KEN', admin_level=1)\n    \"\"\"\n    client = client or GeoRepoClient()\n\n    client.logger.info(\"Finding Admin Boundaries module...\")\n    modules = client.list_modules()\n    admin_module_uuid = None\n\n    for module in modules.get(\"results\", []):\n        if \"Admin Boundaries\" in module[\"name\"]:\n            admin_module_uuid = module[\"uuid\"]\n            client.logger.info(\n                f\"Found Admin Boundaries module: {module['name']} ({admin_module_uuid})\"\n            )\n            break\n\n    if not admin_module_uuid:\n        raise ValueError(\"Admin Boundaries module not found\")\n\n    client.logger.info(f\"Finding datasets in the Admin Boundaries module...\")\n    datasets = client.list_datasets_by_module(admin_module_uuid)\n    global_dataset_uuid = None\n\n    for dataset in datasets.get(\"results\", []):\n        if any(keyword in dataset[\"name\"].lower() for keyword in [\"global\"]):\n            global_dataset_uuid = dataset[\"uuid\"]\n            client.logger.info(\n                f\"Found global dataset: {dataset['name']} ({global_dataset_uuid})\"\n            )\n            break\n\n    if not global_dataset_uuid:\n        if datasets.get(\"results\"):\n            global_dataset_uuid = datasets[\"results\"][0][\"uuid\"]\n            client.logger.info(\n                f\"Using first available dataset: {datasets['results'][0]['name']} ({global_dataset_uuid})\"\n            )\n        else:\n            raise ValueError(\"No datasets found in the Admin Boundaries module\")\n\n    client.logger.info(f\"Finding views in the dataset...\")\n    views = client.list_views_by_dataset(global_dataset_uuid)\n    latest_view_uuid = None\n\n    for view in views.get(\"results\", []):\n        if \"tags\" in view and \"latest\" in view[\"tags\"]:\n            latest_view_uuid = view[\"uuid\"]\n            client.logger.info(\n                f\"Found latest view: {view['name']} ({latest_view_uuid})\"\n            )\n            break\n\n    if not latest_view_uuid:\n        if views.get(\"results\"):\n            latest_view_uuid = views[\"results\"][0][\"uuid\"]\n            client.logger.info(\n                f\"Using first available view: {views['results'][0]['name']} ({latest_view_uuid})\"\n            )\n        else:\n            raise ValueError(\"No views found in the dataset\")\n\n    # Search for the country by ISO3 code\n    client.logger.info(f\"Searching for country with ISO3 code: {iso3_code}...\")\n    country_entity = client.find_country_by_iso3(latest_view_uuid, iso3_code)\n\n    if not country_entity:\n        raise ValueError(f\"Country with ISO3 code '{iso3_code}' not found\")\n\n    country_ucode = country_entity[\"ucode\"]\n    country_name = country_entity[\"name\"]\n    client.logger.info(f\"Found country: {country_name} (Ucode: {country_ucode})\")\n\n    # Search for country-specific view\n    client.logger.info(f\"Checking for country-specific view...\")\n    country_view_uuid = None\n    all_views = []\n\n    # Need to fetch all pages of views\n    page = 1\n    while True:\n        views_page = client.list_views_by_dataset(global_dataset_uuid, page=page)\n        all_views.extend(views_page.get(\"results\", []))\n        if page &gt;= views_page.get(\"total_page\", 1):\n            break\n        page += 1\n\n    # Look for a view specifically for this country\n    for view in all_views:\n        if country_name.lower() in view[\"name\"].lower() and \"latest\" in view.get(\n            \"tags\", []\n        ):\n            country_view_uuid = view[\"uuid\"]\n            client.logger.info(\n                f\"Found country-specific view: {view['name']} ({country_view_uuid})\"\n            )\n            break\n\n    # Get boundaries based on admin level\n    if country_view_uuid:\n        client.logger.info(country_view_uuid)\n        # If we found a view specific to this country, use it\n        client.logger.info(f\"Getting admin boundaries from country-specific view...\")\n        if admin_level is not None:\n            client.logger.info(f\"Fetching admin level {admin_level} boundaries...\")\n\n            # Handle pagination for large datasets\n            all_features = []\n            page = 1\n            while True:\n                result, meta = client.list_entities_by_admin_level(\n                    country_view_uuid,\n                    admin_level,\n                    geom=\"full_geom\",\n                    format=\"geojson\",\n                    page=page,\n                    page_size=50,\n                )\n\n                # Add features to our collection\n                if \"features\" in result:\n                    all_features.extend(result[\"features\"])\n                elif \"results\" in result:\n                    # Convert entities to GeoJSON features if needed\n                    for entity in result[\"results\"]:\n                        if \"geometry\" in entity:\n                            feature = {\n                                \"type\": \"Feature\",\n                                \"properties\": {\n                                    k: v for k, v in entity.items() if k != \"geometry\"\n                                },\n                                \"geometry\": entity[\"geometry\"],\n                            }\n                            all_features.append(feature)\n\n                # Check if there are more pages\n                if page &gt;= meta[\"total_page\"]:\n                    break\n\n                page += 1\n\n            boundaries = {\"type\": \"FeatureCollection\", \"features\": all_features}\n        else:\n            # Get all admin levels by fetching each level separately\n            boundaries = {\"type\": \"FeatureCollection\", \"features\": []}\n\n            # Get dataset details to find available admin levels\n            dataset_details = client.get_dataset_details(global_dataset_uuid)\n            max_level = 0\n\n            for level_info in dataset_details.get(\"dataset_levels\", []):\n                if isinstance(level_info.get(\"level\"), int):\n                    max_level = max(max_level, level_info[\"level\"])\n\n            client.logger.info(f\"Dataset has admin levels from 0 to {max_level}\")\n\n            # Fetch each admin level\n            for level in range(max_level + 1):\n                client.logger.info(f\"Fetching admin level {level}...\")\n                try:\n                    level_data, meta = client.list_entities_by_admin_level(\n                        country_view_uuid, level, geom=\"full_geom\", format=\"geojson\"\n                    )\n\n                    if \"features\" in level_data:\n                        boundaries[\"features\"].extend(level_data[\"features\"])\n                    elif \"results\" in level_data:\n                        # Process each page of results\n                        page = 1\n                        while True:\n                            result, meta = client.list_entities_by_admin_level(\n                                country_view_uuid,\n                                level,\n                                geom=\"full_geom\",\n                                format=\"geojson\",\n                                page=page,\n                            )\n\n                            if \"features\" in result:\n                                boundaries[\"features\"].extend(result[\"features\"])\n\n                            # Check for more pages\n                            if page &gt;= meta[\"total_page\"]:\n                                break\n\n                            page += 1\n\n                except Exception as e:\n                    client.logger.warning(f\"Error fetching admin level {level}: {e}\")\n    else:\n        # Use the global view with filtering\n        client.logger.info(f\"Using global view and filtering by country...\")\n\n        # Function to recursively get all descendants\n        def get_all_children(\n            parent_ucode, view_uuid, level=1, max_depth=5, admin_level_filter=None\n        ):\n            \"\"\"\n            Recursively retrieve all child entities of a parent entity.\n\n            Args:\n                parent_ucode (str): The Ucode of the parent entity.\n                view_uuid (str): The UUID of the view to query.\n                level (int): Current recursion level (for depth limiting).\n                max_depth (int): Maximum recursion depth to prevent infinite loops.\n                admin_level_filter (int, optional): If specified, only return\n                    entities at this specific administrative level.\n\n            Returns:\n                list: List of GeoJSON features for all child entities.\n            \"\"\"\n            if level &gt; max_depth:\n                return []\n\n            try:\n                children = client.list_entity_children(view_uuid, parent_ucode)\n                features = []\n\n                for child in children.get(\"results\", []):\n                    # Skip if we're filtering by admin level and this doesn't match\n                    if (\n                        admin_level_filter is not None\n                        and child.get(\"admin_level\") != admin_level_filter\n                    ):\n                        continue\n\n                    # Get the child with full geometry\n                    child_entity = client.get_entity_by_ucode(child[\"ucode\"])\n                    if \"features\" in child_entity:\n                        features.extend(child_entity[\"features\"])\n\n                    # Recursively get grandchildren if not filtering by admin level\n                    if admin_level_filter is None:\n                        features.extend(\n                            get_all_children(\n                                child[\"ucode\"], view_uuid, level + 1, max_depth\n                            )\n                        )\n\n                return features\n            except Exception as e:\n                client.logger.warning(f\"Error getting children for {parent_ucode}: {e}\")\n                return []\n\n        # Start with the country boundaries\n        boundaries = {\"type\": \"FeatureCollection\", \"features\": []}\n\n        # If admin_level is 0, just get the country entity\n        if admin_level == 0:\n            country_entity = client.get_entity_by_ucode(country_ucode)\n            if \"features\" in country_entity:\n                boundaries[\"features\"].extend(country_entity[\"features\"])\n        # If specific admin level requested, get all entities at that level\n        elif admin_level is not None:\n            children_features = get_all_children(\n                country_ucode,\n                latest_view_uuid,\n                max_depth=admin_level + 1,\n                admin_level_filter=admin_level,\n            )\n            boundaries[\"features\"].extend(children_features)\n        # If no admin_level specified, get all levels\n        else:\n            # Start with the country entity\n            country_entity = client.get_entity_by_ucode(country_ucode)\n            if \"features\" in country_entity:\n                boundaries[\"features\"].extend(country_entity[\"features\"])\n\n            # Get all descendants\n            children_features = get_all_children(\n                country_ucode, latest_view_uuid, max_depth=5\n            )\n            boundaries[\"features\"].extend(children_features)\n\n    return boundaries\n</code></pre>"},{"location":"api/handlers/#gigaspatial.handlers.unicef_georepo.get_country_boundaries_by_iso3--get-all-administrative-levels-for-kenya","title":"Get all administrative levels for Kenya","text":"<p>boundaries = get_country_boundaries_by_iso3('KEN')</p>"},{"location":"api/handlers/#gigaspatial.handlers.unicef_georepo.get_country_boundaries_by_iso3--get-only-province-level-boundaries-for-kenya","title":"Get only province-level boundaries for Kenya","text":"<p>provinces = get_country_boundaries_by_iso3('KEN', admin_level=1)</p>"},{"location":"api/handlers/#gigaspatial.handlers.worldpop","title":"<code>worldpop</code>","text":""},{"location":"api/handlers/#gigaspatial.handlers.worldpop.WorldPopConfig","title":"<code>WorldPopConfig</code>","text":"<p>               Bases: <code>BaseModel</code></p> Source code in <code>gigaspatial/handlers/worldpop.py</code> <pre><code>class WorldPopConfig(BaseModel):\n    # class variables\n    _metadata_cache: ClassVar[Optional[pd.DataFrame]] = None\n\n    # constants\n    CURRENT_MAX_YEAR: int = 2022\n    EARLIEST_YEAR: int = 2000\n    SCHOOL_AGE_YEAR: int = 2020\n\n    # base config\n    WORLDPOP_DB_BASE_URL: HttpUrl = Field(default=\"https://data.worldpop.org/\")\n    SCHOOL_AGE_POPULATION_PATH: str = Field(\n        default=\"GIS/AgeSex_structures/school_age_population/v1/2020/\"\n    )\n    PPP_2021_2022_PATH: str = Field(\n        default=\"GIS/Population/Global_2021_2022_1km_UNadj/\"\n    )\n    DATASETS_METADATA_PATH: str = Field(default=\"assets/wpgpDatasets.csv\")\n\n    # user config\n    base_path: Path = Field(default=global_config.get_path(\"worldpop\", \"bronze\"))\n    country: str = Field(...)\n    year: int = Field(..., ge=EARLIEST_YEAR, le=CURRENT_MAX_YEAR)\n    resolution: Literal[\"HIGH\", \"LOW\"] = Field(\n        default=\"LOW\",\n        description=\"Spatial resolution of the population grid: HIGH (100m) or LOW (1km)\",\n    )\n    un_adjusted: bool = True\n    constrained: bool = False\n    school_age: Optional[Literal[\"PRIMARY\", \"SECONDARY\"]] = None\n    gender: Literal[\"F\", \"M\", \"F_M\"] = \"F_M\"\n\n    @field_validator(\"country\")\n    def validate_country(cls, value: str) -&gt; str:\n        try:\n            return pycountry.countries.lookup(value).alpha_3\n        except LookupError:\n            raise ValueError(f\"Invalid country code provided: {value}\")\n\n    @model_validator(mode=\"after\")\n    def validate_configuration(self):\n        \"\"\"\n        Validate that the configuration is valid based on dataset availability constraints.\n\n        Specific rules:\n        - Post-2020 data is only available at 1km resolution with UN adjustment\n        - School age population data is only available for 2020 at 1km resolution\n        \"\"\"\n        if self.year &gt; self.SCHOOL_AGE_YEAR:\n            if self.resolution != \"LOW\":\n                raise ValueError(\n                    f\"Data for year {self.year} is only available at LOW (1km) resolution\"\n                )\n\n            if not self.un_adjusted:\n                raise ValueError(\n                    f\"Data for year {self.year} is only available with UN adjustment\"\n                )\n\n        if self.school_age:\n            if self.resolution != \"LOW\":\n                raise ValueError(\n                    f\"School age data is only available at LOW (1km) resolution\"\n                )\n\n            if self.year != self.SCHOOL_AGE_YEAR:\n                self.year = self.SCHOOL_AGE_YEAR\n                raise ValueError(f\"School age data is only available for 2020\")\n\n        return self\n\n    @property\n    def dataset_url(self) -&gt; str:\n        \"\"\"Get the URL for the configured dataset. The URL is computed on first access and then cached for subsequent calls.\"\"\"\n        if not hasattr(self, \"_dataset_url\"):\n            self._dataset_url = self._compute_dataset_url()\n        return self._dataset_url\n\n    @property\n    def dataset_path(self) -&gt; Path:\n        \"\"\"Construct and return the path for the configured dataset.\"\"\"\n        url_parts = self.dataset_url.split(\"/\")\n        file_path = (\n            \"/\".join(\n                [url_parts[4], url_parts[5], url_parts[7], self.country, url_parts[-1]]\n            )\n            if self.school_age\n            else \"/\".join([url_parts[4], url_parts[6], self.country, url_parts[-1]])\n        )\n        return self.base_path / file_path\n\n    def _load_datasets_metadata(self) -&gt; pd.DataFrame:\n        \"\"\"Load and return the WorldPop datasets metadata, using cache if available.\"\"\"\n        if WorldPopConfig._metadata_cache is not None:\n            return WorldPopConfig._metadata_cache\n\n        try:\n            WorldPopConfig._metadata_cache = pd.read_csv(\n                str(self.WORLDPOP_DB_BASE_URL) + self.DATASETS_METADATA_PATH\n            )\n            return WorldPopConfig._metadata_cache\n        except (URLError, pd.errors.EmptyDataError) as e:\n            raise RuntimeError(f\"Failed to load WorldPop datasets metadata: {e}\")\n\n    def _compute_dataset_url(self) -&gt; str:\n        \"\"\"Construct and return the URL for the configured dataset.\"\"\"\n        # handle post-2020 datasets\n        if self.year &gt; self.SCHOOL_AGE_YEAR:\n            return (\n                str(self.WORLDPOP_DB_BASE_URL)\n                + self.PPP_2021_2022_PATH\n                + f\"{'' if self.constrained else 'un'}constrained/{self.year}/{self.country}/{self.country.lower()}_ppp_{self.year}_1km_UNadj{'_constrained' if self.constrained else ''}.tif\"\n            )\n\n        # handle school-age population datasets\n        if self.school_age:\n            return (\n                str(self.WORLDPOP_DB_BASE_URL)\n                + self.SCHOOL_AGE_POPULATION_PATH\n                + f\"{self.country}/{self.country}_SAP_1km_2020/{self.country}_{self.gender}_{self.school_age}_2020_1km.tif\"\n            )\n\n        # handle standard population datasets\n        wp_metadata = self._load_datasets_metadata()\n\n        try:\n            dataset_url = (\n                self.WORLDPOP_DB_BASE_URL\n                + wp_metadata[\n                    (wp_metadata.ISO3 == self.country)\n                    &amp; (\n                        wp_metadata.Covariate\n                        == \"ppp_\"\n                        + str(self.year)\n                        + (\"_UNadj\" if self.un_adjusted else \"\")\n                    )\n                ].PathToRaster.values[0]\n            )\n        except IndexError:\n            raise ValueError(\n                f\"No dataset found for country={self.country}, year={self.year}, un_adjusted={self.un_adjusted}\"\n            )\n\n        # handle resolution conversion if needed\n        if self.resolution == \"HIGH\":\n            return dataset_url\n\n        url_parts = dataset_url.split(\"/\")\n        url_parts[5] = (\n            url_parts[5] + \"_1km\" + (\"_UNadj\" if self.un_adjusted else \"\")\n        )  # get 1km folder with UNadj specification\n        url_parts[8] = url_parts[8].replace(\n            str(self.year), str(self.year) + \"_1km_Aggregated\"\n        )  # get filename with 1km res\n        dataset_url = \"/\".join(url_parts)\n\n        return dataset_url\n\n    def __repr__(self) -&gt; str:\n\n        parts = [\n            f\"WorldpopConfig(\",\n            f\"  country='{self.country}'\",\n            f\"  year={self.year}\",\n            f\"  resolution={self.resolution}\",\n            f\"  un_adjusted={self.un_adjusted}\",\n            f\"  constrained={self.constrained}\",\n        ]\n\n        if self.school_age:\n            parts.append(f\"  school_age='{self.school_age}'\")\n            parts.append(f\"  gender='{self.gender}'\")\n\n        parts.append(\")\")\n\n        return \"\\n\".join(parts)\n</code></pre>"},{"location":"api/handlers/#gigaspatial.handlers.worldpop.WorldPopConfig.dataset_path","title":"<code>dataset_path: Path</code>  <code>property</code>","text":"<p>Construct and return the path for the configured dataset.</p>"},{"location":"api/handlers/#gigaspatial.handlers.worldpop.WorldPopConfig.dataset_url","title":"<code>dataset_url: str</code>  <code>property</code>","text":"<p>Get the URL for the configured dataset. The URL is computed on first access and then cached for subsequent calls.</p>"},{"location":"api/handlers/#gigaspatial.handlers.worldpop.WorldPopConfig.validate_configuration","title":"<code>validate_configuration()</code>","text":"<p>Validate that the configuration is valid based on dataset availability constraints.</p> <p>Specific rules: - Post-2020 data is only available at 1km resolution with UN adjustment - School age population data is only available for 2020 at 1km resolution</p> Source code in <code>gigaspatial/handlers/worldpop.py</code> <pre><code>@model_validator(mode=\"after\")\ndef validate_configuration(self):\n    \"\"\"\n    Validate that the configuration is valid based on dataset availability constraints.\n\n    Specific rules:\n    - Post-2020 data is only available at 1km resolution with UN adjustment\n    - School age population data is only available for 2020 at 1km resolution\n    \"\"\"\n    if self.year &gt; self.SCHOOL_AGE_YEAR:\n        if self.resolution != \"LOW\":\n            raise ValueError(\n                f\"Data for year {self.year} is only available at LOW (1km) resolution\"\n            )\n\n        if not self.un_adjusted:\n            raise ValueError(\n                f\"Data for year {self.year} is only available with UN adjustment\"\n            )\n\n    if self.school_age:\n        if self.resolution != \"LOW\":\n            raise ValueError(\n                f\"School age data is only available at LOW (1km) resolution\"\n            )\n\n        if self.year != self.SCHOOL_AGE_YEAR:\n            self.year = self.SCHOOL_AGE_YEAR\n            raise ValueError(f\"School age data is only available for 2020\")\n\n    return self\n</code></pre>"},{"location":"api/handlers/#gigaspatial.handlers.worldpop.WorldPopDownloader","title":"<code>WorldPopDownloader</code>","text":"<p>A class to handle downloads of WorldPop datasets.</p> Source code in <code>gigaspatial/handlers/worldpop.py</code> <pre><code>class WorldPopDownloader:\n    \"\"\"A class to handle downloads of WorldPop datasets.\"\"\"\n\n    def __init__(\n        self,\n        config: Union[WorldPopConfig, dict[str, Union[str, int]]],\n        data_store: Optional[DataStore] = None,\n        logger: Optional[logging.Logger] = None,\n    ):\n        \"\"\"\n        Initialize the downloader.\n\n        Args:\n            config: Configuration for the WorldPop dataset, either as a WorldPopConfig object or a dictionary of parameters\n            data_store: Optional data storage interface. If not provided, uses LocalDataStore.\n            logger: Optional custom logger. If not provided, uses default logger.\n        \"\"\"\n        self.logger = logger or global_config.get_logger(self.__class__.__name__)\n        self.data_store = data_store or LocalDataStore()\n        self.config = (\n            config if isinstance(config, WorldPopConfig) else WorldPopConfig(**config)\n        )\n\n    @classmethod\n    def from_country_year(cls, country: str, year: int, **kwargs):\n        \"\"\"\n        Create a downloader instance from country and year.\n\n        Args:\n            country: Country code or name\n            year: Year of the dataset\n            **kwargs: Additional parameters for WorldPopConfig or the downloader\n        \"\"\"\n        return cls({\"country\": country, \"year\": year}, **kwargs)\n\n    def download_dataset(self) -&gt; str:\n        \"\"\"\n        Download the configured dataset to the provided output path.\n        \"\"\"\n\n        try:\n            response = requests.get(self.config.dataset_url, stream=True)\n            response.raise_for_status()\n\n            output_path = str(self.config.dataset_path)\n\n            total_size = int(response.headers.get(\"content-length\", 0))\n\n            with self.data_store.open(output_path, \"wb\") as file:\n                with tqdm(\n                    total=total_size,\n                    unit=\"B\",\n                    unit_scale=True,\n                    desc=f\"Downloading {os.path.basename(output_path)}\",\n                ) as pbar:\n                    for chunk in response.iter_content(chunk_size=8192):\n                        if chunk:\n                            file.write(chunk)\n                            pbar.update(len(chunk))\n\n            self.logger.debug(f\"Successfully downloaded dataset: {self.config}\")\n\n            return output_path\n\n        except requests.exceptions.RequestException as e:\n            self.logger.error(f\"Failed to download dataset {self.config}: {str(e)}\")\n            return None\n        except Exception as e:\n            self.logger.error(f\"Unexpected error downloading dataset: {str(e)}\")\n            return None\n</code></pre>"},{"location":"api/handlers/#gigaspatial.handlers.worldpop.WorldPopDownloader.__init__","title":"<code>__init__(config, data_store=None, logger=None)</code>","text":"<p>Initialize the downloader.</p> <p>Parameters:</p> Name Type Description Default <code>config</code> <code>Union[WorldPopConfig, dict[str, Union[str, int]]]</code> <p>Configuration for the WorldPop dataset, either as a WorldPopConfig object or a dictionary of parameters</p> required <code>data_store</code> <code>Optional[DataStore]</code> <p>Optional data storage interface. If not provided, uses LocalDataStore.</p> <code>None</code> <code>logger</code> <code>Optional[Logger]</code> <p>Optional custom logger. If not provided, uses default logger.</p> <code>None</code> Source code in <code>gigaspatial/handlers/worldpop.py</code> <pre><code>def __init__(\n    self,\n    config: Union[WorldPopConfig, dict[str, Union[str, int]]],\n    data_store: Optional[DataStore] = None,\n    logger: Optional[logging.Logger] = None,\n):\n    \"\"\"\n    Initialize the downloader.\n\n    Args:\n        config: Configuration for the WorldPop dataset, either as a WorldPopConfig object or a dictionary of parameters\n        data_store: Optional data storage interface. If not provided, uses LocalDataStore.\n        logger: Optional custom logger. If not provided, uses default logger.\n    \"\"\"\n    self.logger = logger or global_config.get_logger(self.__class__.__name__)\n    self.data_store = data_store or LocalDataStore()\n    self.config = (\n        config if isinstance(config, WorldPopConfig) else WorldPopConfig(**config)\n    )\n</code></pre>"},{"location":"api/handlers/#gigaspatial.handlers.worldpop.WorldPopDownloader.download_dataset","title":"<code>download_dataset()</code>","text":"<p>Download the configured dataset to the provided output path.</p> Source code in <code>gigaspatial/handlers/worldpop.py</code> <pre><code>def download_dataset(self) -&gt; str:\n    \"\"\"\n    Download the configured dataset to the provided output path.\n    \"\"\"\n\n    try:\n        response = requests.get(self.config.dataset_url, stream=True)\n        response.raise_for_status()\n\n        output_path = str(self.config.dataset_path)\n\n        total_size = int(response.headers.get(\"content-length\", 0))\n\n        with self.data_store.open(output_path, \"wb\") as file:\n            with tqdm(\n                total=total_size,\n                unit=\"B\",\n                unit_scale=True,\n                desc=f\"Downloading {os.path.basename(output_path)}\",\n            ) as pbar:\n                for chunk in response.iter_content(chunk_size=8192):\n                    if chunk:\n                        file.write(chunk)\n                        pbar.update(len(chunk))\n\n        self.logger.debug(f\"Successfully downloaded dataset: {self.config}\")\n\n        return output_path\n\n    except requests.exceptions.RequestException as e:\n        self.logger.error(f\"Failed to download dataset {self.config}: {str(e)}\")\n        return None\n    except Exception as e:\n        self.logger.error(f\"Unexpected error downloading dataset: {str(e)}\")\n        return None\n</code></pre>"},{"location":"api/handlers/#gigaspatial.handlers.worldpop.WorldPopDownloader.from_country_year","title":"<code>from_country_year(country, year, **kwargs)</code>  <code>classmethod</code>","text":"<p>Create a downloader instance from country and year.</p> <p>Parameters:</p> Name Type Description Default <code>country</code> <code>str</code> <p>Country code or name</p> required <code>year</code> <code>int</code> <p>Year of the dataset</p> required <code>**kwargs</code> <p>Additional parameters for WorldPopConfig or the downloader</p> <code>{}</code> Source code in <code>gigaspatial/handlers/worldpop.py</code> <pre><code>@classmethod\ndef from_country_year(cls, country: str, year: int, **kwargs):\n    \"\"\"\n    Create a downloader instance from country and year.\n\n    Args:\n        country: Country code or name\n        year: Year of the dataset\n        **kwargs: Additional parameters for WorldPopConfig or the downloader\n    \"\"\"\n    return cls({\"country\": country, \"year\": year}, **kwargs)\n</code></pre>"},{"location":"api/handlers/#gigaspatial.handlers.worldpop.read_dataset","title":"<code>read_dataset(data_store, path, compression=None, **kwargs)</code>","text":"<p>Read data from various file formats stored in both local and cloud-based storage.</p>"},{"location":"api/handlers/#gigaspatial.handlers.worldpop.read_dataset--parameters","title":"Parameters:","text":"<p>data_store : DataStore     Instance of DataStore for accessing data storage. path : str, Path     Path to the file in data storage. **kwargs : dict     Additional arguments passed to the specific reader function.</p>"},{"location":"api/handlers/#gigaspatial.handlers.worldpop.read_dataset--returns","title":"Returns:","text":"<p>pandas.DataFrame or geopandas.GeoDataFrame     The data read from the file.</p>"},{"location":"api/handlers/#gigaspatial.handlers.worldpop.read_dataset--raises","title":"Raises:","text":"<p>FileNotFoundError     If the file doesn't exist in blob storage. ValueError     If the file type is unsupported or if there's an error reading the file.</p> Source code in <code>gigaspatial/core/io/readers.py</code> <pre><code>def read_dataset(data_store: DataStore, path: str, compression: str = None, **kwargs):\n    \"\"\"\n    Read data from various file formats stored in both local and cloud-based storage.\n\n    Parameters:\n    ----------\n    data_store : DataStore\n        Instance of DataStore for accessing data storage.\n    path : str, Path\n        Path to the file in data storage.\n    **kwargs : dict\n        Additional arguments passed to the specific reader function.\n\n    Returns:\n    -------\n    pandas.DataFrame or geopandas.GeoDataFrame\n        The data read from the file.\n\n    Raises:\n    ------\n    FileNotFoundError\n        If the file doesn't exist in blob storage.\n    ValueError\n        If the file type is unsupported or if there's an error reading the file.\n    \"\"\"\n\n    # Define supported file formats and their readers\n    BINARY_FORMATS = {\n        \".shp\",\n        \".zip\",\n        \".parquet\",\n        \".gpkg\",\n        \".xlsx\",\n        \".xls\",\n        \".kmz\",\n        \".gz\",\n    }\n\n    PANDAS_READERS = {\n        \".csv\": pd.read_csv,\n        \".xlsx\": lambda f, **kw: pd.read_excel(f, engine=\"openpyxl\", **kw),\n        \".xls\": lambda f, **kw: pd.read_excel(f, engine=\"xlrd\", **kw),\n        \".json\": pd.read_json,\n        # \".gz\": lambda f, **kw: pd.read_csv(f, compression=\"gzip\", **kw),\n    }\n\n    GEO_READERS = {\n        \".shp\": gpd.read_file,\n        \".zip\": gpd.read_file,\n        \".geojson\": gpd.read_file,\n        \".gpkg\": gpd.read_file,\n        \".parquet\": gpd.read_parquet,\n        \".kmz\": read_kmz,\n    }\n\n    COMPRESSION_FORMATS = {\n        \".gz\": \"gzip\",\n        \".bz2\": \"bz2\",\n        \".zip\": \"zip\",\n        \".xz\": \"xz\",\n    }\n\n    try:\n        # Check if file exists\n        if not data_store.file_exists(path):\n            raise FileNotFoundError(f\"File '{path}' not found in blob storage\")\n\n        path_obj = Path(path)\n        suffixes = path_obj.suffixes\n        file_extension = suffixes[-1].lower() if suffixes else \"\"\n\n        if compression is None and file_extension in COMPRESSION_FORMATS:\n            compression_format = COMPRESSION_FORMATS[file_extension]\n\n            # if file has multiple extensions (e.g., .csv.gz), get the inner format\n            if len(suffixes) &gt; 1:\n                inner_extension = suffixes[-2].lower()\n\n                if inner_extension == \".tar\":\n                    raise ValueError(\n                        \"Tar archives (.tar.gz) are not directly supported\"\n                    )\n\n                if inner_extension in PANDAS_READERS:\n                    try:\n                        with data_store.open(path, \"rb\") as f:\n                            return PANDAS_READERS[inner_extension](\n                                f, compression=compression_format, **kwargs\n                            )\n                    except Exception as e:\n                        raise ValueError(f\"Error reading compressed file: {str(e)}\")\n                elif inner_extension in GEO_READERS:\n                    try:\n                        with data_store.open(path, \"rb\") as f:\n                            if compression_format == \"gzip\":\n                                import gzip\n\n                                decompressed_data = gzip.decompress(f.read())\n                                import io\n\n                                return GEO_READERS[inner_extension](\n                                    io.BytesIO(decompressed_data), **kwargs\n                                )\n                            else:\n                                raise ValueError(\n                                    f\"Compression format {compression_format} not supported for geo data\"\n                                )\n                    except Exception as e:\n                        raise ValueError(f\"Error reading compressed geo file: {str(e)}\")\n            else:\n                # if just .gz without clear inner type, assume csv\n                try:\n                    with data_store.open(path, \"rb\") as f:\n                        return pd.read_csv(f, compression=compression_format, **kwargs)\n                except Exception as e:\n                    raise ValueError(\n                        f\"Error reading compressed file as CSV: {str(e)}. \"\n                        f\"If not a CSV, specify the format in the filename (e.g., .json.gz)\"\n                    )\n\n        # Special handling for compressed files\n        if file_extension == \".zip\":\n            # For zip files, we need to use binary mode\n            with data_store.open(path, \"rb\") as f:\n                return gpd.read_file(f)\n\n        # Determine if we need binary mode based on file type\n        mode = \"rb\" if file_extension in BINARY_FORMATS else \"r\"\n\n        # Try reading with appropriate reader\n        if file_extension in PANDAS_READERS:\n            try:\n                with data_store.open(path, mode) as f:\n                    return PANDAS_READERS[file_extension](f, **kwargs)\n            except Exception as e:\n                raise ValueError(f\"Error reading file with pandas: {str(e)}\")\n\n        if file_extension in GEO_READERS:\n            try:\n                with data_store.open(path, \"rb\") as f:\n                    return GEO_READERS[file_extension](f, **kwargs)\n            except Exception as e:\n                # For parquet files, try pandas reader if geopandas fails\n                if file_extension == \".parquet\":\n                    try:\n                        with data_store.open(path, \"rb\") as f:\n                            return pd.read_parquet(f, **kwargs)\n                    except Exception as e2:\n                        raise ValueError(\n                            f\"Failed to read parquet with both geopandas ({str(e)}) \"\n                            f\"and pandas ({str(e2)})\"\n                        )\n                raise ValueError(f\"Error reading file with geopandas: {str(e)}\")\n\n        # If we get here, the file type is unsupported\n        supported_formats = sorted(set(PANDAS_READERS.keys()) | set(GEO_READERS.keys()))\n        supported_compressions = sorted(COMPRESSION_FORMATS.keys())\n        raise ValueError(\n            f\"Unsupported file type: {file_extension}\\n\"\n            f\"Supported formats: {', '.join(supported_formats)}\"\n            f\"Supported compressions: {', '.join(supported_compressions)}\"\n        )\n\n    except Exception as e:\n        if isinstance(e, (FileNotFoundError, ValueError)):\n            raise\n        raise RuntimeError(f\"Unexpected error reading dataset: {str(e)}\")\n</code></pre>"},{"location":"api/handlers/#gigaspatial.handlers.worldpop.read_datasets","title":"<code>read_datasets(data_store, paths, **kwargs)</code>","text":"<p>Read multiple datasets from data storage at once.</p>"},{"location":"api/handlers/#gigaspatial.handlers.worldpop.read_datasets--parameters","title":"Parameters:","text":"<p>data_store : DataStore     Instance of DataStore for accessing data storage. paths : list of str     Paths to files in data storage. **kwargs : dict     Additional arguments passed to read_dataset.</p>"},{"location":"api/handlers/#gigaspatial.handlers.worldpop.read_datasets--returns","title":"Returns:","text":"<p>dict     Dictionary mapping paths to their corresponding DataFrames/GeoDataFrames.</p> Source code in <code>gigaspatial/core/io/readers.py</code> <pre><code>def read_datasets(data_store: DataStore, paths, **kwargs):\n    \"\"\"\n    Read multiple datasets from data storage at once.\n\n    Parameters:\n    ----------\n    data_store : DataStore\n        Instance of DataStore for accessing data storage.\n    paths : list of str\n        Paths to files in data storage.\n    **kwargs : dict\n        Additional arguments passed to read_dataset.\n\n    Returns:\n    -------\n    dict\n        Dictionary mapping paths to their corresponding DataFrames/GeoDataFrames.\n    \"\"\"\n    results = {}\n    errors = {}\n\n    for path in paths:\n        try:\n            results[path] = read_dataset(data_store, path, **kwargs)\n        except Exception as e:\n            errors[path] = str(e)\n\n    if errors:\n        error_msg = \"\\n\".join(f\"- {path}: {error}\" for path, error in errors.items())\n        raise ValueError(f\"Errors reading datasets:\\n{error_msg}\")\n\n    return results\n</code></pre>"},{"location":"api/handlers/#gigaspatial.handlers.worldpop.read_gzipped_json_or_csv","title":"<code>read_gzipped_json_or_csv(file_path, data_store)</code>","text":"<p>Reads a gzipped file, attempting to parse it as JSON (lines=True) or CSV.</p> Source code in <code>gigaspatial/core/io/readers.py</code> <pre><code>def read_gzipped_json_or_csv(file_path, data_store):\n    \"\"\"Reads a gzipped file, attempting to parse it as JSON (lines=True) or CSV.\"\"\"\n\n    with data_store.open(file_path, \"rb\") as f:\n        g = gzip.GzipFile(fileobj=f)\n        text = g.read().decode(\"utf-8\")\n        try:\n            df = pd.read_json(io.StringIO(text), lines=True)\n            return df\n        except json.JSONDecodeError:\n            try:\n                df = pd.read_csv(io.StringIO(text))\n                return df\n            except pd.errors.ParserError:\n                print(f\"Error: Could not parse {file_path} as JSON or CSV.\")\n                return None\n</code></pre>"},{"location":"api/handlers/#gigaspatial.handlers.worldpop.read_kmz","title":"<code>read_kmz(file_obj, **kwargs)</code>","text":"<p>Helper function to read KMZ files and return a GeoDataFrame.</p> Source code in <code>gigaspatial/core/io/readers.py</code> <pre><code>def read_kmz(file_obj, **kwargs):\n    \"\"\"Helper function to read KMZ files and return a GeoDataFrame.\"\"\"\n    try:\n        with zipfile.ZipFile(file_obj) as kmz:\n            # Find the KML file in the archive (usually doc.kml)\n            kml_filename = next(\n                name for name in kmz.namelist() if name.endswith(\".kml\")\n            )\n\n            # Read the KML content\n            kml_content = io.BytesIO(kmz.read(kml_filename))\n\n            gdf = gpd.read_file(kml_content)\n\n            # Validate the GeoDataFrame\n            if gdf.empty:\n                raise ValueError(\n                    \"The KML file is empty or does not contain valid geospatial data.\"\n                )\n\n        return gdf\n\n    except zipfile.BadZipFile:\n        raise ValueError(\"The provided file is not a valid KMZ file.\")\n    except StopIteration:\n        raise ValueError(\"No KML file found in the KMZ archive.\")\n    except Exception as e:\n        raise RuntimeError(f\"An error occurred: {e}\")\n</code></pre>"},{"location":"api/handlers/#gigaspatial.handlers.worldpop.write_dataset","title":"<code>write_dataset(data, data_store, path, **kwargs)</code>","text":"<p>Write DataFrame or GeoDataFrame to various file formats in Azure Blob Storage.</p>"},{"location":"api/handlers/#gigaspatial.handlers.worldpop.write_dataset--parameters","title":"Parameters:","text":"<p>data : pandas.DataFrame or geopandas.GeoDataFrame     The data to write to blob storage. data_store : DataStore     Instance of DataStore for accessing data storage. path : str     Path where the file will be written in data storage. **kwargs : dict     Additional arguments passed to the specific writer function.</p>"},{"location":"api/handlers/#gigaspatial.handlers.worldpop.write_dataset--raises","title":"Raises:","text":"<p>ValueError     If the file type is unsupported or if there's an error writing the file. TypeError     If input data is not a DataFrame or GeoDataFrame.</p> Source code in <code>gigaspatial/core/io/writers.py</code> <pre><code>def write_dataset(data, data_store: DataStore, path, **kwargs):\n    \"\"\"\n    Write DataFrame or GeoDataFrame to various file formats in Azure Blob Storage.\n\n    Parameters:\n    ----------\n    data : pandas.DataFrame or geopandas.GeoDataFrame\n        The data to write to blob storage.\n    data_store : DataStore\n        Instance of DataStore for accessing data storage.\n    path : str\n        Path where the file will be written in data storage.\n    **kwargs : dict\n        Additional arguments passed to the specific writer function.\n\n    Raises:\n    ------\n    ValueError\n        If the file type is unsupported or if there's an error writing the file.\n    TypeError\n        If input data is not a DataFrame or GeoDataFrame.\n    \"\"\"\n\n    # Define supported file formats and their writers\n    BINARY_FORMATS = {\".shp\", \".zip\", \".parquet\", \".gpkg\", \".xlsx\", \".xls\"}\n\n    PANDAS_WRITERS = {\n        \".csv\": lambda df, buf, **kw: df.to_csv(buf, **kw),\n        \".xlsx\": lambda df, buf, **kw: df.to_excel(buf, engine=\"openpyxl\", **kw),\n        \".json\": lambda df, buf, **kw: df.to_json(buf, **kw),\n        \".parquet\": lambda df, buf, **kw: df.to_parquet(buf, **kw),\n    }\n\n    GEO_WRITERS = {\n        \".geojson\": lambda gdf, buf, **kw: gdf.to_file(buf, driver=\"GeoJSON\", **kw),\n        \".gpkg\": lambda gdf, buf, **kw: gdf.to_file(buf, driver=\"GPKG\", **kw),\n        \".parquet\": lambda gdf, buf, **kw: gdf.to_parquet(buf, **kw),\n    }\n\n    try:\n        # Input validation\n        if not isinstance(data, (pd.DataFrame, gpd.GeoDataFrame)):\n            raise TypeError(\"Input data must be a pandas DataFrame or GeoDataFrame\")\n\n        # Get file suffix and ensure it's lowercase\n        suffix = Path(path).suffix.lower()\n\n        # Determine if we need binary mode based on file type\n        mode = \"wb\" if suffix in BINARY_FORMATS else \"w\"\n\n        # Handle different data types and formats\n        if isinstance(data, gpd.GeoDataFrame):\n            if suffix not in GEO_WRITERS:\n                supported_formats = sorted(GEO_WRITERS.keys())\n                raise ValueError(\n                    f\"Unsupported file type for GeoDataFrame: {suffix}\\n\"\n                    f\"Supported formats: {', '.join(supported_formats)}\"\n                )\n\n            try:\n                with data_store.open(path, \"wb\") as f:\n                    GEO_WRITERS[suffix](data, f, **kwargs)\n            except Exception as e:\n                raise ValueError(f\"Error writing GeoDataFrame: {str(e)}\")\n\n        else:  # pandas DataFrame\n            if suffix not in PANDAS_WRITERS:\n                supported_formats = sorted(PANDAS_WRITERS.keys())\n                raise ValueError(\n                    f\"Unsupported file type for DataFrame: {suffix}\\n\"\n                    f\"Supported formats: {', '.join(supported_formats)}\"\n                )\n\n            try:\n                with data_store.open(path, mode) as f:\n                    PANDAS_WRITERS[suffix](data, f, **kwargs)\n            except Exception as e:\n                raise ValueError(f\"Error writing DataFrame: {str(e)}\")\n\n    except Exception as e:\n        if isinstance(e, (TypeError, ValueError)):\n            raise\n        raise RuntimeError(f\"Unexpected error writing dataset: {str(e)}\")\n</code></pre>"},{"location":"api/handlers/#gigaspatial.handlers.worldpop.write_datasets","title":"<code>write_datasets(data_dict, data_store, **kwargs)</code>","text":"<p>Write multiple datasets to data storage at once.</p>"},{"location":"api/handlers/#gigaspatial.handlers.worldpop.write_datasets--parameters","title":"Parameters:","text":"<p>data_dict : dict     Dictionary mapping paths to DataFrames/GeoDataFrames. data_store : DataStore     Instance of DataStore for accessing data storage. **kwargs : dict     Additional arguments passed to write_dataset.</p>"},{"location":"api/handlers/#gigaspatial.handlers.worldpop.write_datasets--raises","title":"Raises:","text":"<p>ValueError     If there are any errors writing the datasets.</p> Source code in <code>gigaspatial/core/io/writers.py</code> <pre><code>def write_datasets(data_dict, data_store: DataStore, **kwargs):\n    \"\"\"\n    Write multiple datasets to data storage at once.\n\n    Parameters:\n    ----------\n    data_dict : dict\n        Dictionary mapping paths to DataFrames/GeoDataFrames.\n    data_store : DataStore\n        Instance of DataStore for accessing data storage.\n    **kwargs : dict\n        Additional arguments passed to write_dataset.\n\n    Raises:\n    ------\n    ValueError\n        If there are any errors writing the datasets.\n    \"\"\"\n    errors = {}\n\n    for path, data in data_dict.items():\n        try:\n            write_dataset(data, data_store, path, **kwargs)\n        except Exception as e:\n            errors[path] = str(e)\n\n    if errors:\n        error_msg = \"\\n\".join(f\"- {path}: {error}\" for path, error in errors.items())\n        raise ValueError(f\"Errors writing datasets:\\n{error_msg}\")\n</code></pre>"},{"location":"api/processing/","title":"Processing Module","text":""},{"location":"api/processing/#gigaspatial.processing","title":"<code>gigaspatial.processing</code>","text":""},{"location":"api/processing/#gigaspatial.processing.DataStore","title":"<code>DataStore</code>","text":"<p>               Bases: <code>ABC</code></p> <p>Abstract base class defining the interface for data store implementations. This class serves as a parent for both local and cloud-based storage solutions.</p> Source code in <code>gigaspatial/core/io/data_store.py</code> <pre><code>class DataStore(ABC):\n    \"\"\"\n    Abstract base class defining the interface for data store implementations.\n    This class serves as a parent for both local and cloud-based storage solutions.\n    \"\"\"\n\n    @abstractmethod\n    def read_file(self, path: str) -&gt; Any:\n        \"\"\"\n        Read contents of a file from the data store.\n\n        Args:\n            path: Path to the file to read\n\n        Returns:\n            Contents of the file\n\n        Raises:\n            IOError: If file cannot be read\n        \"\"\"\n        pass\n\n    @abstractmethod\n    def write_file(self, path: str, data: Any) -&gt; None:\n        \"\"\"\n        Write data to a file in the data store.\n\n        Args:\n            path: Path where to write the file\n            data: Data to write to the file\n\n        Raises:\n            IOError: If file cannot be written\n        \"\"\"\n        pass\n\n    @abstractmethod\n    def file_exists(self, path: str) -&gt; bool:\n        \"\"\"\n        Check if a file exists in the data store.\n\n        Args:\n            path: Path to check\n\n        Returns:\n            True if file exists, False otherwise\n        \"\"\"\n        pass\n\n    @abstractmethod\n    def list_files(self, path: str) -&gt; List[str]:\n        \"\"\"\n        List all files in a directory.\n\n        Args:\n            path: Directory path to list\n\n        Returns:\n            List of file paths in the directory\n        \"\"\"\n        pass\n\n    @abstractmethod\n    def walk(self, top: str) -&gt; Generator:\n        \"\"\"\n        Walk through directory tree, similar to os.walk().\n\n        Args:\n            top: Starting directory for the walk\n\n        Returns:\n            Generator yielding tuples of (dirpath, dirnames, filenames)\n        \"\"\"\n        pass\n\n    @abstractmethod\n    def open(self, file: str, mode: str = \"r\") -&gt; Union[str, bytes]:\n        \"\"\"\n        Context manager for file operations.\n\n        Args:\n            file: Path to the file\n            mode: File mode ('r', 'w', 'rb', 'wb')\n\n        Yields:\n            File-like object\n\n        Raises:\n            IOError: If file cannot be opened\n        \"\"\"\n        pass\n\n    @abstractmethod\n    def is_file(self, path: str) -&gt; bool:\n        \"\"\"\n        Check if path points to a file.\n\n        Args:\n            path: Path to check\n\n        Returns:\n            True if path is a file, False otherwise\n        \"\"\"\n        pass\n\n    @abstractmethod\n    def is_dir(self, path: str) -&gt; bool:\n        \"\"\"\n        Check if path points to a directory.\n\n        Args:\n            path: Path to check\n\n        Returns:\n            True if path is a directory, False otherwise\n        \"\"\"\n        pass\n\n    @abstractmethod\n    def remove(self, path: str) -&gt; None:\n        \"\"\"\n        Remove a file.\n\n        Args:\n            path: Path to the file to remove\n\n        Raises:\n            IOError: If file cannot be removed\n        \"\"\"\n        pass\n\n    @abstractmethod\n    def rmdir(self, dir: str) -&gt; None:\n        \"\"\"\n        Remove a directory and all its contents.\n\n        Args:\n            dir: Path to the directory to remove\n\n        Raises:\n            IOError: If directory cannot be removed\n        \"\"\"\n        pass\n</code></pre>"},{"location":"api/processing/#gigaspatial.processing.DataStore.file_exists","title":"<code>file_exists(path)</code>  <code>abstractmethod</code>","text":"<p>Check if a file exists in the data store.</p> <p>Parameters:</p> Name Type Description Default <code>path</code> <code>str</code> <p>Path to check</p> required <p>Returns:</p> Type Description <code>bool</code> <p>True if file exists, False otherwise</p> Source code in <code>gigaspatial/core/io/data_store.py</code> <pre><code>@abstractmethod\ndef file_exists(self, path: str) -&gt; bool:\n    \"\"\"\n    Check if a file exists in the data store.\n\n    Args:\n        path: Path to check\n\n    Returns:\n        True if file exists, False otherwise\n    \"\"\"\n    pass\n</code></pre>"},{"location":"api/processing/#gigaspatial.processing.DataStore.is_dir","title":"<code>is_dir(path)</code>  <code>abstractmethod</code>","text":"<p>Check if path points to a directory.</p> <p>Parameters:</p> Name Type Description Default <code>path</code> <code>str</code> <p>Path to check</p> required <p>Returns:</p> Type Description <code>bool</code> <p>True if path is a directory, False otherwise</p> Source code in <code>gigaspatial/core/io/data_store.py</code> <pre><code>@abstractmethod\ndef is_dir(self, path: str) -&gt; bool:\n    \"\"\"\n    Check if path points to a directory.\n\n    Args:\n        path: Path to check\n\n    Returns:\n        True if path is a directory, False otherwise\n    \"\"\"\n    pass\n</code></pre>"},{"location":"api/processing/#gigaspatial.processing.DataStore.is_file","title":"<code>is_file(path)</code>  <code>abstractmethod</code>","text":"<p>Check if path points to a file.</p> <p>Parameters:</p> Name Type Description Default <code>path</code> <code>str</code> <p>Path to check</p> required <p>Returns:</p> Type Description <code>bool</code> <p>True if path is a file, False otherwise</p> Source code in <code>gigaspatial/core/io/data_store.py</code> <pre><code>@abstractmethod\ndef is_file(self, path: str) -&gt; bool:\n    \"\"\"\n    Check if path points to a file.\n\n    Args:\n        path: Path to check\n\n    Returns:\n        True if path is a file, False otherwise\n    \"\"\"\n    pass\n</code></pre>"},{"location":"api/processing/#gigaspatial.processing.DataStore.list_files","title":"<code>list_files(path)</code>  <code>abstractmethod</code>","text":"<p>List all files in a directory.</p> <p>Parameters:</p> Name Type Description Default <code>path</code> <code>str</code> <p>Directory path to list</p> required <p>Returns:</p> Type Description <code>List[str]</code> <p>List of file paths in the directory</p> Source code in <code>gigaspatial/core/io/data_store.py</code> <pre><code>@abstractmethod\ndef list_files(self, path: str) -&gt; List[str]:\n    \"\"\"\n    List all files in a directory.\n\n    Args:\n        path: Directory path to list\n\n    Returns:\n        List of file paths in the directory\n    \"\"\"\n    pass\n</code></pre>"},{"location":"api/processing/#gigaspatial.processing.DataStore.open","title":"<code>open(file, mode='r')</code>  <code>abstractmethod</code>","text":"<p>Context manager for file operations.</p> <p>Parameters:</p> Name Type Description Default <code>file</code> <code>str</code> <p>Path to the file</p> required <code>mode</code> <code>str</code> <p>File mode ('r', 'w', 'rb', 'wb')</p> <code>'r'</code> <p>Yields:</p> Type Description <code>Union[str, bytes]</code> <p>File-like object</p> <p>Raises:</p> Type Description <code>IOError</code> <p>If file cannot be opened</p> Source code in <code>gigaspatial/core/io/data_store.py</code> <pre><code>@abstractmethod\ndef open(self, file: str, mode: str = \"r\") -&gt; Union[str, bytes]:\n    \"\"\"\n    Context manager for file operations.\n\n    Args:\n        file: Path to the file\n        mode: File mode ('r', 'w', 'rb', 'wb')\n\n    Yields:\n        File-like object\n\n    Raises:\n        IOError: If file cannot be opened\n    \"\"\"\n    pass\n</code></pre>"},{"location":"api/processing/#gigaspatial.processing.DataStore.read_file","title":"<code>read_file(path)</code>  <code>abstractmethod</code>","text":"<p>Read contents of a file from the data store.</p> <p>Parameters:</p> Name Type Description Default <code>path</code> <code>str</code> <p>Path to the file to read</p> required <p>Returns:</p> Type Description <code>Any</code> <p>Contents of the file</p> <p>Raises:</p> Type Description <code>IOError</code> <p>If file cannot be read</p> Source code in <code>gigaspatial/core/io/data_store.py</code> <pre><code>@abstractmethod\ndef read_file(self, path: str) -&gt; Any:\n    \"\"\"\n    Read contents of a file from the data store.\n\n    Args:\n        path: Path to the file to read\n\n    Returns:\n        Contents of the file\n\n    Raises:\n        IOError: If file cannot be read\n    \"\"\"\n    pass\n</code></pre>"},{"location":"api/processing/#gigaspatial.processing.DataStore.remove","title":"<code>remove(path)</code>  <code>abstractmethod</code>","text":"<p>Remove a file.</p> <p>Parameters:</p> Name Type Description Default <code>path</code> <code>str</code> <p>Path to the file to remove</p> required <p>Raises:</p> Type Description <code>IOError</code> <p>If file cannot be removed</p> Source code in <code>gigaspatial/core/io/data_store.py</code> <pre><code>@abstractmethod\ndef remove(self, path: str) -&gt; None:\n    \"\"\"\n    Remove a file.\n\n    Args:\n        path: Path to the file to remove\n\n    Raises:\n        IOError: If file cannot be removed\n    \"\"\"\n    pass\n</code></pre>"},{"location":"api/processing/#gigaspatial.processing.DataStore.rmdir","title":"<code>rmdir(dir)</code>  <code>abstractmethod</code>","text":"<p>Remove a directory and all its contents.</p> <p>Parameters:</p> Name Type Description Default <code>dir</code> <code>str</code> <p>Path to the directory to remove</p> required <p>Raises:</p> Type Description <code>IOError</code> <p>If directory cannot be removed</p> Source code in <code>gigaspatial/core/io/data_store.py</code> <pre><code>@abstractmethod\ndef rmdir(self, dir: str) -&gt; None:\n    \"\"\"\n    Remove a directory and all its contents.\n\n    Args:\n        dir: Path to the directory to remove\n\n    Raises:\n        IOError: If directory cannot be removed\n    \"\"\"\n    pass\n</code></pre>"},{"location":"api/processing/#gigaspatial.processing.DataStore.walk","title":"<code>walk(top)</code>  <code>abstractmethod</code>","text":"<p>Walk through directory tree, similar to os.walk().</p> <p>Parameters:</p> Name Type Description Default <code>top</code> <code>str</code> <p>Starting directory for the walk</p> required <p>Returns:</p> Type Description <code>Generator</code> <p>Generator yielding tuples of (dirpath, dirnames, filenames)</p> Source code in <code>gigaspatial/core/io/data_store.py</code> <pre><code>@abstractmethod\ndef walk(self, top: str) -&gt; Generator:\n    \"\"\"\n    Walk through directory tree, similar to os.walk().\n\n    Args:\n        top: Starting directory for the walk\n\n    Returns:\n        Generator yielding tuples of (dirpath, dirnames, filenames)\n    \"\"\"\n    pass\n</code></pre>"},{"location":"api/processing/#gigaspatial.processing.DataStore.write_file","title":"<code>write_file(path, data)</code>  <code>abstractmethod</code>","text":"<p>Write data to a file in the data store.</p> <p>Parameters:</p> Name Type Description Default <code>path</code> <code>str</code> <p>Path where to write the file</p> required <code>data</code> <code>Any</code> <p>Data to write to the file</p> required <p>Raises:</p> Type Description <code>IOError</code> <p>If file cannot be written</p> Source code in <code>gigaspatial/core/io/data_store.py</code> <pre><code>@abstractmethod\ndef write_file(self, path: str, data: Any) -&gt; None:\n    \"\"\"\n    Write data to a file in the data store.\n\n    Args:\n        path: Path where to write the file\n        data: Data to write to the file\n\n    Raises:\n        IOError: If file cannot be written\n    \"\"\"\n    pass\n</code></pre>"},{"location":"api/processing/#gigaspatial.processing.LocalDataStore","title":"<code>LocalDataStore</code>","text":"<p>               Bases: <code>DataStore</code></p> <p>Implementation for local filesystem storage.</p> Source code in <code>gigaspatial/core/io/local_data_store.py</code> <pre><code>class LocalDataStore(DataStore):\n    \"\"\"Implementation for local filesystem storage.\"\"\"\n\n    def __init__(self, base_path: Union[str, Path] = \"\"):\n        super().__init__()\n        self.base_path = Path(base_path).resolve()\n\n    def _resolve_path(self, path: str) -&gt; Path:\n        \"\"\"Resolve path relative to base directory.\"\"\"\n        return self.base_path / path\n\n    def read_file(self, path: str) -&gt; bytes:\n        full_path = self._resolve_path(path)\n        with open(full_path, \"rb\") as f:\n            return f.read()\n\n    def write_file(self, path: str, data: Union[bytes, str]) -&gt; None:\n        full_path = self._resolve_path(path)\n        self.mkdir(str(full_path.parent), exist_ok=True)\n\n        if isinstance(data, str):\n            mode = \"w\"\n            encoding = \"utf-8\"\n        else:\n            mode = \"wb\"\n            encoding = None\n\n        with open(full_path, mode, encoding=encoding) as f:\n            f.write(data)\n\n    def file_exists(self, path: str) -&gt; bool:\n        return self._resolve_path(path).is_file()\n\n    def list_files(self, path: str) -&gt; List[str]:\n        full_path = self._resolve_path(path)\n        return [\n            str(f.relative_to(self.base_path))\n            for f in full_path.iterdir()\n            if f.is_file()\n        ]\n\n    def walk(self, top: str) -&gt; Generator[Tuple[str, List[str], List[str]], None, None]:\n        full_path = self._resolve_path(top)\n        for root, dirs, files in os.walk(full_path):\n            rel_root = str(Path(root).relative_to(self.base_path))\n            yield rel_root, dirs, files\n\n    def list_directories(self, path: str) -&gt; List[str]:\n        full_path = self._resolve_path(path)\n\n        if not full_path.exists():\n            return []\n\n        if not full_path.is_dir():\n            return []\n\n        return [d.name for d in full_path.iterdir() if d.is_dir()]\n\n    def open(self, path: str, mode: str = \"r\") -&gt; IO:\n        full_path = self._resolve_path(path)\n        self.mkdir(str(full_path.parent), exist_ok=True)\n        return open(full_path, mode)\n\n    def is_file(self, path: str) -&gt; bool:\n        return self._resolve_path(path).is_file()\n\n    def is_dir(self, path: str) -&gt; bool:\n        return self._resolve_path(path).is_dir()\n\n    def remove(self, path: str) -&gt; None:\n        full_path = self._resolve_path(path)\n        if full_path.is_file():\n            os.remove(full_path)\n\n    def rmdir(self, directory: str) -&gt; None:\n        full_path = self._resolve_path(directory)\n        if full_path.is_dir():\n            os.rmdir(full_path)\n\n    def mkdir(self, path: str, exist_ok: bool = False) -&gt; None:\n        full_path = self._resolve_path(path)\n        full_path.mkdir(parents=True, exist_ok=exist_ok)\n\n    def exists(self, path: str) -&gt; bool:\n        return self._resolve_path(path).exists()\n</code></pre>"},{"location":"api/processing/#gigaspatial.processing.TifProcessor","title":"<code>TifProcessor</code>","text":"<p>A class to handle tif data processing, supporting single-band, RGB, RGBA, and multi-band data.</p> Source code in <code>gigaspatial/processing/tif_processor.py</code> <pre><code>@dataclass(config=ConfigDict(arbitrary_types_allowed=True))\nclass TifProcessor:\n    \"\"\"\n    A class to handle tif data processing, supporting single-band, RGB, RGBA, and multi-band data.\n    \"\"\"\n\n    dataset_path: Union[Path, str]\n    data_store: Optional[DataStore] = None\n    mode: Literal[\"single\", \"rgb\", \"rgba\", \"multi\"] = \"single\"\n\n    def __post_init__(self):\n        \"\"\"Validate inputs and set up logging.\"\"\"\n        self.data_store = self.data_store or LocalDataStore()\n        self.logger = config.get_logger(self.__class__.__name__)\n        self._cache = {}\n\n        if not self.data_store.file_exists(self.dataset_path):\n            raise FileNotFoundError(f\"Dataset not found at {self.dataset_path}\")\n\n        self._load_metadata()\n\n        # Validate mode and band count\n        if self.mode == \"rgba\" and self.count != 4:\n            raise ValueError(\"RGBA mode requires a 4-band TIF file\")\n        if self.mode == \"rgb\" and self.count != 3:\n            raise ValueError(\"RGB mode requires a 3-band TIF file\")\n        if self.mode == \"single\" and self.count != 1:\n            raise ValueError(\"Single mode requires a 1-band TIF file\")\n        if self.mode == \"multi\" and self.count &lt; 2:\n            raise ValueError(\"Multi mode requires a TIF file with 2 or more bands\")\n\n    @contextmanager\n    def open_dataset(self):\n        \"\"\"Context manager for accessing the dataset\"\"\"\n        with self.data_store.open(self.dataset_path, \"rb\") as f:\n            with rasterio.MemoryFile(f.read()) as memfile:\n                with memfile.open() as src:\n                    yield src\n\n    def _load_metadata(self):\n        \"\"\"Load metadata from the TIF file if not already cached\"\"\"\n        if not self._cache:\n            with self.open_dataset() as src:\n                self._cache[\"transform\"] = src.transform\n                self._cache[\"crs\"] = src.crs.to_string()\n                self._cache[\"bounds\"] = src.bounds\n                self._cache[\"width\"] = src.width\n                self._cache[\"height\"] = src.height\n                self._cache[\"resolution\"] = (abs(src.transform.a), abs(src.transform.e))\n                self._cache[\"x_transform\"] = src.transform.a\n                self._cache[\"y_transform\"] = src.transform.e\n                self._cache[\"nodata\"] = src.nodata\n                self._cache[\"count\"] = src.count\n                self._cache[\"dtype\"] = src.dtypes[0]\n\n    @property\n    def transform(self):\n        \"\"\"Get the transform from the TIF file\"\"\"\n        return self._cache[\"transform\"]\n\n    @property\n    def crs(self):\n        \"\"\"Get the coordinate reference system from the TIF file\"\"\"\n        return self._cache[\"crs\"]\n\n    @property\n    def bounds(self):\n        \"\"\"Get the bounds of the TIF file\"\"\"\n        return self._cache[\"bounds\"]\n\n    @property\n    def resolution(self) -&gt; Tuple[float, float]:\n        \"\"\"Get the x and y resolution (pixel width and height or pixel size) from the TIF file\"\"\"\n        return self._cache[\"resolution\"]\n\n    @property\n    def x_transform(self) -&gt; float:\n        \"\"\"Get the x transform from the TIF file\"\"\"\n        return self._cache[\"x_transform\"]\n\n    @property\n    def y_transform(self) -&gt; float:\n        \"\"\"Get the y transform from the TIF file\"\"\"\n        return self._cache[\"y_transform\"]\n\n    @property\n    def count(self) -&gt; int:\n        \"\"\"Get the band count from the TIF file\"\"\"\n        return self._cache[\"count\"]\n\n    @property\n    def nodata(self) -&gt; int:\n        \"\"\"Get the value representing no data in the rasters\"\"\"\n        return self._cache[\"nodata\"]\n\n    @property\n    def tabular(self) -&gt; pd.DataFrame:\n        \"\"\"Get the data from the TIF file\"\"\"\n        if not hasattr(self, \"_tabular\"):\n            try:\n                if self.mode == \"single\":\n                    self._tabular = self._to_band_dataframe(\n                        drop_nodata=True, drop_values=[]\n                    )\n                elif self.mode == \"rgb\":\n                    self._tabular = self._to_rgb_dataframe(drop_nodata=True)\n                elif self.mode == \"rgba\":\n                    self._tabular = self._to_rgba_dataframe(drop_transparent=True)\n                elif self.mode == \"multi\":\n                    self._tabular = self._to_multi_band_dataframe(\n                        drop_nodata=True,\n                        drop_values=[],\n                        band_names=None,  # Use default band naming\n                    )\n                else:\n                    raise ValueError(\n                        f\"Invalid mode: {self.mode}. Must be one of: single, rgb, rgba, multi\"\n                    )\n            except Exception as e:\n                raise ValueError(\n                    f\"Failed to process TIF file in mode '{self.mode}'. \"\n                    f\"Please ensure the file is valid and matches the selected mode. \"\n                    f\"Original error: {str(e)}\"\n                )\n\n        return self._tabular\n\n    def to_dataframe(self) -&gt; pd.DataFrame:\n        return self.tabular\n\n    def get_zoned_geodataframe(self) -&gt; gpd.GeoDataFrame:\n        \"\"\"\n        Convert the processed TIF data into a GeoDataFrame, where each row represents a pixel zone.\n        Each zone is defined by its bounding box, based on pixel resolution and coordinates.\n        \"\"\"\n        self.logger.info(\"Converting data to GeoDataFrame with zones...\")\n\n        df = self.tabular\n\n        x_res, y_res = self.resolution\n\n        # create bounding box for each pixel\n        geometries = [\n            box(lon - x_res / 2, lat - y_res / 2, lon + x_res / 2, lat + y_res / 2)\n            for lon, lat in zip(df[\"lon\"], df[\"lat\"])\n        ]\n\n        gdf = gpd.GeoDataFrame(df, geometry=geometries, crs=self.crs)\n\n        self.logger.info(\"Conversion to GeoDataFrame complete!\")\n        return gdf\n\n    def sample_by_coordinates(\n        self, coordinate_list: List[Tuple[float, float]]\n    ) -&gt; Union[np.ndarray, dict]:\n        self.logger.info(\"Sampling raster values at the coordinates...\")\n\n        with self.open_dataset() as src:\n            if self.mode == \"rgba\":\n                if self.count != 4:\n                    raise ValueError(\"RGBA mode requires a 4-band TIF file\")\n\n                rgba_values = {\"red\": [], \"green\": [], \"blue\": [], \"alpha\": []}\n\n                for band_idx, color in enumerate([\"red\", \"green\", \"blue\", \"alpha\"], 1):\n                    rgba_values[color] = [\n                        vals[0]\n                        for vals in src.sample(coordinate_list, indexes=band_idx)\n                    ]\n\n                return rgba_values\n\n            elif self.mode == \"rgb\":\n                if self.count != 3:\n                    raise ValueError(\"RGB mode requires a 3-band TIF file\")\n\n                rgb_values = {\"red\": [], \"green\": [], \"blue\": []}\n\n                for band_idx, color in enumerate([\"red\", \"green\", \"blue\"], 1):\n                    rgb_values[color] = [\n                        vals[0]\n                        for vals in src.sample(coordinate_list, indexes=band_idx)\n                    ]\n\n                return rgb_values\n            else:\n                if src.count != 1:\n                    raise ValueError(\"Single band mode requires a 1-band TIF file\")\n                return np.array([vals[0] for vals in src.sample(coordinate_list)])\n\n    def sample_by_polygons(\n        self, polygon_list: List[Union[Polygon, MultiPolygon]], stat: str = \"mean\"\n    ) -&gt; np.ndarray:\n        \"\"\"\n        Sample raster values within each polygon of a GeoDataFrame.\n\n        Parameters:\n            polygon_list: List of polygon geometries (can include MultiPolygons).\n            stat (str): Aggregation statistic to compute within each polygon.\n                        Options: \"mean\", \"median\", \"sum\", \"min\", \"max\".\n        Returns:\n            A NumPy array of sampled values\n        \"\"\"\n        self.logger.info(\"Sampling raster values within polygons...\")\n\n        with self.open_dataset() as src:\n            results = []\n\n            for geom in polygon_list:\n                if geom.is_empty:\n                    results.append(np.nan)\n                    continue\n\n                try:\n                    # Mask the raster with the polygon\n                    out_image, _ = mask(src, [geom], crop=True)\n\n                    # Flatten the raster values and remove NoData values\n                    values = out_image[out_image != src.nodata].flatten()\n\n                    # Compute the desired statistic\n                    if len(values) == 0:\n                        results.append(np.nan)\n                    else:\n                        if stat == \"mean\":\n                            results.append(np.mean(values))\n                        elif stat == \"median\":\n                            results.append(np.median(values))\n                        elif stat == \"sum\":\n                            results.append(np.sum(values))\n                        elif stat == \"min\":\n                            results.append(np.min(values))\n                        elif stat == \"max\":\n                            results.append(np.max(values))\n                        else:\n                            raise ValueError(f\"Unknown statistic: {stat}\")\n\n                except Exception as e:\n                    self.logger.error(f\"Error processing polygon: {e}\")\n                    results.append(np.nan)\n\n        return np.array(results)\n\n    def _to_rgba_dataframe(self, drop_transparent: bool = False) -&gt; pd.DataFrame:\n        \"\"\"\n        Convert RGBA TIF to DataFrame with separate columns for R, G, B, A values.\n        \"\"\"\n        self.logger.info(\"Processing RGBA dataset...\")\n\n        with self.open_dataset() as src:\n            if self.count != 4:\n                raise ValueError(\"RGBA mode requires a 4-band TIF file\")\n\n            # Read all four bands\n            red, green, blue, alpha = src.read()\n\n            x_coords, y_coords = self._get_pixel_coordinates()\n\n            if drop_transparent:\n                mask = alpha &gt; 0\n                red = np.extract(mask, red)\n                green = np.extract(mask, green)\n                blue = np.extract(mask, blue)\n                alpha = np.extract(mask, alpha)\n                lons = np.extract(mask, x_coords)\n                lats = np.extract(mask, y_coords)\n            else:\n                lons = x_coords.flatten()\n                lats = y_coords.flatten()\n                red = red.flatten()\n                green = green.flatten()\n                blue = blue.flatten()\n                alpha = alpha.flatten()\n\n            # Create DataFrame with RGBA values\n            data = pd.DataFrame(\n                {\n                    \"lon\": lons,\n                    \"lat\": lats,\n                    \"red\": red,\n                    \"green\": green,\n                    \"blue\": blue,\n                    \"alpha\": alpha,\n                }\n            )\n\n            # Normalize alpha values if they're not in [0, 1] range\n            if data[\"alpha\"].max() &gt; 1:\n                data[\"alpha\"] = data[\"alpha\"] / data[\"alpha\"].max()\n\n        self.logger.info(\"RGBA dataset is processed!\")\n        return data\n\n    def _to_rgb_dataframe(self, drop_nodata: bool = True) -&gt; pd.DataFrame:\n        \"\"\"Convert RGB TIF to DataFrame with separate columns for R, G, B values.\"\"\"\n        if self.mode != \"rgb\":\n            raise ValueError(\"Use appropriate method for current mode\")\n\n        self.logger.info(\"Processing RGB dataset...\")\n\n        with self.open_dataset() as src:\n            if self.count != 3:\n                raise ValueError(\"RGB mode requires a 3-band TIF file\")\n\n            # Read all three bands\n            red, green, blue = src.read()\n\n            x_coords, y_coords = self._get_pixel_coordinates()\n\n            if drop_nodata:\n                nodata_value = src.nodata\n                if nodata_value is not None:\n                    mask = ~(\n                        (red == nodata_value)\n                        | (green == nodata_value)\n                        | (blue == nodata_value)\n                    )\n                    red = np.extract(mask, red)\n                    green = np.extract(mask, green)\n                    blue = np.extract(mask, blue)\n                    lons = np.extract(mask, x_coords)\n                    lats = np.extract(mask, y_coords)\n                else:\n                    lons = x_coords.flatten()\n                    lats = y_coords.flatten()\n                    red = red.flatten()\n                    green = green.flatten()\n                    blue = blue.flatten()\n            else:\n                lons = x_coords.flatten()\n                lats = y_coords.flatten()\n                red = red.flatten()\n                green = green.flatten()\n                blue = blue.flatten()\n\n            data = pd.DataFrame(\n                {\n                    \"lon\": lons,\n                    \"lat\": lats,\n                    \"red\": red,\n                    \"green\": green,\n                    \"blue\": blue,\n                }\n            )\n\n        self.logger.info(\"RGB dataset is processed!\")\n        return data\n\n    def _to_band_dataframe(\n        self, band_number: int = 1, drop_nodata: bool = True, drop_values: list = []\n    ) -&gt; pd.DataFrame:\n        \"\"\"Process single-band TIF to DataFrame.\"\"\"\n        if self.mode != \"single\":\n            raise ValueError(\"Use appropriate method for current mode\")\n\n        self.logger.info(\"Processing single-band dataset...\")\n\n        if band_number &lt;= 0 or band_number &gt; self.count:\n            self.logger.error(\n                f\"Error: Band number {band_number} is out of range. The file has {self.count} bands.\"\n            )\n            return None\n\n        with self.open_dataset() as src:\n\n            band = src.read(band_number)\n\n            x_coords, y_coords = self._get_pixel_coordinates()\n\n            values_to_mask = []\n            if drop_nodata:\n                nodata_value = src.nodata\n                if nodata_value is not None:\n                    values_to_mask.append(nodata_value)\n\n            if drop_values:\n                values_to_mask.extend(drop_values)\n\n            if values_to_mask:\n                data_mask = ~np.isin(band, values_to_mask)\n                pixel_values = np.extract(data_mask, band)\n                lons = np.extract(data_mask, x_coords)\n                lats = np.extract(data_mask, y_coords)\n            else:\n                pixel_values = band.flatten()\n                lons = x_coords.flatten()\n                lats = y_coords.flatten()\n\n            data = pd.DataFrame({\"lon\": lons, \"lat\": lats, \"pixel_value\": pixel_values})\n\n        self.logger.info(\"Dataset is processed!\")\n        return data\n\n    def _to_multi_band_dataframe(\n        self,\n        drop_nodata: bool = True,\n        drop_values: list = [],\n        band_names: Optional[List[str]] = None,\n    ) -&gt; pd.DataFrame:\n        \"\"\"\n        Process multi-band TIF to DataFrame with all bands included.\n\n        Args:\n            drop_nodata (bool): Whether to drop nodata values. Defaults to True.\n            drop_values (list): Additional values to drop from the dataset. Defaults to empty list.\n            band_names (Optional[List[str]]): Custom names for the bands. If None, bands will be named using\n                                            the band descriptions from the GeoTIFF metadata if available,\n                                            otherwise 'band_1', 'band_2', etc.\n\n        Returns:\n            pd.DataFrame: DataFrame containing coordinates and all band values\n        \"\"\"\n        self.logger.info(\"Processing multi-band dataset...\")\n\n        with self.open_dataset() as src:\n            # Read all bands\n            stack = src.read()\n\n            x_coords, y_coords = self._get_pixel_coordinates()\n\n            # Initialize dictionary with coordinates\n            data_dict = {\"lon\": x_coords.flatten(), \"lat\": y_coords.flatten()}\n\n            # Get band descriptions from metadata if available\n            if band_names is None and hasattr(src, \"descriptions\") and src.descriptions:\n                band_names = [\n                    desc if desc else f\"band_{i+1}\"\n                    for i, desc in enumerate(src.descriptions)\n                ]\n\n            # Process each band\n            for band_idx in range(self.count):\n                band_data = stack[band_idx]\n\n                # Handle nodata and other values to drop\n                if drop_nodata or drop_values:\n                    values_to_mask = []\n                    if drop_nodata and src.nodata is not None:\n                        values_to_mask.append(src.nodata)\n                    if drop_values:\n                        values_to_mask.extend(drop_values)\n\n                    if values_to_mask:\n                        data_mask = ~np.isin(band_data, values_to_mask)\n                        band_values = np.extract(data_mask, band_data)\n                        if band_idx == 0:  # Only need to mask coordinates once\n                            data_dict[\"lon\"] = np.extract(data_mask, x_coords)\n                            data_dict[\"lat\"] = np.extract(data_mask, y_coords)\n                    else:\n                        band_values = band_data.flatten()\n                else:\n                    band_values = band_data.flatten()\n\n                # Use custom band names if provided, otherwise use descriptions or default naming\n                band_name = (\n                    band_names[band_idx]\n                    if band_names and len(band_names) &gt; band_idx\n                    else f\"band_{band_idx + 1}\"\n                )\n                data_dict[band_name] = band_values\n\n        self.logger.info(\"Multi-band dataset is processed!\")\n        return pd.DataFrame(data_dict)\n\n    def _get_pixel_coordinates(self):\n        \"\"\"Helper method to generate coordinate arrays for all pixels\"\"\"\n        if \"pixel_coords\" not in self._cache:\n            # use cached values\n            bounds = self._cache[\"bounds\"]\n            width = self._cache[\"width\"]\n            height = self._cache[\"height\"]\n            pixel_size_x = self._cache[\"x_transform\"]\n            pixel_size_y = self._cache[\"y_transform\"]\n\n            self._cache[\"pixel_coords\"] = np.meshgrid(\n                np.linspace(\n                    bounds.left + pixel_size_x / 2,\n                    bounds.right - pixel_size_x / 2,\n                    width,\n                ),\n                np.linspace(\n                    bounds.top + pixel_size_y / 2,\n                    bounds.bottom - pixel_size_y / 2,\n                    height,\n                ),\n            )\n\n        return self._cache[\"pixel_coords\"]\n</code></pre>"},{"location":"api/processing/#gigaspatial.processing.TifProcessor.bounds","title":"<code>bounds</code>  <code>property</code>","text":"<p>Get the bounds of the TIF file</p>"},{"location":"api/processing/#gigaspatial.processing.TifProcessor.count","title":"<code>count: int</code>  <code>property</code>","text":"<p>Get the band count from the TIF file</p>"},{"location":"api/processing/#gigaspatial.processing.TifProcessor.crs","title":"<code>crs</code>  <code>property</code>","text":"<p>Get the coordinate reference system from the TIF file</p>"},{"location":"api/processing/#gigaspatial.processing.TifProcessor.nodata","title":"<code>nodata: int</code>  <code>property</code>","text":"<p>Get the value representing no data in the rasters</p>"},{"location":"api/processing/#gigaspatial.processing.TifProcessor.resolution","title":"<code>resolution: Tuple[float, float]</code>  <code>property</code>","text":"<p>Get the x and y resolution (pixel width and height or pixel size) from the TIF file</p>"},{"location":"api/processing/#gigaspatial.processing.TifProcessor.tabular","title":"<code>tabular: pd.DataFrame</code>  <code>property</code>","text":"<p>Get the data from the TIF file</p>"},{"location":"api/processing/#gigaspatial.processing.TifProcessor.transform","title":"<code>transform</code>  <code>property</code>","text":"<p>Get the transform from the TIF file</p>"},{"location":"api/processing/#gigaspatial.processing.TifProcessor.x_transform","title":"<code>x_transform: float</code>  <code>property</code>","text":"<p>Get the x transform from the TIF file</p>"},{"location":"api/processing/#gigaspatial.processing.TifProcessor.y_transform","title":"<code>y_transform: float</code>  <code>property</code>","text":"<p>Get the y transform from the TIF file</p>"},{"location":"api/processing/#gigaspatial.processing.TifProcessor.__post_init__","title":"<code>__post_init__()</code>","text":"<p>Validate inputs and set up logging.</p> Source code in <code>gigaspatial/processing/tif_processor.py</code> <pre><code>def __post_init__(self):\n    \"\"\"Validate inputs and set up logging.\"\"\"\n    self.data_store = self.data_store or LocalDataStore()\n    self.logger = config.get_logger(self.__class__.__name__)\n    self._cache = {}\n\n    if not self.data_store.file_exists(self.dataset_path):\n        raise FileNotFoundError(f\"Dataset not found at {self.dataset_path}\")\n\n    self._load_metadata()\n\n    # Validate mode and band count\n    if self.mode == \"rgba\" and self.count != 4:\n        raise ValueError(\"RGBA mode requires a 4-band TIF file\")\n    if self.mode == \"rgb\" and self.count != 3:\n        raise ValueError(\"RGB mode requires a 3-band TIF file\")\n    if self.mode == \"single\" and self.count != 1:\n        raise ValueError(\"Single mode requires a 1-band TIF file\")\n    if self.mode == \"multi\" and self.count &lt; 2:\n        raise ValueError(\"Multi mode requires a TIF file with 2 or more bands\")\n</code></pre>"},{"location":"api/processing/#gigaspatial.processing.TifProcessor.get_zoned_geodataframe","title":"<code>get_zoned_geodataframe()</code>","text":"<p>Convert the processed TIF data into a GeoDataFrame, where each row represents a pixel zone. Each zone is defined by its bounding box, based on pixel resolution and coordinates.</p> Source code in <code>gigaspatial/processing/tif_processor.py</code> <pre><code>def get_zoned_geodataframe(self) -&gt; gpd.GeoDataFrame:\n    \"\"\"\n    Convert the processed TIF data into a GeoDataFrame, where each row represents a pixel zone.\n    Each zone is defined by its bounding box, based on pixel resolution and coordinates.\n    \"\"\"\n    self.logger.info(\"Converting data to GeoDataFrame with zones...\")\n\n    df = self.tabular\n\n    x_res, y_res = self.resolution\n\n    # create bounding box for each pixel\n    geometries = [\n        box(lon - x_res / 2, lat - y_res / 2, lon + x_res / 2, lat + y_res / 2)\n        for lon, lat in zip(df[\"lon\"], df[\"lat\"])\n    ]\n\n    gdf = gpd.GeoDataFrame(df, geometry=geometries, crs=self.crs)\n\n    self.logger.info(\"Conversion to GeoDataFrame complete!\")\n    return gdf\n</code></pre>"},{"location":"api/processing/#gigaspatial.processing.TifProcessor.open_dataset","title":"<code>open_dataset()</code>","text":"<p>Context manager for accessing the dataset</p> Source code in <code>gigaspatial/processing/tif_processor.py</code> <pre><code>@contextmanager\ndef open_dataset(self):\n    \"\"\"Context manager for accessing the dataset\"\"\"\n    with self.data_store.open(self.dataset_path, \"rb\") as f:\n        with rasterio.MemoryFile(f.read()) as memfile:\n            with memfile.open() as src:\n                yield src\n</code></pre>"},{"location":"api/processing/#gigaspatial.processing.TifProcessor.sample_by_polygons","title":"<code>sample_by_polygons(polygon_list, stat='mean')</code>","text":"<p>Sample raster values within each polygon of a GeoDataFrame.</p> <p>Parameters:</p> Name Type Description Default <code>polygon_list</code> <code>List[Union[Polygon, MultiPolygon]]</code> <p>List of polygon geometries (can include MultiPolygons).</p> required <code>stat</code> <code>str</code> <p>Aggregation statistic to compute within each polygon.         Options: \"mean\", \"median\", \"sum\", \"min\", \"max\".</p> <code>'mean'</code> Source code in <code>gigaspatial/processing/tif_processor.py</code> <pre><code>def sample_by_polygons(\n    self, polygon_list: List[Union[Polygon, MultiPolygon]], stat: str = \"mean\"\n) -&gt; np.ndarray:\n    \"\"\"\n    Sample raster values within each polygon of a GeoDataFrame.\n\n    Parameters:\n        polygon_list: List of polygon geometries (can include MultiPolygons).\n        stat (str): Aggregation statistic to compute within each polygon.\n                    Options: \"mean\", \"median\", \"sum\", \"min\", \"max\".\n    Returns:\n        A NumPy array of sampled values\n    \"\"\"\n    self.logger.info(\"Sampling raster values within polygons...\")\n\n    with self.open_dataset() as src:\n        results = []\n\n        for geom in polygon_list:\n            if geom.is_empty:\n                results.append(np.nan)\n                continue\n\n            try:\n                # Mask the raster with the polygon\n                out_image, _ = mask(src, [geom], crop=True)\n\n                # Flatten the raster values and remove NoData values\n                values = out_image[out_image != src.nodata].flatten()\n\n                # Compute the desired statistic\n                if len(values) == 0:\n                    results.append(np.nan)\n                else:\n                    if stat == \"mean\":\n                        results.append(np.mean(values))\n                    elif stat == \"median\":\n                        results.append(np.median(values))\n                    elif stat == \"sum\":\n                        results.append(np.sum(values))\n                    elif stat == \"min\":\n                        results.append(np.min(values))\n                    elif stat == \"max\":\n                        results.append(np.max(values))\n                    else:\n                        raise ValueError(f\"Unknown statistic: {stat}\")\n\n            except Exception as e:\n                self.logger.error(f\"Error processing polygon: {e}\")\n                results.append(np.nan)\n\n    return np.array(results)\n</code></pre>"},{"location":"api/processing/#gigaspatial.processing.add_area_in_meters","title":"<code>add_area_in_meters(gdf, area_column_name='area_in_meters')</code>","text":"<p>Calculate the area of (Multi)Polygon geometries in square meters and add it as a new column.</p>"},{"location":"api/processing/#gigaspatial.processing.add_area_in_meters--parameters","title":"Parameters:","text":"<p>gdf : geopandas.GeoDataFrame     GeoDataFrame containing (Multi)Polygon geometries. area_column_name : str, optional     Name of the new column to store the area values. Default is \"area_m2\".</p>"},{"location":"api/processing/#gigaspatial.processing.add_area_in_meters--returns","title":"Returns:","text":"<p>geopandas.GeoDataFrame     The input GeoDataFrame with an additional column for the area in square meters.</p>"},{"location":"api/processing/#gigaspatial.processing.add_area_in_meters--raises","title":"Raises:","text":"<p>ValueError     If the input GeoDataFrame does not contain (Multi)Polygon geometries.</p> Source code in <code>gigaspatial/processing/geo.py</code> <pre><code>def add_area_in_meters(\n    gdf: gpd.GeoDataFrame, area_column_name: str = \"area_in_meters\"\n) -&gt; gpd.GeoDataFrame:\n    \"\"\"\n    Calculate the area of (Multi)Polygon geometries in square meters and add it as a new column.\n\n    Parameters:\n    ----------\n    gdf : geopandas.GeoDataFrame\n        GeoDataFrame containing (Multi)Polygon geometries.\n    area_column_name : str, optional\n        Name of the new column to store the area values. Default is \"area_m2\".\n\n    Returns:\n    -------\n    geopandas.GeoDataFrame\n        The input GeoDataFrame with an additional column for the area in square meters.\n\n    Raises:\n    ------\n    ValueError\n        If the input GeoDataFrame does not contain (Multi)Polygon geometries.\n    \"\"\"\n    # Validate input geometries\n    if not all(gdf.geometry.geom_type.isin([\"Polygon\", \"MultiPolygon\"])):\n        raise ValueError(\n            \"Input GeoDataFrame must contain only Polygon or MultiPolygon geometries.\"\n        )\n\n    # Create a copy of the GeoDataFrame to avoid modifying the original\n    gdf_with_area = gdf.copy()\n\n    # Calculate the UTM CRS for accurate area calculation\n    utm_crs = gdf_with_area.estimate_utm_crs()\n\n    # Transform to UTM CRS and calculate the area in square meters\n    gdf_with_area[area_column_name] = gdf_with_area.to_crs(utm_crs).geometry.area\n\n    return gdf_with_area\n</code></pre>"},{"location":"api/processing/#gigaspatial.processing.add_spatial_jitter","title":"<code>add_spatial_jitter(df, columns=['latitude', 'longitude'], amount=0.0001, seed=None, copy=True)</code>","text":"<p>Add random jitter to duplicated geographic coordinates to create slight separation between overlapping points.</p>"},{"location":"api/processing/#gigaspatial.processing.add_spatial_jitter--parameters","title":"Parameters:","text":"<p>df : pandas.DataFrame     DataFrame containing geographic coordinates. columns : list of str, optional     Column names containing coordinates to jitter. Default is ['latitude', 'longitude']. amount : float or dict, optional     Amount of jitter to add. If float, same amount used for all columns.     If dict, specify amount per column, e.g., {'lat': 0.0001, 'lon': 0.0002}.     Default is 0.0001 (approximately 11 meters at the equator). seed : int, optional     Random seed for reproducibility. Default is None. copy : bool, optional     Whether to create a copy of the input DataFrame. Default is True.</p>"},{"location":"api/processing/#gigaspatial.processing.add_spatial_jitter--returns","title":"Returns:","text":"<p>pandas.DataFrame     DataFrame with jittered coordinates for previously duplicated points.</p>"},{"location":"api/processing/#gigaspatial.processing.add_spatial_jitter--raises","title":"Raises:","text":"<p>ValueError     If columns don't exist or jitter amount is invalid. TypeError     If input types are incorrect.</p> Source code in <code>gigaspatial/processing/geo.py</code> <pre><code>def add_spatial_jitter(\n    df: pd.DataFrame,\n    columns: List[str] = [\"latitude\", \"longitude\"],\n    amount: float = 0.0001,\n    seed=None,\n    copy=True,\n) -&gt; pd.DataFrame:\n    \"\"\"\n    Add random jitter to duplicated geographic coordinates to create slight separation\n    between overlapping points.\n\n    Parameters:\n    ----------\n    df : pandas.DataFrame\n        DataFrame containing geographic coordinates.\n    columns : list of str, optional\n        Column names containing coordinates to jitter. Default is ['latitude', 'longitude'].\n    amount : float or dict, optional\n        Amount of jitter to add. If float, same amount used for all columns.\n        If dict, specify amount per column, e.g., {'lat': 0.0001, 'lon': 0.0002}.\n        Default is 0.0001 (approximately 11 meters at the equator).\n    seed : int, optional\n        Random seed for reproducibility. Default is None.\n    copy : bool, optional\n        Whether to create a copy of the input DataFrame. Default is True.\n\n    Returns:\n    -------\n    pandas.DataFrame\n        DataFrame with jittered coordinates for previously duplicated points.\n\n    Raises:\n    ------\n    ValueError\n        If columns don't exist or jitter amount is invalid.\n    TypeError\n        If input types are incorrect.\n    \"\"\"\n\n    # Input validation\n    if not isinstance(df, pd.DataFrame):\n        raise TypeError(\"Input must be a pandas DataFrame\")\n\n    if not all(col in df.columns for col in columns):\n        raise ValueError(f\"Not all columns {columns} found in DataFrame\")\n\n    # Handle jitter amounts\n    if isinstance(amount, (int, float)):\n        if amount &lt;= 0:\n            raise ValueError(\"Jitter amount must be positive\")\n        jitter_amounts = {col: amount for col in columns}\n    elif isinstance(amount, dict):\n        if not all(col in amount for col in columns):\n            raise ValueError(\"Must specify jitter amount for each column\")\n        if not all(amt &gt; 0 for amt in amount.values()):\n            raise ValueError(\"All jitter amounts must be positive\")\n        jitter_amounts = amount\n    else:\n        raise TypeError(\"amount must be a number or dictionary\")\n\n    # Create copy if requested\n    df_work = df.copy() if copy else df\n\n    # Set random seed if provided\n    if seed is not None:\n        np.random.seed(seed)\n\n    try:\n        # Find duplicated coordinates\n        duplicate_mask = df_work.duplicated(subset=columns, keep=False)\n        n_duplicates = duplicate_mask.sum()\n\n        if n_duplicates &gt; 0:\n            # Add jitter to each column separately\n            for col in columns:\n                jitter = np.random.uniform(\n                    low=-jitter_amounts[col],\n                    high=jitter_amounts[col],\n                    size=n_duplicates,\n                )\n                df_work.loc[duplicate_mask, col] += jitter\n\n            # Validate results (ensure no remaining duplicates)\n            if df_work.duplicated(subset=columns, keep=False).any():\n                # If duplicates remain, recursively add more jitter\n                df_work = add_spatial_jitter(\n                    df_work,\n                    columns=columns,\n                    amount={col: amt * 2 for col, amt in jitter_amounts.items()},\n                    seed=seed,\n                    copy=False,\n                )\n\n        return df_work\n\n    except Exception as e:\n        raise RuntimeError(f\"Error during jittering operation: {str(e)}\")\n</code></pre>"},{"location":"api/processing/#gigaspatial.processing.aggregate_points_to_zones","title":"<code>aggregate_points_to_zones(points, zones, value_columns=None, aggregation='count', point_zone_predicate='within', zone_id_column='zone_id', output_suffix='', drop_geometry=False)</code>","text":"<p>Aggregate point data to zones with flexible aggregation methods.</p> <p>Parameters:</p> Name Type Description Default <code>points</code> <code>Union[DataFrame, GeoDataFrame]</code> <p>Point data to aggregate</p> required <code>zones</code> <code>GeoDataFrame</code> <p>Zones to aggregate points to</p> required <code>value_columns</code> <code>Optional[Union[str, List[str]]]</code> <p>Column(s) containing values to aggregate If None, only counts will be performed.</p> <code>None</code> <code>aggregation</code> <code>Union[str, Dict[str, str]]</code> <p>Aggregation method(s) to use: - Single string: Use same method for all columns (\"count\", \"mean\", \"sum\", \"min\", \"max\") - Dict: Map column names to aggregation methods</p> <code>'count'</code> <code>point_zone_predicate</code> <code>str</code> <p>Spatial predicate for point-to-zone relationship Options: \"within\", \"intersects\", \"contains\"</p> <code>'within'</code> <code>zone_id_column</code> <code>str</code> <p>Column in zones containing zone identifiers</p> <code>'zone_id'</code> <code>output_suffix</code> <code>str</code> <p>Suffix to add to output column names</p> <code>''</code> <code>drop_geometry</code> <code>bool</code> <p>Whether to drop the geometry column from output</p> <code>False</code> <p>Returns:</p> Type Description <code>GeoDataFrame</code> <p>gpd.GeoDataFrame: Zones with aggregated point values</p> Example <p>poi_counts = aggregate_points_to_zones(pois, zones, aggregation=\"count\") poi_value_mean = aggregate_points_to_zones( ...     pois, zones, value_columns=\"score\", aggregation=\"mean\" ... ) poi_multiple = aggregate_points_to_zones( ...     pois, zones, ...     value_columns=[\"score\", \"visits\"], ...     aggregation={\"score\": \"mean\", \"visits\": \"sum\"} ... )</p> Source code in <code>gigaspatial/processing/geo.py</code> <pre><code>def aggregate_points_to_zones(\n    points: Union[pd.DataFrame, gpd.GeoDataFrame],\n    zones: gpd.GeoDataFrame,\n    value_columns: Optional[Union[str, List[str]]] = None,\n    aggregation: Union[str, Dict[str, str]] = \"count\",\n    point_zone_predicate: str = \"within\",\n    zone_id_column: str = \"zone_id\",\n    output_suffix: str = \"\",\n    drop_geometry: bool = False,\n) -&gt; gpd.GeoDataFrame:\n    \"\"\"\n    Aggregate point data to zones with flexible aggregation methods.\n\n    Args:\n        points (Union[pd.DataFrame, gpd.GeoDataFrame]): Point data to aggregate\n        zones (gpd.GeoDataFrame): Zones to aggregate points to\n        value_columns (Optional[Union[str, List[str]]]): Column(s) containing values to aggregate\n            If None, only counts will be performed.\n        aggregation (Union[str, Dict[str, str]]): Aggregation method(s) to use:\n            - Single string: Use same method for all columns (\"count\", \"mean\", \"sum\", \"min\", \"max\")\n            - Dict: Map column names to aggregation methods\n        point_zone_predicate (str): Spatial predicate for point-to-zone relationship\n            Options: \"within\", \"intersects\", \"contains\"\n        zone_id_column (str): Column in zones containing zone identifiers\n        output_suffix (str): Suffix to add to output column names\n        drop_geometry (bool): Whether to drop the geometry column from output\n\n    Returns:\n        gpd.GeoDataFrame: Zones with aggregated point values\n\n    Example:\n        &gt;&gt;&gt; poi_counts = aggregate_points_to_zones(pois, zones, aggregation=\"count\")\n        &gt;&gt;&gt; poi_value_mean = aggregate_points_to_zones(\n        ...     pois, zones, value_columns=\"score\", aggregation=\"mean\"\n        ... )\n        &gt;&gt;&gt; poi_multiple = aggregate_points_to_zones(\n        ...     pois, zones,\n        ...     value_columns=[\"score\", \"visits\"],\n        ...     aggregation={\"score\": \"mean\", \"visits\": \"sum\"}\n        ... )\n    \"\"\"\n    # Input validation\n    if not isinstance(zones, gpd.GeoDataFrame):\n        raise TypeError(\"zones must be a GeoDataFrame\")\n\n    if zone_id_column not in zones.columns:\n        raise ValueError(f\"Zone ID column '{zone_id_column}' not found in zones\")\n\n    # Convert points to GeoDataFrame if necessary\n    if not isinstance(points, gpd.GeoDataFrame):\n        points_gdf = convert_to_geodataframe(points)\n    else:\n        points_gdf = points.copy()\n\n    # Ensure CRS match\n    if points_gdf.crs != zones.crs:\n        points_gdf = points_gdf.to_crs(zones.crs)\n\n    # Handle value columns\n    if value_columns is not None:\n        if isinstance(value_columns, str):\n            value_columns = [value_columns]\n\n        # Validate that all value columns exist\n        missing_cols = [col for col in value_columns if col not in points_gdf.columns]\n        if missing_cols:\n            raise ValueError(f\"Value columns not found in points data: {missing_cols}\")\n\n    # Handle aggregation method\n    agg_funcs = {}\n\n    if isinstance(aggregation, str):\n        if aggregation == \"count\":\n            # Special case for count (doesn't need value columns)\n            agg_funcs[\"__count\"] = \"count\"\n        elif value_columns is not None:\n            # Apply the same aggregation to all value columns\n            agg_funcs = {col: aggregation for col in value_columns}\n        else:\n            raise ValueError(\n                \"Value columns must be specified for aggregation methods other than 'count'\"\n            )\n    elif isinstance(aggregation, dict):\n        # Validate dictionary keys\n        if value_columns is None:\n            raise ValueError(\n                \"Value columns must be specified when using a dictionary of aggregation methods\"\n            )\n\n        missing_aggs = [col for col in value_columns if col not in aggregation]\n        extra_aggs = [col for col in aggregation if col not in value_columns]\n\n        if missing_aggs:\n            raise ValueError(f\"Missing aggregation methods for columns: {missing_aggs}\")\n        if extra_aggs:\n            raise ValueError(\n                f\"Aggregation methods specified for non-existent columns: {extra_aggs}\"\n            )\n\n        agg_funcs = aggregation\n    else:\n        raise TypeError(\"aggregation must be a string or dictionary\")\n\n    # Create a copy of the zones\n    result = zones.copy()\n\n    # Spatial join\n    joined = gpd.sjoin(points_gdf, zones, how=\"inner\", predicate=point_zone_predicate)\n\n    # Perform aggregation\n    if \"geometry\" in joined.columns and not all(\n        value == \"count\" for value in agg_funcs.values()\n    ):\n        # Drop geometry for non-count aggregations to avoid errors\n        joined = joined.drop(columns=[\"geometry\"])\n\n    if \"__count\" in agg_funcs:\n        # Count points per zone\n        counts = (\n            joined.groupby(zone_id_column)\n            .size()\n            .reset_index(name=f\"point_count{output_suffix}\")\n        )\n        result = result.merge(counts, on=zone_id_column, how=\"left\")\n        result[f\"point_count{output_suffix}\"] = (\n            result[f\"point_count{output_suffix}\"].fillna(0).astype(int)\n        )\n    else:\n        # Aggregate values\n        aggregated = joined.groupby(zone_id_column).agg(agg_funcs).reset_index()\n\n        # Rename columns to include aggregation method\n        if len(value_columns) &gt; 0:\n            # Handle MultiIndex columns from pandas aggregation\n            if isinstance(aggregated.columns, pd.MultiIndex):\n                aggregated.columns = [\n                    (\n                        f\"{col[0]}_{col[1]}{output_suffix}\"\n                        if col[0] != zone_id_column\n                        else zone_id_column\n                    )\n                    for col in aggregated.columns\n                ]\n\n            # Merge back to zones\n            result = result.merge(aggregated, on=zone_id_column, how=\"left\")\n\n            # Fill NaN values with zeros\n            for col in result.columns:\n                if (\n                    col != zone_id_column\n                    and col != \"geometry\"\n                    and pd.api.types.is_numeric_dtype(result[col])\n                ):\n                    result[col] = result[col].fillna(0)\n\n    if drop_geometry:\n        result = result.drop(columns=[\"geometry\"])\n        return result\n\n    return result\n</code></pre>"},{"location":"api/processing/#gigaspatial.processing.aggregate_polygons_to_zones","title":"<code>aggregate_polygons_to_zones(polygons, zones, value_columns, aggregation='sum', area_weighted=True, zone_id_column='zone_id', output_suffix='', drop_geometry=False)</code>","text":"<p>Aggregate polygon data to zones with area-weighted values.</p> <p>This function maps polygon data to zones, weighting values by the fractional area of overlap between polygons and zones.</p> <p>Parameters:</p> Name Type Description Default <code>polygons</code> <code>Union[DataFrame, GeoDataFrame]</code> <p>Polygon data to aggregate</p> required <code>zones</code> <code>GeoDataFrame</code> <p>Zones to aggregate polygons to</p> required <code>value_columns</code> <code>Union[str, List[str]]</code> <p>Column(s) containing values to aggregate</p> required <code>aggregation</code> <code>Union[str, Dict[str, str]]</code> <p>Aggregation method(s) to use: - Single string: Use same method for all columns (\"sum\", \"mean\", \"max\", etc.) - Dict: Map column names to aggregation methods</p> <code>'sum'</code> <code>area_weighted</code> <code>bool</code> <p>Whether to weight values by fractional area overlap If False, values are not weighted before aggregation</p> <code>True</code> <code>zone_id_column</code> <code>str</code> <p>Column in zones containing zone identifiers</p> <code>'zone_id'</code> <code>output_suffix</code> <code>str</code> <p>Suffix to add to output column names</p> <code>''</code> <code>drop_geometry</code> <code>bool</code> <p>Whether to drop the geometry column from output</p> <code>False</code> <p>Returns:</p> Type Description <code>GeoDataFrame</code> <p>gpd.GeoDataFrame: Zones with aggregated polygon values</p> Example <p>landuse_stats = aggregate_polygons_to_zones( ...     landuse_polygons, ...     grid_zones, ...     value_columns=[\"area\", \"population\"], ...     aggregation=\"sum\" ... )</p> Source code in <code>gigaspatial/processing/geo.py</code> <pre><code>def aggregate_polygons_to_zones(\n    polygons: Union[pd.DataFrame, gpd.GeoDataFrame],\n    zones: gpd.GeoDataFrame,\n    value_columns: Union[str, List[str]],\n    aggregation: Union[str, Dict[str, str]] = \"sum\",\n    area_weighted: bool = True,\n    zone_id_column: str = \"zone_id\",\n    output_suffix: str = \"\",\n    drop_geometry: bool = False,\n) -&gt; gpd.GeoDataFrame:\n    \"\"\"\n    Aggregate polygon data to zones with area-weighted values.\n\n    This function maps polygon data to zones, weighting values by the\n    fractional area of overlap between polygons and zones.\n\n    Args:\n        polygons (Union[pd.DataFrame, gpd.GeoDataFrame]): Polygon data to aggregate\n        zones (gpd.GeoDataFrame): Zones to aggregate polygons to\n        value_columns (Union[str, List[str]]): Column(s) containing values to aggregate\n        aggregation (Union[str, Dict[str, str]]): Aggregation method(s) to use:\n            - Single string: Use same method for all columns (\"sum\", \"mean\", \"max\", etc.)\n            - Dict: Map column names to aggregation methods\n        area_weighted (bool): Whether to weight values by fractional area overlap\n            If False, values are not weighted before aggregation\n        zone_id_column (str): Column in zones containing zone identifiers\n        output_suffix (str): Suffix to add to output column names\n        drop_geometry (bool): Whether to drop the geometry column from output\n\n    Returns:\n        gpd.GeoDataFrame: Zones with aggregated polygon values\n\n    Example:\n        &gt;&gt;&gt; landuse_stats = aggregate_polygons_to_zones(\n        ...     landuse_polygons,\n        ...     grid_zones,\n        ...     value_columns=[\"area\", \"population\"],\n        ...     aggregation=\"sum\"\n        ... )\n    \"\"\"\n    # Input validation\n    if not isinstance(zones, gpd.GeoDataFrame):\n        raise TypeError(\"zones must be a GeoDataFrame\")\n\n    if zone_id_column not in zones.columns:\n        raise ValueError(f\"Zone ID column '{zone_id_column}' not found in zones\")\n\n    # Convert polygons to GeoDataFrame if necessary\n    if not isinstance(polygons, gpd.GeoDataFrame):\n        try:\n            polygons_gdf = convert_to_geodataframe(polygons)\n        except:\n            raise TypeError(\"polygons must be a GeoDataFrame or convertible to one\")\n    else:\n        polygons_gdf = polygons.copy()\n\n    # Validate geometry types\n    non_polygon_geoms = [\n        geom_type\n        for geom_type in polygons_gdf.geometry.geom_type.unique()\n        if geom_type not in [\"Polygon\", \"MultiPolygon\"]\n    ]\n    if non_polygon_geoms:\n        raise ValueError(\n            f\"Input contains non-polygon geometries: {non_polygon_geoms}. \"\n            \"Use aggregate_points_to_zones for point data.\"\n        )\n\n    # Process value columns\n    if isinstance(value_columns, str):\n        value_columns = [value_columns]\n\n    # Validate that all value columns exist\n    missing_cols = [col for col in value_columns if col not in polygons_gdf.columns]\n    if missing_cols:\n        raise ValueError(f\"Value columns not found in polygons data: {missing_cols}\")\n\n    # Ensure CRS match\n    if polygons_gdf.crs != zones.crs:\n        polygons_gdf = polygons_gdf.to_crs(zones.crs)\n\n    # Handle aggregation method\n    if isinstance(aggregation, str):\n        agg_funcs = {col: aggregation for col in value_columns}\n    elif isinstance(aggregation, dict):\n        # Validate dictionary keys\n        missing_aggs = [col for col in value_columns if col not in aggregation]\n        extra_aggs = [col for col in aggregation if col not in value_columns]\n\n        if missing_aggs:\n            raise ValueError(f\"Missing aggregation methods for columns: {missing_aggs}\")\n        if extra_aggs:\n            raise ValueError(\n                f\"Aggregation methods specified for non-existent columns: {extra_aggs}\"\n            )\n\n        agg_funcs = aggregation\n    else:\n        raise TypeError(\"aggregation must be a string or dictionary\")\n\n    # Create a copy of the zones\n    result = zones.copy()\n\n    if area_weighted:\n        # Use area-weighted aggregation with polygon overlay\n        try:\n            # Compute UTM CRS for accurate area calculations\n            overlay_utm_crs = polygons_gdf.estimate_utm_crs()\n\n            # Prepare polygons for overlay\n            polygons_utm = polygons_gdf.to_crs(overlay_utm_crs)\n            polygons_utm[\"orig_area\"] = polygons_utm.area\n\n            # Keep only necessary columns\n            overlay_cols = value_columns + [\"geometry\", \"orig_area\"]\n            overlay_gdf = polygons_utm[overlay_cols].copy()\n\n            # Prepare zones for overlay\n            zones_utm = zones.to_crs(overlay_utm_crs)\n\n            # Perform the spatial overlay\n            gdf_overlayed = gpd.overlay(\n                overlay_gdf, zones_utm[[zone_id_column, \"geometry\"]], how=\"intersection\"\n            )\n\n            # Calculate fractional areas\n            gdf_overlayed[\"intersection_area\"] = gdf_overlayed.area\n            gdf_overlayed[\"area_fraction\"] = (\n                gdf_overlayed[\"intersection_area\"] / gdf_overlayed[\"orig_area\"]\n            )\n\n            # Apply area weighting to value columns\n            for col in value_columns:\n                gdf_overlayed[col] = gdf_overlayed[col] * gdf_overlayed[\"area_fraction\"]\n\n            # Aggregate by zone ID\n            aggregated = gdf_overlayed.groupby(zone_id_column)[value_columns].agg(\n                agg_funcs\n            )\n\n            # Handle column naming for multi-level index\n            if isinstance(aggregated.columns, pd.MultiIndex):\n                aggregated.columns = [\n                    f\"{col[0]}_{col[1]}{output_suffix}\" for col in aggregated.columns\n                ]\n\n            # Reset index\n            aggregated = aggregated.reset_index()\n\n            # Merge aggregated values back to the zones\n            result = result.merge(aggregated, on=zone_id_column, how=\"left\")\n\n            # Fill NaN values with zeros\n            for col in result.columns:\n                if (\n                    col != zone_id_column\n                    and col != \"geometry\"\n                    and pd.api.types.is_numeric_dtype(result[col])\n                ):\n                    result[col] = result[col].fillna(0)\n\n        except Exception as e:\n            raise RuntimeError(f\"Error during area-weighted aggregation: {e}\")\n\n    else:\n        # Non-weighted aggregation - simpler approach\n        # Perform spatial join\n        joined = gpd.sjoin(polygons_gdf, zones, how=\"inner\", predicate=\"intersects\")\n\n        # Remove geometry column for aggregation\n        if \"geometry\" in joined.columns:\n            joined = joined.drop(columns=[\"geometry\"])\n\n        # Group by zone ID and aggregate\n        aggregated = joined.groupby(zone_id_column)[value_columns].agg(agg_funcs)\n\n        # Handle column naming for multi-level index\n        if isinstance(aggregated.columns, pd.MultiIndex):\n            aggregated.columns = [\n                f\"{col[0]}_{col[1]}{output_suffix}\" for col in aggregated.columns\n            ]\n\n        # Reset index and merge back to zones\n        aggregated = aggregated.reset_index()\n        result = result.merge(aggregated, on=zone_id_column, how=\"left\")\n\n        # Fill NaN values with zeros\n        for col in result.columns:\n            if (\n                col != zone_id_column\n                and col != \"geometry\"\n                and pd.api.types.is_numeric_dtype(result[col])\n            ):\n                result[col] = result[col].fillna(0)\n\n    if drop_geometry:\n        result = result.drop(columns=[\"geometry\"])\n\n    return result\n</code></pre>"},{"location":"api/processing/#gigaspatial.processing.annotate_with_admin_regions","title":"<code>annotate_with_admin_regions(gdf, country_code, data_store=None, admin_id_column_suffix='_giga')</code>","text":"<p>Annotate a GeoDataFrame with administrative region information.</p> <p>Performs a spatial join between the input points and administrative boundaries at levels 1 and 2, resolving conflicts when points intersect multiple admin regions.</p> <p>Parameters:</p> Name Type Description Default <code>gdf</code> <code>GeoDataFrame</code> <p>GeoDataFrame containing points to annotate</p> required <code>country_code</code> <code>str</code> <p>Country code for administrative boundaries</p> required <code>data_store</code> <code>Optional[DataStore]</code> <p>Optional DataStore for loading admin boundary data</p> <code>None</code> <p>Returns:</p> Type Description <code>GeoDataFrame</code> <p>GeoDataFrame with added administrative region columns</p> Source code in <code>gigaspatial/processing/geo.py</code> <pre><code>def annotate_with_admin_regions(\n    gdf: gpd.GeoDataFrame,\n    country_code: str,\n    data_store: Optional[DataStore] = None,\n    admin_id_column_suffix=\"_giga\",\n) -&gt; gpd.GeoDataFrame:\n    \"\"\"\n    Annotate a GeoDataFrame with administrative region information.\n\n    Performs a spatial join between the input points and administrative boundaries\n    at levels 1 and 2, resolving conflicts when points intersect multiple admin regions.\n\n    Args:\n        gdf: GeoDataFrame containing points to annotate\n        country_code: Country code for administrative boundaries\n        data_store: Optional DataStore for loading admin boundary data\n\n    Returns:\n        GeoDataFrame with added administrative region columns\n    \"\"\"\n    from gigaspatial.handlers.boundaries import AdminBoundaries\n\n    if not isinstance(gdf, gpd.GeoDataFrame):\n        raise TypeError(\"gdf must be a GeoDataFrame\")\n\n    if gdf.empty:\n        LOGGER.warning(\"Empty GeoDataFrame provided, returning as-is\")\n        return gdf\n\n    # read country admin data\n    admin1_data = AdminBoundaries.create(\n        country_code=country_code, admin_level=1, data_store=data_store\n    ).to_geodataframe()\n\n    admin1_data.rename(\n        columns={\"id\": f\"admin1_id{admin_id_column_suffix}\", \"name\": \"admin1\"},\n        inplace=True,\n    )\n    admin1_data.drop(columns=[\"name_en\", \"parent_id\", \"country_code\"], inplace=True)\n\n    admin2_data = AdminBoundaries.create(\n        country_code=country_code, admin_level=2, data_store=data_store\n    ).to_geodataframe()\n\n    admin2_data.rename(\n        columns={\n            \"id\": f\"admin2_id{admin_id_column_suffix}\",\n            \"parent_id\": f\"admin1_id{admin_id_column_suffix}\",\n            \"name\": \"admin2\",\n        },\n        inplace=True,\n    )\n    admin2_data.drop(columns=[\"name_en\", \"country_code\"], inplace=True)\n\n    # Join dataframes based on 'admin1_id_giga'\n    admin_data = admin2_data.merge(\n        admin1_data[[f\"admin1_id{admin_id_column_suffix}\", \"admin1\", \"geometry\"]],\n        left_on=f\"admin1_id{admin_id_column_suffix}\",\n        right_on=f\"admin1_id{admin_id_column_suffix}\",\n        how=\"outer\",\n    )\n\n    admin_data[\"geometry\"] = admin_data.apply(\n        lambda x: x.geometry_x if x.geometry_x else x.geometry_y, axis=1\n    )\n\n    admin_data = gpd.GeoDataFrame(\n        admin_data.drop(columns=[\"geometry_x\", \"geometry_y\"]),\n        geometry=\"geometry\",\n        crs=4326,\n    )\n\n    admin_data[\"admin2\"].fillna(\"Unknown\", inplace=True)\n    admin_data[f\"admin2_id{admin_id_column_suffix}\"] = admin_data[\n        f\"admin2_id{admin_id_column_suffix}\"\n    ].replace({np.nan: None})\n\n    if gdf.crs is None:\n        LOGGER.warning(\"Input GeoDataFrame has no CRS, assuming EPSG:4326\")\n        gdf.set_crs(epsg=4326, inplace=True)\n    elif gdf.crs != \"EPSG:4326\":\n        LOGGER.info(f\"Reprojecting from {gdf.crs} to EPSG:4326\")\n        gdf = gdf.to_crs(epsg=4326)\n\n    # spatial join gdf to admins\n    gdf_w_admins = gdf.copy().sjoin(\n        admin_data,\n        how=\"left\",\n        predicate=\"intersects\",\n    )\n\n    # Check for duplicates caused by points intersecting multiple polygons\n    if len(gdf_w_admins) != len(gdf):\n        LOGGER.warning(\n            \"Some points intersect multiple administrative boundaries. Resolving conflicts...\"\n        )\n\n        # Group by original index and select the closest admin area for ties\n        gdf_w_admins[\"distance\"] = gdf_w_admins.apply(\n            lambda row: row.geometry.distance(\n                admin_data.loc[row.index_right, \"geometry\"].centroid\n            ),\n            axis=1,\n        )\n\n        # For points with multiple matches, keep the closest polygon\n        gdf_w_admins = gdf_w_admins.loc[\n            gdf_w_admins.groupby(gdf.index)[\"distance\"].idxmin()\n        ].drop(columns=\"distance\")\n\n    # Drop unnecessary columns and reset the index\n    gdf_w_admins = gdf_w_admins.drop(columns=\"index_right\").reset_index(drop=True)\n\n    return gdf_w_admins\n</code></pre>"},{"location":"api/processing/#gigaspatial.processing.assign_id","title":"<code>assign_id(df, required_columns, id_column='id')</code>","text":"<p>Generate IDs for any entity type in a pandas DataFrame.</p> <p>Parameters:</p> Name Type Description Default <code>df</code> <code>DataFrame</code> <p>Input DataFrame containing entity data</p> required <code>required_columns</code> <code>List[str]</code> <p>List of column names required for ID generation</p> required <code>id_column</code> <code>str</code> <p>Name for the id column that will be generated</p> <code>'id'</code> <p>Returns:</p> Type Description <code>DataFrame</code> <p>pd.DataFrame: DataFrame with generated id column</p> Source code in <code>gigaspatial/processing/utils.py</code> <pre><code>def assign_id(\n    df: pd.DataFrame, required_columns: List[str], id_column: str = \"id\"\n) -&gt; pd.DataFrame:\n    \"\"\"\n    Generate IDs for any entity type in a pandas DataFrame.\n\n    Args:\n        df (pd.DataFrame): Input DataFrame containing entity data\n        required_columns (List[str]): List of column names required for ID generation\n        id_column (str): Name for the id column that will be generated\n\n    Returns:\n        pd.DataFrame: DataFrame with generated id column\n    \"\"\"\n    # Create a copy to avoid modifying the original DataFrame\n    df = df.copy()\n\n    # Check if ID column exists, if not create it with None values\n    if id_column not in df.columns:\n        df[id_column] = None\n\n    # Check required columns exist\n    if not all(col in df.columns for col in required_columns):\n        return df\n\n    # Create identifier concat for UUID generation\n    df[\"identifier_concat\"] = (\n        df[required_columns].astype(str).fillna(\"\").agg(\"\".join, axis=1)\n    )\n\n    # Generate UUIDs only where all required fields are present and no existing ID\n    mask = df[id_column].isna()\n    for col in required_columns:\n        mask &amp;= df[col].notna()\n\n    # Apply UUID generation only where mask is True\n    df.loc[mask, id_column] = df.loc[mask, \"identifier_concat\"].apply(\n        lambda x: str(uuid.uuid3(uuid.NAMESPACE_DNS, x))\n    )\n\n    # Drop temporary column\n    df = df.drop(columns=[\"identifier_concat\"])\n\n    return df\n</code></pre>"},{"location":"api/processing/#gigaspatial.processing.buffer_geodataframe","title":"<code>buffer_geodataframe(gdf, buffer_distance_meters, cap_style='round', copy=True)</code>","text":"<p>Buffers a GeoDataFrame with a given buffer distance in meters.</p> <ul> <li>gdf : geopandas.GeoDataFrame     The GeoDataFrame to be buffered.</li> <li>buffer_distance_meters : float     The buffer distance in meters.</li> <li>cap_style : str, optional     The style of caps. round, flat, square. Default is round.</li> </ul> <ul> <li>geopandas.GeoDataFrame     The buffered GeoDataFrame.</li> </ul> Source code in <code>gigaspatial/processing/geo.py</code> <pre><code>def buffer_geodataframe(\n    gdf: gpd.GeoDataFrame,\n    buffer_distance_meters: float,\n    cap_style: Literal[\"round\", \"square\", \"flat\"] = \"round\",\n    copy=True,\n) -&gt; gpd.GeoDataFrame:\n    \"\"\"\n    Buffers a GeoDataFrame with a given buffer distance in meters.\n\n    Parameters:\n    - gdf : geopandas.GeoDataFrame\n        The GeoDataFrame to be buffered.\n    - buffer_distance_meters : float\n        The buffer distance in meters.\n    - cap_style : str, optional\n        The style of caps. round, flat, square. Default is round.\n\n    Returns:\n    - geopandas.GeoDataFrame\n        The buffered GeoDataFrame.\n    \"\"\"\n\n    # Input validation\n    if not isinstance(gdf, gpd.GeoDataFrame):\n        raise TypeError(\"Input must be a GeoDataFrame\")\n\n    if not isinstance(buffer_distance_meters, (float, int)):\n        raise TypeError(\"Buffer distance must be a number\")\n\n    if cap_style not in [\"round\", \"square\", \"flat\"]:\n        raise ValueError(\"cap_style must be round, flat or square.\")\n\n    if gdf.crs is None:\n        raise ValueError(\"Input GeoDataFrame must have a defined CRS\")\n\n    # Create a copy if requested\n    gdf_work = gdf.copy() if copy else gdf\n\n    # Store input CRS\n    input_crs = gdf_work.crs\n\n    try:\n        # Create a custom UTM CRS based on the calculated UTM zone\n        utm_crs = gdf_work.estimate_utm_crs()\n\n        # Transform to UTM, create buffer, and transform back\n        gdf_work = gdf_work.to_crs(utm_crs)\n        gdf_work[\"geometry\"] = gdf_work[\"geometry\"].buffer(\n            buffer_distance_meters, cap_style=cap_style\n        )\n        gdf_work = gdf_work.to_crs(input_crs)\n\n        return gdf_work\n\n    except Exception as e:\n        raise RuntimeError(f\"Error during buffering operation: {str(e)}\")\n</code></pre>"},{"location":"api/processing/#gigaspatial.processing.calculate_pixels_at_location","title":"<code>calculate_pixels_at_location(gdf, resolution, bbox_size=300, crs='EPSG:3857')</code>","text":"<p>Calculates the number of pixels required to cover a given bounding box around a geographic coordinate, given a resolution in meters per pixel.</p> <p>Parameters:</p> Name Type Description Default <code>gdf</code> <p>a geodataframe with Point geometries that are geographic coordinates</p> required <code>resolution</code> <code>float</code> <p>Desired resolution (meters per pixel).</p> required <code>bbox_size</code> <code>float</code> <p>Bounding box size in meters (default 300m x 300m).</p> <code>300</code> <code>crs</code> <code>str</code> <p>Target projection (default is EPSG:3857).</p> <code>'EPSG:3857'</code> <p>Returns:</p> Name Type Description <code>int</code> <p>Number of pixels per side (width and height).</p> Source code in <code>gigaspatial/processing/sat_images.py</code> <pre><code>def calculate_pixels_at_location(gdf, resolution, bbox_size=300, crs=\"EPSG:3857\"):\n    \"\"\"\n    Calculates the number of pixels required to cover a given bounding box\n    around a geographic coordinate, given a resolution in meters per pixel.\n\n    Parameters:\n        gdf: a geodataframe with Point geometries that are geographic coordinates\n        resolution (float): Desired resolution (meters per pixel).\n        bbox_size (float): Bounding box size in meters (default 300m x 300m).\n        crs (str): Target projection (default is EPSG:3857).\n\n    Returns:\n        int: Number of pixels per side (width and height).\n    \"\"\"\n\n    # Calculate avg lat and lon\n    lon = gdf.geometry.x.mean()\n    lat = gdf.geometry.y.mean()\n\n    # Define projections\n    wgs84 = pyproj.CRS(\"EPSG:4326\")  # Geographic coordinate system\n    mercator = pyproj.CRS(crs)  # Target CRS (EPSG:3857)\n\n    # Transform the center coordinate to EPSG:3857\n    transformer = pyproj.Transformer.from_crs(wgs84, mercator, always_xy=True)\n    x, y = transformer.transform(lon, lat)\n\n    # Calculate scale factor (distortion) at given latitude\n    scale_factor = np.cos(np.radians(lat))  # Mercator scale correction\n\n    # Adjust the effective resolution\n    effective_resolution = resolution * scale_factor\n\n    # Compute number of pixels per side\n    pixels = bbox_size / effective_resolution\n    return int(round(pixels))\n</code></pre>"},{"location":"api/processing/#gigaspatial.processing.convert_to_geodataframe","title":"<code>convert_to_geodataframe(data, lat_col=None, lon_col=None, crs='EPSG:4326')</code>","text":"<p>Convert a pandas DataFrame to a GeoDataFrame, either from latitude/longitude columns or from a WKT geometry column.</p>"},{"location":"api/processing/#gigaspatial.processing.convert_to_geodataframe--parameters","title":"Parameters:","text":"<p>data : pandas.DataFrame     Input DataFrame containing either lat/lon columns or a geometry column. lat_col : str, optional     Name of the latitude column. Default is 'lat'. lon_col : str, optional     Name of the longitude column. Default is 'lon'. crs : str or pyproj.CRS, optional     Coordinate Reference System of the geometry data. Default is 'EPSG:4326'.</p>"},{"location":"api/processing/#gigaspatial.processing.convert_to_geodataframe--returns","title":"Returns:","text":"<p>geopandas.GeoDataFrame     A GeoDataFrame containing the input data with a geometry column.</p>"},{"location":"api/processing/#gigaspatial.processing.convert_to_geodataframe--raises","title":"Raises:","text":"<p>TypeError     If input is not a pandas DataFrame. ValueError     If required columns are missing or contain invalid data.</p> Source code in <code>gigaspatial/processing/geo.py</code> <pre><code>def convert_to_geodataframe(\n    data: pd.DataFrame, lat_col: str = None, lon_col: str = None, crs=\"EPSG:4326\"\n) -&gt; gpd.GeoDataFrame:\n    \"\"\"\n    Convert a pandas DataFrame to a GeoDataFrame, either from latitude/longitude columns\n    or from a WKT geometry column.\n\n    Parameters:\n    ----------\n    data : pandas.DataFrame\n        Input DataFrame containing either lat/lon columns or a geometry column.\n    lat_col : str, optional\n        Name of the latitude column. Default is 'lat'.\n    lon_col : str, optional\n        Name of the longitude column. Default is 'lon'.\n    crs : str or pyproj.CRS, optional\n        Coordinate Reference System of the geometry data. Default is 'EPSG:4326'.\n\n    Returns:\n    -------\n    geopandas.GeoDataFrame\n        A GeoDataFrame containing the input data with a geometry column.\n\n    Raises:\n    ------\n    TypeError\n        If input is not a pandas DataFrame.\n    ValueError\n        If required columns are missing or contain invalid data.\n    \"\"\"\n\n    # Input validation\n    if not isinstance(data, pd.DataFrame):\n        raise TypeError(\"Input 'data' must be a pandas DataFrame\")\n\n    # Create a copy to avoid modifying the input\n    df = data.copy()\n\n    try:\n        if \"geometry\" not in df.columns:\n            # If column names not provided, try to detect them\n            if lat_col is None or lon_col is None:\n                try:\n                    detected_lat, detected_lon = detect_coordinate_columns(df)\n                    lat_col = lat_col or detected_lat\n                    lon_col = lon_col or detected_lon\n                except ValueError as e:\n                    raise ValueError(\n                        f\"Could not automatically detect coordinate columns and no \"\n                        f\"'geometry' column found. Error: {str(e)}\"\n                    )\n\n            # Validate latitude/longitude columns exist\n            if lat_col not in df.columns or lon_col not in df.columns:\n                raise ValueError(\n                    f\"Could not find columns: {lat_col} and/or {lon_col} in the DataFrame\"\n                )\n\n            # Check for missing values\n            if df[lat_col].isna().any() or df[lon_col].isna().any():\n                raise ValueError(\n                    f\"Missing values found in {lat_col} and/or {lon_col} columns\"\n                )\n\n            # Create geometry from lat/lon\n            geometry = gpd.points_from_xy(x=df[lon_col], y=df[lat_col])\n\n        else:\n            # Check if geometry column already contains valid geometries\n            if df[\"geometry\"].apply(lambda x: isinstance(x, base.BaseGeometry)).all():\n                geometry = df[\"geometry\"]\n            elif df[\"geometry\"].apply(lambda x: isinstance(x, str)).all():\n                # Convert WKT strings to geometry objects\n                geometry = df[\"geometry\"].apply(wkt.loads)\n            else:\n                raise ValueError(\n                    \"Invalid geometry format: contains mixed or unsupported types\"\n                )\n\n        # drop the WKT column if conversion was done\n        if (\n            \"geometry\" in df.columns\n            and not df[\"geometry\"]\n            .apply(lambda x: isinstance(x, base.BaseGeometry))\n            .all()\n        ):\n            df = df.drop(columns=[\"geometry\"])\n\n        return gpd.GeoDataFrame(df, geometry=geometry, crs=crs)\n\n    except Exception as e:\n        raise RuntimeError(f\"Error converting to GeoDataFrame: {str(e)}\")\n</code></pre>"},{"location":"api/processing/#gigaspatial.processing.detect_coordinate_columns","title":"<code>detect_coordinate_columns(data, lat_keywords=None, lon_keywords=None, case_sensitive=False)</code>","text":"<p>Detect latitude and longitude columns in a DataFrame using keyword matching.</p>"},{"location":"api/processing/#gigaspatial.processing.detect_coordinate_columns--parameters","title":"Parameters:","text":"<p>data : pandas.DataFrame     DataFrame to search for coordinate columns. lat_keywords : list of str, optional     Keywords for identifying latitude columns. If None, uses default keywords. lon_keywords : list of str, optional     Keywords for identifying longitude columns. If None, uses default keywords. case_sensitive : bool, optional     Whether to perform case-sensitive matching. Default is False.</p>"},{"location":"api/processing/#gigaspatial.processing.detect_coordinate_columns--returns","title":"Returns:","text":"<p>tuple[str, str]     Names of detected (latitude, longitude) columns.</p>"},{"location":"api/processing/#gigaspatial.processing.detect_coordinate_columns--raises","title":"Raises:","text":"<p>ValueError     If no unique pair of latitude/longitude columns can be found. TypeError     If input data is not a pandas DataFrame.</p> Source code in <code>gigaspatial/processing/geo.py</code> <pre><code>def detect_coordinate_columns(\n    data, lat_keywords=None, lon_keywords=None, case_sensitive=False\n) -&gt; Tuple[str, str]:\n    \"\"\"\n    Detect latitude and longitude columns in a DataFrame using keyword matching.\n\n    Parameters:\n    ----------\n    data : pandas.DataFrame\n        DataFrame to search for coordinate columns.\n    lat_keywords : list of str, optional\n        Keywords for identifying latitude columns. If None, uses default keywords.\n    lon_keywords : list of str, optional\n        Keywords for identifying longitude columns. If None, uses default keywords.\n    case_sensitive : bool, optional\n        Whether to perform case-sensitive matching. Default is False.\n\n    Returns:\n    -------\n    tuple[str, str]\n        Names of detected (latitude, longitude) columns.\n\n    Raises:\n    ------\n    ValueError\n        If no unique pair of latitude/longitude columns can be found.\n    TypeError\n        If input data is not a pandas DataFrame.\n    \"\"\"\n\n    # Default keywords if none provided\n    default_lat = [\n        \"latitude\",\n        \"lat\",\n        \"y\",\n        \"lat_\",\n        \"lat(s)\",\n        \"_lat\",\n        \"ylat\",\n        \"latitude_y\",\n    ]\n    default_lon = [\n        \"longitude\",\n        \"lon\",\n        \"long\",\n        \"x\",\n        \"lon_\",\n        \"lon(e)\",\n        \"long(e)\",\n        \"_lon\",\n        \"xlon\",\n        \"longitude_x\",\n    ]\n\n    lat_keywords = lat_keywords or default_lat\n    lon_keywords = lon_keywords or default_lon\n\n    # Input validation\n    if not isinstance(data, pd.DataFrame):\n        raise TypeError(\"Input must be a pandas DataFrame\")\n\n    if not data.columns.is_unique:\n        raise ValueError(\"DataFrame contains duplicate column names\")\n\n    def create_pattern(keywords):\n        \"\"\"Create regex pattern from keywords.\"\"\"\n        return \"|\".join(rf\"\\b{re.escape(keyword)}\\b\" for keyword in keywords)\n\n    def find_matching_columns(columns, pattern, case_sensitive) -&gt; List:\n        \"\"\"Find columns matching the pattern.\"\"\"\n        flags = 0 if case_sensitive else re.IGNORECASE\n        return [col for col in columns if re.search(pattern, col, flags=flags)]\n\n    try:\n        # Create patterns\n        lat_pattern = create_pattern(lat_keywords)\n        lon_pattern = create_pattern(lon_keywords)\n\n        # Find matching columns\n        lat_cols = find_matching_columns(data.columns, lat_pattern, case_sensitive)\n        lon_cols = find_matching_columns(data.columns, lon_pattern, case_sensitive)\n\n        # Remove any longitude matches from latitude columns and vice versa\n        lat_cols = [col for col in lat_cols if col not in lon_cols]\n        lon_cols = [col for col in lon_cols if col not in lat_cols]\n\n        # Detailed error messages based on what was found\n        if not lat_cols and not lon_cols:\n            columns_list = \"\\n\".join(f\"- {col}\" for col in data.columns)\n            raise ValueError(\n                f\"No latitude or longitude columns found. Available columns are:\\n{columns_list}\\n\"\n                f\"Consider adding more keywords or checking column names.\"\n            )\n\n        if not lat_cols:\n            found_lons = \", \".join(lon_cols)\n            raise ValueError(\n                f\"Found longitude columns ({found_lons}) but no latitude columns. \"\n                \"Check latitude keywords or column names.\"\n            )\n\n        if not lon_cols:\n            found_lats = \", \".join(lat_cols)\n            raise ValueError(\n                f\"Found latitude columns ({found_lats}) but no longitude columns. \"\n                \"Check longitude keywords or column names.\"\n            )\n\n        if len(lat_cols) &gt; 1 or len(lon_cols) &gt; 1:\n            raise ValueError(\n                f\"Multiple possible coordinate columns found:\\n\"\n                f\"Latitude candidates: {lat_cols}\\n\"\n                f\"Longitude candidates: {lon_cols}\\n\"\n                \"Please specify more precise keywords.\"\n            )\n\n        return lat_cols[0], lon_cols[0]\n\n    except Exception as e:\n        if isinstance(e, ValueError):\n            raise\n        raise RuntimeError(f\"Error detecting coordinate columns: {str(e)}\")\n</code></pre>"},{"location":"api/processing/#gigaspatial.processing.get_centroids","title":"<code>get_centroids(gdf)</code>","text":"<p>Calculate the centroids of a (Multi)Polygon GeoDataFrame.</p>"},{"location":"api/processing/#gigaspatial.processing.get_centroids--parameters","title":"Parameters:","text":"<p>gdf : geopandas.GeoDataFrame     GeoDataFrame containing (Multi)Polygon geometries.</p>"},{"location":"api/processing/#gigaspatial.processing.get_centroids--returns","title":"Returns:","text":"<p>geopandas.GeoDataFrame     A new GeoDataFrame with Point geometries representing the centroids.</p>"},{"location":"api/processing/#gigaspatial.processing.get_centroids--raises","title":"Raises:","text":"<p>ValueError     If the input GeoDataFrame does not contain (Multi)Polygon geometries.</p> Source code in <code>gigaspatial/processing/geo.py</code> <pre><code>def get_centroids(gdf: gpd.GeoDataFrame) -&gt; gpd.GeoDataFrame:\n    \"\"\"\n    Calculate the centroids of a (Multi)Polygon GeoDataFrame.\n\n    Parameters:\n    ----------\n    gdf : geopandas.GeoDataFrame\n        GeoDataFrame containing (Multi)Polygon geometries.\n\n    Returns:\n    -------\n    geopandas.GeoDataFrame\n        A new GeoDataFrame with Point geometries representing the centroids.\n\n    Raises:\n    ------\n    ValueError\n        If the input GeoDataFrame does not contain (Multi)Polygon geometries.\n    \"\"\"\n    # Validate input geometries\n    if not all(gdf.geometry.geom_type.isin([\"Polygon\", \"MultiPolygon\"])):\n        raise ValueError(\n            \"Input GeoDataFrame must contain only Polygon or MultiPolygon geometries.\"\n        )\n\n    # Calculate centroids\n    centroids = gdf.copy()\n    centroids[\"geometry\"] = centroids.geometry.centroid\n\n    return centroids\n</code></pre>"},{"location":"api/processing/#gigaspatial.processing.map_points_within_polygons","title":"<code>map_points_within_polygons(base_points_gdf, polygon_gdf)</code>","text":"<p>Maps whether each point in <code>base_points_gdf</code> is within any polygon in <code>polygon_gdf</code>.</p>"},{"location":"api/processing/#gigaspatial.processing.map_points_within_polygons--parameters","title":"Parameters:","text":"<p>base_points_gdf : geopandas.GeoDataFrame     GeoDataFrame containing point geometries to check. polygon_gdf : geopandas.GeoDataFrame     GeoDataFrame containing polygon geometries.</p>"},{"location":"api/processing/#gigaspatial.processing.map_points_within_polygons--returns","title":"Returns:","text":"<p>geopandas.GeoDataFrame     The <code>base_points_gdf</code> with an additional column <code>is_within</code> (True/False).</p>"},{"location":"api/processing/#gigaspatial.processing.map_points_within_polygons--raises","title":"Raises:","text":"<p>ValueError     If the geometries in either GeoDataFrame are invalid or not of the expected type.</p> Source code in <code>gigaspatial/processing/geo.py</code> <pre><code>def map_points_within_polygons(base_points_gdf, polygon_gdf):\n    \"\"\"\n    Maps whether each point in `base_points_gdf` is within any polygon in `polygon_gdf`.\n\n    Parameters:\n    ----------\n    base_points_gdf : geopandas.GeoDataFrame\n        GeoDataFrame containing point geometries to check.\n    polygon_gdf : geopandas.GeoDataFrame\n        GeoDataFrame containing polygon geometries.\n\n    Returns:\n    -------\n    geopandas.GeoDataFrame\n        The `base_points_gdf` with an additional column `is_within` (True/False).\n\n    Raises:\n    ------\n    ValueError\n        If the geometries in either GeoDataFrame are invalid or not of the expected type.\n    \"\"\"\n    # Validate input GeoDataFrames\n    if not all(base_points_gdf.geometry.geom_type == \"Point\"):\n        raise ValueError(\"`base_points_gdf` must contain only Point geometries.\")\n    if not all(polygon_gdf.geometry.geom_type.isin([\"Polygon\", \"MultiPolygon\"])):\n        raise ValueError(\n            \"`polygon_gdf` must contain only Polygon or MultiPolygon geometries.\"\n        )\n\n    if not base_points_gdf.crs == polygon_gdf.crs:\n        raise ValueError(\"CRS of `base_points_gdf` and `polygon_gdf` must match.\")\n\n    # Perform spatial join to check if points fall within any polygon\n    joined_gdf = gpd.sjoin(\n        base_points_gdf, polygon_gdf[[\"geometry\"]], how=\"left\", predicate=\"within\"\n    )\n\n    # Add `is_within` column to base_points_gdf\n    base_points_gdf[\"is_within\"] = base_points_gdf.index.isin(\n        set(joined_gdf.index[~joined_gdf.index_right.isna()])\n    )\n\n    return base_points_gdf\n</code></pre>"},{"location":"api/processing/#gigaspatial.processing.sample_multiple_tifs_by_coordinates","title":"<code>sample_multiple_tifs_by_coordinates(tif_processors, coordinate_list)</code>","text":"<p>Sample raster values from multiple TIFF files for given coordinates.</p> <p>Parameters: - tif_processors: List of TifProcessor instances. - coordinate_list: List of (x, y) coordinates.</p> <p>Returns: - A NumPy array of sampled values, taking the first non-nodata value encountered.</p> Source code in <code>gigaspatial/processing/tif_processor.py</code> <pre><code>def sample_multiple_tifs_by_coordinates(\n    tif_processors: List[TifProcessor], coordinate_list: List[Tuple[float, float]]\n):\n    \"\"\"\n    Sample raster values from multiple TIFF files for given coordinates.\n\n    Parameters:\n    - tif_processors: List of TifProcessor instances.\n    - coordinate_list: List of (x, y) coordinates.\n\n    Returns:\n    - A NumPy array of sampled values, taking the first non-nodata value encountered.\n    \"\"\"\n    sampled_values = np.full(len(coordinate_list), np.nan, dtype=np.float32)\n\n    for tp in tif_processors:\n        values = tp.sample_by_coordinates(coordinate_list=coordinate_list)\n\n        if tp.nodata is not None:\n            mask = (np.isnan(sampled_values)) &amp; (\n                values != tp.nodata\n            )  # Replace only NaNs\n        else:\n            mask = np.isnan(sampled_values)  # No explicit nodata, replace all NaNs\n\n        sampled_values[mask] = values[mask]  # Update only missing values\n\n    return sampled_values\n</code></pre>"},{"location":"api/processing/#gigaspatial.processing.sample_multiple_tifs_by_polygons","title":"<code>sample_multiple_tifs_by_polygons(tif_processors, polygon_list, stat='mean')</code>","text":"<p>Sample raster values from multiple TIFF files for polygons in a list and join the results.</p> <p>Parameters: - tif_processors: List of TifProcessor instances. - polygon_list: List of polygon geometries (can include MultiPolygons). - stat: Aggregation statistic to compute within each polygon (mean, median, sum, min, max).</p> <p>Returns: - A NumPy array of sampled values, taking the first non-nodata value encountered.</p> Source code in <code>gigaspatial/processing/tif_processor.py</code> <pre><code>def sample_multiple_tifs_by_polygons(\n    tif_processors: List[TifProcessor],\n    polygon_list: List[Union[Polygon, MultiPolygon]],\n    stat: str = \"mean\",\n) -&gt; np.ndarray:\n    \"\"\"\n    Sample raster values from multiple TIFF files for polygons in a list and join the results.\n\n    Parameters:\n    - tif_processors: List of TifProcessor instances.\n    - polygon_list: List of polygon geometries (can include MultiPolygons).\n    - stat: Aggregation statistic to compute within each polygon (mean, median, sum, min, max).\n\n    Returns:\n    - A NumPy array of sampled values, taking the first non-nodata value encountered.\n    \"\"\"\n    sampled_values = np.full(len(polygon_list), np.nan, dtype=np.float32)\n\n    for tp in tif_processors:\n        values = tp.sample_by_polygons(polygon_list=polygon_list, stat=stat)\n\n        mask = np.isnan(sampled_values)  # replace all NaNs\n\n        sampled_values[mask] = values[mask]  # Update only values with samapled value\n\n    return sampled_values\n</code></pre>"},{"location":"api/processing/#gigaspatial.processing.simplify_geometries","title":"<code>simplify_geometries(gdf, tolerance=0.01, preserve_topology=True, geometry_column='geometry')</code>","text":"<p>Simplify geometries in a GeoDataFrame to reduce file size and improve visualization performance.</p>"},{"location":"api/processing/#gigaspatial.processing.simplify_geometries--parameters","title":"Parameters","text":"<p>gdf : geopandas.GeoDataFrame     GeoDataFrame containing geometries to simplify. tolerance : float, optional     Tolerance for simplification. Larger values simplify more but reduce detail (default is 0.01). preserve_topology : bool, optional     Whether to preserve topology while simplifying. Preserving topology prevents invalid geometries (default is True). geometry_column : str, optional     Name of the column containing geometries (default is \"geometry\").</p>"},{"location":"api/processing/#gigaspatial.processing.simplify_geometries--returns","title":"Returns","text":"<p>geopandas.GeoDataFrame     A new GeoDataFrame with simplified geometries.</p>"},{"location":"api/processing/#gigaspatial.processing.simplify_geometries--raises","title":"Raises","text":"<p>ValueError     If the specified geometry column does not exist or contains invalid geometries. TypeError     If the geometry column does not contain valid geometries.</p>"},{"location":"api/processing/#gigaspatial.processing.simplify_geometries--examples","title":"Examples","text":"<p>Simplify geometries in a GeoDataFrame:</p> <p>simplified_gdf = simplify_geometries(gdf, tolerance=0.05)</p> Source code in <code>gigaspatial/processing/geo.py</code> <pre><code>def simplify_geometries(\n    gdf: gpd.GeoDataFrame,\n    tolerance: float = 0.01,\n    preserve_topology: bool = True,\n    geometry_column: str = \"geometry\",\n) -&gt; gpd.GeoDataFrame:\n    \"\"\"\n    Simplify geometries in a GeoDataFrame to reduce file size and improve visualization performance.\n\n    Parameters\n    ----------\n    gdf : geopandas.GeoDataFrame\n        GeoDataFrame containing geometries to simplify.\n    tolerance : float, optional\n        Tolerance for simplification. Larger values simplify more but reduce detail (default is 0.01).\n    preserve_topology : bool, optional\n        Whether to preserve topology while simplifying. Preserving topology prevents invalid geometries (default is True).\n    geometry_column : str, optional\n        Name of the column containing geometries (default is \"geometry\").\n\n    Returns\n    -------\n    geopandas.GeoDataFrame\n        A new GeoDataFrame with simplified geometries.\n\n    Raises\n    ------\n    ValueError\n        If the specified geometry column does not exist or contains invalid geometries.\n    TypeError\n        If the geometry column does not contain valid geometries.\n\n    Examples\n    --------\n    Simplify geometries in a GeoDataFrame:\n    &gt;&gt;&gt; simplified_gdf = simplify_geometries(gdf, tolerance=0.05)\n    \"\"\"\n\n    # Check if the specified geometry column exists\n    if geometry_column not in gdf.columns:\n        raise ValueError(\n            f\"Geometry column '{geometry_column}' not found in the GeoDataFrame.\"\n        )\n\n    # Check if the specified column contains geometries\n    if not gpd.GeoSeries(gdf[geometry_column]).is_valid.all():\n        raise TypeError(\n            f\"Geometry column '{geometry_column}' contains invalid geometries.\"\n        )\n\n    # Simplify geometries (non-destructive)\n    gdf_simplified = gdf.copy()\n    gdf_simplified[geometry_column] = gdf_simplified[geometry_column].simplify(\n        tolerance=tolerance, preserve_topology=preserve_topology\n    )\n\n    return gdf_simplified\n</code></pre>"},{"location":"api/processing/#gigaspatial.processing.geo","title":"<code>geo</code>","text":""},{"location":"api/processing/#gigaspatial.processing.geo.add_area_in_meters","title":"<code>add_area_in_meters(gdf, area_column_name='area_in_meters')</code>","text":"<p>Calculate the area of (Multi)Polygon geometries in square meters and add it as a new column.</p>"},{"location":"api/processing/#gigaspatial.processing.geo.add_area_in_meters--parameters","title":"Parameters:","text":"<p>gdf : geopandas.GeoDataFrame     GeoDataFrame containing (Multi)Polygon geometries. area_column_name : str, optional     Name of the new column to store the area values. Default is \"area_m2\".</p>"},{"location":"api/processing/#gigaspatial.processing.geo.add_area_in_meters--returns","title":"Returns:","text":"<p>geopandas.GeoDataFrame     The input GeoDataFrame with an additional column for the area in square meters.</p>"},{"location":"api/processing/#gigaspatial.processing.geo.add_area_in_meters--raises","title":"Raises:","text":"<p>ValueError     If the input GeoDataFrame does not contain (Multi)Polygon geometries.</p> Source code in <code>gigaspatial/processing/geo.py</code> <pre><code>def add_area_in_meters(\n    gdf: gpd.GeoDataFrame, area_column_name: str = \"area_in_meters\"\n) -&gt; gpd.GeoDataFrame:\n    \"\"\"\n    Calculate the area of (Multi)Polygon geometries in square meters and add it as a new column.\n\n    Parameters:\n    ----------\n    gdf : geopandas.GeoDataFrame\n        GeoDataFrame containing (Multi)Polygon geometries.\n    area_column_name : str, optional\n        Name of the new column to store the area values. Default is \"area_m2\".\n\n    Returns:\n    -------\n    geopandas.GeoDataFrame\n        The input GeoDataFrame with an additional column for the area in square meters.\n\n    Raises:\n    ------\n    ValueError\n        If the input GeoDataFrame does not contain (Multi)Polygon geometries.\n    \"\"\"\n    # Validate input geometries\n    if not all(gdf.geometry.geom_type.isin([\"Polygon\", \"MultiPolygon\"])):\n        raise ValueError(\n            \"Input GeoDataFrame must contain only Polygon or MultiPolygon geometries.\"\n        )\n\n    # Create a copy of the GeoDataFrame to avoid modifying the original\n    gdf_with_area = gdf.copy()\n\n    # Calculate the UTM CRS for accurate area calculation\n    utm_crs = gdf_with_area.estimate_utm_crs()\n\n    # Transform to UTM CRS and calculate the area in square meters\n    gdf_with_area[area_column_name] = gdf_with_area.to_crs(utm_crs).geometry.area\n\n    return gdf_with_area\n</code></pre>"},{"location":"api/processing/#gigaspatial.processing.geo.add_spatial_jitter","title":"<code>add_spatial_jitter(df, columns=['latitude', 'longitude'], amount=0.0001, seed=None, copy=True)</code>","text":"<p>Add random jitter to duplicated geographic coordinates to create slight separation between overlapping points.</p>"},{"location":"api/processing/#gigaspatial.processing.geo.add_spatial_jitter--parameters","title":"Parameters:","text":"<p>df : pandas.DataFrame     DataFrame containing geographic coordinates. columns : list of str, optional     Column names containing coordinates to jitter. Default is ['latitude', 'longitude']. amount : float or dict, optional     Amount of jitter to add. If float, same amount used for all columns.     If dict, specify amount per column, e.g., {'lat': 0.0001, 'lon': 0.0002}.     Default is 0.0001 (approximately 11 meters at the equator). seed : int, optional     Random seed for reproducibility. Default is None. copy : bool, optional     Whether to create a copy of the input DataFrame. Default is True.</p>"},{"location":"api/processing/#gigaspatial.processing.geo.add_spatial_jitter--returns","title":"Returns:","text":"<p>pandas.DataFrame     DataFrame with jittered coordinates for previously duplicated points.</p>"},{"location":"api/processing/#gigaspatial.processing.geo.add_spatial_jitter--raises","title":"Raises:","text":"<p>ValueError     If columns don't exist or jitter amount is invalid. TypeError     If input types are incorrect.</p> Source code in <code>gigaspatial/processing/geo.py</code> <pre><code>def add_spatial_jitter(\n    df: pd.DataFrame,\n    columns: List[str] = [\"latitude\", \"longitude\"],\n    amount: float = 0.0001,\n    seed=None,\n    copy=True,\n) -&gt; pd.DataFrame:\n    \"\"\"\n    Add random jitter to duplicated geographic coordinates to create slight separation\n    between overlapping points.\n\n    Parameters:\n    ----------\n    df : pandas.DataFrame\n        DataFrame containing geographic coordinates.\n    columns : list of str, optional\n        Column names containing coordinates to jitter. Default is ['latitude', 'longitude'].\n    amount : float or dict, optional\n        Amount of jitter to add. If float, same amount used for all columns.\n        If dict, specify amount per column, e.g., {'lat': 0.0001, 'lon': 0.0002}.\n        Default is 0.0001 (approximately 11 meters at the equator).\n    seed : int, optional\n        Random seed for reproducibility. Default is None.\n    copy : bool, optional\n        Whether to create a copy of the input DataFrame. Default is True.\n\n    Returns:\n    -------\n    pandas.DataFrame\n        DataFrame with jittered coordinates for previously duplicated points.\n\n    Raises:\n    ------\n    ValueError\n        If columns don't exist or jitter amount is invalid.\n    TypeError\n        If input types are incorrect.\n    \"\"\"\n\n    # Input validation\n    if not isinstance(df, pd.DataFrame):\n        raise TypeError(\"Input must be a pandas DataFrame\")\n\n    if not all(col in df.columns for col in columns):\n        raise ValueError(f\"Not all columns {columns} found in DataFrame\")\n\n    # Handle jitter amounts\n    if isinstance(amount, (int, float)):\n        if amount &lt;= 0:\n            raise ValueError(\"Jitter amount must be positive\")\n        jitter_amounts = {col: amount for col in columns}\n    elif isinstance(amount, dict):\n        if not all(col in amount for col in columns):\n            raise ValueError(\"Must specify jitter amount for each column\")\n        if not all(amt &gt; 0 for amt in amount.values()):\n            raise ValueError(\"All jitter amounts must be positive\")\n        jitter_amounts = amount\n    else:\n        raise TypeError(\"amount must be a number or dictionary\")\n\n    # Create copy if requested\n    df_work = df.copy() if copy else df\n\n    # Set random seed if provided\n    if seed is not None:\n        np.random.seed(seed)\n\n    try:\n        # Find duplicated coordinates\n        duplicate_mask = df_work.duplicated(subset=columns, keep=False)\n        n_duplicates = duplicate_mask.sum()\n\n        if n_duplicates &gt; 0:\n            # Add jitter to each column separately\n            for col in columns:\n                jitter = np.random.uniform(\n                    low=-jitter_amounts[col],\n                    high=jitter_amounts[col],\n                    size=n_duplicates,\n                )\n                df_work.loc[duplicate_mask, col] += jitter\n\n            # Validate results (ensure no remaining duplicates)\n            if df_work.duplicated(subset=columns, keep=False).any():\n                # If duplicates remain, recursively add more jitter\n                df_work = add_spatial_jitter(\n                    df_work,\n                    columns=columns,\n                    amount={col: amt * 2 for col, amt in jitter_amounts.items()},\n                    seed=seed,\n                    copy=False,\n                )\n\n        return df_work\n\n    except Exception as e:\n        raise RuntimeError(f\"Error during jittering operation: {str(e)}\")\n</code></pre>"},{"location":"api/processing/#gigaspatial.processing.geo.aggregate_points_to_zones","title":"<code>aggregate_points_to_zones(points, zones, value_columns=None, aggregation='count', point_zone_predicate='within', zone_id_column='zone_id', output_suffix='', drop_geometry=False)</code>","text":"<p>Aggregate point data to zones with flexible aggregation methods.</p> <p>Parameters:</p> Name Type Description Default <code>points</code> <code>Union[DataFrame, GeoDataFrame]</code> <p>Point data to aggregate</p> required <code>zones</code> <code>GeoDataFrame</code> <p>Zones to aggregate points to</p> required <code>value_columns</code> <code>Optional[Union[str, List[str]]]</code> <p>Column(s) containing values to aggregate If None, only counts will be performed.</p> <code>None</code> <code>aggregation</code> <code>Union[str, Dict[str, str]]</code> <p>Aggregation method(s) to use: - Single string: Use same method for all columns (\"count\", \"mean\", \"sum\", \"min\", \"max\") - Dict: Map column names to aggregation methods</p> <code>'count'</code> <code>point_zone_predicate</code> <code>str</code> <p>Spatial predicate for point-to-zone relationship Options: \"within\", \"intersects\", \"contains\"</p> <code>'within'</code> <code>zone_id_column</code> <code>str</code> <p>Column in zones containing zone identifiers</p> <code>'zone_id'</code> <code>output_suffix</code> <code>str</code> <p>Suffix to add to output column names</p> <code>''</code> <code>drop_geometry</code> <code>bool</code> <p>Whether to drop the geometry column from output</p> <code>False</code> <p>Returns:</p> Type Description <code>GeoDataFrame</code> <p>gpd.GeoDataFrame: Zones with aggregated point values</p> Example <p>poi_counts = aggregate_points_to_zones(pois, zones, aggregation=\"count\") poi_value_mean = aggregate_points_to_zones( ...     pois, zones, value_columns=\"score\", aggregation=\"mean\" ... ) poi_multiple = aggregate_points_to_zones( ...     pois, zones, ...     value_columns=[\"score\", \"visits\"], ...     aggregation={\"score\": \"mean\", \"visits\": \"sum\"} ... )</p> Source code in <code>gigaspatial/processing/geo.py</code> <pre><code>def aggregate_points_to_zones(\n    points: Union[pd.DataFrame, gpd.GeoDataFrame],\n    zones: gpd.GeoDataFrame,\n    value_columns: Optional[Union[str, List[str]]] = None,\n    aggregation: Union[str, Dict[str, str]] = \"count\",\n    point_zone_predicate: str = \"within\",\n    zone_id_column: str = \"zone_id\",\n    output_suffix: str = \"\",\n    drop_geometry: bool = False,\n) -&gt; gpd.GeoDataFrame:\n    \"\"\"\n    Aggregate point data to zones with flexible aggregation methods.\n\n    Args:\n        points (Union[pd.DataFrame, gpd.GeoDataFrame]): Point data to aggregate\n        zones (gpd.GeoDataFrame): Zones to aggregate points to\n        value_columns (Optional[Union[str, List[str]]]): Column(s) containing values to aggregate\n            If None, only counts will be performed.\n        aggregation (Union[str, Dict[str, str]]): Aggregation method(s) to use:\n            - Single string: Use same method for all columns (\"count\", \"mean\", \"sum\", \"min\", \"max\")\n            - Dict: Map column names to aggregation methods\n        point_zone_predicate (str): Spatial predicate for point-to-zone relationship\n            Options: \"within\", \"intersects\", \"contains\"\n        zone_id_column (str): Column in zones containing zone identifiers\n        output_suffix (str): Suffix to add to output column names\n        drop_geometry (bool): Whether to drop the geometry column from output\n\n    Returns:\n        gpd.GeoDataFrame: Zones with aggregated point values\n\n    Example:\n        &gt;&gt;&gt; poi_counts = aggregate_points_to_zones(pois, zones, aggregation=\"count\")\n        &gt;&gt;&gt; poi_value_mean = aggregate_points_to_zones(\n        ...     pois, zones, value_columns=\"score\", aggregation=\"mean\"\n        ... )\n        &gt;&gt;&gt; poi_multiple = aggregate_points_to_zones(\n        ...     pois, zones,\n        ...     value_columns=[\"score\", \"visits\"],\n        ...     aggregation={\"score\": \"mean\", \"visits\": \"sum\"}\n        ... )\n    \"\"\"\n    # Input validation\n    if not isinstance(zones, gpd.GeoDataFrame):\n        raise TypeError(\"zones must be a GeoDataFrame\")\n\n    if zone_id_column not in zones.columns:\n        raise ValueError(f\"Zone ID column '{zone_id_column}' not found in zones\")\n\n    # Convert points to GeoDataFrame if necessary\n    if not isinstance(points, gpd.GeoDataFrame):\n        points_gdf = convert_to_geodataframe(points)\n    else:\n        points_gdf = points.copy()\n\n    # Ensure CRS match\n    if points_gdf.crs != zones.crs:\n        points_gdf = points_gdf.to_crs(zones.crs)\n\n    # Handle value columns\n    if value_columns is not None:\n        if isinstance(value_columns, str):\n            value_columns = [value_columns]\n\n        # Validate that all value columns exist\n        missing_cols = [col for col in value_columns if col not in points_gdf.columns]\n        if missing_cols:\n            raise ValueError(f\"Value columns not found in points data: {missing_cols}\")\n\n    # Handle aggregation method\n    agg_funcs = {}\n\n    if isinstance(aggregation, str):\n        if aggregation == \"count\":\n            # Special case for count (doesn't need value columns)\n            agg_funcs[\"__count\"] = \"count\"\n        elif value_columns is not None:\n            # Apply the same aggregation to all value columns\n            agg_funcs = {col: aggregation for col in value_columns}\n        else:\n            raise ValueError(\n                \"Value columns must be specified for aggregation methods other than 'count'\"\n            )\n    elif isinstance(aggregation, dict):\n        # Validate dictionary keys\n        if value_columns is None:\n            raise ValueError(\n                \"Value columns must be specified when using a dictionary of aggregation methods\"\n            )\n\n        missing_aggs = [col for col in value_columns if col not in aggregation]\n        extra_aggs = [col for col in aggregation if col not in value_columns]\n\n        if missing_aggs:\n            raise ValueError(f\"Missing aggregation methods for columns: {missing_aggs}\")\n        if extra_aggs:\n            raise ValueError(\n                f\"Aggregation methods specified for non-existent columns: {extra_aggs}\"\n            )\n\n        agg_funcs = aggregation\n    else:\n        raise TypeError(\"aggregation must be a string or dictionary\")\n\n    # Create a copy of the zones\n    result = zones.copy()\n\n    # Spatial join\n    joined = gpd.sjoin(points_gdf, zones, how=\"inner\", predicate=point_zone_predicate)\n\n    # Perform aggregation\n    if \"geometry\" in joined.columns and not all(\n        value == \"count\" for value in agg_funcs.values()\n    ):\n        # Drop geometry for non-count aggregations to avoid errors\n        joined = joined.drop(columns=[\"geometry\"])\n\n    if \"__count\" in agg_funcs:\n        # Count points per zone\n        counts = (\n            joined.groupby(zone_id_column)\n            .size()\n            .reset_index(name=f\"point_count{output_suffix}\")\n        )\n        result = result.merge(counts, on=zone_id_column, how=\"left\")\n        result[f\"point_count{output_suffix}\"] = (\n            result[f\"point_count{output_suffix}\"].fillna(0).astype(int)\n        )\n    else:\n        # Aggregate values\n        aggregated = joined.groupby(zone_id_column).agg(agg_funcs).reset_index()\n\n        # Rename columns to include aggregation method\n        if len(value_columns) &gt; 0:\n            # Handle MultiIndex columns from pandas aggregation\n            if isinstance(aggregated.columns, pd.MultiIndex):\n                aggregated.columns = [\n                    (\n                        f\"{col[0]}_{col[1]}{output_suffix}\"\n                        if col[0] != zone_id_column\n                        else zone_id_column\n                    )\n                    for col in aggregated.columns\n                ]\n\n            # Merge back to zones\n            result = result.merge(aggregated, on=zone_id_column, how=\"left\")\n\n            # Fill NaN values with zeros\n            for col in result.columns:\n                if (\n                    col != zone_id_column\n                    and col != \"geometry\"\n                    and pd.api.types.is_numeric_dtype(result[col])\n                ):\n                    result[col] = result[col].fillna(0)\n\n    if drop_geometry:\n        result = result.drop(columns=[\"geometry\"])\n        return result\n\n    return result\n</code></pre>"},{"location":"api/processing/#gigaspatial.processing.geo.aggregate_polygons_to_zones","title":"<code>aggregate_polygons_to_zones(polygons, zones, value_columns, aggregation='sum', area_weighted=True, zone_id_column='zone_id', output_suffix='', drop_geometry=False)</code>","text":"<p>Aggregate polygon data to zones with area-weighted values.</p> <p>This function maps polygon data to zones, weighting values by the fractional area of overlap between polygons and zones.</p> <p>Parameters:</p> Name Type Description Default <code>polygons</code> <code>Union[DataFrame, GeoDataFrame]</code> <p>Polygon data to aggregate</p> required <code>zones</code> <code>GeoDataFrame</code> <p>Zones to aggregate polygons to</p> required <code>value_columns</code> <code>Union[str, List[str]]</code> <p>Column(s) containing values to aggregate</p> required <code>aggregation</code> <code>Union[str, Dict[str, str]]</code> <p>Aggregation method(s) to use: - Single string: Use same method for all columns (\"sum\", \"mean\", \"max\", etc.) - Dict: Map column names to aggregation methods</p> <code>'sum'</code> <code>area_weighted</code> <code>bool</code> <p>Whether to weight values by fractional area overlap If False, values are not weighted before aggregation</p> <code>True</code> <code>zone_id_column</code> <code>str</code> <p>Column in zones containing zone identifiers</p> <code>'zone_id'</code> <code>output_suffix</code> <code>str</code> <p>Suffix to add to output column names</p> <code>''</code> <code>drop_geometry</code> <code>bool</code> <p>Whether to drop the geometry column from output</p> <code>False</code> <p>Returns:</p> Type Description <code>GeoDataFrame</code> <p>gpd.GeoDataFrame: Zones with aggregated polygon values</p> Example <p>landuse_stats = aggregate_polygons_to_zones( ...     landuse_polygons, ...     grid_zones, ...     value_columns=[\"area\", \"population\"], ...     aggregation=\"sum\" ... )</p> Source code in <code>gigaspatial/processing/geo.py</code> <pre><code>def aggregate_polygons_to_zones(\n    polygons: Union[pd.DataFrame, gpd.GeoDataFrame],\n    zones: gpd.GeoDataFrame,\n    value_columns: Union[str, List[str]],\n    aggregation: Union[str, Dict[str, str]] = \"sum\",\n    area_weighted: bool = True,\n    zone_id_column: str = \"zone_id\",\n    output_suffix: str = \"\",\n    drop_geometry: bool = False,\n) -&gt; gpd.GeoDataFrame:\n    \"\"\"\n    Aggregate polygon data to zones with area-weighted values.\n\n    This function maps polygon data to zones, weighting values by the\n    fractional area of overlap between polygons and zones.\n\n    Args:\n        polygons (Union[pd.DataFrame, gpd.GeoDataFrame]): Polygon data to aggregate\n        zones (gpd.GeoDataFrame): Zones to aggregate polygons to\n        value_columns (Union[str, List[str]]): Column(s) containing values to aggregate\n        aggregation (Union[str, Dict[str, str]]): Aggregation method(s) to use:\n            - Single string: Use same method for all columns (\"sum\", \"mean\", \"max\", etc.)\n            - Dict: Map column names to aggregation methods\n        area_weighted (bool): Whether to weight values by fractional area overlap\n            If False, values are not weighted before aggregation\n        zone_id_column (str): Column in zones containing zone identifiers\n        output_suffix (str): Suffix to add to output column names\n        drop_geometry (bool): Whether to drop the geometry column from output\n\n    Returns:\n        gpd.GeoDataFrame: Zones with aggregated polygon values\n\n    Example:\n        &gt;&gt;&gt; landuse_stats = aggregate_polygons_to_zones(\n        ...     landuse_polygons,\n        ...     grid_zones,\n        ...     value_columns=[\"area\", \"population\"],\n        ...     aggregation=\"sum\"\n        ... )\n    \"\"\"\n    # Input validation\n    if not isinstance(zones, gpd.GeoDataFrame):\n        raise TypeError(\"zones must be a GeoDataFrame\")\n\n    if zone_id_column not in zones.columns:\n        raise ValueError(f\"Zone ID column '{zone_id_column}' not found in zones\")\n\n    # Convert polygons to GeoDataFrame if necessary\n    if not isinstance(polygons, gpd.GeoDataFrame):\n        try:\n            polygons_gdf = convert_to_geodataframe(polygons)\n        except:\n            raise TypeError(\"polygons must be a GeoDataFrame or convertible to one\")\n    else:\n        polygons_gdf = polygons.copy()\n\n    # Validate geometry types\n    non_polygon_geoms = [\n        geom_type\n        for geom_type in polygons_gdf.geometry.geom_type.unique()\n        if geom_type not in [\"Polygon\", \"MultiPolygon\"]\n    ]\n    if non_polygon_geoms:\n        raise ValueError(\n            f\"Input contains non-polygon geometries: {non_polygon_geoms}. \"\n            \"Use aggregate_points_to_zones for point data.\"\n        )\n\n    # Process value columns\n    if isinstance(value_columns, str):\n        value_columns = [value_columns]\n\n    # Validate that all value columns exist\n    missing_cols = [col for col in value_columns if col not in polygons_gdf.columns]\n    if missing_cols:\n        raise ValueError(f\"Value columns not found in polygons data: {missing_cols}\")\n\n    # Ensure CRS match\n    if polygons_gdf.crs != zones.crs:\n        polygons_gdf = polygons_gdf.to_crs(zones.crs)\n\n    # Handle aggregation method\n    if isinstance(aggregation, str):\n        agg_funcs = {col: aggregation for col in value_columns}\n    elif isinstance(aggregation, dict):\n        # Validate dictionary keys\n        missing_aggs = [col for col in value_columns if col not in aggregation]\n        extra_aggs = [col for col in aggregation if col not in value_columns]\n\n        if missing_aggs:\n            raise ValueError(f\"Missing aggregation methods for columns: {missing_aggs}\")\n        if extra_aggs:\n            raise ValueError(\n                f\"Aggregation methods specified for non-existent columns: {extra_aggs}\"\n            )\n\n        agg_funcs = aggregation\n    else:\n        raise TypeError(\"aggregation must be a string or dictionary\")\n\n    # Create a copy of the zones\n    result = zones.copy()\n\n    if area_weighted:\n        # Use area-weighted aggregation with polygon overlay\n        try:\n            # Compute UTM CRS for accurate area calculations\n            overlay_utm_crs = polygons_gdf.estimate_utm_crs()\n\n            # Prepare polygons for overlay\n            polygons_utm = polygons_gdf.to_crs(overlay_utm_crs)\n            polygons_utm[\"orig_area\"] = polygons_utm.area\n\n            # Keep only necessary columns\n            overlay_cols = value_columns + [\"geometry\", \"orig_area\"]\n            overlay_gdf = polygons_utm[overlay_cols].copy()\n\n            # Prepare zones for overlay\n            zones_utm = zones.to_crs(overlay_utm_crs)\n\n            # Perform the spatial overlay\n            gdf_overlayed = gpd.overlay(\n                overlay_gdf, zones_utm[[zone_id_column, \"geometry\"]], how=\"intersection\"\n            )\n\n            # Calculate fractional areas\n            gdf_overlayed[\"intersection_area\"] = gdf_overlayed.area\n            gdf_overlayed[\"area_fraction\"] = (\n                gdf_overlayed[\"intersection_area\"] / gdf_overlayed[\"orig_area\"]\n            )\n\n            # Apply area weighting to value columns\n            for col in value_columns:\n                gdf_overlayed[col] = gdf_overlayed[col] * gdf_overlayed[\"area_fraction\"]\n\n            # Aggregate by zone ID\n            aggregated = gdf_overlayed.groupby(zone_id_column)[value_columns].agg(\n                agg_funcs\n            )\n\n            # Handle column naming for multi-level index\n            if isinstance(aggregated.columns, pd.MultiIndex):\n                aggregated.columns = [\n                    f\"{col[0]}_{col[1]}{output_suffix}\" for col in aggregated.columns\n                ]\n\n            # Reset index\n            aggregated = aggregated.reset_index()\n\n            # Merge aggregated values back to the zones\n            result = result.merge(aggregated, on=zone_id_column, how=\"left\")\n\n            # Fill NaN values with zeros\n            for col in result.columns:\n                if (\n                    col != zone_id_column\n                    and col != \"geometry\"\n                    and pd.api.types.is_numeric_dtype(result[col])\n                ):\n                    result[col] = result[col].fillna(0)\n\n        except Exception as e:\n            raise RuntimeError(f\"Error during area-weighted aggregation: {e}\")\n\n    else:\n        # Non-weighted aggregation - simpler approach\n        # Perform spatial join\n        joined = gpd.sjoin(polygons_gdf, zones, how=\"inner\", predicate=\"intersects\")\n\n        # Remove geometry column for aggregation\n        if \"geometry\" in joined.columns:\n            joined = joined.drop(columns=[\"geometry\"])\n\n        # Group by zone ID and aggregate\n        aggregated = joined.groupby(zone_id_column)[value_columns].agg(agg_funcs)\n\n        # Handle column naming for multi-level index\n        if isinstance(aggregated.columns, pd.MultiIndex):\n            aggregated.columns = [\n                f\"{col[0]}_{col[1]}{output_suffix}\" for col in aggregated.columns\n            ]\n\n        # Reset index and merge back to zones\n        aggregated = aggregated.reset_index()\n        result = result.merge(aggregated, on=zone_id_column, how=\"left\")\n\n        # Fill NaN values with zeros\n        for col in result.columns:\n            if (\n                col != zone_id_column\n                and col != \"geometry\"\n                and pd.api.types.is_numeric_dtype(result[col])\n            ):\n                result[col] = result[col].fillna(0)\n\n    if drop_geometry:\n        result = result.drop(columns=[\"geometry\"])\n\n    return result\n</code></pre>"},{"location":"api/processing/#gigaspatial.processing.geo.annotate_with_admin_regions","title":"<code>annotate_with_admin_regions(gdf, country_code, data_store=None, admin_id_column_suffix='_giga')</code>","text":"<p>Annotate a GeoDataFrame with administrative region information.</p> <p>Performs a spatial join between the input points and administrative boundaries at levels 1 and 2, resolving conflicts when points intersect multiple admin regions.</p> <p>Parameters:</p> Name Type Description Default <code>gdf</code> <code>GeoDataFrame</code> <p>GeoDataFrame containing points to annotate</p> required <code>country_code</code> <code>str</code> <p>Country code for administrative boundaries</p> required <code>data_store</code> <code>Optional[DataStore]</code> <p>Optional DataStore for loading admin boundary data</p> <code>None</code> <p>Returns:</p> Type Description <code>GeoDataFrame</code> <p>GeoDataFrame with added administrative region columns</p> Source code in <code>gigaspatial/processing/geo.py</code> <pre><code>def annotate_with_admin_regions(\n    gdf: gpd.GeoDataFrame,\n    country_code: str,\n    data_store: Optional[DataStore] = None,\n    admin_id_column_suffix=\"_giga\",\n) -&gt; gpd.GeoDataFrame:\n    \"\"\"\n    Annotate a GeoDataFrame with administrative region information.\n\n    Performs a spatial join between the input points and administrative boundaries\n    at levels 1 and 2, resolving conflicts when points intersect multiple admin regions.\n\n    Args:\n        gdf: GeoDataFrame containing points to annotate\n        country_code: Country code for administrative boundaries\n        data_store: Optional DataStore for loading admin boundary data\n\n    Returns:\n        GeoDataFrame with added administrative region columns\n    \"\"\"\n    from gigaspatial.handlers.boundaries import AdminBoundaries\n\n    if not isinstance(gdf, gpd.GeoDataFrame):\n        raise TypeError(\"gdf must be a GeoDataFrame\")\n\n    if gdf.empty:\n        LOGGER.warning(\"Empty GeoDataFrame provided, returning as-is\")\n        return gdf\n\n    # read country admin data\n    admin1_data = AdminBoundaries.create(\n        country_code=country_code, admin_level=1, data_store=data_store\n    ).to_geodataframe()\n\n    admin1_data.rename(\n        columns={\"id\": f\"admin1_id{admin_id_column_suffix}\", \"name\": \"admin1\"},\n        inplace=True,\n    )\n    admin1_data.drop(columns=[\"name_en\", \"parent_id\", \"country_code\"], inplace=True)\n\n    admin2_data = AdminBoundaries.create(\n        country_code=country_code, admin_level=2, data_store=data_store\n    ).to_geodataframe()\n\n    admin2_data.rename(\n        columns={\n            \"id\": f\"admin2_id{admin_id_column_suffix}\",\n            \"parent_id\": f\"admin1_id{admin_id_column_suffix}\",\n            \"name\": \"admin2\",\n        },\n        inplace=True,\n    )\n    admin2_data.drop(columns=[\"name_en\", \"country_code\"], inplace=True)\n\n    # Join dataframes based on 'admin1_id_giga'\n    admin_data = admin2_data.merge(\n        admin1_data[[f\"admin1_id{admin_id_column_suffix}\", \"admin1\", \"geometry\"]],\n        left_on=f\"admin1_id{admin_id_column_suffix}\",\n        right_on=f\"admin1_id{admin_id_column_suffix}\",\n        how=\"outer\",\n    )\n\n    admin_data[\"geometry\"] = admin_data.apply(\n        lambda x: x.geometry_x if x.geometry_x else x.geometry_y, axis=1\n    )\n\n    admin_data = gpd.GeoDataFrame(\n        admin_data.drop(columns=[\"geometry_x\", \"geometry_y\"]),\n        geometry=\"geometry\",\n        crs=4326,\n    )\n\n    admin_data[\"admin2\"].fillna(\"Unknown\", inplace=True)\n    admin_data[f\"admin2_id{admin_id_column_suffix}\"] = admin_data[\n        f\"admin2_id{admin_id_column_suffix}\"\n    ].replace({np.nan: None})\n\n    if gdf.crs is None:\n        LOGGER.warning(\"Input GeoDataFrame has no CRS, assuming EPSG:4326\")\n        gdf.set_crs(epsg=4326, inplace=True)\n    elif gdf.crs != \"EPSG:4326\":\n        LOGGER.info(f\"Reprojecting from {gdf.crs} to EPSG:4326\")\n        gdf = gdf.to_crs(epsg=4326)\n\n    # spatial join gdf to admins\n    gdf_w_admins = gdf.copy().sjoin(\n        admin_data,\n        how=\"left\",\n        predicate=\"intersects\",\n    )\n\n    # Check for duplicates caused by points intersecting multiple polygons\n    if len(gdf_w_admins) != len(gdf):\n        LOGGER.warning(\n            \"Some points intersect multiple administrative boundaries. Resolving conflicts...\"\n        )\n\n        # Group by original index and select the closest admin area for ties\n        gdf_w_admins[\"distance\"] = gdf_w_admins.apply(\n            lambda row: row.geometry.distance(\n                admin_data.loc[row.index_right, \"geometry\"].centroid\n            ),\n            axis=1,\n        )\n\n        # For points with multiple matches, keep the closest polygon\n        gdf_w_admins = gdf_w_admins.loc[\n            gdf_w_admins.groupby(gdf.index)[\"distance\"].idxmin()\n        ].drop(columns=\"distance\")\n\n    # Drop unnecessary columns and reset the index\n    gdf_w_admins = gdf_w_admins.drop(columns=\"index_right\").reset_index(drop=True)\n\n    return gdf_w_admins\n</code></pre>"},{"location":"api/processing/#gigaspatial.processing.geo.buffer_geodataframe","title":"<code>buffer_geodataframe(gdf, buffer_distance_meters, cap_style='round', copy=True)</code>","text":"<p>Buffers a GeoDataFrame with a given buffer distance in meters.</p> <ul> <li>gdf : geopandas.GeoDataFrame     The GeoDataFrame to be buffered.</li> <li>buffer_distance_meters : float     The buffer distance in meters.</li> <li>cap_style : str, optional     The style of caps. round, flat, square. Default is round.</li> </ul> <ul> <li>geopandas.GeoDataFrame     The buffered GeoDataFrame.</li> </ul> Source code in <code>gigaspatial/processing/geo.py</code> <pre><code>def buffer_geodataframe(\n    gdf: gpd.GeoDataFrame,\n    buffer_distance_meters: float,\n    cap_style: Literal[\"round\", \"square\", \"flat\"] = \"round\",\n    copy=True,\n) -&gt; gpd.GeoDataFrame:\n    \"\"\"\n    Buffers a GeoDataFrame with a given buffer distance in meters.\n\n    Parameters:\n    - gdf : geopandas.GeoDataFrame\n        The GeoDataFrame to be buffered.\n    - buffer_distance_meters : float\n        The buffer distance in meters.\n    - cap_style : str, optional\n        The style of caps. round, flat, square. Default is round.\n\n    Returns:\n    - geopandas.GeoDataFrame\n        The buffered GeoDataFrame.\n    \"\"\"\n\n    # Input validation\n    if not isinstance(gdf, gpd.GeoDataFrame):\n        raise TypeError(\"Input must be a GeoDataFrame\")\n\n    if not isinstance(buffer_distance_meters, (float, int)):\n        raise TypeError(\"Buffer distance must be a number\")\n\n    if cap_style not in [\"round\", \"square\", \"flat\"]:\n        raise ValueError(\"cap_style must be round, flat or square.\")\n\n    if gdf.crs is None:\n        raise ValueError(\"Input GeoDataFrame must have a defined CRS\")\n\n    # Create a copy if requested\n    gdf_work = gdf.copy() if copy else gdf\n\n    # Store input CRS\n    input_crs = gdf_work.crs\n\n    try:\n        # Create a custom UTM CRS based on the calculated UTM zone\n        utm_crs = gdf_work.estimate_utm_crs()\n\n        # Transform to UTM, create buffer, and transform back\n        gdf_work = gdf_work.to_crs(utm_crs)\n        gdf_work[\"geometry\"] = gdf_work[\"geometry\"].buffer(\n            buffer_distance_meters, cap_style=cap_style\n        )\n        gdf_work = gdf_work.to_crs(input_crs)\n\n        return gdf_work\n\n    except Exception as e:\n        raise RuntimeError(f\"Error during buffering operation: {str(e)}\")\n</code></pre>"},{"location":"api/processing/#gigaspatial.processing.geo.convert_to_geodataframe","title":"<code>convert_to_geodataframe(data, lat_col=None, lon_col=None, crs='EPSG:4326')</code>","text":"<p>Convert a pandas DataFrame to a GeoDataFrame, either from latitude/longitude columns or from a WKT geometry column.</p>"},{"location":"api/processing/#gigaspatial.processing.geo.convert_to_geodataframe--parameters","title":"Parameters:","text":"<p>data : pandas.DataFrame     Input DataFrame containing either lat/lon columns or a geometry column. lat_col : str, optional     Name of the latitude column. Default is 'lat'. lon_col : str, optional     Name of the longitude column. Default is 'lon'. crs : str or pyproj.CRS, optional     Coordinate Reference System of the geometry data. Default is 'EPSG:4326'.</p>"},{"location":"api/processing/#gigaspatial.processing.geo.convert_to_geodataframe--returns","title":"Returns:","text":"<p>geopandas.GeoDataFrame     A GeoDataFrame containing the input data with a geometry column.</p>"},{"location":"api/processing/#gigaspatial.processing.geo.convert_to_geodataframe--raises","title":"Raises:","text":"<p>TypeError     If input is not a pandas DataFrame. ValueError     If required columns are missing or contain invalid data.</p> Source code in <code>gigaspatial/processing/geo.py</code> <pre><code>def convert_to_geodataframe(\n    data: pd.DataFrame, lat_col: str = None, lon_col: str = None, crs=\"EPSG:4326\"\n) -&gt; gpd.GeoDataFrame:\n    \"\"\"\n    Convert a pandas DataFrame to a GeoDataFrame, either from latitude/longitude columns\n    or from a WKT geometry column.\n\n    Parameters:\n    ----------\n    data : pandas.DataFrame\n        Input DataFrame containing either lat/lon columns or a geometry column.\n    lat_col : str, optional\n        Name of the latitude column. Default is 'lat'.\n    lon_col : str, optional\n        Name of the longitude column. Default is 'lon'.\n    crs : str or pyproj.CRS, optional\n        Coordinate Reference System of the geometry data. Default is 'EPSG:4326'.\n\n    Returns:\n    -------\n    geopandas.GeoDataFrame\n        A GeoDataFrame containing the input data with a geometry column.\n\n    Raises:\n    ------\n    TypeError\n        If input is not a pandas DataFrame.\n    ValueError\n        If required columns are missing or contain invalid data.\n    \"\"\"\n\n    # Input validation\n    if not isinstance(data, pd.DataFrame):\n        raise TypeError(\"Input 'data' must be a pandas DataFrame\")\n\n    # Create a copy to avoid modifying the input\n    df = data.copy()\n\n    try:\n        if \"geometry\" not in df.columns:\n            # If column names not provided, try to detect them\n            if lat_col is None or lon_col is None:\n                try:\n                    detected_lat, detected_lon = detect_coordinate_columns(df)\n                    lat_col = lat_col or detected_lat\n                    lon_col = lon_col or detected_lon\n                except ValueError as e:\n                    raise ValueError(\n                        f\"Could not automatically detect coordinate columns and no \"\n                        f\"'geometry' column found. Error: {str(e)}\"\n                    )\n\n            # Validate latitude/longitude columns exist\n            if lat_col not in df.columns or lon_col not in df.columns:\n                raise ValueError(\n                    f\"Could not find columns: {lat_col} and/or {lon_col} in the DataFrame\"\n                )\n\n            # Check for missing values\n            if df[lat_col].isna().any() or df[lon_col].isna().any():\n                raise ValueError(\n                    f\"Missing values found in {lat_col} and/or {lon_col} columns\"\n                )\n\n            # Create geometry from lat/lon\n            geometry = gpd.points_from_xy(x=df[lon_col], y=df[lat_col])\n\n        else:\n            # Check if geometry column already contains valid geometries\n            if df[\"geometry\"].apply(lambda x: isinstance(x, base.BaseGeometry)).all():\n                geometry = df[\"geometry\"]\n            elif df[\"geometry\"].apply(lambda x: isinstance(x, str)).all():\n                # Convert WKT strings to geometry objects\n                geometry = df[\"geometry\"].apply(wkt.loads)\n            else:\n                raise ValueError(\n                    \"Invalid geometry format: contains mixed or unsupported types\"\n                )\n\n        # drop the WKT column if conversion was done\n        if (\n            \"geometry\" in df.columns\n            and not df[\"geometry\"]\n            .apply(lambda x: isinstance(x, base.BaseGeometry))\n            .all()\n        ):\n            df = df.drop(columns=[\"geometry\"])\n\n        return gpd.GeoDataFrame(df, geometry=geometry, crs=crs)\n\n    except Exception as e:\n        raise RuntimeError(f\"Error converting to GeoDataFrame: {str(e)}\")\n</code></pre>"},{"location":"api/processing/#gigaspatial.processing.geo.detect_coordinate_columns","title":"<code>detect_coordinate_columns(data, lat_keywords=None, lon_keywords=None, case_sensitive=False)</code>","text":"<p>Detect latitude and longitude columns in a DataFrame using keyword matching.</p>"},{"location":"api/processing/#gigaspatial.processing.geo.detect_coordinate_columns--parameters","title":"Parameters:","text":"<p>data : pandas.DataFrame     DataFrame to search for coordinate columns. lat_keywords : list of str, optional     Keywords for identifying latitude columns. If None, uses default keywords. lon_keywords : list of str, optional     Keywords for identifying longitude columns. If None, uses default keywords. case_sensitive : bool, optional     Whether to perform case-sensitive matching. Default is False.</p>"},{"location":"api/processing/#gigaspatial.processing.geo.detect_coordinate_columns--returns","title":"Returns:","text":"<p>tuple[str, str]     Names of detected (latitude, longitude) columns.</p>"},{"location":"api/processing/#gigaspatial.processing.geo.detect_coordinate_columns--raises","title":"Raises:","text":"<p>ValueError     If no unique pair of latitude/longitude columns can be found. TypeError     If input data is not a pandas DataFrame.</p> Source code in <code>gigaspatial/processing/geo.py</code> <pre><code>def detect_coordinate_columns(\n    data, lat_keywords=None, lon_keywords=None, case_sensitive=False\n) -&gt; Tuple[str, str]:\n    \"\"\"\n    Detect latitude and longitude columns in a DataFrame using keyword matching.\n\n    Parameters:\n    ----------\n    data : pandas.DataFrame\n        DataFrame to search for coordinate columns.\n    lat_keywords : list of str, optional\n        Keywords for identifying latitude columns. If None, uses default keywords.\n    lon_keywords : list of str, optional\n        Keywords for identifying longitude columns. If None, uses default keywords.\n    case_sensitive : bool, optional\n        Whether to perform case-sensitive matching. Default is False.\n\n    Returns:\n    -------\n    tuple[str, str]\n        Names of detected (latitude, longitude) columns.\n\n    Raises:\n    ------\n    ValueError\n        If no unique pair of latitude/longitude columns can be found.\n    TypeError\n        If input data is not a pandas DataFrame.\n    \"\"\"\n\n    # Default keywords if none provided\n    default_lat = [\n        \"latitude\",\n        \"lat\",\n        \"y\",\n        \"lat_\",\n        \"lat(s)\",\n        \"_lat\",\n        \"ylat\",\n        \"latitude_y\",\n    ]\n    default_lon = [\n        \"longitude\",\n        \"lon\",\n        \"long\",\n        \"x\",\n        \"lon_\",\n        \"lon(e)\",\n        \"long(e)\",\n        \"_lon\",\n        \"xlon\",\n        \"longitude_x\",\n    ]\n\n    lat_keywords = lat_keywords or default_lat\n    lon_keywords = lon_keywords or default_lon\n\n    # Input validation\n    if not isinstance(data, pd.DataFrame):\n        raise TypeError(\"Input must be a pandas DataFrame\")\n\n    if not data.columns.is_unique:\n        raise ValueError(\"DataFrame contains duplicate column names\")\n\n    def create_pattern(keywords):\n        \"\"\"Create regex pattern from keywords.\"\"\"\n        return \"|\".join(rf\"\\b{re.escape(keyword)}\\b\" for keyword in keywords)\n\n    def find_matching_columns(columns, pattern, case_sensitive) -&gt; List:\n        \"\"\"Find columns matching the pattern.\"\"\"\n        flags = 0 if case_sensitive else re.IGNORECASE\n        return [col for col in columns if re.search(pattern, col, flags=flags)]\n\n    try:\n        # Create patterns\n        lat_pattern = create_pattern(lat_keywords)\n        lon_pattern = create_pattern(lon_keywords)\n\n        # Find matching columns\n        lat_cols = find_matching_columns(data.columns, lat_pattern, case_sensitive)\n        lon_cols = find_matching_columns(data.columns, lon_pattern, case_sensitive)\n\n        # Remove any longitude matches from latitude columns and vice versa\n        lat_cols = [col for col in lat_cols if col not in lon_cols]\n        lon_cols = [col for col in lon_cols if col not in lat_cols]\n\n        # Detailed error messages based on what was found\n        if not lat_cols and not lon_cols:\n            columns_list = \"\\n\".join(f\"- {col}\" for col in data.columns)\n            raise ValueError(\n                f\"No latitude or longitude columns found. Available columns are:\\n{columns_list}\\n\"\n                f\"Consider adding more keywords or checking column names.\"\n            )\n\n        if not lat_cols:\n            found_lons = \", \".join(lon_cols)\n            raise ValueError(\n                f\"Found longitude columns ({found_lons}) but no latitude columns. \"\n                \"Check latitude keywords or column names.\"\n            )\n\n        if not lon_cols:\n            found_lats = \", \".join(lat_cols)\n            raise ValueError(\n                f\"Found latitude columns ({found_lats}) but no longitude columns. \"\n                \"Check longitude keywords or column names.\"\n            )\n\n        if len(lat_cols) &gt; 1 or len(lon_cols) &gt; 1:\n            raise ValueError(\n                f\"Multiple possible coordinate columns found:\\n\"\n                f\"Latitude candidates: {lat_cols}\\n\"\n                f\"Longitude candidates: {lon_cols}\\n\"\n                \"Please specify more precise keywords.\"\n            )\n\n        return lat_cols[0], lon_cols[0]\n\n    except Exception as e:\n        if isinstance(e, ValueError):\n            raise\n        raise RuntimeError(f\"Error detecting coordinate columns: {str(e)}\")\n</code></pre>"},{"location":"api/processing/#gigaspatial.processing.geo.get_centroids","title":"<code>get_centroids(gdf)</code>","text":"<p>Calculate the centroids of a (Multi)Polygon GeoDataFrame.</p>"},{"location":"api/processing/#gigaspatial.processing.geo.get_centroids--parameters","title":"Parameters:","text":"<p>gdf : geopandas.GeoDataFrame     GeoDataFrame containing (Multi)Polygon geometries.</p>"},{"location":"api/processing/#gigaspatial.processing.geo.get_centroids--returns","title":"Returns:","text":"<p>geopandas.GeoDataFrame     A new GeoDataFrame with Point geometries representing the centroids.</p>"},{"location":"api/processing/#gigaspatial.processing.geo.get_centroids--raises","title":"Raises:","text":"<p>ValueError     If the input GeoDataFrame does not contain (Multi)Polygon geometries.</p> Source code in <code>gigaspatial/processing/geo.py</code> <pre><code>def get_centroids(gdf: gpd.GeoDataFrame) -&gt; gpd.GeoDataFrame:\n    \"\"\"\n    Calculate the centroids of a (Multi)Polygon GeoDataFrame.\n\n    Parameters:\n    ----------\n    gdf : geopandas.GeoDataFrame\n        GeoDataFrame containing (Multi)Polygon geometries.\n\n    Returns:\n    -------\n    geopandas.GeoDataFrame\n        A new GeoDataFrame with Point geometries representing the centroids.\n\n    Raises:\n    ------\n    ValueError\n        If the input GeoDataFrame does not contain (Multi)Polygon geometries.\n    \"\"\"\n    # Validate input geometries\n    if not all(gdf.geometry.geom_type.isin([\"Polygon\", \"MultiPolygon\"])):\n        raise ValueError(\n            \"Input GeoDataFrame must contain only Polygon or MultiPolygon geometries.\"\n        )\n\n    # Calculate centroids\n    centroids = gdf.copy()\n    centroids[\"geometry\"] = centroids.geometry.centroid\n\n    return centroids\n</code></pre>"},{"location":"api/processing/#gigaspatial.processing.geo.map_points_within_polygons","title":"<code>map_points_within_polygons(base_points_gdf, polygon_gdf)</code>","text":"<p>Maps whether each point in <code>base_points_gdf</code> is within any polygon in <code>polygon_gdf</code>.</p>"},{"location":"api/processing/#gigaspatial.processing.geo.map_points_within_polygons--parameters","title":"Parameters:","text":"<p>base_points_gdf : geopandas.GeoDataFrame     GeoDataFrame containing point geometries to check. polygon_gdf : geopandas.GeoDataFrame     GeoDataFrame containing polygon geometries.</p>"},{"location":"api/processing/#gigaspatial.processing.geo.map_points_within_polygons--returns","title":"Returns:","text":"<p>geopandas.GeoDataFrame     The <code>base_points_gdf</code> with an additional column <code>is_within</code> (True/False).</p>"},{"location":"api/processing/#gigaspatial.processing.geo.map_points_within_polygons--raises","title":"Raises:","text":"<p>ValueError     If the geometries in either GeoDataFrame are invalid or not of the expected type.</p> Source code in <code>gigaspatial/processing/geo.py</code> <pre><code>def map_points_within_polygons(base_points_gdf, polygon_gdf):\n    \"\"\"\n    Maps whether each point in `base_points_gdf` is within any polygon in `polygon_gdf`.\n\n    Parameters:\n    ----------\n    base_points_gdf : geopandas.GeoDataFrame\n        GeoDataFrame containing point geometries to check.\n    polygon_gdf : geopandas.GeoDataFrame\n        GeoDataFrame containing polygon geometries.\n\n    Returns:\n    -------\n    geopandas.GeoDataFrame\n        The `base_points_gdf` with an additional column `is_within` (True/False).\n\n    Raises:\n    ------\n    ValueError\n        If the geometries in either GeoDataFrame are invalid or not of the expected type.\n    \"\"\"\n    # Validate input GeoDataFrames\n    if not all(base_points_gdf.geometry.geom_type == \"Point\"):\n        raise ValueError(\"`base_points_gdf` must contain only Point geometries.\")\n    if not all(polygon_gdf.geometry.geom_type.isin([\"Polygon\", \"MultiPolygon\"])):\n        raise ValueError(\n            \"`polygon_gdf` must contain only Polygon or MultiPolygon geometries.\"\n        )\n\n    if not base_points_gdf.crs == polygon_gdf.crs:\n        raise ValueError(\"CRS of `base_points_gdf` and `polygon_gdf` must match.\")\n\n    # Perform spatial join to check if points fall within any polygon\n    joined_gdf = gpd.sjoin(\n        base_points_gdf, polygon_gdf[[\"geometry\"]], how=\"left\", predicate=\"within\"\n    )\n\n    # Add `is_within` column to base_points_gdf\n    base_points_gdf[\"is_within\"] = base_points_gdf.index.isin(\n        set(joined_gdf.index[~joined_gdf.index_right.isna()])\n    )\n\n    return base_points_gdf\n</code></pre>"},{"location":"api/processing/#gigaspatial.processing.geo.simplify_geometries","title":"<code>simplify_geometries(gdf, tolerance=0.01, preserve_topology=True, geometry_column='geometry')</code>","text":"<p>Simplify geometries in a GeoDataFrame to reduce file size and improve visualization performance.</p>"},{"location":"api/processing/#gigaspatial.processing.geo.simplify_geometries--parameters","title":"Parameters","text":"<p>gdf : geopandas.GeoDataFrame     GeoDataFrame containing geometries to simplify. tolerance : float, optional     Tolerance for simplification. Larger values simplify more but reduce detail (default is 0.01). preserve_topology : bool, optional     Whether to preserve topology while simplifying. Preserving topology prevents invalid geometries (default is True). geometry_column : str, optional     Name of the column containing geometries (default is \"geometry\").</p>"},{"location":"api/processing/#gigaspatial.processing.geo.simplify_geometries--returns","title":"Returns","text":"<p>geopandas.GeoDataFrame     A new GeoDataFrame with simplified geometries.</p>"},{"location":"api/processing/#gigaspatial.processing.geo.simplify_geometries--raises","title":"Raises","text":"<p>ValueError     If the specified geometry column does not exist or contains invalid geometries. TypeError     If the geometry column does not contain valid geometries.</p>"},{"location":"api/processing/#gigaspatial.processing.geo.simplify_geometries--examples","title":"Examples","text":"<p>Simplify geometries in a GeoDataFrame:</p> <p>simplified_gdf = simplify_geometries(gdf, tolerance=0.05)</p> Source code in <code>gigaspatial/processing/geo.py</code> <pre><code>def simplify_geometries(\n    gdf: gpd.GeoDataFrame,\n    tolerance: float = 0.01,\n    preserve_topology: bool = True,\n    geometry_column: str = \"geometry\",\n) -&gt; gpd.GeoDataFrame:\n    \"\"\"\n    Simplify geometries in a GeoDataFrame to reduce file size and improve visualization performance.\n\n    Parameters\n    ----------\n    gdf : geopandas.GeoDataFrame\n        GeoDataFrame containing geometries to simplify.\n    tolerance : float, optional\n        Tolerance for simplification. Larger values simplify more but reduce detail (default is 0.01).\n    preserve_topology : bool, optional\n        Whether to preserve topology while simplifying. Preserving topology prevents invalid geometries (default is True).\n    geometry_column : str, optional\n        Name of the column containing geometries (default is \"geometry\").\n\n    Returns\n    -------\n    geopandas.GeoDataFrame\n        A new GeoDataFrame with simplified geometries.\n\n    Raises\n    ------\n    ValueError\n        If the specified geometry column does not exist or contains invalid geometries.\n    TypeError\n        If the geometry column does not contain valid geometries.\n\n    Examples\n    --------\n    Simplify geometries in a GeoDataFrame:\n    &gt;&gt;&gt; simplified_gdf = simplify_geometries(gdf, tolerance=0.05)\n    \"\"\"\n\n    # Check if the specified geometry column exists\n    if geometry_column not in gdf.columns:\n        raise ValueError(\n            f\"Geometry column '{geometry_column}' not found in the GeoDataFrame.\"\n        )\n\n    # Check if the specified column contains geometries\n    if not gpd.GeoSeries(gdf[geometry_column]).is_valid.all():\n        raise TypeError(\n            f\"Geometry column '{geometry_column}' contains invalid geometries.\"\n        )\n\n    # Simplify geometries (non-destructive)\n    gdf_simplified = gdf.copy()\n    gdf_simplified[geometry_column] = gdf_simplified[geometry_column].simplify(\n        tolerance=tolerance, preserve_topology=preserve_topology\n    )\n\n    return gdf_simplified\n</code></pre>"},{"location":"api/processing/#gigaspatial.processing.sat_images","title":"<code>sat_images</code>","text":""},{"location":"api/processing/#gigaspatial.processing.sat_images.calculate_pixels_at_location","title":"<code>calculate_pixels_at_location(gdf, resolution, bbox_size=300, crs='EPSG:3857')</code>","text":"<p>Calculates the number of pixels required to cover a given bounding box around a geographic coordinate, given a resolution in meters per pixel.</p> <p>Parameters:</p> Name Type Description Default <code>gdf</code> <p>a geodataframe with Point geometries that are geographic coordinates</p> required <code>resolution</code> <code>float</code> <p>Desired resolution (meters per pixel).</p> required <code>bbox_size</code> <code>float</code> <p>Bounding box size in meters (default 300m x 300m).</p> <code>300</code> <code>crs</code> <code>str</code> <p>Target projection (default is EPSG:3857).</p> <code>'EPSG:3857'</code> <p>Returns:</p> Name Type Description <code>int</code> <p>Number of pixels per side (width and height).</p> Source code in <code>gigaspatial/processing/sat_images.py</code> <pre><code>def calculate_pixels_at_location(gdf, resolution, bbox_size=300, crs=\"EPSG:3857\"):\n    \"\"\"\n    Calculates the number of pixels required to cover a given bounding box\n    around a geographic coordinate, given a resolution in meters per pixel.\n\n    Parameters:\n        gdf: a geodataframe with Point geometries that are geographic coordinates\n        resolution (float): Desired resolution (meters per pixel).\n        bbox_size (float): Bounding box size in meters (default 300m x 300m).\n        crs (str): Target projection (default is EPSG:3857).\n\n    Returns:\n        int: Number of pixels per side (width and height).\n    \"\"\"\n\n    # Calculate avg lat and lon\n    lon = gdf.geometry.x.mean()\n    lat = gdf.geometry.y.mean()\n\n    # Define projections\n    wgs84 = pyproj.CRS(\"EPSG:4326\")  # Geographic coordinate system\n    mercator = pyproj.CRS(crs)  # Target CRS (EPSG:3857)\n\n    # Transform the center coordinate to EPSG:3857\n    transformer = pyproj.Transformer.from_crs(wgs84, mercator, always_xy=True)\n    x, y = transformer.transform(lon, lat)\n\n    # Calculate scale factor (distortion) at given latitude\n    scale_factor = np.cos(np.radians(lat))  # Mercator scale correction\n\n    # Adjust the effective resolution\n    effective_resolution = resolution * scale_factor\n\n    # Compute number of pixels per side\n    pixels = bbox_size / effective_resolution\n    return int(round(pixels))\n</code></pre>"},{"location":"api/processing/#gigaspatial.processing.tif_processor","title":"<code>tif_processor</code>","text":""},{"location":"api/processing/#gigaspatial.processing.tif_processor.TifProcessor","title":"<code>TifProcessor</code>","text":"<p>A class to handle tif data processing, supporting single-band, RGB, RGBA, and multi-band data.</p> Source code in <code>gigaspatial/processing/tif_processor.py</code> <pre><code>@dataclass(config=ConfigDict(arbitrary_types_allowed=True))\nclass TifProcessor:\n    \"\"\"\n    A class to handle tif data processing, supporting single-band, RGB, RGBA, and multi-band data.\n    \"\"\"\n\n    dataset_path: Union[Path, str]\n    data_store: Optional[DataStore] = None\n    mode: Literal[\"single\", \"rgb\", \"rgba\", \"multi\"] = \"single\"\n\n    def __post_init__(self):\n        \"\"\"Validate inputs and set up logging.\"\"\"\n        self.data_store = self.data_store or LocalDataStore()\n        self.logger = config.get_logger(self.__class__.__name__)\n        self._cache = {}\n\n        if not self.data_store.file_exists(self.dataset_path):\n            raise FileNotFoundError(f\"Dataset not found at {self.dataset_path}\")\n\n        self._load_metadata()\n\n        # Validate mode and band count\n        if self.mode == \"rgba\" and self.count != 4:\n            raise ValueError(\"RGBA mode requires a 4-band TIF file\")\n        if self.mode == \"rgb\" and self.count != 3:\n            raise ValueError(\"RGB mode requires a 3-band TIF file\")\n        if self.mode == \"single\" and self.count != 1:\n            raise ValueError(\"Single mode requires a 1-band TIF file\")\n        if self.mode == \"multi\" and self.count &lt; 2:\n            raise ValueError(\"Multi mode requires a TIF file with 2 or more bands\")\n\n    @contextmanager\n    def open_dataset(self):\n        \"\"\"Context manager for accessing the dataset\"\"\"\n        with self.data_store.open(self.dataset_path, \"rb\") as f:\n            with rasterio.MemoryFile(f.read()) as memfile:\n                with memfile.open() as src:\n                    yield src\n\n    def _load_metadata(self):\n        \"\"\"Load metadata from the TIF file if not already cached\"\"\"\n        if not self._cache:\n            with self.open_dataset() as src:\n                self._cache[\"transform\"] = src.transform\n                self._cache[\"crs\"] = src.crs.to_string()\n                self._cache[\"bounds\"] = src.bounds\n                self._cache[\"width\"] = src.width\n                self._cache[\"height\"] = src.height\n                self._cache[\"resolution\"] = (abs(src.transform.a), abs(src.transform.e))\n                self._cache[\"x_transform\"] = src.transform.a\n                self._cache[\"y_transform\"] = src.transform.e\n                self._cache[\"nodata\"] = src.nodata\n                self._cache[\"count\"] = src.count\n                self._cache[\"dtype\"] = src.dtypes[0]\n\n    @property\n    def transform(self):\n        \"\"\"Get the transform from the TIF file\"\"\"\n        return self._cache[\"transform\"]\n\n    @property\n    def crs(self):\n        \"\"\"Get the coordinate reference system from the TIF file\"\"\"\n        return self._cache[\"crs\"]\n\n    @property\n    def bounds(self):\n        \"\"\"Get the bounds of the TIF file\"\"\"\n        return self._cache[\"bounds\"]\n\n    @property\n    def resolution(self) -&gt; Tuple[float, float]:\n        \"\"\"Get the x and y resolution (pixel width and height or pixel size) from the TIF file\"\"\"\n        return self._cache[\"resolution\"]\n\n    @property\n    def x_transform(self) -&gt; float:\n        \"\"\"Get the x transform from the TIF file\"\"\"\n        return self._cache[\"x_transform\"]\n\n    @property\n    def y_transform(self) -&gt; float:\n        \"\"\"Get the y transform from the TIF file\"\"\"\n        return self._cache[\"y_transform\"]\n\n    @property\n    def count(self) -&gt; int:\n        \"\"\"Get the band count from the TIF file\"\"\"\n        return self._cache[\"count\"]\n\n    @property\n    def nodata(self) -&gt; int:\n        \"\"\"Get the value representing no data in the rasters\"\"\"\n        return self._cache[\"nodata\"]\n\n    @property\n    def tabular(self) -&gt; pd.DataFrame:\n        \"\"\"Get the data from the TIF file\"\"\"\n        if not hasattr(self, \"_tabular\"):\n            try:\n                if self.mode == \"single\":\n                    self._tabular = self._to_band_dataframe(\n                        drop_nodata=True, drop_values=[]\n                    )\n                elif self.mode == \"rgb\":\n                    self._tabular = self._to_rgb_dataframe(drop_nodata=True)\n                elif self.mode == \"rgba\":\n                    self._tabular = self._to_rgba_dataframe(drop_transparent=True)\n                elif self.mode == \"multi\":\n                    self._tabular = self._to_multi_band_dataframe(\n                        drop_nodata=True,\n                        drop_values=[],\n                        band_names=None,  # Use default band naming\n                    )\n                else:\n                    raise ValueError(\n                        f\"Invalid mode: {self.mode}. Must be one of: single, rgb, rgba, multi\"\n                    )\n            except Exception as e:\n                raise ValueError(\n                    f\"Failed to process TIF file in mode '{self.mode}'. \"\n                    f\"Please ensure the file is valid and matches the selected mode. \"\n                    f\"Original error: {str(e)}\"\n                )\n\n        return self._tabular\n\n    def to_dataframe(self) -&gt; pd.DataFrame:\n        return self.tabular\n\n    def get_zoned_geodataframe(self) -&gt; gpd.GeoDataFrame:\n        \"\"\"\n        Convert the processed TIF data into a GeoDataFrame, where each row represents a pixel zone.\n        Each zone is defined by its bounding box, based on pixel resolution and coordinates.\n        \"\"\"\n        self.logger.info(\"Converting data to GeoDataFrame with zones...\")\n\n        df = self.tabular\n\n        x_res, y_res = self.resolution\n\n        # create bounding box for each pixel\n        geometries = [\n            box(lon - x_res / 2, lat - y_res / 2, lon + x_res / 2, lat + y_res / 2)\n            for lon, lat in zip(df[\"lon\"], df[\"lat\"])\n        ]\n\n        gdf = gpd.GeoDataFrame(df, geometry=geometries, crs=self.crs)\n\n        self.logger.info(\"Conversion to GeoDataFrame complete!\")\n        return gdf\n\n    def sample_by_coordinates(\n        self, coordinate_list: List[Tuple[float, float]]\n    ) -&gt; Union[np.ndarray, dict]:\n        self.logger.info(\"Sampling raster values at the coordinates...\")\n\n        with self.open_dataset() as src:\n            if self.mode == \"rgba\":\n                if self.count != 4:\n                    raise ValueError(\"RGBA mode requires a 4-band TIF file\")\n\n                rgba_values = {\"red\": [], \"green\": [], \"blue\": [], \"alpha\": []}\n\n                for band_idx, color in enumerate([\"red\", \"green\", \"blue\", \"alpha\"], 1):\n                    rgba_values[color] = [\n                        vals[0]\n                        for vals in src.sample(coordinate_list, indexes=band_idx)\n                    ]\n\n                return rgba_values\n\n            elif self.mode == \"rgb\":\n                if self.count != 3:\n                    raise ValueError(\"RGB mode requires a 3-band TIF file\")\n\n                rgb_values = {\"red\": [], \"green\": [], \"blue\": []}\n\n                for band_idx, color in enumerate([\"red\", \"green\", \"blue\"], 1):\n                    rgb_values[color] = [\n                        vals[0]\n                        for vals in src.sample(coordinate_list, indexes=band_idx)\n                    ]\n\n                return rgb_values\n            else:\n                if src.count != 1:\n                    raise ValueError(\"Single band mode requires a 1-band TIF file\")\n                return np.array([vals[0] for vals in src.sample(coordinate_list)])\n\n    def sample_by_polygons(\n        self, polygon_list: List[Union[Polygon, MultiPolygon]], stat: str = \"mean\"\n    ) -&gt; np.ndarray:\n        \"\"\"\n        Sample raster values within each polygon of a GeoDataFrame.\n\n        Parameters:\n            polygon_list: List of polygon geometries (can include MultiPolygons).\n            stat (str): Aggregation statistic to compute within each polygon.\n                        Options: \"mean\", \"median\", \"sum\", \"min\", \"max\".\n        Returns:\n            A NumPy array of sampled values\n        \"\"\"\n        self.logger.info(\"Sampling raster values within polygons...\")\n\n        with self.open_dataset() as src:\n            results = []\n\n            for geom in polygon_list:\n                if geom.is_empty:\n                    results.append(np.nan)\n                    continue\n\n                try:\n                    # Mask the raster with the polygon\n                    out_image, _ = mask(src, [geom], crop=True)\n\n                    # Flatten the raster values and remove NoData values\n                    values = out_image[out_image != src.nodata].flatten()\n\n                    # Compute the desired statistic\n                    if len(values) == 0:\n                        results.append(np.nan)\n                    else:\n                        if stat == \"mean\":\n                            results.append(np.mean(values))\n                        elif stat == \"median\":\n                            results.append(np.median(values))\n                        elif stat == \"sum\":\n                            results.append(np.sum(values))\n                        elif stat == \"min\":\n                            results.append(np.min(values))\n                        elif stat == \"max\":\n                            results.append(np.max(values))\n                        else:\n                            raise ValueError(f\"Unknown statistic: {stat}\")\n\n                except Exception as e:\n                    self.logger.error(f\"Error processing polygon: {e}\")\n                    results.append(np.nan)\n\n        return np.array(results)\n\n    def _to_rgba_dataframe(self, drop_transparent: bool = False) -&gt; pd.DataFrame:\n        \"\"\"\n        Convert RGBA TIF to DataFrame with separate columns for R, G, B, A values.\n        \"\"\"\n        self.logger.info(\"Processing RGBA dataset...\")\n\n        with self.open_dataset() as src:\n            if self.count != 4:\n                raise ValueError(\"RGBA mode requires a 4-band TIF file\")\n\n            # Read all four bands\n            red, green, blue, alpha = src.read()\n\n            x_coords, y_coords = self._get_pixel_coordinates()\n\n            if drop_transparent:\n                mask = alpha &gt; 0\n                red = np.extract(mask, red)\n                green = np.extract(mask, green)\n                blue = np.extract(mask, blue)\n                alpha = np.extract(mask, alpha)\n                lons = np.extract(mask, x_coords)\n                lats = np.extract(mask, y_coords)\n            else:\n                lons = x_coords.flatten()\n                lats = y_coords.flatten()\n                red = red.flatten()\n                green = green.flatten()\n                blue = blue.flatten()\n                alpha = alpha.flatten()\n\n            # Create DataFrame with RGBA values\n            data = pd.DataFrame(\n                {\n                    \"lon\": lons,\n                    \"lat\": lats,\n                    \"red\": red,\n                    \"green\": green,\n                    \"blue\": blue,\n                    \"alpha\": alpha,\n                }\n            )\n\n            # Normalize alpha values if they're not in [0, 1] range\n            if data[\"alpha\"].max() &gt; 1:\n                data[\"alpha\"] = data[\"alpha\"] / data[\"alpha\"].max()\n\n        self.logger.info(\"RGBA dataset is processed!\")\n        return data\n\n    def _to_rgb_dataframe(self, drop_nodata: bool = True) -&gt; pd.DataFrame:\n        \"\"\"Convert RGB TIF to DataFrame with separate columns for R, G, B values.\"\"\"\n        if self.mode != \"rgb\":\n            raise ValueError(\"Use appropriate method for current mode\")\n\n        self.logger.info(\"Processing RGB dataset...\")\n\n        with self.open_dataset() as src:\n            if self.count != 3:\n                raise ValueError(\"RGB mode requires a 3-band TIF file\")\n\n            # Read all three bands\n            red, green, blue = src.read()\n\n            x_coords, y_coords = self._get_pixel_coordinates()\n\n            if drop_nodata:\n                nodata_value = src.nodata\n                if nodata_value is not None:\n                    mask = ~(\n                        (red == nodata_value)\n                        | (green == nodata_value)\n                        | (blue == nodata_value)\n                    )\n                    red = np.extract(mask, red)\n                    green = np.extract(mask, green)\n                    blue = np.extract(mask, blue)\n                    lons = np.extract(mask, x_coords)\n                    lats = np.extract(mask, y_coords)\n                else:\n                    lons = x_coords.flatten()\n                    lats = y_coords.flatten()\n                    red = red.flatten()\n                    green = green.flatten()\n                    blue = blue.flatten()\n            else:\n                lons = x_coords.flatten()\n                lats = y_coords.flatten()\n                red = red.flatten()\n                green = green.flatten()\n                blue = blue.flatten()\n\n            data = pd.DataFrame(\n                {\n                    \"lon\": lons,\n                    \"lat\": lats,\n                    \"red\": red,\n                    \"green\": green,\n                    \"blue\": blue,\n                }\n            )\n\n        self.logger.info(\"RGB dataset is processed!\")\n        return data\n\n    def _to_band_dataframe(\n        self, band_number: int = 1, drop_nodata: bool = True, drop_values: list = []\n    ) -&gt; pd.DataFrame:\n        \"\"\"Process single-band TIF to DataFrame.\"\"\"\n        if self.mode != \"single\":\n            raise ValueError(\"Use appropriate method for current mode\")\n\n        self.logger.info(\"Processing single-band dataset...\")\n\n        if band_number &lt;= 0 or band_number &gt; self.count:\n            self.logger.error(\n                f\"Error: Band number {band_number} is out of range. The file has {self.count} bands.\"\n            )\n            return None\n\n        with self.open_dataset() as src:\n\n            band = src.read(band_number)\n\n            x_coords, y_coords = self._get_pixel_coordinates()\n\n            values_to_mask = []\n            if drop_nodata:\n                nodata_value = src.nodata\n                if nodata_value is not None:\n                    values_to_mask.append(nodata_value)\n\n            if drop_values:\n                values_to_mask.extend(drop_values)\n\n            if values_to_mask:\n                data_mask = ~np.isin(band, values_to_mask)\n                pixel_values = np.extract(data_mask, band)\n                lons = np.extract(data_mask, x_coords)\n                lats = np.extract(data_mask, y_coords)\n            else:\n                pixel_values = band.flatten()\n                lons = x_coords.flatten()\n                lats = y_coords.flatten()\n\n            data = pd.DataFrame({\"lon\": lons, \"lat\": lats, \"pixel_value\": pixel_values})\n\n        self.logger.info(\"Dataset is processed!\")\n        return data\n\n    def _to_multi_band_dataframe(\n        self,\n        drop_nodata: bool = True,\n        drop_values: list = [],\n        band_names: Optional[List[str]] = None,\n    ) -&gt; pd.DataFrame:\n        \"\"\"\n        Process multi-band TIF to DataFrame with all bands included.\n\n        Args:\n            drop_nodata (bool): Whether to drop nodata values. Defaults to True.\n            drop_values (list): Additional values to drop from the dataset. Defaults to empty list.\n            band_names (Optional[List[str]]): Custom names for the bands. If None, bands will be named using\n                                            the band descriptions from the GeoTIFF metadata if available,\n                                            otherwise 'band_1', 'band_2', etc.\n\n        Returns:\n            pd.DataFrame: DataFrame containing coordinates and all band values\n        \"\"\"\n        self.logger.info(\"Processing multi-band dataset...\")\n\n        with self.open_dataset() as src:\n            # Read all bands\n            stack = src.read()\n\n            x_coords, y_coords = self._get_pixel_coordinates()\n\n            # Initialize dictionary with coordinates\n            data_dict = {\"lon\": x_coords.flatten(), \"lat\": y_coords.flatten()}\n\n            # Get band descriptions from metadata if available\n            if band_names is None and hasattr(src, \"descriptions\") and src.descriptions:\n                band_names = [\n                    desc if desc else f\"band_{i+1}\"\n                    for i, desc in enumerate(src.descriptions)\n                ]\n\n            # Process each band\n            for band_idx in range(self.count):\n                band_data = stack[band_idx]\n\n                # Handle nodata and other values to drop\n                if drop_nodata or drop_values:\n                    values_to_mask = []\n                    if drop_nodata and src.nodata is not None:\n                        values_to_mask.append(src.nodata)\n                    if drop_values:\n                        values_to_mask.extend(drop_values)\n\n                    if values_to_mask:\n                        data_mask = ~np.isin(band_data, values_to_mask)\n                        band_values = np.extract(data_mask, band_data)\n                        if band_idx == 0:  # Only need to mask coordinates once\n                            data_dict[\"lon\"] = np.extract(data_mask, x_coords)\n                            data_dict[\"lat\"] = np.extract(data_mask, y_coords)\n                    else:\n                        band_values = band_data.flatten()\n                else:\n                    band_values = band_data.flatten()\n\n                # Use custom band names if provided, otherwise use descriptions or default naming\n                band_name = (\n                    band_names[band_idx]\n                    if band_names and len(band_names) &gt; band_idx\n                    else f\"band_{band_idx + 1}\"\n                )\n                data_dict[band_name] = band_values\n\n        self.logger.info(\"Multi-band dataset is processed!\")\n        return pd.DataFrame(data_dict)\n\n    def _get_pixel_coordinates(self):\n        \"\"\"Helper method to generate coordinate arrays for all pixels\"\"\"\n        if \"pixel_coords\" not in self._cache:\n            # use cached values\n            bounds = self._cache[\"bounds\"]\n            width = self._cache[\"width\"]\n            height = self._cache[\"height\"]\n            pixel_size_x = self._cache[\"x_transform\"]\n            pixel_size_y = self._cache[\"y_transform\"]\n\n            self._cache[\"pixel_coords\"] = np.meshgrid(\n                np.linspace(\n                    bounds.left + pixel_size_x / 2,\n                    bounds.right - pixel_size_x / 2,\n                    width,\n                ),\n                np.linspace(\n                    bounds.top + pixel_size_y / 2,\n                    bounds.bottom - pixel_size_y / 2,\n                    height,\n                ),\n            )\n\n        return self._cache[\"pixel_coords\"]\n</code></pre>"},{"location":"api/processing/#gigaspatial.processing.tif_processor.TifProcessor.bounds","title":"<code>bounds</code>  <code>property</code>","text":"<p>Get the bounds of the TIF file</p>"},{"location":"api/processing/#gigaspatial.processing.tif_processor.TifProcessor.count","title":"<code>count: int</code>  <code>property</code>","text":"<p>Get the band count from the TIF file</p>"},{"location":"api/processing/#gigaspatial.processing.tif_processor.TifProcessor.crs","title":"<code>crs</code>  <code>property</code>","text":"<p>Get the coordinate reference system from the TIF file</p>"},{"location":"api/processing/#gigaspatial.processing.tif_processor.TifProcessor.nodata","title":"<code>nodata: int</code>  <code>property</code>","text":"<p>Get the value representing no data in the rasters</p>"},{"location":"api/processing/#gigaspatial.processing.tif_processor.TifProcessor.resolution","title":"<code>resolution: Tuple[float, float]</code>  <code>property</code>","text":"<p>Get the x and y resolution (pixel width and height or pixel size) from the TIF file</p>"},{"location":"api/processing/#gigaspatial.processing.tif_processor.TifProcessor.tabular","title":"<code>tabular: pd.DataFrame</code>  <code>property</code>","text":"<p>Get the data from the TIF file</p>"},{"location":"api/processing/#gigaspatial.processing.tif_processor.TifProcessor.transform","title":"<code>transform</code>  <code>property</code>","text":"<p>Get the transform from the TIF file</p>"},{"location":"api/processing/#gigaspatial.processing.tif_processor.TifProcessor.x_transform","title":"<code>x_transform: float</code>  <code>property</code>","text":"<p>Get the x transform from the TIF file</p>"},{"location":"api/processing/#gigaspatial.processing.tif_processor.TifProcessor.y_transform","title":"<code>y_transform: float</code>  <code>property</code>","text":"<p>Get the y transform from the TIF file</p>"},{"location":"api/processing/#gigaspatial.processing.tif_processor.TifProcessor.__post_init__","title":"<code>__post_init__()</code>","text":"<p>Validate inputs and set up logging.</p> Source code in <code>gigaspatial/processing/tif_processor.py</code> <pre><code>def __post_init__(self):\n    \"\"\"Validate inputs and set up logging.\"\"\"\n    self.data_store = self.data_store or LocalDataStore()\n    self.logger = config.get_logger(self.__class__.__name__)\n    self._cache = {}\n\n    if not self.data_store.file_exists(self.dataset_path):\n        raise FileNotFoundError(f\"Dataset not found at {self.dataset_path}\")\n\n    self._load_metadata()\n\n    # Validate mode and band count\n    if self.mode == \"rgba\" and self.count != 4:\n        raise ValueError(\"RGBA mode requires a 4-band TIF file\")\n    if self.mode == \"rgb\" and self.count != 3:\n        raise ValueError(\"RGB mode requires a 3-band TIF file\")\n    if self.mode == \"single\" and self.count != 1:\n        raise ValueError(\"Single mode requires a 1-band TIF file\")\n    if self.mode == \"multi\" and self.count &lt; 2:\n        raise ValueError(\"Multi mode requires a TIF file with 2 or more bands\")\n</code></pre>"},{"location":"api/processing/#gigaspatial.processing.tif_processor.TifProcessor.get_zoned_geodataframe","title":"<code>get_zoned_geodataframe()</code>","text":"<p>Convert the processed TIF data into a GeoDataFrame, where each row represents a pixel zone. Each zone is defined by its bounding box, based on pixel resolution and coordinates.</p> Source code in <code>gigaspatial/processing/tif_processor.py</code> <pre><code>def get_zoned_geodataframe(self) -&gt; gpd.GeoDataFrame:\n    \"\"\"\n    Convert the processed TIF data into a GeoDataFrame, where each row represents a pixel zone.\n    Each zone is defined by its bounding box, based on pixel resolution and coordinates.\n    \"\"\"\n    self.logger.info(\"Converting data to GeoDataFrame with zones...\")\n\n    df = self.tabular\n\n    x_res, y_res = self.resolution\n\n    # create bounding box for each pixel\n    geometries = [\n        box(lon - x_res / 2, lat - y_res / 2, lon + x_res / 2, lat + y_res / 2)\n        for lon, lat in zip(df[\"lon\"], df[\"lat\"])\n    ]\n\n    gdf = gpd.GeoDataFrame(df, geometry=geometries, crs=self.crs)\n\n    self.logger.info(\"Conversion to GeoDataFrame complete!\")\n    return gdf\n</code></pre>"},{"location":"api/processing/#gigaspatial.processing.tif_processor.TifProcessor.open_dataset","title":"<code>open_dataset()</code>","text":"<p>Context manager for accessing the dataset</p> Source code in <code>gigaspatial/processing/tif_processor.py</code> <pre><code>@contextmanager\ndef open_dataset(self):\n    \"\"\"Context manager for accessing the dataset\"\"\"\n    with self.data_store.open(self.dataset_path, \"rb\") as f:\n        with rasterio.MemoryFile(f.read()) as memfile:\n            with memfile.open() as src:\n                yield src\n</code></pre>"},{"location":"api/processing/#gigaspatial.processing.tif_processor.TifProcessor.sample_by_polygons","title":"<code>sample_by_polygons(polygon_list, stat='mean')</code>","text":"<p>Sample raster values within each polygon of a GeoDataFrame.</p> <p>Parameters:</p> Name Type Description Default <code>polygon_list</code> <code>List[Union[Polygon, MultiPolygon]]</code> <p>List of polygon geometries (can include MultiPolygons).</p> required <code>stat</code> <code>str</code> <p>Aggregation statistic to compute within each polygon.         Options: \"mean\", \"median\", \"sum\", \"min\", \"max\".</p> <code>'mean'</code> Source code in <code>gigaspatial/processing/tif_processor.py</code> <pre><code>def sample_by_polygons(\n    self, polygon_list: List[Union[Polygon, MultiPolygon]], stat: str = \"mean\"\n) -&gt; np.ndarray:\n    \"\"\"\n    Sample raster values within each polygon of a GeoDataFrame.\n\n    Parameters:\n        polygon_list: List of polygon geometries (can include MultiPolygons).\n        stat (str): Aggregation statistic to compute within each polygon.\n                    Options: \"mean\", \"median\", \"sum\", \"min\", \"max\".\n    Returns:\n        A NumPy array of sampled values\n    \"\"\"\n    self.logger.info(\"Sampling raster values within polygons...\")\n\n    with self.open_dataset() as src:\n        results = []\n\n        for geom in polygon_list:\n            if geom.is_empty:\n                results.append(np.nan)\n                continue\n\n            try:\n                # Mask the raster with the polygon\n                out_image, _ = mask(src, [geom], crop=True)\n\n                # Flatten the raster values and remove NoData values\n                values = out_image[out_image != src.nodata].flatten()\n\n                # Compute the desired statistic\n                if len(values) == 0:\n                    results.append(np.nan)\n                else:\n                    if stat == \"mean\":\n                        results.append(np.mean(values))\n                    elif stat == \"median\":\n                        results.append(np.median(values))\n                    elif stat == \"sum\":\n                        results.append(np.sum(values))\n                    elif stat == \"min\":\n                        results.append(np.min(values))\n                    elif stat == \"max\":\n                        results.append(np.max(values))\n                    else:\n                        raise ValueError(f\"Unknown statistic: {stat}\")\n\n            except Exception as e:\n                self.logger.error(f\"Error processing polygon: {e}\")\n                results.append(np.nan)\n\n    return np.array(results)\n</code></pre>"},{"location":"api/processing/#gigaspatial.processing.tif_processor.sample_multiple_tifs_by_coordinates","title":"<code>sample_multiple_tifs_by_coordinates(tif_processors, coordinate_list)</code>","text":"<p>Sample raster values from multiple TIFF files for given coordinates.</p> <p>Parameters: - tif_processors: List of TifProcessor instances. - coordinate_list: List of (x, y) coordinates.</p> <p>Returns: - A NumPy array of sampled values, taking the first non-nodata value encountered.</p> Source code in <code>gigaspatial/processing/tif_processor.py</code> <pre><code>def sample_multiple_tifs_by_coordinates(\n    tif_processors: List[TifProcessor], coordinate_list: List[Tuple[float, float]]\n):\n    \"\"\"\n    Sample raster values from multiple TIFF files for given coordinates.\n\n    Parameters:\n    - tif_processors: List of TifProcessor instances.\n    - coordinate_list: List of (x, y) coordinates.\n\n    Returns:\n    - A NumPy array of sampled values, taking the first non-nodata value encountered.\n    \"\"\"\n    sampled_values = np.full(len(coordinate_list), np.nan, dtype=np.float32)\n\n    for tp in tif_processors:\n        values = tp.sample_by_coordinates(coordinate_list=coordinate_list)\n\n        if tp.nodata is not None:\n            mask = (np.isnan(sampled_values)) &amp; (\n                values != tp.nodata\n            )  # Replace only NaNs\n        else:\n            mask = np.isnan(sampled_values)  # No explicit nodata, replace all NaNs\n\n        sampled_values[mask] = values[mask]  # Update only missing values\n\n    return sampled_values\n</code></pre>"},{"location":"api/processing/#gigaspatial.processing.tif_processor.sample_multiple_tifs_by_polygons","title":"<code>sample_multiple_tifs_by_polygons(tif_processors, polygon_list, stat='mean')</code>","text":"<p>Sample raster values from multiple TIFF files for polygons in a list and join the results.</p> <p>Parameters: - tif_processors: List of TifProcessor instances. - polygon_list: List of polygon geometries (can include MultiPolygons). - stat: Aggregation statistic to compute within each polygon (mean, median, sum, min, max).</p> <p>Returns: - A NumPy array of sampled values, taking the first non-nodata value encountered.</p> Source code in <code>gigaspatial/processing/tif_processor.py</code> <pre><code>def sample_multiple_tifs_by_polygons(\n    tif_processors: List[TifProcessor],\n    polygon_list: List[Union[Polygon, MultiPolygon]],\n    stat: str = \"mean\",\n) -&gt; np.ndarray:\n    \"\"\"\n    Sample raster values from multiple TIFF files for polygons in a list and join the results.\n\n    Parameters:\n    - tif_processors: List of TifProcessor instances.\n    - polygon_list: List of polygon geometries (can include MultiPolygons).\n    - stat: Aggregation statistic to compute within each polygon (mean, median, sum, min, max).\n\n    Returns:\n    - A NumPy array of sampled values, taking the first non-nodata value encountered.\n    \"\"\"\n    sampled_values = np.full(len(polygon_list), np.nan, dtype=np.float32)\n\n    for tp in tif_processors:\n        values = tp.sample_by_polygons(polygon_list=polygon_list, stat=stat)\n\n        mask = np.isnan(sampled_values)  # replace all NaNs\n\n        sampled_values[mask] = values[mask]  # Update only values with samapled value\n\n    return sampled_values\n</code></pre>"},{"location":"api/processing/#gigaspatial.processing.utils","title":"<code>utils</code>","text":""},{"location":"api/processing/#gigaspatial.processing.utils.assign_id","title":"<code>assign_id(df, required_columns, id_column='id')</code>","text":"<p>Generate IDs for any entity type in a pandas DataFrame.</p> <p>Parameters:</p> Name Type Description Default <code>df</code> <code>DataFrame</code> <p>Input DataFrame containing entity data</p> required <code>required_columns</code> <code>List[str]</code> <p>List of column names required for ID generation</p> required <code>id_column</code> <code>str</code> <p>Name for the id column that will be generated</p> <code>'id'</code> <p>Returns:</p> Type Description <code>DataFrame</code> <p>pd.DataFrame: DataFrame with generated id column</p> Source code in <code>gigaspatial/processing/utils.py</code> <pre><code>def assign_id(\n    df: pd.DataFrame, required_columns: List[str], id_column: str = \"id\"\n) -&gt; pd.DataFrame:\n    \"\"\"\n    Generate IDs for any entity type in a pandas DataFrame.\n\n    Args:\n        df (pd.DataFrame): Input DataFrame containing entity data\n        required_columns (List[str]): List of column names required for ID generation\n        id_column (str): Name for the id column that will be generated\n\n    Returns:\n        pd.DataFrame: DataFrame with generated id column\n    \"\"\"\n    # Create a copy to avoid modifying the original DataFrame\n    df = df.copy()\n\n    # Check if ID column exists, if not create it with None values\n    if id_column not in df.columns:\n        df[id_column] = None\n\n    # Check required columns exist\n    if not all(col in df.columns for col in required_columns):\n        return df\n\n    # Create identifier concat for UUID generation\n    df[\"identifier_concat\"] = (\n        df[required_columns].astype(str).fillna(\"\").agg(\"\".join, axis=1)\n    )\n\n    # Generate UUIDs only where all required fields are present and no existing ID\n    mask = df[id_column].isna()\n    for col in required_columns:\n        mask &amp;= df[col].notna()\n\n    # Apply UUID generation only where mask is True\n    df.loc[mask, id_column] = df.loc[mask, \"identifier_concat\"].apply(\n        lambda x: str(uuid.uuid3(uuid.NAMESPACE_DNS, x))\n    )\n\n    # Drop temporary column\n    df = df.drop(columns=[\"identifier_concat\"])\n\n    return df\n</code></pre>"},{"location":"examples/","title":"Examples","text":"<p>Welcome to the examples section of the <code>gigaspatial</code> package. Here, you\u2019ll find practical examples that demonstrate how to use the package for various geospatial data tasks.</p>"},{"location":"examples/#getting-started","title":"Getting Started","text":"<p>If you\u2019re new to <code>gigaspatial</code>, start with the Quick Start Guide to learn the basics.</p>"},{"location":"examples/#example-categories","title":"Example Categories","text":""},{"location":"examples/#1-data-downloading","title":"1. Data Downloading","text":"<p>Learn how to download geospatial data from various sources.</p> <ul> <li>Downloading GHSL Data</li> <li>Fetching OSM Data</li> </ul>"},{"location":"examples/#2-data-processing","title":"2. Data Processing","text":"<p>Explore examples of processing geospatial data, such as GeoTIFF files.</p> <ul> <li>Processing GeoTIFF Files</li> </ul>"},{"location":"examples/#3-data-storage-in-progress","title":"3. Data Storage (In Progress)","text":"<p>Discover how to store and retrieve geospatial data in different formats.</p> <ul> <li>Saving Data to GeoJSON (Coming Soon)</li> <li>Storing Data in a Database (Coming Soon)</li> </ul>"},{"location":"examples/#4-data-visualization-in-progress","title":"4. Data Visualization (In Progress)","text":"<p>Learn how to visualize geospatial data using popular libraries.</p> <ul> <li>Plotting Data with Matplotlib (Coming Soon)</li> <li>Creating Interactive Maps with Folium (Coming Soon)</li> </ul>"},{"location":"examples/#5-advanced-use-cases-in-progress","title":"5. Advanced Use Cases (In Progress)","text":"<p>Explore advanced examples that combine multiple functionalities.</p> <ul> <li>Building a Geospatial Pipeline (Coming Soon)</li> <li>Integrating with External APIs (Coming Soon)</li> </ul>"},{"location":"examples/#contributing-examples","title":"Contributing Examples","text":"<p>If you\u2019d like to contribute your own examples, please follow the Contributing Guidelines.</p>"},{"location":"examples/#feedback","title":"Feedback","text":"<p>If you have suggestions for additional examples or encounter any issues, feel free to open an issue or join our Discord community.</p>"},{"location":"examples/advanced/","title":"Advanced Examples","text":"<p>It's in development.</p>"},{"location":"examples/basic/","title":"Data Handler Examples","text":"<p>This guide provides examples of how to use various data handlers in GigaSpatial to access and process different types of spatial data.</p>"},{"location":"examples/basic/#population-data-worldpop","title":"Population Data (WorldPop)","text":"<pre><code>from gigaspatial.handlers import WorldPopHandler\n\n# Get population data for a specific country and year\nconfig = {\n    \"country_code\": \"KEN\",\n    \"year\": 2020,\n}\n\n# Initialize the WorldPop handler\nworldpop = WorldPopDownloader(config = config)\npath_to_data = worldpop.download_dataset()\n</code></pre>"},{"location":"examples/basic/#building-footprints","title":"Building Footprints","text":""},{"location":"examples/basic/#google-open-buildings","title":"Google Open Buildings","text":"<pre><code>from gigaspatial.handlers import GoogleOpenBuildingsHandler\nfrom gigaspatial.generators import PoiViewGenerator\nfrom gigaspatial.core.io import LocalDataStore\nimport geopandas as gpd\n\n# Initialize data store and handlers\ndata_store = LocalDataStore()\ngob_handler = GoogleOpenBuildingsHandler(data_store=data_store)\n\n# Example 1: Load building data for a country\ncountry_code = \"KEN\"  # Kenya\npolygons_gdf = gob_handler.load_polygons(country_code, ensure_available=True)\nprint(f\"Loaded {len(polygons_gdf)} building polygons\")\n\n# Example 2: Load building data for specific points\npoints = [(36.8219, -1.2921), (36.8172, -1.2867)]  # Nairobi coordinates\npoints_gdf = gob_handler.load_polygons(points, ensure_available=True)\nprint(f\"Loaded {len(points_gdf)} building polygons for points\")\n\n# Example 3: Map nearest buildings to points using PoiViewGenerator\n# Initialize POI view generator with points\npoi_generator = PoiViewGenerator(points=points)\n\n# Map nearest Google buildings to POIs\nresult_gdf = poi_generator.map_google_buildings(gob_handler)\nprint(\"\\nPOIs with nearest building information:\")\nprint(result_gdf[[\"poi_id\", \"nearest_google_building_id\", \"nearest_google_building_distance\"]].head())\n\n# Example 4: Save the enriched POI view\noutput_path = poi_generator.save_view(\"nairobi_buildings\", output_format=\"geojson\")\nprint(f\"\\nSaved enriched POI view to: {output_path}\")\n</code></pre> <p>This example demonstrates: 1. Loading building data for a country or specific points 2. Using PoiViewGenerator to map nearest buildings to points of interest 3. Saving the enriched POI view with building information</p> <p>The resulting GeoDataFrame includes: - Original POI information - Nearest building ID - Distance to the nearest building</p>"},{"location":"examples/basic/#microsoft-global-buildings","title":"Microsoft Global Buildings","text":"<pre><code>from gigaspatial.handlers import MSBuildingsDownloader\n\n# Initialize the handler\nmgb = MSBuildingsDownloader()\n\npoints = [(1.25214, 5.5124), (3.45234, 12.51232)]\n\n# Get building footprints\nlist_of_paths = mgb.download(\n    points=points\n)\n</code></pre>"},{"location":"examples/basic/#satellite-imagery","title":"Satellite Imagery","text":""},{"location":"examples/basic/#maxar-imagery","title":"Maxar Imagery","text":"<pre><code>from gigaspatial.handlers import MaxarImageHandler\n\n# Initialize woith default config which reads credentials config from your environment\nmaxar = MaxarImageDownloader()\n\n# Download imagery\nmaxar.download_images_by_coordinates(\n    data=coordinates,\n    res_meters_pixel=0.6,\n    output_dir=\"bronze/maxar\",\n    bbox_size = 300.0,\n    image_prefix = \"maxar_\"\n)\n</code></pre>"},{"location":"examples/basic/#mapbox-imagery","title":"Mapbox Imagery","text":"<pre><code>from gigaspatial.handlers import MapboxImageDownloader\n\n# Initialize with your access token or config will be read from your environment\nmapbox = MapboxImageDownloader(access_token=\"your_access_token\", style_id=\"mapbox/satellite-v9\")\n\n# Get satellite imagery\nmapbox.download_images_by_coordinates(\n    data=coordinates,\n    res_meters_pixel=300.0,\n    output_dir=\"bronze/mapbox\",\n    image_size=(256,256),\n    image_prefix=\"mapbox_\"\n)\n</code></pre>"},{"location":"examples/basic/#internet-speed-data-ookla","title":"Internet Speed Data (Ookla)","text":"<pre><code>from gigaspatial.core.io import LocalDataStore\nfrom gigaspatial.handlers import (\n    OoklaSpeedtestTileConfig, CountryOoklaTiles\n)\n\n# Initialize OoklaSpeedtestTileConfig for a specific quarter and year\nookla_config = OoklaSpeedtestTileConfig(\n    service_type=\"fixed\", year=2023, quarter=3, data_store=LocalDataStore())\n\n# Download and read the Ookla tile data\ndf = ookla_config.read_tile()\nprint(df.head())  # Display the first few rows of the dataset\n\n# Generate country-specific Ookla tiles\ncountry_ookla_tiles = CountryOoklaTiles.from_country(\"KEN\", ookla_config)\n\n# Convert to DataFrame and display\ncountry_df = country_ookla_tiles.to_dataframe()\nprint(country_df.head())\n\n# Convert to GeoDataFrame and display\ncountry_gdf = country_ookla_tiles.to_geodataframe()\nprint(country_gdf.head())\n</code></pre>"},{"location":"examples/basic/#administrative-boundaries","title":"Administrative Boundaries","text":"<pre><code>from gigaspatial.handlers import AdminBoundaries\n\n# Load level-1 administrative boundaries for Kenya\nadmin_boundaries = AdminBoundaries.create(country_code=\"KEN\", admin_level=1)\n\n# Convert to a GeoDataFrame\ngdf = admin_boundaries.to_geodataframe()\n</code></pre>"},{"location":"examples/basic/#openstreetmap-data","title":"OpenStreetMap Data","text":"<pre><code>from gigaspatial.handlers.osm import OSMAmenityFetcher\n\n# Example 1: Fetching school amenities in Kenya\nfetcher = OSMAmenityFetcher(country_iso2=\"KE\", amenity_types=[\"school\"])\nschools_df = fetcher.get_locations()\nprint(schools_df.head())\n\n# Example 2: Fetching hospital and clinic amenities in Tanzania\nfetcher = OSMAmenityFetcher(country_iso2=\"TZ\", amenity_types=[\"hospital\", \"clinic\"])\nhealthcare_df = fetcher.get_locations()\nprint(healthcare_df.head())\n\n# Example 3: Fetching restaurant amenities in Ghana since 2020\nfetcher = OSMAmenityFetcher(country_iso2=\"GH\", amenity_types=[\"restaurant\"])\nrestaurants_df = fetcher.get_locations(since_year=2020)\nprint(restaurants_df.head())\n</code></pre>"},{"location":"examples/use-cases/","title":"Use Cases","text":"<p>It's in development.</p>"},{"location":"examples/downloading/ghsl/","title":"Downloading and Processing GHSL Data","text":"<p>This example demonstrates how to download and process data from the Global Human Settlement Layer (GHSL) using the <code>GHSLDataHandler</code> class.</p>"},{"location":"examples/downloading/ghsl/#prerequisites","title":"Prerequisites","text":"<p>Ensure you have installed the <code>gigaspatial</code> package and set up the necessary configuration. Follow the Installation Guide if you haven't already.</p>"},{"location":"examples/downloading/ghsl/#example-code","title":"Example Code","text":"<pre><code>from gigaspatial.handlers import GHSLDataHandler\n\n# Initialize the handler with desired product and parameters\nghsl_handler = GHSLDataHandler(\n    product=\"GHS_BUILT_S\",  # Built-up surface\n    year=2020,\n    resolution=100,  # 100m resolution\n)\n\n# Download and load data for a specific country\ncountry_code = \"TUR\"\ndownloaded_files = ghsl_handler.load_data(country_code, ensure_available=True)\n\n# Load the data into a DataFrame\ndf = ghsl_handler.load_into_dataframe(country_code, ensure_available=True)\nprint(df.head())\n\n# You can also load data for specific points\npoints = [(38.404581, 27.4816677), (39.8915702, 32.7809618)]  # Example coordinates\ndf_points = ghsl_handler.load_into_dataframe(points, ensure_available=True)\n</code></pre>"},{"location":"examples/downloading/ghsl/#explanation","title":"Explanation","text":"<ul> <li>GHSLDataHandler: This class provides a unified interface for downloading and processing GHSL data.</li> <li>Available Products:</li> <li><code>GHS_BUILT_S</code>: Built-up surface</li> <li><code>GHS_BUILT_H_AGBH</code>: Average building height</li> <li><code>GHS_BUILT_H_ANBH</code>: Average number of building heights</li> <li><code>GHS_BUILT_V</code>: Building volume</li> <li><code>GHS_POP</code>: Population</li> <li><code>GHS_SMOD</code>: Settlement model</li> <li>Parameters:</li> <li><code>product</code>: The GHSL product to use</li> <li><code>year</code>: The year of the data (default: 2020)</li> <li><code>resolution</code>: The resolution in meters (default: 100)</li> <li>Methods:</li> <li><code>load_data()</code>: Downloads and loads the data</li> <li><code>load_into_dataframe()</code>: Loads the data into a pandas DataFrame</li> </ul>"},{"location":"examples/downloading/ghsl/#next-steps","title":"Next Steps","text":"<p>Once the data is downloaded and processed, you can: 1. Store the data using the <code>DataStore</code> class 2. Visualize the data using <code>geopandas</code> and <code>matplotlib</code> 3. Process the data further using the Processing Examples</p> <p>Back to Examples</p>"},{"location":"examples/downloading/osm/","title":"Downloading OSM Data","text":"<p>This example demonstrates how to fetch and process OpenStreetMap (OSM) data using the <code>OSMLocationFetcher</code> class.</p>"},{"location":"examples/downloading/osm/#prerequisites","title":"Prerequisites","text":"<p>Ensure you have installed the <code>gigaspatial</code> package and set up the necessary configuration. Follow the Installation Guide if you haven\u2019t already.</p>"},{"location":"examples/downloading/osm/#example-code","title":"Example Code","text":"<pre><code>from gigaspatial.handlers.osm import OSMLocationFetcher\n\n# Initialize the fetcher\nfetcher = OSMLocationFetcher(\n    country=\"Spain\",\n    location_types=[\"amenity\", \"building\", \"shop\"]\n)\n\n# Fetch and process OSM locations\nlocations = fetcher.fetch_locations(since_year=2020, handle_duplicates=\"combine\")\nprint(locations.head())\n</code></pre>"},{"location":"examples/downloading/osm/#explanation","title":"Explanation","text":"<ul> <li>OSMLocationFetcher: This class fetches and processes location data from OpenStreetMap.</li> <li>fetch_locations: This method fetches and processes OSM data based on the specified criteria.</li> </ul>"},{"location":"examples/downloading/osm/#next-steps","title":"Next Steps","text":"<p>Once the data is fetched, you can process it using the Processing Examples.</p> <p>Back to Examples</p>"},{"location":"examples/processing/tif/","title":"Processing GeoTIFF Files","text":"<p>This example demonstrates how to process GeoTIFF files using the <code>TifProcessor</code> class.</p>"},{"location":"examples/processing/tif/#prerequisites","title":"Prerequisites","text":"<p>Ensure you have installed the <code>gigaspatial</code> package and set up the necessary configuration. Follow the Installation Guide if you haven\u2019t already.</p>"},{"location":"examples/processing/tif/#example-code","title":"Example Code","text":"<pre><code>from gigaspatial.processing.tif_processor import TifProcessor\n\n# Initialize the processor\nprocessor = TifProcessor(\"/path/to/ghsl/data/ghsl_data.tif\")\n\n# Process the GeoTIFF file\nprocessed_data = processor.to_dataframe()\nprint(processed_data.head())\n</code></pre>"},{"location":"examples/processing/tif/#explanation","title":"Explanation","text":"<ul> <li>TifProcessor: This class processes GeoTIFF files and extracts relevant data.</li> <li>process: This method processes the GeoTIFF file and returns the data as a NumPy array.</li> </ul>"},{"location":"examples/processing/tif/#next-steps","title":"Next Steps","text":"<p>Once the data is processed, you can store it using the Storage Examples.</p> <p>Back to Examples</p>"},{"location":"getting-started/installation/","title":"Installation Guide","text":"<p>This guide will walk you through the steps to install the <code>gigaspatial</code> package on your system. The package is compatible with Python 3.10 and above.</p>"},{"location":"getting-started/installation/#prerequisites","title":"Prerequisites","text":"<p>Before installing <code>gigaspatial</code>, ensure you have Python installed on your system. You can check your Python version by running:</p> <pre><code>python --version\n</code></pre> <p>If Python is not installed, you can download it from the official Python website.</p>"},{"location":"getting-started/installation/#installing-from-pypi","title":"Installing from PyPI","text":"<p>The easiest way to install <code>gigaspatial</code> is directly from PyPI using pip:</p> <pre><code>pip install giga-spatial\n</code></pre> <p>This will install the latest stable version of the package along with all its dependencies.</p>"},{"location":"getting-started/installation/#installing-from-source","title":"Installing from Source","text":"<p>If you need to install a specific version or want to contribute to the development, you can install from the source:</p> <ol> <li> <p>Clone the Repository:    <pre><code>git clone https://github.com/unicef/giga-spatial.git\ncd giga-spatial\n</code></pre></p> </li> <li> <p>Install the Package:    <pre><code>pip install .\n</code></pre></p> </li> </ol>"},{"location":"getting-started/installation/#installing-in-development-mode","title":"Installing in Development Mode","text":"<p>If you plan to contribute to the package or modify the source code, you can install it in development mode. This allows you to make changes to the code without reinstalling the package:</p> <pre><code>pip install -e .\n</code></pre>"},{"location":"getting-started/installation/#installing-dependencies","title":"Installing Dependencies","text":"<p>The package dependencies are automatically installed when you install <code>gigaspatial</code>. However, if you need to install them manually, you can use:</p> <pre><code>pip install -r requirements.txt\n</code></pre>"},{"location":"getting-started/installation/#verifying-the-installation","title":"Verifying the Installation","text":"<p>After installation, you can verify that the package is installed correctly by running:</p> <pre><code>python -c \"import gigaspatial; print(gigaspatial.__version__)\"\n</code></pre> <p>This should print the version of the installed package.</p>"},{"location":"getting-started/installation/#troubleshooting","title":"Troubleshooting","text":"<p>If you encounter any issues during installation, consider the following:</p> <ul> <li> <p>Ensure <code>pip</code> is up-to-date:   <pre><code>pip install --upgrade pip\n</code></pre></p> </li> <li> <p>Check for conflicting dependencies: If you have other Python packages installed that might conflict with <code>gigaspatial</code>, consider using a virtual environment.</p> </li> <li> <p>Use a Virtual Environment: To avoid conflicts with other Python packages, you can create a virtual environment:   <pre><code>python -m venv venv\nsource venv/bin/activate  # On Windows, use `venv\\Scripts\\activate`\npip install giga-spatial  # or pip install . if installing from source\n</code></pre></p> </li> </ul>"},{"location":"getting-started/installation/#next-steps","title":"Next Steps","text":"<p>Once the installation is complete, you can proceed to the Quick Start Guide to begin using the <code>gigaspatial</code> package.</p>"},{"location":"getting-started/quickstart/","title":"Quick Start Guide","text":"<p>This guide will walk you through the basic usage of the <code>gigaspatial</code> package. By the end of this guide, you will be able to download, process, and store geospatial data using the package.</p>"},{"location":"getting-started/quickstart/#prerequisites","title":"Prerequisites","text":"<p>Before you begin, ensure that you have installed the <code>gigaspatial</code> package. If you haven't installed it yet, follow the Installation Guide.</p>"},{"location":"getting-started/quickstart/#importing-the-package","title":"Importing the Package","text":"<p>Start by importing the <code>gigaspatial</code> package:</p> <pre><code>import gigaspatial as gs\n</code></pre>"},{"location":"getting-started/quickstart/#setting-up-configuration","title":"Setting Up Configuration","text":"<p>The <code>gigaspatial</code> package uses a configuration file (<code>config.py</code>) to manage paths, API keys, and other settings. You can customize the configuration as needed.</p>"},{"location":"getting-started/quickstart/#using-environment-variables","title":"Using Environment Variables","text":"<p>The package can read configuration settings from an environment file (e.g., <code>.env</code>). Here's an example of how to set up the <code>.env</code> file based on the <code>env_sample</code>:</p> <pre><code># Paths for different data types\nBRONZE_DIR=/path/to/your/bronze_tier_data\nSILVER_DIR=/path/to/your/silver_tier_data\nGOLD_DIR=/path/to/your/gold_tier_data\nVIEWS_DIR=/path/to/your/views_data\nADMIN_BOUNDARIES_DIR=/path/to/your/admin_boundaries_data\n\n# API keys and tokens\nMAPBOX_ACCESS_TOKEN=your_mapbox_token_here\nMAXAR_USERNAME=your_maxar_username_here\nMAXAR_PASSWORD=your_maxar_password_here\nMAXAR_CONNECTION_STRING=your_maxar_key_here\n</code></pre> <p>The <code>config.py</code> file will automatically read these environment variables and set the paths and keys accordingly.</p>"},{"location":"getting-started/quickstart/#setting-paths-manually","title":"Setting Paths Manually","text":"<p>You can also set paths manually in your code:</p> <pre><code>from gigaspatial.config import config\n\n# Example: Setting custom data storage paths\nconfig.set_path(\"bronze\", \"/path/to/your/bronze_tier_data\")\nconfig.set_path(\"gold\", \"/path/to/your/gold_tier_data\")\nconfig.set_path(\"views\", \"/path/to/your/views_data\")\n</code></pre> <p>API keys and tokens should be set through environment variables.</p>"},{"location":"getting-started/quickstart/#downloading-and-processing-geospatial-data","title":"Downloading and Processing Geospatial Data","text":"<p>The <code>gigaspatial</code> package provides several handlers for different types of geospatial data. Here are examples for two commonly used handlers:</p>"},{"location":"getting-started/quickstart/#ghsl-global-human-settlement-layer-data","title":"GHSL (Global Human Settlement Layer) Data","text":"<p>The <code>GHSLDataHandler</code> provides access to various GHSL products including built-up surface, building height, population, and settlement model data:</p> <pre><code>from gigaspatial.handlers import GHSLDataHandler\n\n# Initialize the handler with desired product and parameters\nghsl_handler = GHSLDataHandler(\n    product=\"GHS_BUILT_S\",  # Built-up surface\n    year=2020,\n    resolution=100,  # 100m resolution\n)\n\n# Download data for a specific country\ncountry_code = \"TUR\"\ndownloaded_files = ghsl_handler.load_data(country_code, ensure_available = True)\n\n# Load the data into a DataFrame\ndf = ghsl_handler.load_into_dataframe(country_code, ensure_available = True)\nprint(df.head())\n\n# You can also load data for specific points or geometries\npoints = [(38.404581,27.4816677), (39.8915702, 32.7809618)]\ndf_points = ghsl_handler.load_into_dataframe(points, ensure_available = True)\n</code></pre>"},{"location":"getting-started/quickstart/#google-open-buildings-data","title":"Google Open Buildings Data","text":"<p>The <code>GoogleOpenBuildingsHandler</code> provides access to Google's Open Buildings dataset, which includes building footprints and points:</p> <pre><code>from gigaspatial.handlers import GoogleOpenBuildingsHandler\n\n# Initialize the handler\ngob_handler = GoogleOpenBuildingsHandler()\n\n# Download and load building polygons for a country\ncountry_code = \"TUR\"\npolygons_gdf = gob_handler.load_polygons(country_code, ensure_available = True)\n\n# Download and load building points for a country\npoints_gdf = gob_handler.load_points(country_code, ensure_available = True)\n\n# You can also load data for specific points or geometries\npoints = [(38.404581, 27.4816677), (39.8915702, 32.7809618)]\npolygons_gdf = gob_handler.load_polygons(points, ensure_available = True)\n</code></pre>"},{"location":"getting-started/quickstart/#storing-geospatial-data","title":"Storing Geospatial Data","text":"<p>You can store the processed data in various formats using the <code>DataStore</code> class from the <code>core.io</code> module. Here's an example of saving data to a parquet file:</p> <pre><code>from gigaspatial.core.io import LocalDataStore\n\n# Initialize the data store\ndata_store = LocalDataStore()\n\n# Save the processed data to a parquet file\nwith data_store.open(\"/path/to/your/output/processed_data.parquet\", \"rb\") as f:\n    processed_data.to_parquet(f)\n</code></pre> <p>If your dataset is already a <code>pandas.DataFrame</code> or <code>geopandas.GeoDataFrame</code>, <code>write_dataset</code> method from the <code>core.io.writers</code> module can be used to write the dataset in various formats. </p> <pre><code>from gigaspatial.core.io.writers import write_dataset\n\n# Save the processed data to a GeoJSON file\nwrite_dataset(data=processed_data, data_store=data_store, path=\"/path/to/your/output/processed_data.geojson\")\n</code></pre>"},{"location":"getting-started/quickstart/#visualizing-geospatial-data","title":"Visualizing Geospatial Data","text":"<p>To visualize the geospatial data, you can use libraries like <code>geopandas</code> and <code>matplotlib</code>. Here's an example of plotting the processed data on a map:</p> <pre><code>import geopandas as gpd\nimport matplotlib.pyplot as plt\n\n# Load the GeoJSON file\ngdf = gpd.read_file(\"/path/to/your/output/processed_data.geojson\")\n\n# Plot the data\ngdf.plot()\nplt.show()\n</code></pre> <p><code>geopandas.GeoDataFrame.explore</code> can also be used to visualise the data on interactive map based on <code>GeoPandas</code> and <code>folium/leaflet.js</code>: <pre><code># Visualize the data\ngdf.explore(\"population\", cmap=\"Blues\")\n</code></pre></p>"},{"location":"getting-started/quickstart/#next-steps","title":"Next Steps","text":"<p>Now that you have a basic understanding of how to use the <code>gigaspatial</code> package, you can explore more advanced features and configurations. Check out the User Guide for detailed documentation and examples.</p>"},{"location":"getting-started/quickstart/#additional-resources","title":"Additional Resources","text":"<ul> <li>API Documentation: Detailed documentation of all classes and functions.</li> <li>Examples: Real-world examples and use cases.</li> <li>Changelog: Information about the latest updates and changes.</li> </ul>"},{"location":"user-guide/","title":"User Guide","text":"<p>Welcome to the User Guide for the <code>gigaspatial</code> package. This guide provides detailed documentation on how to use the package for various geospatial data tasks.</p>"},{"location":"user-guide/#getting-started","title":"Getting Started","text":"<p>If you\u2019re new to <code>gigaspatial</code>, start with the Quick Start Guide to learn the basics.</p>"},{"location":"user-guide/#core-concepts","title":"Core Concepts","text":""},{"location":"user-guide/#1-configuration","title":"1. Configuration","text":"<p>Learn how to configure the package, including setting paths, API keys, and other settings.</p> <ul> <li>Configuration Overview</li> <li>Using Environment Variables</li> <li>Setting Paths and Keys Manually</li> </ul>"},{"location":"user-guide/#additional-resources","title":"Additional Resources","text":"<ul> <li>Examples: Real-world examples and use cases.</li> <li>API Reference: Detailed documentation of all classes and functions.</li> <li>Changelog: Information about the latest updates and changes.</li> <li>Contributing: Guidelines for contributing to the project.</li> </ul>"},{"location":"user-guide/#support","title":"Support","text":"<p>If you encounter any issues or have questions, feel free to open an issue or join our Discord community.</p>"},{"location":"user-guide/configuration/","title":"Configuration","text":"<p>The <code>gigaspatial</code> package uses a configuration file (<code>config.py</code>) to manage paths, API keys, and other settings. This guide explains how to configure the package to suit your needs.</p>"},{"location":"user-guide/configuration/#using-environment-variables","title":"Using Environment Variables","text":"<p>The package can read configuration settings from an environment file (e.g., <code>.env</code>). Here\u2019s an example of how to set up the <code>.env</code> file based on the <code>env_sample</code>:</p> <pre><code># Paths for different data types\nBRONZE_DIR=/path/to/your/bronze_tier_data\nSILVER_DIR=/path/to/your/silver_tier_data\nGOLD_DIR=/path/to/your/gold_tier_data\nVIEWS_DIR=/path/to/your/views_data\nADMIN_BOUNDARIES_DIR=/path/to/your/admin_boundaries_data\n\n# API keys and tokens\nMAPBOX_ACCESS_TOKEN=your_mapbox_token_here\nMAXAR_USERNAME=your_maxar_username_here\nMAXAR_PASSWORD=your_maxar_password_here\nMAXAR_CONNECTION_STRING=your_maxar_key_here\n</code></pre> <p>The <code>config.py</code> file will automatically read these environment variables and set the paths and keys accordingly.</p>"},{"location":"user-guide/configuration/#setting-paths-manually","title":"Setting Paths Manually","text":"<p>You can also set paths and keys manually in your code. Here\u2019s an example:</p> <pre><code>from gigaspatial.config import config\n\n# Example: Setting custom data storage paths\nconfig.set_path(\"bronze\", \"/path/to/your/bronze_tier_data\")\nconfig.set_path(\"gold\", \"/path/to/your/gold_tier_data\")\nconfig.set_path(\"views\", \"/path/to/your/views_data\")\n</code></pre> <p>API keys and tokens should be set through environment variables.</p>"},{"location":"user-guide/configuration/#verifying-the-configuration","title":"Verifying the Configuration","text":"<p>After setting up the configuration, you can verify it by printing the current settings:</p> <pre><code>from gigaspatial.config import config\n\n# Print all configuration settings\nprint(config)\n</code></pre>"},{"location":"user-guide/configuration/#troubleshooting","title":"Troubleshooting","text":"<p>If you encounter issues with the configuration, consider the following:</p> <ul> <li>Ensure <code>.env</code> File Exists: Make sure the <code>.env</code> file is in the root directory of your project.</li> <li>Check Environment Variables: Verify that the environment variables are correctly set in the <code>.env</code> file.</li> <li>Use Absolute Paths: When setting paths manually, use absolute paths to avoid issues with relative paths.</li> </ul>"},{"location":"user-guide/configuration/#next-steps","title":"Next Steps","text":"<p>Once the configuration is set up, you can proceed to the Data Handling Guide to start using the <code>gigaspatial</code> package.</p> <p>Back to User Guide</p>"}]}