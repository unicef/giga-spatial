{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Welcome to GigaSpatial","text":"<p>GigaSpatial is a powerful Python package designed for spatial data analysis and processing, providing efficient tools and utilities for handling geographic information systems (GIS) data.</p>"},{"location":"#features","title":"Features","text":"<ul> <li>Efficient Data Processing: Fast and memory-efficient processing of large spatial datasets</li> <li>Multiple Format Support: Support for various spatial data formats</li> <li>Advanced Analysis Tools: Comprehensive set of spatial analysis tools</li> <li>Easy Integration: Seamless integration with popular GIS and data science libraries</li> <li>Scalable Solutions: Designed to handle both small and large-scale spatial data processing tasks</li> </ul>"},{"location":"#quick-links","title":"Quick Links","text":"<ul> <li>Installation Guide</li> <li>Quick Start Tutorial</li> <li>API Reference</li> <li>Example Gallery</li> </ul>"},{"location":"#getting-help","title":"Getting Help","text":"<p>If you need help using GigaSpatial, please check out our:</p> <ul> <li>User Guide for detailed usage instructions</li> <li>API Reference for detailed function and class documentation</li> <li>GitHub Issues for bug reports and feature requests</li> <li>Contributing Guide for guidelines on how to contribute to the project</li> </ul>"},{"location":"#license","title":"License","text":"<p>GigaSpatial is released under the AGPL-3.0 License. See the LICENSE file for more details. </p>"},{"location":"changelog/","title":"Changelog","text":"<p>All notable changes to this project will be documented in this file.</p>"},{"location":"changelog/#v073-2025-11-11","title":"[v0.7.3] - 2025-11-11","text":""},{"location":"changelog/#added","title":"Added","text":"<ul> <li> <p>SnowflakeDataStore Support</p> <ul> <li>New <code>SnowflakeDataStore</code> class implementing the <code>DataStore</code> interface for Snowflake stages.</li> <li>Supports file operations (read, write, list, delete) on Snowflake internal stages.</li> <li>Integrated with <code>gigaspatial/config.py</code> for centralized configuration via environment variables.</li> <li>Provides directory-like operations (<code>mkdir</code>, <code>rmdir</code>, <code>walk</code>, <code>is_dir</code>, <code>is_file</code>) for conceptual directories in Snowflake stages.</li> <li>Includes context manager support and connection management.</li> <li>Full compatibility with existing <code>DataStore</code> abstraction.</li> </ul> </li> <li> <p>BaseHandler: Config-Level Data Unit Caching</p> <ul> <li><code>BaseHandlerConfig</code> now maintains internal <code>_unit_cache</code> for data unit and geometry caching.</li> <li>Cache stores tuples of <code>(units, search_geometry)</code> for efficient reuse across handler, downloader, and reader operations.</li> <li>New methods:<ul> <li><code>get_cached_search_geometry()</code>: Retrieve cached geometry for a source.</li> <li><code>clear_unit_cache()</code>: Clear cached data for testing or manual refreshes.</li> <li><code>_cache_key()</code>: Generate canonical cache keys from various source types.</li> </ul> </li> <li>Benefits all components (handler, downloader, reader) regardless of entry point.</li> </ul> </li> <li> <p>Unified Geometry Extraction in BaseHandlerConfig</p> <ul> <li>New <code>extract_search_geometry()</code> method providing standardized geometry extraction from various source types:<ul> <li>Country codes (via <code>AdminBoundaries</code>)</li> <li>Shapely geometries (<code>BaseGeometry</code>)</li> <li>GeoDataFrames with automatic CRS handling</li> <li>Lists of points or coordinate tuples (converted to <code>MultiPoint</code>)</li> </ul> </li> <li>Centralizes geometry conversion logic, eliminating duplication across handler methods.</li> </ul> </li> <li> <p>BaseHandler: Crop-to-Source Feature for Handlers</p> <ul> <li>New <code>crop_to_source</code> parameter in <code>BaseHandlerReader.load()</code> and <code>BaseHandler.load_data()</code> methods.</li> <li>Allows users to load data clipped to exact source boundaries rather than full data units (e.g., tiles).</li> <li>Particularly useful for tile-based datasets (Google Open Buildings, GHSL) where tiles extend beyond requested regions.</li> <li>Implemented <code>crop_to_geometry()</code> method in <code>BaseHandlerReader</code> for spatial filtering:<ul> <li>Supports <code>(Geo)DataFrame</code> clipping using geometry intersection.</li> <li>Supports raster clipping using <code>TifProcessor</code>'s <code>clip_to_geometry</code> method.</li> <li>Extensible for future cropping implementations.</li> </ul> </li> <li>Search geometries are now cached alongside data units for efficient cropping operations.</li> </ul> </li> <li> <p>S2 Zonal View Generator (<code>S2ViewGenerator</code>)</p> <ul> <li>New generator for producing zonal views using Google S2 cells (levels 0\u201330).</li> <li>Supports sources:<ul> <li>Country name (<code>str</code>) via <code>CountryS2Cells.create(...)</code></li> <li>Shapely geometry or <code>gpd.GeoDataFrame</code> via <code>S2Cells.from_spatial(...)</code></li> <li>Points (<code>List[Point | (lon, lat)]</code>) via <code>S2Cells.from_points(...)</code></li> <li>Explicit cells (<code>List[int | str]</code>, S2 IDs or tokens) via <code>S2Cells.from_cells(...)</code></li> </ul> </li> <li>Uses <code>cell_token</code> as the zone identifier.</li> <li>Includes <code>map_wp_pop()</code> convenience method (auto-uses stored country when available).</li> </ul> </li> <li> <p>H3 Zonal View Generator (<code>H3ViewGenerator</code>)</p> <ul> <li>New generator for producing zonal views using H3 hexagons (resolutions 0\u201315).</li> <li>Supports sources:<ul> <li>Country name (<code>str</code>) via <code>CountryH3Hexagons.create(...)</code></li> <li>Shapely geometry or <code>gpd.GeoDataFrame</code> via <code>H3Hexagons.from_spatial(...)</code></li> <li>Points (<code>List[Point | (lon, lat)]</code>) via <code>H3Hexagons.from_spatial(...)</code></li> <li>Explicit H3 indexes (<code>List[str]</code>) via <code>H3Hexagons.from_hexagons(...)</code></li> </ul> </li> <li>Uses <code>h3</code> as the zone identifier.</li> <li>Includes <code>map_wp_pop()</code> convenience method (auto-uses stored country when available).</li> </ul> </li> <li> <p>TifProcessor: MultiPoint clipping support</p> <ul> <li><code>_prepare_geometry_for_clipping()</code> now accepts <code>MultiPoint</code> inputs and uses their bounding box for raster clipping.</li> <li>Enables passing collections of points as a <code>MultiPoint</code> to <code>clip_to_geometry()</code> without pre-converting to a polygon.</li> </ul> </li> </ul>"},{"location":"changelog/#changed","title":"Changed","text":"<ul> <li> <p>Configuration</p> <ul> <li>Added Snowflake connection parameters to <code>gigaspatial/config.py</code>:<ul> <li><code>SNOWFLAKE_ACCOUNT</code>, <code>SNOWFLAKE_USER</code>, <code>SNOWFLAKE_PASSWORD</code></li> <li><code>SNOWFLAKE_WAREHOUSE</code>, <code>SNOWFLAKE_DATABASE</code>, <code>SNOWFLAKE_SCHEMA</code></li> <li><code>SNOWFLAKE_STAGE_NAME</code></li> </ul> </li> <li>Added Snowflake configuration variables to <code>.env_sample</code></li> </ul> </li> <li> <p>BaseHandler</p> <ul> <li> <p>Streamlined Data Unit Resolution</p> <ul> <li>Consolidated <code>get_relevant_data_units_by_country()</code>, <code>get_relevant_data_units_by_points()</code>, and <code>get_relevant_data_units_by_geometry()</code> into a unified workflow.</li> <li>All source types now convert to geometry via <code>extract_search_geometry()</code> before unit resolution.</li> <li>Subclasses now only need to implement <code>get_relevant_data_units_by_geometry()</code> for custom logic.</li> <li>Significantly reduces code duplication in handler subclasses.</li> </ul> </li> <li> <p>Optimized Handler Workflow</p> <ul> <li>Eliminated redundant <code>get_relevant_data_units()</code> calls across handler, downloader, and reader operations.</li> <li><code>ensure_data_available()</code> now uses cached units and paths, preventing multiple lookups per request.</li> <li>Data unit resolution occurs at most once per unique source query, improving performance for:<ul> <li>Repeated <code>load_data()</code> calls with the same source.</li> <li>Operations involving both download and read steps.</li> <li>Direct usage of downloader or reader components.</li> </ul> </li> </ul> </li> </ul> </li> <li> <p>Enhanced BaseHandlerReader</p> <ul> <li><code>resolve_source_paths()</code> now primarily handles explicit file paths.</li> <li>Geometry/country/point conversion delegated to handler and config layers.</li> <li><code>load()</code> method updated to support <code>crop_to_source</code> parameter with automatic geometry retrieval from cache.</li> <li>Fallback geometry computation if cache miss occurs (e.g., when reader used independently).</li> </ul> </li> <li> <p>BaseHandlerConfig Caching Logic</p> <ul> <li><code>get_relevant_data_units()</code> now checks cache before computing units.</li> <li>Added <code>force_recompute</code> parameter to bypass cache when needed (e.g., <code>force_download=True</code>).</li> <li>Cache operations include debug logging for transparency during development.</li> </ul> </li> <li> <p>TifProcessor temp-file handling</p> <ul> <li>Simplified <code>_create_clipped_processor()</code> to mirror <code>_reproject_to_temp_file</code>: write clipped output to the new processor\u2019s <code>_temp_dir</code>, set <code>_clipped_file_path</code>, update <code>dataset_path</code>, and reload metadata.</li> <li><code>open_dataset()</code> now prioritizes <code>_merged_file_path</code>, <code>_reprojected_file_path</code>, then <code>_clipped_file_path</code>, and opens local files directly.</li> <li>Clipped processors consistently use <code>LocalDataStore()</code> for local temp files to avoid data-store path resolution issues.</li> </ul> </li> </ul>"},{"location":"changelog/#fixed","title":"Fixed","text":"<ul> <li>TifProcessor: clip_to_geometry() open failure after merge<ul> <li>Fixed a bug where <code>open_dataset()</code> failed for processors returned by <code>clip_to_geometry()</code> when the source was initialized with multiple paths and loaded via handlers with <code>merge_rasters=True</code>.</li> <li>The clipped raster is now saved directly into the new processor\u2019s temp directory and tracked via <code>_clipped_file_path</code>, ensuring reliable access by <code>open_dataset()</code>.</li> <li>Absolute path checks in <code>__post_init__</code> now use <code>os.path.exists()</code> for absolute paths with <code>LocalDataStore</code>, preventing false negatives for temp files.</li> </ul> </li> </ul>"},{"location":"changelog/#performance","title":"Performance","text":"<ul> <li>Significant reduction in redundant computations in handlers:<ul> <li>Single geometry extraction per source query (previously up to 3 times).</li> <li>Single data unit resolution per source query (previously 2-3 times).</li> <li>Cached geometry reuse for cropping operations.</li> <li>Benefits scale with:<ul> <li>Number of repeated queries.</li> <li>Complexity of geometry extraction (especially country boundaries).</li> <li>Number of data units per query.</li> </ul> </li> </ul> </li> </ul>"},{"location":"changelog/#developer-notes","title":"Developer Notes","text":"<ul> <li>Subclass implementations should now:<ul> <li>Only override <code>get_relevant_data_units_by_geometry()</code> for custom unit resolution.</li> <li>Use <code>extract_search_geometry()</code> for any geometry conversion needs.</li> <li>Optionally override <code>crop_to_geometry()</code> for dataset-specific cropping logic.</li> </ul> </li> </ul>"},{"location":"changelog/#dependencies","title":"Dependencies","text":"<ul> <li>Added <code>snowflake-connector-python&gt;=3.0.0</code> as a new dependency</li> </ul>"},{"location":"changelog/#v072-2025-10-27","title":"[v0.7.2] - 2025-10-27","text":""},{"location":"changelog/#added_1","title":"Added","text":"<ul> <li> <p>Ookla Speedtest Handler Integration (<code>OoklaSpeedtestHandler</code>)</p> <ul> <li>New classes <code>OoklaSpeedtestHandler</code>, <code>OoklaSpeedtestConfig</code>, <code>OoklaSpeedtestDownloader</code>, and <code>OoklaSpeedtestReader</code> for managing Ookla Speedtest data.</li> <li><code>OoklaSpeedtestHandler.load_data</code> method supports Mercator tile filtering by country or spatial geometry and includes an optional <code>process_geospatial</code> parameter for WKT to GeoDataFrame conversion.</li> <li>In <code>OoklaSpeedtestConfig</code>, <code>year</code> and <code>quarter</code> fields are optional (defaulting to <code>None</code>) and <code>__post_init__</code> logs warnings if they are not explicitly provided, using the latest available data.</li> <li>In <code>OoklaSpeedtestReader</code>, <code>resolve_source_paths</code> method overridden to appropriately handle <code>None</code> or non-path sources by returning the <code>DATASET_URL</code>.</li> <li><code>OoklaSpeedtestHandler</code>, the <code>__init__</code> method requires <code>type</code> as a mandatory argument, with <code>year</code> and <code>quarter</code> being optional.</li> </ul> </li> <li> <p>S2 Grid Generation Support (<code>S2Cells</code>)</p> <ul> <li>Introduced <code>S2Cells</code> class for managing Google S2 cell grids using the <code>s2sphere</code> library.</li> <li>Supports S2 levels 0-30, providing finer granularity than H3 (30 levels vs 15).</li> <li>Provides multiple creation methods:<ul> <li><code>from_cells()</code>: Create from lists of S2 cell IDs (integers or tokens).</li> <li><code>from_bounds()</code>: Create from geographic bounding box coordinates.</li> <li><code>from_spatial()</code>: Create from various spatial sources (geometries, GeoDataFrames, points).</li> <li><code>from_json()</code>: Load S2Cells from JSON files via DataStore.</li> </ul> </li> <li>Includes methods for spatial operations:<ul> <li><code>get_neighbors()</code>: Get edge neighbors (4 per cell) with optional corner neighbors (8 total).</li> <li><code>get_children()</code>: Navigate to higher resolution child cells.</li> <li><code>get_parents()</code>: Navigate to lower resolution parent cells.</li> <li><code>filter_cells()</code>: Filter cells by a given set of cell IDs.</li> </ul> </li> <li>Provides conversion methods:<ul> <li><code>to_dataframe()</code>: Convert to pandas DataFrame with cell IDs, tokens, and centroid coordinates.</li> <li><code>to_geoms()</code>: Convert cells to shapely Polygon geometries (square cells).</li> <li><code>to_geodataframe()</code>: Convert to GeoPandas GeoDataFrame with geometry column.</li> </ul> </li> <li>Supports saving to JSON, Parquet, or GeoJSON files via <code>save()</code> method.</li> <li>Includes <code>average_cell_area</code> property for approximate area calculation based on S2 level.</li> </ul> </li> <li> <p>Country-Specific S2 Cells (<code>CountryS2Cells</code>)</p> <ul> <li>Extends <code>S2Cells</code> for generating S2 grids constrained by country boundaries.</li> <li>Integrates with <code>AdminBoundaries</code> to fetch country geometries for precise cell generation.</li> <li>Factory method <code>create()</code> enforces proper instantiation with country code validation via <code>pycountry</code>.</li> </ul> </li> <li> <p>Expanded <code>write_dataset</code> to support generic JSON objects.</p> <ul> <li>The <code>write_dataset</code> function can now write any serializable Python object (like a dict or list) directly to a <code>.json</code> file by leveraging the dedicated write_json helper.</li> </ul> </li> <li> <p>NASA SRTM Elevation Data Handler (<code>NasaSRTMHandler</code>)</p> <ul> <li>New handler classes for downloading and processing NASA SRTM elevation data (30m and 90m resolution).</li> <li>Supports Earthdata authentication via <code>EARTHDATA_USERNAME</code> and <code>EARTHDATA_PASSWORD</code> environment variables.</li> <li><code>NasaSRTMConfig</code> provides dynamic 1\u00b0x1\u00b0 tile grid generation covering the global extent.</li> <li><code>NasaSRTMDownloader</code> supports parallel downloads of SRTM .hgt.zip tiles using multiprocessing.</li> <li><code>NasaSRTMReader</code> loads SRTM data with options to return as pandas DataFrame or list of <code>SRTMParser</code> objects.</li> <li>Integrated with <code>BaseHandler</code> architecture for consistent data lifecycle management.</li> </ul> </li> <li> <p>SRTM Parser (<code>SRTMParser</code>)</p> <ul> <li>Efficient parser for NASA SRTM .hgt.zip files using memory mapping.</li> <li>Supports both SRTM-1 (3601x3601, 1 arc-second) and SRTM-3 (1201x1201, 3 arc-second) formats.</li> <li>Provides methods for:<ul> <li><code>get_elevation(latitude, longitude)</code>: Get interpolated elevation for specific coordinates.</li> <li><code>get_elevation_batch(coordinates)</code>: Batch elevation queries with NumPy array support.</li> <li><code>to_dataframe()</code>: Convert elevation data to pandas DataFrame with optional NaN filtering.</li> <li>Automatic tile coordinate extraction from filename (e.g., N37E023, S10W120).</li> </ul> </li> </ul> </li> <li> <p>SRTM Manager (<code>SRTMManager</code>)</p> <ul> <li>Manager class for accessing elevation data across multiple SRTM tiles with lazy loading.</li> <li>Implements LRU caching (default cache size: 10 tiles) for efficient memory usage.</li> <li>Methods include:<ul> <li><code>get_elevation(latitude, longitude)</code>: Get interpolated elevation for any coordinate.</li> <li><code>get_elevation_batch(coordinates)</code>: Batch elevation queries across multiple tiles.</li> <li><code>get_elevation_profile(latitudes, longitudes)</code>: Generate elevation profiles along paths.</li> <li><code>check_coverage(latitude, longitude)</code>: Check if a coordinate has SRTM coverage.</li> <li><code>get_available_tiles()</code>: List available SRTM tiles.</li> <li><code>clear_cache()</code> and <code>get_cache_info()</code>: Cache management utilities.</li> </ul> </li> <li>Automatically handles tile boundary crossings for elevation profiles.</li> </ul> </li> <li> <p>Earthdata Session (<code>EarthdataSession</code>)</p> <ul> <li>Custom <code>requests.Session</code> subclass for NASA Earthdata authentication.</li> <li>Maintains Authorization headers through redirects to/from Earthdata hosts.</li> <li>Required for accessing NASA's SRTM data repository.</li> </ul> </li> </ul>"},{"location":"changelog/#changed_1","title":"Changed","text":"<ul> <li> <p>ADLSDataStore Enhancements</p> <ul> <li>Modified <code>__init__</code> method to support initialization using either <code>ADLS_CONNECTION_STRING</code> or a combination of <code>ADLS_ACCOUNT_URL</code> and <code>ADLS_SAS_TOKEN</code>.</li> <li>Improved flexibility for authenticating with Azure Data Lake Storage.</li> </ul> </li> <li> <p>Configuration</p> <ul> <li>Added <code>ADLS_ACCOUNT_URL</code> and <code>ADLS_SAS_TOKEN</code> to <code>gigaspatial/config.py</code> and <code>.env_sample</code> for alternative ADLS authentication.</li> <li>Added <code>EARTHDATA_USERNAME</code> and <code>EARTHDATA_PASSWORD</code> to <code>gigaspatial/config.py</code> and <code>.env_sample</code> for NASA Earthdata authentication.</li> </ul> </li> </ul>"},{"location":"changelog/#fixed_1","title":"Fixed","text":"<ul> <li>WorldPop: <code>RuntimeError</code> during <code>school_age=True</code> data availability check:<ul> <li>Resolved a <code>RuntimeError: Could not ensure data availability for loading</code> that occurred when <code>school_age=True</code> and WorldPop data was not yet present in the data store.</li> <li><code>WPPopulationConfig.get_data_unit_paths</code> now correctly returns the original <code>.zip</code> URLs to trigger the download/extraction process when filtered <code>.tif</code> files are missing.</li> <li>After successful download and extraction, it now accurately identifies and returns the paths to the local <code>.tif</code> files, allowing <code>BaseHandler</code> to confirm availability and proceed with loading.</li> </ul> </li> <li> <p>WorldPop: <code>list index out of range</code> when no datasets found:</p> <ul> <li>Added a <code>RuntimeError</code> in <code>WPPopulationConfig.get_relevant_data_units_by_country</code> when <code>self.client.search_datasets</code> returns no results, providing a clearer error message with the search parameters.</li> </ul> </li> <li> <p>WorldPop: Incomplete downloads with <code>min_age</code>/<code>max_age</code> filters for non-school-age <code>age_structures</code>:</p> <ul> <li>Fixed an issue where <code>load_data</code> with <code>min_age</code> or <code>max_age</code> filters (when <code>school_age=False</code>) resulted in incomplete downloads.</li> <li><code>WPPopulationConfig.get_data_unit_paths</code> now returns all potential <code>.tif</code> URLs for non-school-age <code>age_structures</code> during the initial availability check, ensuring all necessary files are downloaded.</li> <li>Age/sex filtering is now deferred and applied by <code>WPPopulationReader.load_from_paths</code> using <code>WPPopulationConfig._filter_age_sex_paths</code> after download, guaranteeing data integrity.</li> </ul> </li> <li> <p>HealthSitesFetcher</p> <ul> <li>Ensured correct Coordinate Reference System (CRS) assignment (<code>EPSG:4326</code>) when returning <code>GeoDataFrame</code> from fetched health facility data.</li> </ul> </li> </ul>"},{"location":"changelog/#dependencies_1","title":"Dependencies","text":"<ul> <li>Added <code>s2sphere</code> as a new dependency for S2 geometry operations</li> </ul>"},{"location":"changelog/#v071-2025-10-15","title":"[v0.7.1] - 2025-10-15","text":""},{"location":"changelog/#added_2","title":"Added","text":"<ul> <li> <p>Healthsites.io API Integration (<code>HealthSitesFetcher</code>):</p> <ul> <li>New class <code>HealthSitesFetcher</code> to fetch and process health facility data from the Healthsites.io API.</li> <li>Supports filtering by country, bounding box extent, and date ranges (<code>from_date</code>, <code>to_date</code>).</li> <li>Provides methods for:<ul> <li><code>fetch_facilities()</code>: Retrieves health facility locations, returning a <code>pd.DataFrame</code> or <code>gpd.GeoDataFrame</code> based on output format.</li> <li><code>fetch_statistics()</code>: Fetches aggregated statistics for health facilities based on provided filters.</li> <li><code>fetch_facility_by_id()</code>: Retrieves details for a specific facility using its OSM type and ID.</li> </ul> </li> <li>Includes robust handling for API pagination, different output formats (JSON, GeoJSON), and nested data structures.</li> <li>Integrates with <code>OSMLocationFetcher</code> and <code>pycountry</code> to standardize country names to OSM English names for consistent querying.</li> <li>Configurable parameters for API URL, API key, page size, flat properties, tag format, output format, and request sleep time.</li> </ul> </li> <li> <p>OSMLocationFetcher Enhancements:</p> <ul> <li>Historical Data Fetching (<code>fetch_locations_changed_between</code>):<ul> <li>New method <code>fetch_locations_changed_between()</code> to retrieve OSM objects that were created or modified within a specified date range. This enables historical analysis and change tracking.</li> <li>Defaults <code>include_metadata</code> to <code>True</code> for this method, as it's typically used for change tracking.</li> </ul> </li> <li>Comprehensive OSM Country Information (<code>get_osm_countries</code>):<ul> <li>New static method <code>get_osm_countries()</code> to fetch country-level administrative boundaries directly from the OSM database.</li> <li>Supports fetching all countries or a specific country by ISO 3166-1 alpha-3 code.</li> <li>Option to include various name variants (e.g., <code>name:en</code>, <code>official_name</code>) and ISO codes.</li> </ul> </li> <li>Metadata Inclusion in Fetched Locations:<ul> <li>Added <code>include_metadata</code> parameter to <code>fetch_locations()</code> to optionally retrieve change tracking metadata (timestamp, version, changeset, user, uid) for each fetched OSM element.</li> <li>This metadata is now extracted and included in the DataFrame for nodes, relations, and ways.</li> </ul> </li> <li>Flexible Date Filtering in Overpass Queries:<ul> <li>Introduced <code>date_filter_type</code> (<code>newer</code>, <code>changed</code>) and <code>start_date</code>/<code>end_date</code> parameters to <code>_build_queries()</code> for more granular control over time-based filtering in Overpass QL.</li> </ul> </li> <li>Date Normalization Utility:<ul> <li>Added <code>_normalize_date()</code> helper method to convert various date inputs (string, datetime object) into a standardized ISO 8601 format for Overpass API queries.</li> </ul> </li> </ul> </li> <li> <p>TifProcessor</p> <ul> <li>Comprehensive Memory Management:<ul> <li>Introduced <code>_check_available_memory()</code>, <code>_estimate_memory_usage()</code>, and <code>_memory_guard()</code> methods for proactive memory assessment across various operations.</li> <li>Added warnings (<code>ResourceWarning</code>) for potentially high memory usage in batched operations, with suggestions for optimizing <code>n_workers</code>.</li> </ul> </li> <li>Chunked DataFrame Conversion:<ul> <li>Implemented <code>to_dataframe_chunked()</code> for memory-efficient processing of large rasters by converting them to DataFrames in manageable chunks.</li> <li>Automatic calculation of optimal <code>chunk_size</code> based on target memory usage via <code>_calculate_optimal_chunk_size()</code>.</li> <li>New helper methods: <code>_get_chunk_windows()</code>, <code>_get_chunk_coordinates()</code>.</li> </ul> </li> <li>Raster Clipping Functionality:<ul> <li><code>clip_to_geometry()</code>: New method to clip rasters to arbitrary geometries (Shapely, GeoDataFrame, GeoSeries, GeoJSON-like dicts).</li> <li><code>clip_to_bounds()</code>: New method to clip rasters to rectangular bounding boxes, supporting optional CRS transformation for the bounds.</li> <li>New helper methods for clipping: <code>_prepare_geometry_for_clipping()</code>, <code>_validate_geometry_crs()</code>, <code>_create_clipped_processor()</code>.</li> </ul> </li> </ul> </li> <li> <p>WorldPopDownloader Zip Handling:</p> <ul> <li>Modified <code>download_data_unit</code> in <code>WPPopulationDownloader</code> to correctly handle <code>.zip</code> files (e.g., school age datasets) by downloading them to a temporary location and extracting the contained <code>.tif</code> files.</li> <li>Updated <code>download_data_units</code> to correctly flatten the list of paths returned by <code>download_data_unit</code> when zip extraction results in multiple files.</li> <li>Adjusted <code>WPPopulationConfig.get_data_unit_paths</code> to correctly identify and return paths for extracted <code>.tif</code> files from zip resources. It is now intelligently resolves paths. For school-age datasets, it returns paths to extracted <code>.tif</code> files if available; otherwise, it returns the original <code>.zip</code> path(s) to trigger download and extraction.</li> <li>Added filter support to <code>WPPopulationConfig.get_data_unit_paths</code> hence to the <code>WPPopulationHandler</code> for:<ul> <li>School-age datasets: supports <code>sex</code> (e.g., \"F\", \"M\", \"F_M\") and <code>education_level</code> (e.g., \"PRIMARY\", \"SECONDARY\") filters on extracted <code>.tif</code> filenames.</li> <li>Non-school-age age_structures: supports <code>sex</code>, <code>ages</code>, <code>min_age</code>, and <code>max_age</code> filters on <code>.tif</code> filenames.</li> </ul> </li> </ul> </li> <li> <p>WorldPop: Filtered aggregation in <code>GeometryBasedZonalViewGenerator.map_wp_pop</code>:</p> <ul> <li><code>map_wp_pop</code> now enforces a single country input when <code>handler.config.project</code> is \"age_structures\".</li> <li>When <code>predicate</code> is \"centroid_within\" and the project is \"age_structures\", individual <code>TifProcessor</code> objects (representing age/sex combinations) are loaded, sampled with <code>map_rasters(stat=\"sum\")</code>, and their results are summed per zone, preventing unintended merging.</li> </ul> </li> <li> <p>PoiViewGenerator: Filtered aggregation in <code>PoiViewGenerator.map_wp_pop</code>:</p> <ul> <li><code>map_wp_pop</code> now enforces a single country input when <code>handler.config.project</code> is \"age_structures\".</li> <li>When <code>predicate</code> is \"centroid_within\" and the project is \"age_structures\", individual <code>TifProcessor</code> objects (representing age/sex combinations) are loaded, sampled with <code>map_zonal_stats(stat=\"sum\")</code>, and their results are summed per POI, preventing unintended merging.</li> </ul> </li> <li> <p>TifProcessor Multi-Raster Merging in Handlers and Generators:</p> <ul> <li>Extended <code>_load_raster_data</code> in <code>BaseHandlerReader</code> to support an optional <code>merge_rasters</code> argument. When <code>True</code> and multiple raster paths are provided, <code>TifProcessor</code> now merges them into a single <code>TifProcessor</code> object during loading.</li> <li>Integrated <code>merge_rasters</code> argument into <code>GHSLDataReader</code> and <code>WPPopulationReader</code>'s <code>load_from_paths</code> and <code>load</code> methods, enabling control over raster merging at the reader level.</li> <li>Propagated <code>merge_rasters</code> to <code>GHSLDataHandler</code>'s <code>load_into_dataframe</code>, and <code>load_into_geodataframe</code> methods for consistent behavior across the handler interface.</li> </ul> </li> </ul>"},{"location":"changelog/#changed_2","title":"Changed","text":"<ul> <li> <p>TifProcessor</p> <ul> <li>Unified DataFrame Conversion:<ul> <li>Refactored <code>to_dataframe()</code> to act as a universal entry point, dynamically routing to internal, more efficient methods for single and multi-band processing.</li> <li>Deprecated the individual <code>_to_band_dataframe()</code>, <code>_to_rgb_dataframe()</code>, <code>_to_rgba_dataframe()</code>, and <code>_to_multi_band_dataframe()</code> methods in favor of the new unified <code>_to_dataframe()</code>.</li> <li><code>to_dataframe()</code> now includes a <code>check_memory</code> parameter.</li> </ul> </li> <li>Optimized <code>open_dataset</code> Context Manager:<ul> <li>The <code>open_dataset</code> context manager now directly opens local files when <code>LocalDataStore</code> is used, avoiding unnecessary <code>rasterio.MemoryFile</code> creation for improved performance and reduced memory overhead.</li> </ul> </li> <li>Enhanced <code>to_geodataframe</code> and <code>to_graph</code>:<ul> <li>Added <code>check_memory</code> parameter to <code>to_geodataframe()</code> and <code>to_graph()</code> for memory pre-checks.</li> </ul> </li> <li>Refined <code>sample_by_polygons_batched</code>:<ul> <li>Included <code>check_memory</code> parameter for memory checks before batch processing.</li> <li>Implemented platform-specific warnings for potential multiprocessing issues on Windows/macOS.</li> </ul> </li> <li>Improved Multiprocessing Initialization:<ul> <li>The <code>_initializer_worker()</code> method now prioritizes merged, reprojected, or original local file paths for opening, ensuring workers access the most relevant data.</li> </ul> </li> <li>Modular Masking and Coordinate Extraction:<ul> <li>Introduced new private helper methods: <code>_extract_coordinates_with_mask()</code>, <code>_build_data_mask()</code>, <code>_build_multi_band_mask()</code>, and <code>_bands_to_dict()</code> to centralize and improve data masking and coordinate extraction logic.</li> </ul> </li> <li>Streamlined Band-Mode Validation:<ul> <li>Moved the logic for validating <code>mode</code> and band count compatibility into a dedicated <code>_validate_mode_band_compatibility()</code> method for better code organization.</li> </ul> </li> </ul> </li> <li> <p>GigaSchoolLocationFetcher</p> <ul> <li><code>fetch_locations()</code> method:<ul> <li>Added <code>process_geospatial</code> parameter (defaults to <code>False</code>) to optionally process geospatial data and return a <code>gpd.GeoDataFrame</code>.</li> </ul> </li> <li><code>_process_geospatial_data()</code> method:<ul> <li>Modified to return a <code>gpd.GeoDataFrame</code> by converting the <code>pd.DataFrame</code> with a <code>geometry</code> column and <code>EPSG:4326</code> CRS.</li> </ul> </li> </ul> </li> <li> <p>OSMLocationFetcher Refactoring:</p> <ul> <li>Unified Query Execution and Processing: Refactored the core logic for executing Overpass queries and processing their results into a new private method <code>_execute_and_process_queries()</code>. This centralizes common steps and reduces code duplication between <code>fetch_locations()</code> and the new <code>fetch_locations_changed_between()</code>.</li> <li>Enhanced <code>_build_queries</code>: Modified <code>_build_queries</code> to accept <code>date_filter_type</code>, <code>start_date</code>, <code>end_date</code>, and <code>include_metadata</code> to construct more dynamic and feature-rich Overpass QL queries.</li> <li>Updated <code>fetch_locations</code> Signature:<ul> <li>Replaced <code>since_year</code> parameter with <code>since_date</code> (which can be a <code>str</code> or <code>datetime</code> object) for more precise time-based filtering.</li> <li>Added <code>include_metadata</code> parameter.</li> </ul> </li> <li>Improved Logging of Category Distribution:<ul> <li>Modified the logging for category distribution to correctly handle cases where categories are combined into a list (when <code>handle_duplicates='combine'</code>).</li> </ul> </li> <li><code>since_year</code> Parameter: Removed <code>since_year</code> from <code>fetch_locations()</code> as its functionality is now superseded by the more flexible <code>since_date</code> parameter and the <code>_build_queries</code> enhancements.</li> </ul> </li> <li> <p><code>PoiViewGenerator</code> Mapping Methods (<code>map_zonal_stats</code>, <code>map_nearest_points</code>, <code>map_google_buildings</code>, <code>map_ms_buildings</code>, <code>map_built_s</code>, <code>map_smod</code>):</p> <ul> <li>Changed <code>map_zonal_stats</code> and <code>map_nearest_points</code> to return <code>pd.DataFrame</code> results (including <code>'poi_id'</code> and new mapped columns) instead of directly updating the internal view.</li> <li>Updated <code>map_google_buildings</code>, <code>map_ms_buildings</code>, <code>map_built_s</code>, and <code>map_smod</code> to capture the <code>pd.DataFrame</code> returned by their respective underlying mapping calls (<code>map_nearest_points</code> or <code>map_zonal_stats</code>) and then explicitly call <code>self._update_view()</code> with these results.</li> <li>This enhances modularity and allows for more flexible result handling and accumulation.</li> </ul> </li> <li> <p><code>ZonalViewGenerator.map_rasters</code> Enhancements:</p> <ul> <li>Modified <code>map_rasters</code> to accept <code>raster_data</code> as either a single <code>TifProcessor</code> or a <code>List[TifProcessor]</code>.</li> <li>Implemented internal merging of <code>List[TifProcessor]</code> into a single <code>TifProcessor</code> before performing zonal statistics.</li> <li>Replaced <code>sample_multiple_tifs_by_polygons</code> with the <code>TifProcessor.sample_by_polygons</code> method.</li> </ul> </li> </ul>"},{"location":"changelog/#fixed_2","title":"Fixed","text":"<ul> <li>TifProcessor:<ul> <li><code>to_graph()</code> Sparse Matrix Creation:<ul> <li>Corrected the sparse matrix creation logic in <code>to_graph()</code> to ensure proper symmetric graph representation when <code>graph_type=\"sparse\"</code>.</li> </ul> </li> <li>Coordinate System Handling in <code>_initializer_worker</code>:<ul> <li>Ensured that <code>_initializer_worker</code> correctly handles different data storage scenarios to provide the correct dataset handle to worker processes, preventing <code>RuntimeError</code> due to uninitialized raster datasets.</li> </ul> </li> </ul> </li> </ul>"},{"location":"changelog/#removed","title":"Removed","text":"<ul> <li>OSMLocationFetcher<ul> <li>Redundant Category Distribution Logging: Removed the explicit category distribution logging for <code>handle_duplicates == \"separate\"</code> since the <code>value_counts()</code> method on the 'category' column already provides this.</li> </ul> </li> </ul>"},{"location":"changelog/#v070-2025-09-17","title":"[v0.7.0] - 2025-09-17","text":""},{"location":"changelog/#added_3","title":"Added","text":"<ul> <li> <p>TifProcessor Revamp</p> <ul> <li>Explicit Reprojection Method: Introduced <code>reproject_to()</code> method, allowing on-demand reprojection of rasters to a new CRS with customizable <code>resampling_method</code> and <code>resolution</code>.</li> <li>Reprojection Resolution Control: Added <code>reprojection_resolution</code> parameter to <code>TifProcessor</code> for precise control over output pixel size during reprojection.</li> <li>Advanced Raster Information: Added <code>get_raster_info()</code> method to retrieve a comprehensive dictionary of raster metadata.</li> <li>Graph Conversion Capabilities: Implemented <code>to_graph()</code> method to convert raster data into a graph (NetworkX or sparse matrix) based on pixel adjacency (4- or 8-connectivity).</li> <li>Internal Refactoring: <code>_reproject_to_temp_file</code>: Introduced <code>_reproject_to_temp_file</code> as a helper for reprojection into temporary files.</li> </ul> </li> <li> <p>H3 Grid Generation</p> <ul> <li>H3 Grid Generation Module (<code>gigaspatial/grid/h3.py</code>):<ul> <li>Introduced <code>H3Hexagons</code> class for managing H3 cell IDs.</li> <li>Supports creation from lists of hexagons, geographic bounds, spatial geometries, or points.</li> <li>Provides methods to convert H3 hexagons to pandas DataFrames and GeoPandas GeoDataFrames.</li> <li>Includes functionalities for filtering, getting k-ring neighbors, compacting hexagons, and getting children/parents at different resolutions.</li> <li>Allows saving H3Hexagons to JSON, Parquet, or GeoJSON files.</li> </ul> </li> <li>Country-Specific H3 Hexagons (<code>CountryH3Hexagons</code>):<ul> <li>Extends <code>H3Hexagons</code> for generating H3 grids constrained by country boundaries.</li> <li>Integrates with <code>AdminBoundaries</code> to fetch country geometries for precise H3 cell generation.</li> </ul> </li> </ul> </li> <li> <p>Documentation</p> <ul> <li>Improved <code>tif.md</code> example to showcase multi-raster initialization, explicit reprojection, and graph conversion.</li> </ul> </li> </ul>"},{"location":"changelog/#changed_3","title":"Changed","text":"<ul> <li>TifProcessor</li> <li>Improved Temporary File Management: Refactored temporary file handling for merging and reprojection using <code>tempfile.mkdtemp()</code> and <code>shutil.rmtree</code> for more robust and reliable cleanup. Integrated with context manager (<code>__enter__</code>, <code>__exit__</code>) and added a dedicated <code>cleanup()</code> method.</li> <li>Reprojection during Initialization: Implemented automatic reprojection of single rasters to a specified <code>target_crs</code> during <code>TifProcessor</code> initialization.</li> <li>Enhanced <code>open_dataset</code> Context Manager: The <code>open_dataset</code> context manager now intelligently opens the most up-to-date (merged or reprojected) version of the dataset.</li> <li>More Flexible Multi-Dataset Validation: Modified <code>_validate_multiple_datasets</code> to issue a warning instead of raising an error for CRS mismatches when <code>target_crs</code> is not set.</li> <li> <p>Optimized <code>_get_reprojection_profile</code>: Dynamically calculates transform and dimensions based on <code>reprojection_resolution</code> and added LZW compression to reprojected TIFF files to reduce file size.</p> </li> <li> <p>ADLSDataStore Enhancements</p> <ul> <li>New <code>copy_file</code> method: Implemented a new method for copying individual files within ADLS, with an option to overwrite existing files.</li> <li>New <code>rename</code> method: Added a new method to rename (move) files in ADLS, which internally uses <code>copy_file</code> and then deletes the source, with options for overwrite, waiting for copy completion, and polling.</li> <li>Revamped <code>rmdir</code> method: Modified <code>rmdir</code> to perform batch deletions of blobs, addressing the Azure Blob batch delete limit (256 sub-requests) and improving efficiency for large directories.</li> </ul> </li> <li> <p>LocalDataStore Enhancements</p> <ul> <li>New <code>copy_file</code> method: Implemented a new method for copying individual files.</li> </ul> </li> </ul>"},{"location":"changelog/#removed_1","title":"Removed","text":"<ul> <li>Removed deprecated <code>tabular</code> property and <code>get_zoned_geodataframe</code> method from <code>TifProcessor</code>. Users should now use <code>to_dataframe()</code> and <code>to_geodataframe()</code> respectively.</li> </ul>"},{"location":"changelog/#dependencies_2","title":"Dependencies","text":"<ul> <li>Added <code>networkx</code> and <code>h3</code> as new dependencies.</li> </ul>"},{"location":"changelog/#fixed_3","title":"Fixed","text":"<ul> <li>Several small fixes and improvements to aggregation methods.</li> </ul>"},{"location":"changelog/#v069-2025-07-26","title":"[v0.6.9] - 2025-07-26","text":""},{"location":"changelog/#fixed_4","title":"Fixed","text":"<ul> <li>Resolved a bug in the handler base class where non-hashable types (dicts) were incorrectly used as dictionary keys in <code>unit_to_path</code> mapping, preventing potential runtime errors during data availability checks.</li> </ul>"},{"location":"changelog/#v068-2025-07-26","title":"[v0.6.8] - 2025-07-26","text":""},{"location":"changelog/#added_4","title":"Added","text":"<ul> <li>OSMLocationFetcher Enhancements</li> <li>Support for querying OSM locations by arbitrary administrative levels (e.g., states, provinces, cities), in addition to country-level queries.</li> <li>New optional parameters:<ul> <li><code>admin_level</code>: Specify OSM administrative level (e.g., 4 for states, 6 for counties).</li> <li><code>admin_value</code>: Name of the administrative area to query (e.g., \"California\").</li> </ul> </li> <li> <p>New static method <code>get_admin_names(admin_level, country=None)</code>:</p> <ul> <li>Fetch all administrative area names for a given <code>admin_level</code>, optionally filtered by country.</li> <li>Helps users discover valid admin area names for constructing precise queries.</li> </ul> </li> <li> <p>Multi-Raster Merging Support in TifProcessor</p> </li> <li>Added ability to initialize <code>TifProcessor</code> with multiple raster datasets.</li> <li>Merges rasters on load with configurable strategies:<ul> <li>Supported <code>merge_method</code> options: <code>first</code>, <code>last</code>, <code>min</code>, <code>max</code>, <code>mean</code>.</li> </ul> </li> <li>Supports on-the-fly reprojection for rasters with differing coordinate reference systems via <code>target_crs</code>.</li> <li>Handles resampling using <code>resampling_method</code> (default: <code>nearest</code>).</li> <li>Comprehensive validation to ensure compatibility of input rasters (e.g., resolution, nodata, dtype).</li> <li>Temporary file management for merged output with automatic cleanup.</li> <li>Backward compatible with single-raster use cases.</li> </ul> <p>New TifProcessor Parameters:   - <code>merge_method</code> (default: <code>first</code>) \u2013 How to combine pixel values across rasters.   - <code>target_crs</code> (optional) \u2013 CRS to reproject rasters before merging.   - <code>resampling_method</code> \u2013 Resampling method for reprojection.</p> <p>New Properties:   - <code>is_merged</code>: Indicates whether the current instance represents merged rasters.   - <code>source_count</code>: Number of raster datasets merged.</p>"},{"location":"changelog/#changed_4","title":"Changed","text":"<ul> <li>OSMLocationFetcher Overpass Query Logic</li> <li>Refactored Overpass QL query builder to support subnational queries using <code>admin_level</code> and <code>admin_value</code>.</li> <li>Improved flexibility and precision for spatial data collection across different administrative hierarchies.</li> </ul>"},{"location":"changelog/#breaking-changes","title":"Breaking Changes","text":"<ul> <li>None. All changes are fully backward compatible.</li> </ul>"},{"location":"changelog/#v067-2025-07-16","title":"[v0.6.7] - 2025-07-16","text":""},{"location":"changelog/#fixed_5","title":"Fixed","text":"<ul> <li>Fixed a bug in WorldPopHandler/ADLSDataStore integration where a <code>Path</code> object was passed instead of a string, causing a <code>quote_from_bytes() expected bytes</code> error during download.</li> </ul>"},{"location":"changelog/#v066-2025-07-15","title":"[v0.6.6] - 2025-07-15","text":""},{"location":"changelog/#added_5","title":"Added","text":"<ul> <li><code>AdminBoundaries.from_global_country_boundaries(scale=\"medium\")</code></li> <li>New class method to load global admin level 0 boundaries from Natural Earth.</li> <li> <p>Supports <code>\"large\"</code> (10m), <code>\"medium\"</code> (50m), and <code>\"small\"</code> (110m) scale options.</p> </li> <li> <p>WorldPop Handler Refactor (API Integration)</p> </li> <li>Introduced <code>WPPopulationHandler</code>, <code>WPPopulationConfig</code>, <code>WPPopulationDownloader</code>, and <code>WPPopulationReader</code>.</li> <li>Uses new <code>WorldPopRestClient</code> to dynamically query the WorldPop REST API.</li> <li>Replaces static metadata files and hardcoded logic with API-based discovery and download.</li> <li>Country code lookup and dataset filtering now handled at runtime.</li> <li> <p>Improved validation, extensibility, logging, and error handling.</p> </li> <li> <p>POI-Based WorldPop Mapping</p> </li> <li> <p><code>PoiViewGenerator.map_wp_pop()</code> method:</p> <ul> <li>Maps WorldPop population data around POIs using flexible spatial predicates:</li> <li><code>\"centroid_within\"</code>, <code>\"intersects\"</code>, <code>\"fractional\"</code> (1000m only), <code>\"within\"</code></li> <li>Supports configurable radius and resolution (100m or 1000m).</li> <li>Aggregates population data and appends it to the view.</li> </ul> </li> <li> <p>Geometry-Based Zonal WorldPop Mapping</p> </li> <li><code>GeometryBasedZonalViewGenerator.map_wp_pop()</code> method:<ul> <li>Maps WorldPop population data to polygons/zones using:</li> <li><code>\"intersects\"</code> or <code>\"fractional\"</code> predicate</li> <li>Returns zonal population sums as a new view column.</li> <li>Handles predicate-dependent data loading (raster vs. GeoDataFrame).</li> </ul> </li> </ul>"},{"location":"changelog/#changed_5","title":"Changed","text":"<ul> <li>Refactored <code>BaseHandler.ensure_data_available</code></li> <li>More efficient data check and download logic.</li> <li>Downloads only missing units unless <code>force_download=True</code>.</li> <li> <p>Cleaner structure and better reuse of <code>get_relevant_data_units()</code>.</p> </li> <li> <p>Refactored WorldPop Module</p> </li> <li>Complete handler redesign using API-based architecture.</li> <li>Dataset paths and URLs are now dynamically constructed from API metadata.</li> <li>Resolution/year validation is more robust and descriptive.</li> <li>Removed static constants, gender/school_age toggles, and local CSV dependency.</li> </ul>"},{"location":"changelog/#fixed_6","title":"Fixed","text":"<ul> <li>Several small fixes and improvements to zonal aggregation methods, especially around CRS consistency, missing values, and result alignment.</li> </ul>"},{"location":"changelog/#v065-2025-07-01","title":"[v0.6.5] - 2025-07-01","text":""},{"location":"changelog/#added_6","title":"Added","text":"<ul> <li> <p><code>MercatorTiles.get_quadkeys_from_points()</code>   New static method for efficient 1:1 point-to-quadkey mapping using coordinate-based logic, improving performance over spatial joins.</p> </li> <li> <p><code>AdminBoundariesViewGenerator</code>   New generator class for producing zonal views based on administrative boundaries (e.g., districts, provinces) with flexible source and admin level support.</p> </li> <li> <p>Zonal View Generator Enhancements </p> </li> <li><code>_view</code>: Internal attribute for accumulating mapped statistics.  </li> <li><code>view</code>: Exposes current state of zonal view.  </li> <li><code>add_variable_to_view()</code>: Adds mapped data from <code>map_points</code>, <code>map_polygons</code>, or <code>map_rasters</code> with robust validation and zone alignment.  </li> <li> <p><code>to_dataframe()</code> and <code>to_geodataframe()</code> methods added for exporting current view in tabular or spatial formats.</p> </li> <li> <p><code>PoiViewGenerator</code> Enhancements </p> </li> <li>Consistent <code>_view</code> DataFrame for storing mapped results.  </li> <li><code>_update_view()</code>: Central method to update POI data.  </li> <li><code>save_view()</code>: Improved format handling (CSV, Parquet, GeoJSON, etc.) with geometry recovery.  </li> <li><code>to_dataframe()</code> and <code>to_geodataframe()</code> methods added for convenient export of enriched POI view.  </li> <li> <p>Robust duplicate ID detection and CRS validation in <code>map_zonal_stats</code>.</p> </li> <li> <p><code>TifProcessor</code> Enhancements </p> </li> <li><code>sample_by_polygons_batched()</code>: Parallel polygon sampling.  </li> <li>Enhanced <code>sample_by_polygons()</code> with nodata masking and multiple stats.  </li> <li> <p><code>warn_on_error</code>: Flag to suppress sampling warnings.</p> </li> <li> <p>GeoTIFF Multi-Band Support </p> </li> <li><code>multi</code> mode added for multi-band raster support.  </li> <li>Auto-detects band names via metadata.  </li> <li> <p>Strict validation of band count based on mode (<code>single</code>, <code>rgb</code>, <code>rgba</code>, <code>multi</code>).</p> </li> <li> <p>Spatial Distance Graph Algorithm </p> </li> <li><code>build_distance_graph()</code> added for fast KD-tree-based spatial matching.  </li> <li>Supports both <code>DataFrame</code> and <code>GeoDataFrame</code> inputs.  </li> <li>Outputs a <code>networkx.Graph</code> with optional DataFrame of matches.  </li> <li> <p>Handles projections, self-match exclusion, and includes verbose stats/logs.</p> </li> <li> <p>Database Integration (Experimental) </p> </li> <li>Added <code>DBConnection</code> class in <code>core/io/database.py</code> for unified Trino and PostgreSQL access.  </li> <li>Supports schema/table introspection, query execution, and reading into <code>pandas</code> or <code>dask</code>.  </li> <li>Handles connection creation, credential management, and diagnostics.  </li> <li> <p>Utility methods for schema/view/table/column listings and parameterized queries.</p> </li> <li> <p>GHSL Population Mapping </p> </li> <li><code>map_ghsl_pop()</code> method added to <code>GeometryBasedZonalViewGenerator</code>.  </li> <li>Aggregates GHSL population rasters to user-defined zones.  </li> <li>Supports <code>intersects</code> and <code>fractional</code> predicates (latter for 1000m resolution only).  </li> <li>Returns population statistics (e.g., <code>sum</code>) with customizable column prefix.</li> </ul>"},{"location":"changelog/#changed_6","title":"Changed","text":"<ul> <li> <p><code>MercatorTiles.from_points()</code> now internally uses <code>get_quadkeys_from_points()</code> for better performance.</p> </li> <li> <p><code>map_points()</code> and <code>map_rasters()</code> now return <code>Dict[zone_id, value]</code> to support direct usage with <code>add_variable_to_view()</code>.</p> </li> <li> <p>Refactored <code>aggregate_polygons_to_zones()</code> </p> </li> <li><code>area_weighted</code> deprecated in favor of <code>predicate</code>.  </li> <li>Supports flexible predicates like <code>\"within\"</code>, <code>\"fractional\"</code> for spatial aggregation.  </li> <li> <p><code>map_polygons()</code> updated to reflect this change.</p> </li> <li> <p>Optional Admin Boundaries Configuration </p> </li> <li><code>ADMIN_BOUNDARIES_DATA_DIR</code> is now optional.  </li> <li><code>AdminBoundaries.create()</code> only attempts to load if explicitly configured or path is provided.  </li> <li>Improved documentation and fallback behavior for missing configs.</li> </ul>"},{"location":"changelog/#fixed_7","title":"Fixed","text":"<ul> <li>GHSL Downloader </li> <li>ZIP files are now downloaded into a temporary cache directory using <code>requests.get()</code>.  </li> <li> <p>Avoids unnecessary writes and ensures cleanup.</p> </li> <li> <p><code>TifProcessor</code> </p> </li> <li>Removed polygon sampling warnings unless explicitly enabled.</li> </ul>"},{"location":"changelog/#deprecated","title":"Deprecated","text":"<ul> <li><code>TifProcessor.tabular</code> \u2192 use <code>to_dataframe()</code> instead.  </li> <li><code>TifProcessor.get_zoned_geodataframe()</code> \u2192 use <code>to_geodataframe()</code> instead.  </li> <li><code>area_weighted</code> \u2192 use <code>predicate</code> in aggregation methods instead.</li> </ul>"},{"location":"changelog/#v064-2025-06-19","title":"[v0.6.4] - 2025-06-19","text":""},{"location":"changelog/#added_7","title":"Added","text":"<ul> <li>GigaSchoolProfileFetcher </li> <li>New class to fetch and process school profile data from the Giga School Profile API  </li> <li>Supports paginated fetching, filtering by country and school ID  </li> <li> <p>Includes methods to generate connectivity summary statistics by region, connection type, and source</p> </li> <li> <p>GigaSchoolMeasurementsFetcher </p> </li> <li>New class to fetch and process daily real-time connectivity measurements from the Giga API  </li> <li>Supports filtering by date range and school  </li> <li> <p>Includes performance summary generation (download/upload speeds, latency, quality flags)</p> </li> <li> <p>AdminBoundaries.from_geoboundaries </p> </li> <li>New class method to download and process geoBoundaries data by country and admin level  </li> <li> <p>Automatically handles HDX dataset discovery, downloading, and fallback logic</p> </li> <li> <p>HDXConfig.search_datasets </p> </li> <li>Static method to search HDX datasets without full handler initialization  </li> <li>Supports query string, sort order, result count, HDX site selection, and custom user agent</li> </ul>"},{"location":"changelog/#fixed_8","title":"Fixed","text":"<ul> <li>Typo in <code>MaxarImageDownloader</code> causing runtime error</li> </ul>"},{"location":"changelog/#documentation","title":"Documentation","text":"<ul> <li>Improved Configuration Guide (<code>docs/user-guide/configuration.md</code>)  </li> <li>Added comprehensive table of environment variables with defaults and descriptions  </li> <li>Synced <code>.env_sample</code> and <code>config.py</code> with docs  </li> <li>Example <code>.env</code> file and guidance on path overrides using <code>config.set_path</code> </li> <li>New section on <code>config.ensure_directories_exist</code> and troubleshooting tips  </li> <li>Clearer handling of credentials and security notes  </li> <li>Improved formatting and structure for clarity</li> </ul>"},{"location":"changelog/#v063-2025-06-16","title":"[v0.6.3] - 2025-06-16","text":""},{"location":"changelog/#added_8","title":"Added","text":"<ul> <li>Major refactor of <code>HDX</code> module to align with unified <code>BaseHandler</code> architecture:</li> <li><code>HDXConfig</code>: fully aligned with <code>BaseHandlerConfig</code> structure.</li> <li>Added flexible pattern matching for resource filtering.</li> <li>Improved data unit resolution by country, geometry, and points.</li> <li>Enhanced resource filtering with exact and regex options.</li> <li><code>HDXDownloader</code> fully aligned with <code>BaseHandlerDownloader</code>:</li> <li>Simplified sequential download logic.</li> <li>Improved error handling, validation, and logging.</li> <li><code>HDXReader</code> fully aligned with <code>BaseHandlerReader</code>:</li> <li>Added <code>resolve_source_paths</code> and <code>load_all_resources</code> methods.</li> <li>Simplified source handling for single and multiple files.</li> <li>Cleaned up redundant and dataset-specific logic.</li> <li> <p>Introduced <code>HDXHandler</code> as unified orchestration layer using factory methods.</p> </li> <li> <p>Refactor of <code>RelativeWealthIndex (RWI)</code> module:</p> </li> <li>Added new <code>RWIHandler</code> class aligned with <code>HDXHandler</code> and <code>BaseHandler</code>.</li> <li>Simplified class names: <code>RWIDownloader</code> and <code>RWIReader</code>.</li> <li>Enhanced configuration with <code>latest_only</code> flag to select newest resources automatically.</li> <li>Simplified resource filtering and country resolution logic.</li> <li> <p>Improved code maintainability, type hints, and error handling.</p> </li> <li> <p>New raster multi-band support in TifProcessor:</p> </li> <li>Added new <code>multi</code> mode for handling multi-band raster datasets.</li> <li>Automatic band name detection from raster metadata.</li> <li>Added strict mode validation (<code>single</code>, <code>rgb</code>, <code>rgba</code>, <code>multi</code>).</li> <li>Enhanced error handling for invalid modes and band counts.</li> </ul>"},{"location":"changelog/#fixed_9","title":"Fixed","text":"<ul> <li>Fixed GHSL tiles loading behavior for correct coordinate system handling:</li> <li>Moved <code>TILES_URL</code> formatting and tile loading to <code>validate_configuration</code>.</li> <li>Ensures proper tile loading after CRS validation.</li> </ul>"},{"location":"changelog/#documentation_1","title":"Documentation","text":"<ul> <li>Updated and standardized API references across documentation.</li> <li>Standardized handler method names and usage examples.</li> <li>Added building enrichment examples for POI processing.</li> <li>Updated installation instructions.</li> </ul>"},{"location":"changelog/#deprecated_1","title":"Deprecated","text":"<ul> <li>Deprecated direct imports from individual handler modules.</li> </ul>"},{"location":"changelog/#v062-2025-06-11","title":"[v0.6.2] - 2025-06-11","text":""},{"location":"changelog/#added_9","title":"Added","text":"<ul> <li>New <code>ROOT_DATA_DIR</code> configuration option to set a base directory for all data tiers</li> <li>Can be configured via environment variable <code>ROOT_DATA_DIR</code> or <code>.env</code> file</li> <li>Defaults to current directory (<code>.</code>) if not specified</li> <li>All tier data paths (bronze, silver, gold, views) are now constructed relative to this root directory</li> <li>Example: Setting <code>ROOT_DATA_DIR=/data/gigaspatial</code> will store all data under <code>/data/gigaspatial/bronze</code>, <code>/data/gigaspatial/silver</code>, etc.</li> </ul>"},{"location":"changelog/#fixed_10","title":"Fixed","text":"<ul> <li>Fixed URL formatting in GHSL tiles by using Enum value instead of Enum member</li> <li>Ensures consistent URL formatting with numeric values (4326) instead of Enum names (WGS84)</li> <li> <p>Fixes URL formatting issue across different Python environments</p> </li> <li> <p>Refactored GHSL downloader to follow DataStore abstraction</p> </li> <li>Directory creation is now handled by DataStore implementation</li> <li>Removed redundant directory creation logic from download_data_unit method</li> <li>Improves separation of concerns and makes the code more maintainable</li> </ul>"},{"location":"changelog/#v061-2025-06-09","title":"[v0.6.1] - 2025-06-09","text":""},{"location":"changelog/#fixed_11","title":"Fixed","text":"<ul> <li>Gracefully handle missing or invalid GeoRepo API key in <code>AdminBoundaries.create()</code>:</li> <li>Wrapped <code>GeoRepoClient</code> initialization in a <code>try-except</code> block</li> <li>Added fallback to GADM if GeoRepo client fails</li> <li>Improved logging for better debugging and transparency</li> </ul>"},{"location":"changelog/#v060-2025-06-09","title":"[v0.6.0] - 2025-06-09","text":""},{"location":"changelog/#added_10","title":"Added","text":""},{"location":"changelog/#poi-view-generator","title":"POI View Generator","text":"<ul> <li><code>map_zonal_stats</code>: New method for enriched spatial mapping with support for:</li> <li>Raster point sampling (value at POI location)</li> <li>Raster zonal statistics (with buffer zone)</li> <li>Polygon aggregation (with optional area-weighted averaging)</li> <li>Auto-generated POI IDs in <code>_init_points_gdf</code> for consistent point tracking.</li> <li>Support for area-weighted aggregation for polygon-based statistics.</li> </ul>"},{"location":"changelog/#basehandler-orchestration-layer","title":"BaseHandler Orchestration Layer","text":"<ul> <li>New abstract <code>BaseHandler</code> class providing unified lifecycle orchestration for config, downloader, and reader.</li> <li>High-level interface methods:</li> <li><code>ensure_data_available()</code></li> <li><code>load_data()</code></li> <li><code>download_and_load()</code></li> <li><code>get_available_data_info()</code></li> <li>Integrated factory pattern for safe and standardized component creation.</li> <li>Built-in context manager support for resource cleanup.</li> <li>Fully backwards compatible with existing handler architecture.</li> </ul>"},{"location":"changelog/#handlers-updated-to-use-basehandler","title":"Handlers Updated to Use BaseHandler","text":"<ul> <li><code>GoogleOpenBuildingsHandler</code></li> <li><code>MicrosoftBuildingsHandler</code></li> <li><code>GHSLDataHandler</code></li> <li>All now inherit from <code>BaseHandler</code>, supporting standardized behavior and cleaner APIs.</li> </ul>"},{"location":"changelog/#changed_7","title":"Changed","text":""},{"location":"changelog/#poi-view-generator_1","title":"POI View Generator","text":"<ul> <li><code>map_built_s</code> and <code>map_smod</code> now internally use the new <code>map_zonal_stats</code> method.</li> <li><code>tif_processors</code> renamed to <code>data</code> to support both raster and polygon inputs.</li> <li>Removed parameters:</li> <li><code>id_column</code> (now handled internally)</li> <li><code>area_column</code> (now automatically calculated)</li> </ul>"},{"location":"changelog/#internals-and-usability","title":"Internals and Usability","text":"<ul> <li>Improved error handling with clearer validation messages.</li> <li>Enhanced logging for better visibility during enrichment.</li> <li>More consistent use of coordinate column naming.</li> <li>Refined type hints and parameter documentation across key methods.</li> </ul>"},{"location":"changelog/#notes","title":"Notes","text":"<ul> <li>Removed legacy POI generator classes and redundant <code>poi.py</code> file.</li> <li>Simplified imports and removed unused handler dependencies.</li> <li>All POI generator methods now include updated docstrings, parameter explanations, and usage examples.</li> <li>Added docs on the new <code>BaseHandler</code> interface and handler refactors.</li> </ul>"},{"location":"changelog/#v050-2025-06-02","title":"[v0.5.0] - 2025-06-02","text":""},{"location":"changelog/#changed_8","title":"Changed","text":"<ul> <li>Refactored data loading architecture:</li> <li>Introduced dedicated reader classes for major datasets (Microsoft Global Buildings, Google Open Buildings, GHSL), each inheriting from a new <code>BaseHandlerReader</code>.</li> <li>Centralized file existence checks and raster/tabular loading methods in <code>BaseHandlerReader</code>.</li> <li> <p>Improved maintainability by encapsulating dataset-specific logic inside each reader class.</p> </li> <li> <p>Modularized source resolution:</p> </li> <li> <p>Each reader now supports resolving data by country, geometry, or individual points, improving code reuse and flexibility.</p> </li> <li> <p>Unified POI enrichment:</p> </li> <li>Merged all POI generators (Google Open Buildings, Microsoft Global Buildings, GHSL Built Surface, GHSL SMOD) into a single <code>PoiViewGenerator</code> class.</li> <li>Supports flexible inputs: list of <code>(lat, lon)</code> tuples, list of dicts, DataFrame, or GeoDataFrame.</li> <li>Maintains consistent internal state via <code>points_gdf</code>, updated after each mapping.</li> <li> <p>Enables chained enrichment of POI data using multiple datasets.</p> </li> <li> <p>Modernized internal data access:</p> </li> <li>All data loading now uses dedicated handler/reader classes, improving consistency and long-term maintainability.</li> </ul>"},{"location":"changelog/#fixed_12","title":"Fixed","text":"<ul> <li>Full DataStore integration:</li> <li>Fixed <code>OpenCellID</code> and <code>HDX</code> handlers to fully support the <code>DataStore</code> abstraction.</li> <li>All file reads, writes, and checks now use the configured <code>DataStore</code> (local or cloud).</li> <li>Temporary files are only used during downloads; final data is always stored and accessed via the DataStore interface.</li> </ul>"},{"location":"changelog/#removed_2","title":"Removed","text":"<ul> <li>Removed deprecated POI generator classes and the now-obsolete poi submodule. All enrichment is handled through the unified <code>PoiViewGenerator</code>.</li> </ul>"},{"location":"changelog/#notes_1","title":"Notes","text":"<ul> <li>This release finalizes the architectural refactors started in <code>v0.5.0</code>.</li> <li>While marked stable, please report any issues or regressions from the new modular structure.</li> </ul>"},{"location":"changelog/#v050b1-2025-05-27","title":"[v0.5.0b1] - 2025-05-27","text":""},{"location":"changelog/#added_11","title":"Added","text":"<ul> <li>New Handlers:</li> <li><code>hdx.py</code>: Handler for downloading and managing Humanitarian Data Exchange datasets.</li> <li><code>rwi.py</code>: Handler for the Relative Wealth Index dataset.</li> <li><code>opencellid.py</code>: Handler for OpenCellID tower locations.</li> <li><code>unicef_georepo.py</code>: Integration with UNICEF\u2019s GeoRepo asset repository.</li> <li>Zonal Generators:</li> <li>Introduced the <code>generators/zonal/</code> module to support spatial aggregations of various data types (points, polygons, rasters)     to zonal geometries such as grid tiles or catchment areas.</li> <li>New Geo-Processing Methods:</li> <li>Added methods to compute centroids of (Multi)Polygon geometries.</li> <li>Added methods to calculate area of (Multi)Polygon geometries in square meters.</li> </ul>"},{"location":"changelog/#changed_9","title":"Changed","text":"<ul> <li>Refactored:</li> <li><code>config.py</code>: Added support for new environment variables (OpenCellID and UNICEF GeoRepo keys).</li> <li><code>geo.py</code>: Enhanced spatial join functions for improved performance and clarity.</li> <li><code>handlers/</code>: <ul> <li>Minor robustness improvements in <code>google_open_buildings</code> and <code>microsoft_global_buildings</code>.</li> <li>Added a new class method in <code>boundaries</code> for initializing admin boundaries from UNICEF GeoRepo.</li> </ul> </li> <li><code>core/io/</code>: <ul> <li>Added <code>list_directories</code> method to both ADLS and local storage backends.</li> </ul> </li> <li>Documentation &amp; Project Structure:</li> <li>Updated <code>.env_sample</code> and <code>.gitignore</code> to align with new environment variables and data handling practices.</li> </ul>"},{"location":"changelog/#dependencies_3","title":"Dependencies","text":"<ul> <li>Updated <code>requirements.txt</code> and <code>setup.py</code> to reflect new dependencies and ensure compatibility.</li> </ul>"},{"location":"changelog/#notes_2","title":"Notes","text":"<ul> <li>This is a pre-release (<code>v0.5.0b1</code>) and is intended for testing and feedback.</li> <li>Some new modules, especially in <code>handlers</code> and <code>generators</code>, are experimental and may be refined in upcoming releases.</li> </ul>"},{"location":"changelog/#v041-2025-04-17","title":"[v0.4.1] - 2025-04-17","text":""},{"location":"changelog/#added_12","title":"Added","text":"<ul> <li>Documentation:<ul> <li>Added API Reference documentation for all modules, classes, and functions.</li> <li>Added a Configuration Guide to explain how to set up paths, API keys, and other.</li> </ul> </li> <li>TifProcessor: added new to_dataframe method.</li> <li>config: added set_path method for dynamic path management.</li> </ul>"},{"location":"changelog/#changed_10","title":"Changed","text":"<ul> <li>Documentation:<ul> <li>Restructured the <code>docs/</code> directory to improve organization and navigation.</li> <li>Updated the <code>index.md</code> for the User Guide to provide a clear overview of available documentation.</li> <li>Updated Examples for downloading, processing, and storing geospatial data - more to come.</li> </ul> </li> <li>README:<ul> <li>Updated the README with a clear description of the package\u2019s purpose and key features.</li> <li>Added a section on View Generators to explain spatial context enrichment and mapping to grid or POI locations.</li> <li>Included a Supported Datasets section with an image of dataset provider logos.</li> </ul> </li> </ul>"},{"location":"changelog/#fixed_13","title":"Fixed","text":"<ul> <li>Handled errors when processing nodes, relations, and ways in OSMLocationFetcher.</li> <li>Made <code>admin1</code> and <code>admin1_id_giga</code> optional in GigaEntity instances for countries with no admin level 1 divisions.</li> </ul>"},{"location":"changelog/#v040-2025-04-01","title":"[v0.4.0] - 2025-04-01","text":""},{"location":"changelog/#added_13","title":"Added","text":"<ul> <li>POI View Generators: Introduced a new module, generators, containing a base class for POI view generation.</li> <li>Expanded POI Support: Added new classes for generating POI views from:<ul> <li>Google Open Buildings</li> <li>Microsoft Global Buildings</li> <li>GHSL Settlement Model</li> <li>GHSL Built Surface</li> </ul> </li> <li>New Reader: Added read_gzipped_json_or_csv to handle compressed JSON/CSV files.</li> </ul>"},{"location":"changelog/#changed_11","title":"Changed","text":"<ul> <li>ADLSDataStore Enhancements: Updated methods to match LocalDataStore for improved consistency.</li> <li>Geo Processing Updates:<ul> <li>Improved convert_to_dataframe for more efficient data conversion.</li> <li>Enhanced annotate_with_admin_regions to improve spatial joins.</li> </ul> </li> <li>New TifProcessor Methods:<ul> <li>sample_by_polygons for polygon-based raster sampling.</li> <li>sample_multiple_tifs_by_coordinates &amp; sample_multiple_tifs_by_polygons to manage multi-raster sampling.</li> </ul> </li> <li>Fixed Global Config Handling: Resolved issues with handling configurations inside classes.</li> </ul>"},{"location":"changelog/#v032-2025-03-21","title":"[v0.3.2] - 2025-03-21","text":""},{"location":"changelog/#added_14","title":"Added","text":"<ul> <li>Added a method to efficiently assign unique IDs to features.</li> </ul>"},{"location":"changelog/#changed_12","title":"Changed","text":"<ul> <li>Enhanced logging for better debugging and clarity.</li> </ul>"},{"location":"changelog/#fixed_14","title":"Fixed","text":"<ul> <li>Minor bug fix in config.py</li> </ul>"},{"location":"changelog/#031-2025-03-20","title":"[0.3.1] - 2025-03-20","text":""},{"location":"changelog/#added_15","title":"Added","text":"<ul> <li>Enhanced AdminBoundaries handler with improved error handling for cases where administrative level data is unavailable for a country.</li> <li>Added pyproject.toml and setup.py, enabling pip install support for the package.</li> <li>Introduced a new method annotate_with_admin_regions in geo.py to perform spatial joins between input points and administrative boundaries (levels 1 and 2), handling conflicts where points intersect multiple admin regions.</li> </ul>"},{"location":"changelog/#removed_3","title":"Removed","text":"<ul> <li>Removed the utils module containing logger.py and integrated LOG_FORMAT and get_logger into config.py for a more streamlined logging approach.</li> </ul>"},{"location":"changelog/#030-2025-03-18","title":"[0.3.0] - 2025-03-18","text":""},{"location":"changelog/#added_16","title":"Added","text":"<ul> <li>Compression support in readers for improved efficiency  </li> <li>New GHSL data handler to manage GHSL dataset downloads  </li> </ul>"},{"location":"changelog/#fixed_15","title":"Fixed","text":"<ul> <li>Small fixes/improvements in Microsoft Buildings, Maxar, and Overture handlers  </li> </ul>"},{"location":"changelog/#v022-2025-03-12","title":"[v0.2.2] - 2025-03-12","text":"<ul> <li> <p>Refactored Handlers: Improved structure and performance of maxar_image.py, osm.py and overture.py to enhance geospatial data handling.</p> </li> <li> <p>Documentation Improvements:</p> <ul> <li>Updated index.md, advanced.md, and use-cases.md for better clarity.</li> <li>Added installation.md under docs/getting-started for setup guidance.</li> <li>Refined API documentation in docs/api/index.md.</li> </ul> </li> <li> <p>Configuration &amp; Setup Enhancements:     \u2022   Improved .gitignore to exclude unnecessary files.     \u2022   Updated mkdocs.yml for better documentation structuring.</p> </li> <li>Bug Fixes &amp; Minor Optimizations: Small fixes and improvements across the codebase for stability and maintainability.</li> </ul>"},{"location":"changelog/#v021-2025-02-28","title":"[v0.2.1] - 2025-02-28","text":""},{"location":"changelog/#added_17","title":"Added","text":"<ul> <li>Introduced WorldPopDownloader feature to handlers</li> <li>Refactored TifProcessor class for better performance</li> </ul>"},{"location":"changelog/#fixed_16","title":"Fixed","text":"<ul> <li>Minor bug fixes and performance improvements</li> </ul>"},{"location":"changelog/#v020-maxarimagedownloader-bug-fixes-2025-02-24","title":"[v0.2.0] - MaxarImageDownloader &amp; Bug Fixes - 2025-02-24","text":"<ul> <li>New Handler: MaxarImageDownloader for downloading Maxar images.</li> <li>Bug Fixes: Various improvements and bug fixes.</li> <li>Enhancements: Minor optimizations in handlers.</li> </ul>"},{"location":"changelog/#v011-2025-02-24","title":"[v0.1.1] - 2025-02-24","text":""},{"location":"changelog/#added_18","title":"Added","text":"<ul> <li>Local Data Store: Introduced a new local data store alongside ADLS to improve data storage and read/write functionality.</li> <li>Boundaries Handler: Added <code>boundaries.py</code>, a new handler that allows to read administrative boundaries from GADM.</li> </ul>"},{"location":"changelog/#changed_13","title":"Changed","text":"<ul> <li>Handler Refactoring: Refactored existing handlers to improve modularity and data handling.</li> <li>Configuration Management: Added <code>config.py</code> to manage paths, runtime settings, and environment variables.</li> </ul>"},{"location":"changelog/#removed_4","title":"Removed","text":"<ul> <li>Administrative Schema: Removed <code>administrative.py</code> since its functionality is now handled by the <code>boundaries</code> handler.</li> <li>Globals Module: Removed <code>globals.py</code> and replaced it with <code>config.py</code> for better configuration management.</li> </ul>"},{"location":"changelog/#updated-files","title":"Updated Files","text":"<ul> <li><code>config.py</code></li> <li><code>boundaries.py</code></li> <li><code>google_open_buildings.py</code></li> <li><code>mapbox_image.py</code></li> <li><code>microsoft_global_buildings.py</code></li> <li><code>ookla_speedtest.py</code></li> <li><code>mercator_tiles.py</code></li> <li><code>adls_data_store.py</code></li> <li><code>data_store.py</code></li> <li><code>local_data_store.py</code></li> <li><code>readers.py</code></li> <li><code>writers.py</code></li> <li><code>entity.py</code></li> </ul>"},{"location":"changelog/#v010-2025-02-07","title":"[v0.1.0] - 2025-02-07","text":""},{"location":"changelog/#added_19","title":"Added","text":"<ul> <li>New data handlers: <code>google_open_buildings.py</code>, <code>microsoft_global_buildings.py</code>, <code>overture.py</code>, <code>mapbox_image.py</code>, <code>osm.py</code></li> <li>Processing functions in <code>tif_processor.py</code>, <code>geo.py</code> and <code>transform.py</code></li> <li>Grid generation modules: <code>h3_tiles.py</code>, <code>mercator_tiles.py</code></li> <li>View managers: <code>grid_view.py</code> and <code>national_view.py</code></li> <li>Schemas: <code>administrative.py</code></li> </ul>"},{"location":"changelog/#changed_14","title":"Changed","text":"<ul> <li>Updated <code>requirements.txt</code> with new dependencies</li> <li>Improved logging and data storage mechanisms</li> </ul>"},{"location":"changelog/#removed_5","title":"Removed","text":"<ul> <li>Deprecated views: <code>h3_view.py</code>, <code>mercator_view.py</code></li> </ul>"},{"location":"contributing/","title":"Contribution Guidelines","text":"<p>Thank you for considering contributing to Giga! We value your input and aim to make the contribution process as accessible and transparent as possible. Whether you're interested in reporting bugs, discussing code, submitting fixes, proposing features, becoming a maintainer, or engaging with the Giga community, we welcome your involvement.</p>"},{"location":"contributing/#how-to-contribute-to-our-giga-project","title":"How to Contribute to our Giga Project?","text":"<ol> <li>Familiarize Yourself: Before contributing, familiarize yourself with the project by reviewing the README, code of conduct, and existing issues or pull requests.</li> <li>Issues and Feature Requests: Check the issue tracker for existing issues or create a new one to report bugs, suggest improvements, or propose new features.</li> <li>Fork and Branch: Fork the repository and create a branch for your contribution. Branch names should be descriptive (e.g., feature/add-new-functionality, bugfix/issue-description).</li> <li>Code Changes: Make changes or additions following our coding conventions and style guide. Ensure to write clear commit messages that explain the purpose of each commit.</li> <li>Testing: If applicable, include tests for the changes made to ensure code reliability. Ensure existing tests pass.</li> <li>Documentation: Update relevant documentation, including README files or any other necessary guides.</li> <li>Pull Request: Open a pull request (PR) against the main branch. Clearly describe the changes introduced, referencing any related issues.</li> </ol>"},{"location":"contributing/#report-a-bug-or-suggestion","title":"Report a Bug or Suggestion","text":"<ul> <li>Bug Reports: Help us understand and address issues by submitting detailed bug reports via GitHub issues. Include as many relevant details as possible in the provided template to expedite resolutions.</li> <li>Suggestions: Share your ideas, feedback, or stay updated on Giga by joining our Discord channel.</li> </ul>"},{"location":"contributing/#making-changes-and-pull-requests","title":"Making Changes and Pull Requests","text":"<p>To contribute code changes:</p> <ol> <li>Fork the repository and create a new branch for your contribution  <p><code>git checkout -b 'my-contribution'</code>.</p> </li> <li>Make your changes on the created branch.</li> <li>Commit with clear messages describing the updates.</li> <li>Submit a pull request in the main repository, ensuring the following:</li> <li>Clear use case or demonstration of bug fix/new feature.</li> <li>Inclusion of relevant tests (unit, functional, and fuzz tests where applicable).</li> <li>Adherence to code style guidelines.</li> <li>No breaking changes to the existing test suite.</li> <li>Bug fixes accompanied by tests to prevent regression.</li> <li>Update of relevant comments and documentation reflecting code behavior changes.</li> </ol>"},{"location":"contributing/#contributing-with-an-issue","title":"Contributing with an Issue","text":"<p>If you encounter a bug but aren't sure how to fix it or submit a pull request, you can create an issue. Issues serve as avenues for bug reports, feedback, and general discussions within the GigaSpatial GitHub repository.</p>"},{"location":"contributing/#other-ways-to-contribute","title":"Other Ways to Contribute","text":"<p>Beyond code contributions:</p> <ul> <li>Feedback and Insights: Share your expertise and experiences related to cash transfer by contacting us at giga@unicef.org.</li> <li>Documentation: Contribute to our journey by sharing reports, case studies, articles, blogs, or surveys. Contact us to contribute and learn more via giga@unicef.org.</li> <li>Designs: If you're passionate about UI/UX, animations, graphics, tutorials, etc., contact us to create visuals for the Giga community via giga@unicef.org.</li> </ul>"},{"location":"contributing/#connect-with-giga-contributors","title":"Connect with Giga Contributors","text":"<p>Connect with fellow contributors via our Discord channel to engage with the Giga community: Click</p>"},{"location":"license/","title":"License","text":"<pre><code>                GNU AFFERO GENERAL PUBLIC LICENSE\n                   Version 3, 19 November 2007\n</code></pre> <p>Copyright (C) 2007 Free Software Foundation, Inc. https://fsf.org/  Everyone is permitted to copy and distribute verbatim copies  of this license document, but changing it is not allowed.</p> <pre><code>                        Preamble\n</code></pre> <p>The GNU Affero General Public License is a free, copyleft license for software and other kinds of works, specifically designed to ensure cooperation with the community in the case of network server software.</p> <p>The licenses for most software and other practical works are designed to take away your freedom to share and change the works.  By contrast, our General Public Licenses are intended to guarantee your freedom to share and change all versions of a program--to make sure it remains free software for all its users.</p> <p>When we speak of free software, we are referring to freedom, not price.  Our General Public Licenses are designed to make sure that you have the freedom to distribute copies of free software (and charge for them if you wish), that you receive source code or can get it if you want it, that you can change the software or use pieces of it in new free programs, and that you know you can do these things.</p> <p>Developers that use our General Public Licenses protect your rights with two steps: (1) assert copyright on the software, and (2) offer you this License which gives you legal permission to copy, distribute and/or modify the software.</p> <p>A secondary benefit of defending all users' freedom is that improvements made in alternate versions of the program, if they receive widespread use, become available for other developers to incorporate.  Many developers of free software are heartened and encouraged by the resulting cooperation.  However, in the case of software used on network servers, this result may fail to come about. The GNU General Public License permits making a modified version and letting the public access it on a server without ever releasing its source code to the public.</p> <p>The GNU Affero General Public License is designed specifically to ensure that, in such cases, the modified source code becomes available to the community.  It requires the operator of a network server to provide the source code of the modified version running there to the users of that server.  Therefore, public use of a modified version, on a publicly accessible server, gives the public access to the source code of the modified version.</p> <p>An older license, called the Affero General Public License and published by Affero, was designed to accomplish similar goals.  This is a different license, not a version of the Affero GPL, but Affero has released a new version of the Affero GPL which permits relicensing under this license.</p> <p>The precise terms and conditions for copying, distribution and modification follow.</p> <pre><code>                   TERMS AND CONDITIONS\n</code></pre> <ol> <li>Definitions.</li> </ol> <p>\"This License\" refers to version 3 of the GNU Affero General Public License.</p> <p>\"Copyright\" also means copyright-like laws that apply to other kinds of works, such as semiconductor masks.</p> <p>\"The Program\" refers to any copyrightable work licensed under this License.  Each licensee is addressed as \"you\".  \"Licensees\" and \"recipients\" may be individuals or organizations.</p> <p>To \"modify\" a work means to copy from or adapt all or part of the work in a fashion requiring copyright permission, other than the making of an exact copy.  The resulting work is called a \"modified version\" of the earlier work or a work \"based on\" the earlier work.</p> <p>A \"covered work\" means either the unmodified Program or a work based on the Program.</p> <p>To \"propagate\" a work means to do anything with it that, without permission, would make you directly or secondarily liable for infringement under applicable copyright law, except executing it on a computer or modifying a private copy.  Propagation includes copying, distribution (with or without modification), making available to the public, and in some countries other activities as well.</p> <p>To \"convey\" a work means any kind of propagation that enables other parties to make or receive copies.  Mere interaction with a user through a computer network, with no transfer of a copy, is not conveying.</p> <p>An interactive user interface displays \"Appropriate Legal Notices\" to the extent that it includes a convenient and prominently visible feature that (1) displays an appropriate copyright notice, and (2) tells the user that there is no warranty for the work (except to the extent that warranties are provided), that licensees may convey the work under this License, and how to view a copy of this License.  If the interface presents a list of user commands or options, such as a menu, a prominent item in the list meets this criterion.</p> <ol> <li>Source Code.</li> </ol> <p>The \"source code\" for a work means the preferred form of the work for making modifications to it.  \"Object code\" means any non-source form of a work.</p> <p>A \"Standard Interface\" means an interface that either is an official standard defined by a recognized standards body, or, in the case of interfaces specified for a particular programming language, one that is widely used among developers working in that language.</p> <p>The \"System Libraries\" of an executable work include anything, other than the work as a whole, that (a) is included in the normal form of packaging a Major Component, but which is not part of that Major Component, and (b) serves only to enable use of the work with that Major Component, or to implement a Standard Interface for which an implementation is available to the public in source code form.  A \"Major Component\", in this context, means a major essential component (kernel, window system, and so on) of the specific operating system (if any) on which the executable work runs, or a compiler used to produce the work, or an object code interpreter used to run it.</p> <p>The \"Corresponding Source\" for a work in object code form means all the source code needed to generate, install, and (for an executable work) run the object code and to modify the work, including scripts to control those activities.  However, it does not include the work's System Libraries, or general-purpose tools or generally available free programs which are used unmodified in performing those activities but which are not part of the work.  For example, Corresponding Source includes interface definition files associated with source files for the work, and the source code for shared libraries and dynamically linked subprograms that the work is specifically designed to require, such as by intimate data communication or control flow between those subprograms and other parts of the work.</p> <p>The Corresponding Source need not include anything that users can regenerate automatically from other parts of the Corresponding Source.</p> <p>The Corresponding Source for a work in source code form is that same work.</p> <ol> <li>Basic Permissions.</li> </ol> <p>All rights granted under this License are granted for the term of copyright on the Program, and are irrevocable provided the stated conditions are met.  This License explicitly affirms your unlimited permission to run the unmodified Program.  The output from running a covered work is covered by this License only if the output, given its content, constitutes a covered work.  This License acknowledges your rights of fair use or other equivalent, as provided by copyright law.</p> <p>You may make, run and propagate covered works that you do not convey, without conditions so long as your license otherwise remains in force.  You may convey covered works to others for the sole purpose of having them make modifications exclusively for you, or provide you with facilities for running those works, provided that you comply with the terms of this License in conveying all material for which you do not control copyright.  Those thus making or running the covered works for you must do so exclusively on your behalf, under your direction and control, on terms that prohibit them from making any copies of your copyrighted material outside their relationship with you.</p> <p>Conveying under any other circumstances is permitted solely under the conditions stated below.  Sublicensing is not allowed; section 10 makes it unnecessary.</p> <ol> <li>Protecting Users' Legal Rights From Anti-Circumvention Law.</li> </ol> <p>No covered work shall be deemed part of an effective technological measure under any applicable law fulfilling obligations under article 11 of the WIPO copyright treaty adopted on 20 December 1996, or similar laws prohibiting or restricting circumvention of such measures.</p> <p>When you convey a covered work, you waive any legal power to forbid circumvention of technological measures to the extent such circumvention is effected by exercising rights under this License with respect to the covered work, and you disclaim any intention to limit operation or modification of the work as a means of enforcing, against the work's users, your or third parties' legal rights to forbid circumvention of technological measures.</p> <ol> <li>Conveying Verbatim Copies.</li> </ol> <p>You may convey verbatim copies of the Program's source code as you receive it, in any medium, provided that you conspicuously and appropriately publish on each copy an appropriate copyright notice; keep intact all notices stating that this License and any non-permissive terms added in accord with section 7 apply to the code; keep intact all notices of the absence of any warranty; and give all recipients a copy of this License along with the Program.</p> <p>You may charge any price or no price for each copy that you convey, and you may offer support or warranty protection for a fee.</p> <ol> <li>Conveying Modified Source Versions.</li> </ol> <p>You may convey a work based on the Program, or the modifications to produce it from the Program, in the form of source code under the terms of section 4, provided that you also meet all of these conditions:</p> <pre><code>a) The work must carry prominent notices stating that you modified\nit, and giving a relevant date.\n\nb) The work must carry prominent notices stating that it is\nreleased under this License and any conditions added under section\n7.  This requirement modifies the requirement in section 4 to\n\"keep intact all notices\".\n\nc) You must license the entire work, as a whole, under this\nLicense to anyone who comes into possession of a copy.  This\nLicense will therefore apply, along with any applicable section 7\nadditional terms, to the whole of the work, and all its parts,\nregardless of how they are packaged.  This License gives no\npermission to license the work in any other way, but it does not\ninvalidate such permission if you have separately received it.\n\nd) If the work has interactive user interfaces, each must display\nAppropriate Legal Notices; however, if the Program has interactive\ninterfaces that do not display Appropriate Legal Notices, your\nwork need not make them do so.\n</code></pre> <p>A compilation of a covered work with other separate and independent works, which are not by their nature extensions of the covered work, and which are not combined with it such as to form a larger program, in or on a volume of a storage or distribution medium, is called an \"aggregate\" if the compilation and its resulting copyright are not used to limit the access or legal rights of the compilation's users beyond what the individual works permit.  Inclusion of a covered work in an aggregate does not cause this License to apply to the other parts of the aggregate.</p> <ol> <li>Conveying Non-Source Forms.</li> </ol> <p>You may convey a covered work in object code form under the terms of sections 4 and 5, provided that you also convey the machine-readable Corresponding Source under the terms of this License, in one of these ways:</p> <pre><code>a) Convey the object code in, or embodied in, a physical product\n(including a physical distribution medium), accompanied by the\nCorresponding Source fixed on a durable physical medium\ncustomarily used for software interchange.\n\nb) Convey the object code in, or embodied in, a physical product\n(including a physical distribution medium), accompanied by a\nwritten offer, valid for at least three years and valid for as\nlong as you offer spare parts or customer support for that product\nmodel, to give anyone who possesses the object code either (1) a\ncopy of the Corresponding Source for all the software in the\nproduct that is covered by this License, on a durable physical\nmedium customarily used for software interchange, for a price no\nmore than your reasonable cost of physically performing this\nconveying of source, or (2) access to copy the\nCorresponding Source from a network server at no charge.\n\nc) Convey individual copies of the object code with a copy of the\nwritten offer to provide the Corresponding Source.  This\nalternative is allowed only occasionally and noncommercially, and\nonly if you received the object code with such an offer, in accord\nwith subsection 6b.\n\nd) Convey the object code by offering access from a designated\nplace (gratis or for a charge), and offer equivalent access to the\nCorresponding Source in the same way through the same place at no\nfurther charge.  You need not require recipients to copy the\nCorresponding Source along with the object code.  If the place to\ncopy the object code is a network server, the Corresponding Source\nmay be on a different server (operated by you or a third party)\nthat supports equivalent copying facilities, provided you maintain\nclear directions next to the object code saying where to find the\nCorresponding Source.  Regardless of what server hosts the\nCorresponding Source, you remain obligated to ensure that it is\navailable for as long as needed to satisfy these requirements.\n\ne) Convey the object code using peer-to-peer transmission, provided\nyou inform other peers where the object code and Corresponding\nSource of the work are being offered to the general public at no\ncharge under subsection 6d.\n</code></pre> <p>A separable portion of the object code, whose source code is excluded from the Corresponding Source as a System Library, need not be included in conveying the object code work.</p> <p>A \"User Product\" is either (1) a \"consumer product\", which means any tangible personal property which is normally used for personal, family, or household purposes, or (2) anything designed or sold for incorporation into a dwelling.  In determining whether a product is a consumer product, doubtful cases shall be resolved in favor of coverage.  For a particular product received by a particular user, \"normally used\" refers to a typical or common use of that class of product, regardless of the status of the particular user or of the way in which the particular user actually uses, or expects or is expected to use, the product.  A product is a consumer product regardless of whether the product has substantial commercial, industrial or non-consumer uses, unless such uses represent the only significant mode of use of the product.</p> <p>\"Installation Information\" for a User Product means any methods, procedures, authorization keys, or other information required to install and execute modified versions of a covered work in that User Product from a modified version of its Corresponding Source.  The information must suffice to ensure that the continued functioning of the modified object code is in no case prevented or interfered with solely because modification has been made.</p> <p>If you convey an object code work under this section in, or with, or specifically for use in, a User Product, and the conveying occurs as part of a transaction in which the right of possession and use of the User Product is transferred to the recipient in perpetuity or for a fixed term (regardless of how the transaction is characterized), the Corresponding Source conveyed under this section must be accompanied by the Installation Information.  But this requirement does not apply if neither you nor any third party retains the ability to install modified object code on the User Product (for example, the work has been installed in ROM).</p> <p>The requirement to provide Installation Information does not include a requirement to continue to provide support service, warranty, or updates for a work that has been modified or installed by the recipient, or for the User Product in which it has been modified or installed.  Access to a network may be denied when the modification itself materially and adversely affects the operation of the network or violates the rules and protocols for communication across the network.</p> <p>Corresponding Source conveyed, and Installation Information provided, in accord with this section must be in a format that is publicly documented (and with an implementation available to the public in source code form), and must require no special password or key for unpacking, reading or copying.</p> <ol> <li>Additional Terms.</li> </ol> <p>\"Additional permissions\" are terms that supplement the terms of this License by making exceptions from one or more of its conditions. Additional permissions that are applicable to the entire Program shall be treated as though they were included in this License, to the extent that they are valid under applicable law.  If additional permissions apply only to part of the Program, that part may be used separately under those permissions, but the entire Program remains governed by this License without regard to the additional permissions.</p> <p>When you convey a copy of a covered work, you may at your option remove any additional permissions from that copy, or from any part of it.  (Additional permissions may be written to require their own removal in certain cases when you modify the work.)  You may place additional permissions on material, added by you to a covered work, for which you have or can give appropriate copyright permission.</p> <p>Notwithstanding any other provision of this License, for material you add to a covered work, you may (if authorized by the copyright holders of that material) supplement the terms of this License with terms:</p> <pre><code>a) Disclaiming warranty or limiting liability differently from the\nterms of sections 15 and 16 of this License; or\n\nb) Requiring preservation of specified reasonable legal notices or\nauthor attributions in that material or in the Appropriate Legal\nNotices displayed by works containing it; or\n\nc) Prohibiting misrepresentation of the origin of that material, or\nrequiring that modified versions of such material be marked in\nreasonable ways as different from the original version; or\n\nd) Limiting the use for publicity purposes of names of licensors or\nauthors of the material; or\n\ne) Declining to grant rights under trademark law for use of some\ntrade names, trademarks, or service marks; or\n\nf) Requiring indemnification of licensors and authors of that\nmaterial by anyone who conveys the material (or modified versions of\nit) with contractual assumptions of liability to the recipient, for\nany liability that these contractual assumptions directly impose on\nthose licensors and authors.\n</code></pre> <p>All other non-permissive additional terms are considered \"further restrictions\" within the meaning of section 10.  If the Program as you received it, or any part of it, contains a notice stating that it is governed by this License along with a term that is a further restriction, you may remove that term.  If a license document contains a further restriction but permits relicensing or conveying under this License, you may add to a covered work material governed by the terms of that license document, provided that the further restriction does not survive such relicensing or conveying.</p> <p>If you add terms to a covered work in accord with this section, you must place, in the relevant source files, a statement of the additional terms that apply to those files, or a notice indicating where to find the applicable terms.</p> <p>Additional terms, permissive or non-permissive, may be stated in the form of a separately written license, or stated as exceptions; the above requirements apply either way.</p> <ol> <li>Termination.</li> </ol> <p>You may not propagate or modify a covered work except as expressly provided under this License.  Any attempt otherwise to propagate or modify it is void, and will automatically terminate your rights under this License (including any patent licenses granted under the third paragraph of section 11).</p> <p>However, if you cease all violation of this License, then your license from a particular copyright holder is reinstated (a) provisionally, unless and until the copyright holder explicitly and finally terminates your license, and (b) permanently, if the copyright holder fails to notify you of the violation by some reasonable means prior to 60 days after the cessation.</p> <p>Moreover, your license from a particular copyright holder is reinstated permanently if the copyright holder notifies you of the violation by some reasonable means, this is the first time you have received notice of violation of this License (for any work) from that copyright holder, and you cure the violation prior to 30 days after your receipt of the notice.</p> <p>Termination of your rights under this section does not terminate the licenses of parties who have received copies or rights from you under this License.  If your rights have been terminated and not permanently reinstated, you do not qualify to receive new licenses for the same material under section 10.</p> <ol> <li>Acceptance Not Required for Having Copies.</li> </ol> <p>You are not required to accept this License in order to receive or run a copy of the Program.  Ancillary propagation of a covered work occurring solely as a consequence of using peer-to-peer transmission to receive a copy likewise does not require acceptance.  However, nothing other than this License grants you permission to propagate or modify any covered work.  These actions infringe copyright if you do not accept this License.  Therefore, by modifying or propagating a covered work, you indicate your acceptance of this License to do so.</p> <ol> <li>Automatic Licensing of Downstream Recipients.</li> </ol> <p>Each time you convey a covered work, the recipient automatically receives a license from the original licensors, to run, modify and propagate that work, subject to this License.  You are not responsible for enforcing compliance by third parties with this License.</p> <p>An \"entity transaction\" is a transaction transferring control of an organization, or substantially all assets of one, or subdividing an organization, or merging organizations.  If propagation of a covered work results from an entity transaction, each party to that transaction who receives a copy of the work also receives whatever licenses to the work the party's predecessor in interest had or could give under the previous paragraph, plus a right to possession of the Corresponding Source of the work from the predecessor in interest, if the predecessor has it or can get it with reasonable efforts.</p> <p>You may not impose any further restrictions on the exercise of the rights granted or affirmed under this License.  For example, you may not impose a license fee, royalty, or other charge for exercise of rights granted under this License, and you may not initiate litigation (including a cross-claim or counterclaim in a lawsuit) alleging that any patent claim is infringed by making, using, selling, offering for sale, or importing the Program or any portion of it.</p> <ol> <li>Patents.</li> </ol> <p>A \"contributor\" is a copyright holder who authorizes use under this License of the Program or a work on which the Program is based.  The work thus licensed is called the contributor's \"contributor version\".</p> <p>A contributor's \"essential patent claims\" are all patent claims owned or controlled by the contributor, whether already acquired or hereafter acquired, that would be infringed by some manner, permitted by this License, of making, using, or selling its contributor version, but do not include claims that would be infringed only as a consequence of further modification of the contributor version.  For purposes of this definition, \"control\" includes the right to grant patent sublicenses in a manner consistent with the requirements of this License.</p> <p>Each contributor grants you a non-exclusive, worldwide, royalty-free patent license under the contributor's essential patent claims, to make, use, sell, offer for sale, import and otherwise run, modify and propagate the contents of its contributor version.</p> <p>In the following three paragraphs, a \"patent license\" is any express agreement or commitment, however denominated, not to enforce a patent (such as an express permission to practice a patent or covenant not to sue for patent infringement).  To \"grant\" such a patent license to a party means to make such an agreement or commitment not to enforce a patent against the party.</p> <p>If you convey a covered work, knowingly relying on a patent license, and the Corresponding Source of the work is not available for anyone to copy, free of charge and under the terms of this License, through a publicly available network server or other readily accessible means, then you must either (1) cause the Corresponding Source to be so available, or (2) arrange to deprive yourself of the benefit of the patent license for this particular work, or (3) arrange, in a manner consistent with the requirements of this License, to extend the patent license to downstream recipients.  \"Knowingly relying\" means you have actual knowledge that, but for the patent license, your conveying the covered work in a country, or your recipient's use of the covered work in a country, would infringe one or more identifiable patents in that country that you have reason to believe are valid.</p> <p>If, pursuant to or in connection with a single transaction or arrangement, you convey, or propagate by procuring conveyance of, a covered work, and grant a patent license to some of the parties receiving the covered work authorizing them to use, propagate, modify or convey a specific copy of the covered work, then the patent license you grant is automatically extended to all recipients of the covered work and works based on it.</p> <p>A patent license is \"discriminatory\" if it does not include within the scope of its coverage, prohibits the exercise of, or is conditioned on the non-exercise of one or more of the rights that are specifically granted under this License.  You may not convey a covered work if you are a party to an arrangement with a third party that is in the business of distributing software, under which you make payment to the third party based on the extent of your activity of conveying the work, and under which the third party grants, to any of the parties who would receive the covered work from you, a discriminatory patent license (a) in connection with copies of the covered work conveyed by you (or copies made from those copies), or (b) primarily for and in connection with specific products or compilations that contain the covered work, unless you entered into that arrangement, or that patent license was granted, prior to 28 March 2007.</p> <p>Nothing in this License shall be construed as excluding or limiting any implied license or other defenses to infringement that may otherwise be available to you under applicable patent law.</p> <ol> <li>No Surrender of Others' Freedom.</li> </ol> <p>If conditions are imposed on you (whether by court order, agreement or otherwise) that contradict the conditions of this License, they do not excuse you from the conditions of this License.  If you cannot convey a covered work so as to satisfy simultaneously your obligations under this License and any other pertinent obligations, then as a consequence you may not convey it at all.  For example, if you agree to terms that obligate you to collect a royalty for further conveying from those to whom you convey the Program, the only way you could satisfy both those terms and this License would be to refrain entirely from conveying the Program.</p> <ol> <li>Remote Network Interaction; Use with the GNU General Public License.</li> </ol> <p>Notwithstanding any other provision of this License, if you modify the Program, your modified version must prominently offer all users interacting with it remotely through a computer network (if your version supports such interaction) an opportunity to receive the Corresponding Source of your version by providing access to the Corresponding Source from a network server at no charge, through some standard or customary means of facilitating copying of software.  This Corresponding Source shall include the Corresponding Source for any work covered by version 3 of the GNU General Public License that is incorporated pursuant to the following paragraph.</p> <p>Notwithstanding any other provision of this License, you have permission to link or combine any covered work with a work licensed under version 3 of the GNU General Public License into a single combined work, and to convey the resulting work.  The terms of this License will continue to apply to the part which is the covered work, but the work with which it is combined will remain governed by version 3 of the GNU General Public License.</p> <ol> <li>Revised Versions of this License.</li> </ol> <p>The Free Software Foundation may publish revised and/or new versions of the GNU Affero General Public License from time to time.  Such new versions will be similar in spirit to the present version, but may differ in detail to address new problems or concerns.</p> <p>Each version is given a distinguishing version number.  If the Program specifies that a certain numbered version of the GNU Affero General Public License \"or any later version\" applies to it, you have the option of following the terms and conditions either of that numbered version or of any later version published by the Free Software Foundation.  If the Program does not specify a version number of the GNU Affero General Public License, you may choose any version ever published by the Free Software Foundation.</p> <p>If the Program specifies that a proxy can decide which future versions of the GNU Affero General Public License can be used, that proxy's public statement of acceptance of a version permanently authorizes you to choose that version for the Program.</p> <p>Later license versions may give you additional or different permissions.  However, no additional obligations are imposed on any author or copyright holder as a result of your choosing to follow a later version.</p> <ol> <li>Disclaimer of Warranty.</li> </ol> <p>THERE IS NO WARRANTY FOR THE PROGRAM, TO THE EXTENT PERMITTED BY APPLICABLE LAW.  EXCEPT WHEN OTHERWISE STATED IN WRITING THE COPYRIGHT HOLDERS AND/OR OTHER PARTIES PROVIDE THE PROGRAM \"AS IS\" WITHOUT WARRANTY OF ANY KIND, EITHER EXPRESSED OR IMPLIED, INCLUDING, BUT NOT LIMITED TO, THE IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE.  THE ENTIRE RISK AS TO THE QUALITY AND PERFORMANCE OF THE PROGRAM IS WITH YOU.  SHOULD THE PROGRAM PROVE DEFECTIVE, YOU ASSUME THE COST OF ALL NECESSARY SERVICING, REPAIR OR CORRECTION.</p> <ol> <li>Limitation of Liability.</li> </ol> <p>IN NO EVENT UNLESS REQUIRED BY APPLICABLE LAW OR AGREED TO IN WRITING WILL ANY COPYRIGHT HOLDER, OR ANY OTHER PARTY WHO MODIFIES AND/OR CONVEYS THE PROGRAM AS PERMITTED ABOVE, BE LIABLE TO YOU FOR DAMAGES, INCLUDING ANY GENERAL, SPECIAL, INCIDENTAL OR CONSEQUENTIAL DAMAGES ARISING OUT OF THE USE OR INABILITY TO USE THE PROGRAM (INCLUDING BUT NOT LIMITED TO LOSS OF DATA OR DATA BEING RENDERED INACCURATE OR LOSSES SUSTAINED BY YOU OR THIRD PARTIES OR A FAILURE OF THE PROGRAM TO OPERATE WITH ANY OTHER PROGRAMS), EVEN IF SUCH HOLDER OR OTHER PARTY HAS BEEN ADVISED OF THE POSSIBILITY OF SUCH DAMAGES.</p> <ol> <li>Interpretation of Sections 15 and 16.</li> </ol> <p>If the disclaimer of warranty and limitation of liability provided above cannot be given local legal effect according to their terms, reviewing courts shall apply local law that most closely approximates an absolute waiver of all civil liability in connection with the Program, unless a warranty or assumption of liability accompanies a copy of the Program in return for a fee.</p> <pre><code>                 END OF TERMS AND CONDITIONS\n\n        How to Apply These Terms to Your New Programs\n</code></pre> <p>If you develop a new program, and you want it to be of the greatest possible use to the public, the best way to achieve this is to make it free software which everyone can redistribute and change under these terms.</p> <p>To do so, attach the following notices to the program.  It is safest to attach them to the start of each source file to most effectively state the exclusion of warranty; and each file should have at least the \"copyright\" line and a pointer to where the full notice is found.</p> <pre><code>&lt;one line to give the program's name and a brief idea of what it does.&gt;\nCopyright (C) &lt;year&gt;  &lt;name of author&gt;\n\nThis program is free software: you can redistribute it and/or modify\nit under the terms of the GNU Affero General Public License as published\nby the Free Software Foundation, either version 3 of the License, or\n(at your option) any later version.\n\nThis program is distributed in the hope that it will be useful,\nbut WITHOUT ANY WARRANTY; without even the implied warranty of\nMERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the\nGNU Affero General Public License for more details.\n\nYou should have received a copy of the GNU Affero General Public License\nalong with this program.  If not, see &lt;https://www.gnu.org/licenses/&gt;.\n</code></pre> <p>Also add information on how to contact you by electronic and paper mail.</p> <p>If your software can interact with users remotely through a computer network, you should also make sure that it provides a way for users to get its source.  For example, if your program is a web application, its interface could display a \"Source\" link that leads users to an archive of the code.  There are many ways you could offer source, and different solutions will be better for different programs; see section 13 for the specific requirements.</p> <p>You should also get your employer (if you work as a programmer) or school, if any, to sign a \"copyright disclaimer\" for the program, if necessary. For more information on this, and how to apply and follow the GNU AGPL, see https://www.gnu.org/licenses/.</p>"},{"location":"api/","title":"API Reference","text":"<p>Welcome to the API reference for the <code>gigaspatial</code> package. This documentation provides detailed information about the modules, classes, and functions available in the package.</p>"},{"location":"api/#modules","title":"Modules","text":"<p>The <code>gigaspatial</code> package is organized into several modules, each serving a specific purpose:</p>"},{"location":"api/#1-handlers","title":"1. Handlers","text":"<p>The <code>handlers</code> module contains classes for downloading and processing geospatial data from various sources, such as OpenStreetMap (OSM) and the Global Human Settlement Layer (GHSL).</p> <ul> <li>OSMLocationFetcher: Fetches and processes location data from OpenStreetMap.</li> <li>GHSLDataDownloader: Downloads and processes data from the Global Human Settlement Layer.</li> </ul> <p>Learn more about the Handlers module</p>"},{"location":"api/#2-processing","title":"2. Processing","text":"<p>The <code>processing</code> module provides tools for processing geospatial data, such as GeoTIFF files.</p> <ul> <li>TifProcessor: Processes GeoTIFF files and extracts relevant data.</li> </ul> <p>Learn more about the Processing module</p>"},{"location":"api/#3-core","title":"3. Core","text":"<p>The <code>core</code> module contains essential utilities and base classes used throughout the package.</p> <ul> <li>DataStore: Handles the storage and retrieval of geospatial data in various formats.</li> <li>Config: Manages configuration settings, such as paths and API keys.</li> </ul> <p>Learn more about the Core module</p>"},{"location":"api/#4-generators","title":"4. Generators","text":"<p>The <code>generators</code> module includes tools for generating geospatial data, such as grids and synthetic datasets.</p> <p>Learn more about the Generators module</p>"},{"location":"api/#5-grid","title":"5. Grid","text":"<p>The <code>grid</code> module provides utilities for working with geospatial grids, such as creating and manipulating grid-based data.</p> <p>Learn more about the Grid module</p>"},{"location":"api/#getting-started","title":"Getting Started","text":"<p>To get started with the <code>gigaspatial</code> package, follow the Quick Start Guide.</p>"},{"location":"api/#additional-resources","title":"Additional Resources","text":"<ul> <li>Examples: Real-world examples and use cases.</li> <li>Changelog: Information about the latest updates and changes.</li> <li>Contributing: Guidelines for contributing to the project.</li> </ul>"},{"location":"api/#support","title":"Support","text":"<p>If you encounter any issues or have questions, feel free to open an issue or join our Discord community.</p>"},{"location":"api/core/","title":"Core Module","text":""},{"location":"api/core/#gigaspatial.core","title":"<code>gigaspatial.core</code>","text":""},{"location":"api/core/#gigaspatial.core.io","title":"<code>io</code>","text":""},{"location":"api/core/#gigaspatial.core.io.DataStore","title":"<code>DataStore</code>","text":"<p>               Bases: <code>ABC</code></p> <p>Abstract base class defining the interface for data store implementations. This class serves as a parent for both local and cloud-based storage solutions.</p> Source code in <code>gigaspatial/core/io/data_store.py</code> <pre><code>class DataStore(ABC):\n    \"\"\"\n    Abstract base class defining the interface for data store implementations.\n    This class serves as a parent for both local and cloud-based storage solutions.\n    \"\"\"\n\n    @abstractmethod\n    def read_file(self, path: str) -&gt; Any:\n        \"\"\"\n        Read contents of a file from the data store.\n\n        Args:\n            path: Path to the file to read\n\n        Returns:\n            Contents of the file\n\n        Raises:\n            IOError: If file cannot be read\n        \"\"\"\n        pass\n\n    @abstractmethod\n    def write_file(self, path: str, data: Any) -&gt; None:\n        \"\"\"\n        Write data to a file in the data store.\n\n        Args:\n            path: Path where to write the file\n            data: Data to write to the file\n\n        Raises:\n            IOError: If file cannot be written\n        \"\"\"\n        pass\n\n    @abstractmethod\n    def file_exists(self, path: str) -&gt; bool:\n        \"\"\"\n        Check if a file exists in the data store.\n\n        Args:\n            path: Path to check\n\n        Returns:\n            True if file exists, False otherwise\n        \"\"\"\n        pass\n\n    @abstractmethod\n    def list_files(self, path: str) -&gt; List[str]:\n        \"\"\"\n        List all files in a directory.\n\n        Args:\n            path: Directory path to list\n\n        Returns:\n            List of file paths in the directory\n        \"\"\"\n        pass\n\n    @abstractmethod\n    def walk(self, top: str) -&gt; Generator:\n        \"\"\"\n        Walk through directory tree, similar to os.walk().\n\n        Args:\n            top: Starting directory for the walk\n\n        Returns:\n            Generator yielding tuples of (dirpath, dirnames, filenames)\n        \"\"\"\n        pass\n\n    @abstractmethod\n    def open(self, file: str, mode: str = \"r\") -&gt; Union[str, bytes]:\n        \"\"\"\n        Context manager for file operations.\n\n        Args:\n            file: Path to the file\n            mode: File mode ('r', 'w', 'rb', 'wb')\n\n        Yields:\n            File-like object\n\n        Raises:\n            IOError: If file cannot be opened\n        \"\"\"\n        pass\n\n    @abstractmethod\n    def is_file(self, path: str) -&gt; bool:\n        \"\"\"\n        Check if path points to a file.\n\n        Args:\n            path: Path to check\n\n        Returns:\n            True if path is a file, False otherwise\n        \"\"\"\n        pass\n\n    @abstractmethod\n    def is_dir(self, path: str) -&gt; bool:\n        \"\"\"\n        Check if path points to a directory.\n\n        Args:\n            path: Path to check\n\n        Returns:\n            True if path is a directory, False otherwise\n        \"\"\"\n        pass\n\n    @abstractmethod\n    def remove(self, path: str) -&gt; None:\n        \"\"\"\n        Remove a file.\n\n        Args:\n            path: Path to the file to remove\n\n        Raises:\n            IOError: If file cannot be removed\n        \"\"\"\n        pass\n\n    @abstractmethod\n    def rmdir(self, dir: str) -&gt; None:\n        \"\"\"\n        Remove a directory and all its contents.\n\n        Args:\n            dir: Path to the directory to remove\n\n        Raises:\n            IOError: If directory cannot be removed\n        \"\"\"\n        pass\n</code></pre>"},{"location":"api/core/#gigaspatial.core.io.DataStore.file_exists","title":"<code>file_exists(path)</code>  <code>abstractmethod</code>","text":"<p>Check if a file exists in the data store.</p> <p>Parameters:</p> Name Type Description Default <code>path</code> <code>str</code> <p>Path to check</p> required <p>Returns:</p> Type Description <code>bool</code> <p>True if file exists, False otherwise</p> Source code in <code>gigaspatial/core/io/data_store.py</code> <pre><code>@abstractmethod\ndef file_exists(self, path: str) -&gt; bool:\n    \"\"\"\n    Check if a file exists in the data store.\n\n    Args:\n        path: Path to check\n\n    Returns:\n        True if file exists, False otherwise\n    \"\"\"\n    pass\n</code></pre>"},{"location":"api/core/#gigaspatial.core.io.DataStore.is_dir","title":"<code>is_dir(path)</code>  <code>abstractmethod</code>","text":"<p>Check if path points to a directory.</p> <p>Parameters:</p> Name Type Description Default <code>path</code> <code>str</code> <p>Path to check</p> required <p>Returns:</p> Type Description <code>bool</code> <p>True if path is a directory, False otherwise</p> Source code in <code>gigaspatial/core/io/data_store.py</code> <pre><code>@abstractmethod\ndef is_dir(self, path: str) -&gt; bool:\n    \"\"\"\n    Check if path points to a directory.\n\n    Args:\n        path: Path to check\n\n    Returns:\n        True if path is a directory, False otherwise\n    \"\"\"\n    pass\n</code></pre>"},{"location":"api/core/#gigaspatial.core.io.DataStore.is_file","title":"<code>is_file(path)</code>  <code>abstractmethod</code>","text":"<p>Check if path points to a file.</p> <p>Parameters:</p> Name Type Description Default <code>path</code> <code>str</code> <p>Path to check</p> required <p>Returns:</p> Type Description <code>bool</code> <p>True if path is a file, False otherwise</p> Source code in <code>gigaspatial/core/io/data_store.py</code> <pre><code>@abstractmethod\ndef is_file(self, path: str) -&gt; bool:\n    \"\"\"\n    Check if path points to a file.\n\n    Args:\n        path: Path to check\n\n    Returns:\n        True if path is a file, False otherwise\n    \"\"\"\n    pass\n</code></pre>"},{"location":"api/core/#gigaspatial.core.io.DataStore.list_files","title":"<code>list_files(path)</code>  <code>abstractmethod</code>","text":"<p>List all files in a directory.</p> <p>Parameters:</p> Name Type Description Default <code>path</code> <code>str</code> <p>Directory path to list</p> required <p>Returns:</p> Type Description <code>List[str]</code> <p>List of file paths in the directory</p> Source code in <code>gigaspatial/core/io/data_store.py</code> <pre><code>@abstractmethod\ndef list_files(self, path: str) -&gt; List[str]:\n    \"\"\"\n    List all files in a directory.\n\n    Args:\n        path: Directory path to list\n\n    Returns:\n        List of file paths in the directory\n    \"\"\"\n    pass\n</code></pre>"},{"location":"api/core/#gigaspatial.core.io.DataStore.open","title":"<code>open(file, mode='r')</code>  <code>abstractmethod</code>","text":"<p>Context manager for file operations.</p> <p>Parameters:</p> Name Type Description Default <code>file</code> <code>str</code> <p>Path to the file</p> required <code>mode</code> <code>str</code> <p>File mode ('r', 'w', 'rb', 'wb')</p> <code>'r'</code> <p>Yields:</p> Type Description <code>Union[str, bytes]</code> <p>File-like object</p> <p>Raises:</p> Type Description <code>IOError</code> <p>If file cannot be opened</p> Source code in <code>gigaspatial/core/io/data_store.py</code> <pre><code>@abstractmethod\ndef open(self, file: str, mode: str = \"r\") -&gt; Union[str, bytes]:\n    \"\"\"\n    Context manager for file operations.\n\n    Args:\n        file: Path to the file\n        mode: File mode ('r', 'w', 'rb', 'wb')\n\n    Yields:\n        File-like object\n\n    Raises:\n        IOError: If file cannot be opened\n    \"\"\"\n    pass\n</code></pre>"},{"location":"api/core/#gigaspatial.core.io.DataStore.read_file","title":"<code>read_file(path)</code>  <code>abstractmethod</code>","text":"<p>Read contents of a file from the data store.</p> <p>Parameters:</p> Name Type Description Default <code>path</code> <code>str</code> <p>Path to the file to read</p> required <p>Returns:</p> Type Description <code>Any</code> <p>Contents of the file</p> <p>Raises:</p> Type Description <code>IOError</code> <p>If file cannot be read</p> Source code in <code>gigaspatial/core/io/data_store.py</code> <pre><code>@abstractmethod\ndef read_file(self, path: str) -&gt; Any:\n    \"\"\"\n    Read contents of a file from the data store.\n\n    Args:\n        path: Path to the file to read\n\n    Returns:\n        Contents of the file\n\n    Raises:\n        IOError: If file cannot be read\n    \"\"\"\n    pass\n</code></pre>"},{"location":"api/core/#gigaspatial.core.io.DataStore.remove","title":"<code>remove(path)</code>  <code>abstractmethod</code>","text":"<p>Remove a file.</p> <p>Parameters:</p> Name Type Description Default <code>path</code> <code>str</code> <p>Path to the file to remove</p> required <p>Raises:</p> Type Description <code>IOError</code> <p>If file cannot be removed</p> Source code in <code>gigaspatial/core/io/data_store.py</code> <pre><code>@abstractmethod\ndef remove(self, path: str) -&gt; None:\n    \"\"\"\n    Remove a file.\n\n    Args:\n        path: Path to the file to remove\n\n    Raises:\n        IOError: If file cannot be removed\n    \"\"\"\n    pass\n</code></pre>"},{"location":"api/core/#gigaspatial.core.io.DataStore.rmdir","title":"<code>rmdir(dir)</code>  <code>abstractmethod</code>","text":"<p>Remove a directory and all its contents.</p> <p>Parameters:</p> Name Type Description Default <code>dir</code> <code>str</code> <p>Path to the directory to remove</p> required <p>Raises:</p> Type Description <code>IOError</code> <p>If directory cannot be removed</p> Source code in <code>gigaspatial/core/io/data_store.py</code> <pre><code>@abstractmethod\ndef rmdir(self, dir: str) -&gt; None:\n    \"\"\"\n    Remove a directory and all its contents.\n\n    Args:\n        dir: Path to the directory to remove\n\n    Raises:\n        IOError: If directory cannot be removed\n    \"\"\"\n    pass\n</code></pre>"},{"location":"api/core/#gigaspatial.core.io.DataStore.walk","title":"<code>walk(top)</code>  <code>abstractmethod</code>","text":"<p>Walk through directory tree, similar to os.walk().</p> <p>Parameters:</p> Name Type Description Default <code>top</code> <code>str</code> <p>Starting directory for the walk</p> required <p>Returns:</p> Type Description <code>Generator</code> <p>Generator yielding tuples of (dirpath, dirnames, filenames)</p> Source code in <code>gigaspatial/core/io/data_store.py</code> <pre><code>@abstractmethod\ndef walk(self, top: str) -&gt; Generator:\n    \"\"\"\n    Walk through directory tree, similar to os.walk().\n\n    Args:\n        top: Starting directory for the walk\n\n    Returns:\n        Generator yielding tuples of (dirpath, dirnames, filenames)\n    \"\"\"\n    pass\n</code></pre>"},{"location":"api/core/#gigaspatial.core.io.DataStore.write_file","title":"<code>write_file(path, data)</code>  <code>abstractmethod</code>","text":"<p>Write data to a file in the data store.</p> <p>Parameters:</p> Name Type Description Default <code>path</code> <code>str</code> <p>Path where to write the file</p> required <code>data</code> <code>Any</code> <p>Data to write to the file</p> required <p>Raises:</p> Type Description <code>IOError</code> <p>If file cannot be written</p> Source code in <code>gigaspatial/core/io/data_store.py</code> <pre><code>@abstractmethod\ndef write_file(self, path: str, data: Any) -&gt; None:\n    \"\"\"\n    Write data to a file in the data store.\n\n    Args:\n        path: Path where to write the file\n        data: Data to write to the file\n\n    Raises:\n        IOError: If file cannot be written\n    \"\"\"\n    pass\n</code></pre>"},{"location":"api/core/#gigaspatial.core.io.read_dataset","title":"<code>read_dataset(data_store, path, compression=None, **kwargs)</code>","text":"<p>Read data from various file formats stored in both local and cloud-based storage.</p>"},{"location":"api/core/#gigaspatial.core.io.read_dataset--parameters","title":"Parameters:","text":"<p>data_store : DataStore     Instance of DataStore for accessing data storage. path : str, Path     Path to the file in data storage. **kwargs : dict     Additional arguments passed to the specific reader function.</p>"},{"location":"api/core/#gigaspatial.core.io.read_dataset--returns","title":"Returns:","text":"<p>pandas.DataFrame or geopandas.GeoDataFrame     The data read from the file.</p>"},{"location":"api/core/#gigaspatial.core.io.read_dataset--raises","title":"Raises:","text":"<p>FileNotFoundError     If the file doesn't exist in blob storage. ValueError     If the file type is unsupported or if there's an error reading the file.</p> Source code in <code>gigaspatial/core/io/readers.py</code> <pre><code>def read_dataset(data_store: DataStore, path: str, compression: str = None, **kwargs):\n    \"\"\"\n    Read data from various file formats stored in both local and cloud-based storage.\n\n    Parameters:\n    ----------\n    data_store : DataStore\n        Instance of DataStore for accessing data storage.\n    path : str, Path\n        Path to the file in data storage.\n    **kwargs : dict\n        Additional arguments passed to the specific reader function.\n\n    Returns:\n    -------\n    pandas.DataFrame or geopandas.GeoDataFrame\n        The data read from the file.\n\n    Raises:\n    ------\n    FileNotFoundError\n        If the file doesn't exist in blob storage.\n    ValueError\n        If the file type is unsupported or if there's an error reading the file.\n    \"\"\"\n\n    # Define supported file formats and their readers\n    BINARY_FORMATS = {\n        \".shp\",\n        \".zip\",\n        \".parquet\",\n        \".gpkg\",\n        \".xlsx\",\n        \".xls\",\n        \".kmz\",\n        \".gz\",\n    }\n\n    PANDAS_READERS = {\n        \".csv\": pd.read_csv,\n        \".xlsx\": lambda f, **kw: pd.read_excel(f, engine=\"openpyxl\", **kw),\n        \".xls\": lambda f, **kw: pd.read_excel(f, engine=\"xlrd\", **kw),\n        \".json\": pd.read_json,\n        # \".gz\": lambda f, **kw: pd.read_csv(f, compression=\"gzip\", **kw),\n    }\n\n    GEO_READERS = {\n        \".shp\": gpd.read_file,\n        \".zip\": gpd.read_file,\n        \".geojson\": gpd.read_file,\n        \".gpkg\": gpd.read_file,\n        \".parquet\": gpd.read_parquet,\n        \".kmz\": read_kmz,\n    }\n\n    COMPRESSION_FORMATS = {\n        \".gz\": \"gzip\",\n        \".bz2\": \"bz2\",\n        \".zip\": \"zip\",\n        \".xz\": \"xz\",\n    }\n\n    try:\n        # Check if file exists\n        if not data_store.file_exists(path):\n            raise FileNotFoundError(f\"File '{path}' not found in blob storage\")\n\n        path_obj = Path(path)\n        suffixes = path_obj.suffixes\n        file_extension = suffixes[-1].lower() if suffixes else \"\"\n\n        if compression is None and file_extension in COMPRESSION_FORMATS:\n            compression_format = COMPRESSION_FORMATS[file_extension]\n\n            # if file has multiple extensions (e.g., .csv.gz), get the inner format\n            if len(suffixes) &gt; 1:\n                inner_extension = suffixes[-2].lower()\n\n                if inner_extension == \".tar\":\n                    raise ValueError(\n                        \"Tar archives (.tar.gz) are not directly supported\"\n                    )\n\n                if inner_extension in PANDAS_READERS:\n                    try:\n                        with data_store.open(path, \"rb\") as f:\n                            return PANDAS_READERS[inner_extension](\n                                f, compression=compression_format, **kwargs\n                            )\n                    except Exception as e:\n                        raise ValueError(f\"Error reading compressed file: {str(e)}\")\n                elif inner_extension in GEO_READERS:\n                    try:\n                        with data_store.open(path, \"rb\") as f:\n                            if compression_format == \"gzip\":\n                                import gzip\n\n                                decompressed_data = gzip.decompress(f.read())\n                                import io\n\n                                return GEO_READERS[inner_extension](\n                                    io.BytesIO(decompressed_data), **kwargs\n                                )\n                            else:\n                                raise ValueError(\n                                    f\"Compression format {compression_format} not supported for geo data\"\n                                )\n                    except Exception as e:\n                        raise ValueError(f\"Error reading compressed geo file: {str(e)}\")\n            else:\n                # if just .gz without clear inner type, assume csv\n                try:\n                    with data_store.open(path, \"rb\") as f:\n                        return pd.read_csv(f, compression=compression_format, **kwargs)\n                except Exception as e:\n                    raise ValueError(\n                        f\"Error reading compressed file as CSV: {str(e)}. \"\n                        f\"If not a CSV, specify the format in the filename (e.g., .json.gz)\"\n                    )\n\n        # Special handling for compressed files\n        if file_extension == \".zip\":\n            # For zip files, we need to use binary mode\n            with data_store.open(path, \"rb\") as f:\n                return gpd.read_file(f)\n\n        # Determine if we need binary mode based on file type\n        mode = \"rb\" if file_extension in BINARY_FORMATS else \"r\"\n\n        # Try reading with appropriate reader\n        if file_extension in PANDAS_READERS:\n            try:\n                with data_store.open(path, mode) as f:\n                    return PANDAS_READERS[file_extension](f, **kwargs)\n            except Exception as e:\n                raise ValueError(f\"Error reading file with pandas: {str(e)}\")\n\n        if file_extension in GEO_READERS:\n            try:\n                with data_store.open(path, \"rb\") as f:\n                    return GEO_READERS[file_extension](f, **kwargs)\n            except Exception as e:\n                # For parquet files, try pandas reader if geopandas fails\n                if file_extension == \".parquet\":\n                    try:\n                        with data_store.open(path, \"rb\") as f:\n                            return pd.read_parquet(f, **kwargs)\n                    except Exception as e2:\n                        raise ValueError(\n                            f\"Failed to read parquet with both geopandas ({str(e)}) \"\n                            f\"and pandas ({str(e2)})\"\n                        )\n                raise ValueError(f\"Error reading file with geopandas: {str(e)}\")\n\n        # If we get here, the file type is unsupported\n        supported_formats = sorted(set(PANDAS_READERS.keys()) | set(GEO_READERS.keys()))\n        supported_compressions = sorted(COMPRESSION_FORMATS.keys())\n        raise ValueError(\n            f\"Unsupported file type: {file_extension}\\n\"\n            f\"Supported formats: {', '.join(supported_formats)}\"\n            f\"Supported compressions: {', '.join(supported_compressions)}\"\n        )\n\n    except Exception as e:\n        if isinstance(e, (FileNotFoundError, ValueError)):\n            raise\n        raise RuntimeError(f\"Unexpected error reading dataset: {str(e)}\")\n</code></pre>"},{"location":"api/core/#gigaspatial.core.io.read_datasets","title":"<code>read_datasets(data_store, paths, **kwargs)</code>","text":"<p>Read multiple datasets from data storage at once.</p>"},{"location":"api/core/#gigaspatial.core.io.read_datasets--parameters","title":"Parameters:","text":"<p>data_store : DataStore     Instance of DataStore for accessing data storage. paths : list of str     Paths to files in data storage. **kwargs : dict     Additional arguments passed to read_dataset.</p>"},{"location":"api/core/#gigaspatial.core.io.read_datasets--returns","title":"Returns:","text":"<p>dict     Dictionary mapping paths to their corresponding DataFrames/GeoDataFrames.</p> Source code in <code>gigaspatial/core/io/readers.py</code> <pre><code>def read_datasets(data_store: DataStore, paths, **kwargs):\n    \"\"\"\n    Read multiple datasets from data storage at once.\n\n    Parameters:\n    ----------\n    data_store : DataStore\n        Instance of DataStore for accessing data storage.\n    paths : list of str\n        Paths to files in data storage.\n    **kwargs : dict\n        Additional arguments passed to read_dataset.\n\n    Returns:\n    -------\n    dict\n        Dictionary mapping paths to their corresponding DataFrames/GeoDataFrames.\n    \"\"\"\n    results = {}\n    errors = {}\n\n    for path in paths:\n        try:\n            results[path] = read_dataset(data_store, path, **kwargs)\n        except Exception as e:\n            errors[path] = str(e)\n\n    if errors:\n        error_msg = \"\\n\".join(f\"- {path}: {error}\" for path, error in errors.items())\n        raise ValueError(f\"Errors reading datasets:\\n{error_msg}\")\n\n    return results\n</code></pre>"},{"location":"api/core/#gigaspatial.core.io.read_gzipped_json_or_csv","title":"<code>read_gzipped_json_or_csv(file_path, data_store)</code>","text":"<p>Reads a gzipped file, attempting to parse it as JSON (lines=True) or CSV.</p> Source code in <code>gigaspatial/core/io/readers.py</code> <pre><code>def read_gzipped_json_or_csv(file_path, data_store):\n    \"\"\"Reads a gzipped file, attempting to parse it as JSON (lines=True) or CSV.\"\"\"\n\n    with data_store.open(file_path, \"rb\") as f:\n        g = gzip.GzipFile(fileobj=f)\n        text = g.read().decode(\"utf-8\")\n        try:\n            df = pd.read_json(io.StringIO(text), lines=True)\n            return df\n        except json.JSONDecodeError:\n            try:\n                df = pd.read_csv(io.StringIO(text))\n                return df\n            except pd.errors.ParserError:\n                print(f\"Error: Could not parse {file_path} as JSON or CSV.\")\n                return None\n</code></pre>"},{"location":"api/core/#gigaspatial.core.io.read_kmz","title":"<code>read_kmz(file_obj, **kwargs)</code>","text":"<p>Helper function to read KMZ files and return a GeoDataFrame.</p> Source code in <code>gigaspatial/core/io/readers.py</code> <pre><code>def read_kmz(file_obj, **kwargs):\n    \"\"\"Helper function to read KMZ files and return a GeoDataFrame.\"\"\"\n    try:\n        with zipfile.ZipFile(file_obj) as kmz:\n            # Find the KML file in the archive (usually doc.kml)\n            kml_filename = next(\n                name for name in kmz.namelist() if name.endswith(\".kml\")\n            )\n\n            # Read the KML content\n            kml_content = io.BytesIO(kmz.read(kml_filename))\n\n            gdf = gpd.read_file(kml_content)\n\n            # Validate the GeoDataFrame\n            if gdf.empty:\n                raise ValueError(\n                    \"The KML file is empty or does not contain valid geospatial data.\"\n                )\n\n        return gdf\n\n    except zipfile.BadZipFile:\n        raise ValueError(\"The provided file is not a valid KMZ file.\")\n    except StopIteration:\n        raise ValueError(\"No KML file found in the KMZ archive.\")\n    except Exception as e:\n        raise RuntimeError(f\"An error occurred: {e}\")\n</code></pre>"},{"location":"api/core/#gigaspatial.core.io.write_dataset","title":"<code>write_dataset(data, data_store, path, **kwargs)</code>","text":"<p>Write DataFrame, GeoDataFrame, or a generic object (for JSON) to various file formats in DataStore.</p>"},{"location":"api/core/#gigaspatial.core.io.write_dataset--parameters","title":"Parameters:","text":"<p>data : pandas.DataFrame, geopandas.GeoDataFrame, or any object     The data to write to data storage. data_store : DataStore     Instance of DataStore for accessing data storage. path : str     Path where the file will be written in data storage. **kwargs : dict     Additional arguments passed to the specific writer function.</p>"},{"location":"api/core/#gigaspatial.core.io.write_dataset--raises","title":"Raises:","text":"<p>ValueError     If the file type is unsupported or if there's an error writing the file. TypeError         If input data is not a DataFrame, GeoDataFrame, AND not a generic object         intended for a .json file.</p> Source code in <code>gigaspatial/core/io/writers.py</code> <pre><code>def write_dataset(data, data_store: DataStore, path, **kwargs):\n    \"\"\"\n    Write DataFrame, GeoDataFrame, or a generic object (for JSON)\n    to various file formats in DataStore.\n\n    Parameters:\n    ----------\n    data : pandas.DataFrame, geopandas.GeoDataFrame, or any object\n        The data to write to data storage.\n    data_store : DataStore\n        Instance of DataStore for accessing data storage.\n    path : str\n        Path where the file will be written in data storage.\n    **kwargs : dict\n        Additional arguments passed to the specific writer function.\n\n    Raises:\n    ------\n    ValueError\n        If the file type is unsupported or if there's an error writing the file.\n    TypeError\n            If input data is not a DataFrame, GeoDataFrame, AND not a generic object\n            intended for a .json file.\n    \"\"\"\n\n    # Define supported file formats and their writers\n    BINARY_FORMATS = {\".shp\", \".zip\", \".parquet\", \".gpkg\", \".xlsx\", \".xls\"}\n\n    PANDAS_WRITERS = {\n        \".csv\": lambda df, buf, **kw: df.to_csv(buf, **kw),\n        \".xlsx\": lambda df, buf, **kw: df.to_excel(buf, engine=\"openpyxl\", **kw),\n        \".json\": lambda df, buf, **kw: df.to_json(buf, **kw),\n        \".parquet\": lambda df, buf, **kw: df.to_parquet(buf, **kw),\n    }\n\n    GEO_WRITERS = {\n        \".geojson\": lambda gdf, buf, **kw: gdf.to_file(buf, driver=\"GeoJSON\", **kw),\n        \".gpkg\": lambda gdf, buf, **kw: gdf.to_file(buf, driver=\"GPKG\", **kw),\n        \".parquet\": lambda gdf, buf, **kw: gdf.to_parquet(buf, **kw),\n    }\n\n    try:\n        # Get file suffix and ensure it's lowercase\n        suffix = Path(path).suffix.lower()\n\n        # 1. Handle generic JSON data\n        is_dataframe_like = isinstance(data, (pd.DataFrame, gpd.GeoDataFrame))\n        if not is_dataframe_like:\n            if suffix == \".json\":\n                try:\n                    # Pass generic data directly to the write_json function\n                    write_json(data, data_store, path, **kwargs)\n                    return  # Successfully wrote JSON, so exit\n                except Exception as e:\n                    raise ValueError(f\"Error writing generic JSON data: {str(e)}\")\n            else:\n                # Raise an error if it's not a DataFrame/GeoDataFrame and not a .json file\n                raise TypeError(\n                    \"Input data must be a pandas DataFrame or GeoDataFrame, \"\n                    \"or a generic object destined for a '.json' file.\"\n                )\n\n        # 2. Handle DataFrame/GeoDataFrame\n        # Determine if we need binary mode based on file type\n        mode = \"wb\" if suffix in BINARY_FORMATS else \"w\"\n\n        # Handle different data types and formats\n        if isinstance(data, gpd.GeoDataFrame):\n            if suffix not in GEO_WRITERS:\n                supported_formats = sorted(GEO_WRITERS.keys())\n                raise ValueError(\n                    f\"Unsupported file type for GeoDataFrame: {suffix}\\n\"\n                    f\"Supported formats: {', '.join(supported_formats)}\"\n                )\n\n            try:\n                with data_store.open(path, \"wb\") as f:\n                    GEO_WRITERS[suffix](data, f, **kwargs)\n            except Exception as e:\n                raise ValueError(f\"Error writing GeoDataFrame: {str(e)}\")\n\n        else:  # pandas DataFrame\n            if suffix not in PANDAS_WRITERS:\n                supported_formats = sorted(PANDAS_WRITERS.keys())\n                raise ValueError(\n                    f\"Unsupported file type for DataFrame: {suffix}\\n\"\n                    f\"Supported formats: {', '.join(supported_formats)}\"\n                )\n\n            try:\n                with data_store.open(path, mode) as f:\n                    PANDAS_WRITERS[suffix](data, f, **kwargs)\n            except Exception as e:\n                raise ValueError(f\"Error writing DataFrame: {str(e)}\")\n\n    except Exception as e:\n        if isinstance(e, (TypeError, ValueError)):\n            raise\n        raise RuntimeError(f\"Unexpected error writing dataset: {str(e)}\")\n</code></pre>"},{"location":"api/core/#gigaspatial.core.io.write_datasets","title":"<code>write_datasets(data_dict, data_store, **kwargs)</code>","text":"<p>Write multiple datasets to data storage at once.</p>"},{"location":"api/core/#gigaspatial.core.io.write_datasets--parameters","title":"Parameters:","text":"<p>data_dict : dict     Dictionary mapping paths to DataFrames/GeoDataFrames. data_store : DataStore     Instance of DataStore for accessing data storage. **kwargs : dict     Additional arguments passed to write_dataset.</p>"},{"location":"api/core/#gigaspatial.core.io.write_datasets--raises","title":"Raises:","text":"<p>ValueError     If there are any errors writing the datasets.</p> Source code in <code>gigaspatial/core/io/writers.py</code> <pre><code>def write_datasets(data_dict, data_store: DataStore, **kwargs):\n    \"\"\"\n    Write multiple datasets to data storage at once.\n\n    Parameters:\n    ----------\n    data_dict : dict\n        Dictionary mapping paths to DataFrames/GeoDataFrames.\n    data_store : DataStore\n        Instance of DataStore for accessing data storage.\n    **kwargs : dict\n        Additional arguments passed to write_dataset.\n\n    Raises:\n    ------\n    ValueError\n        If there are any errors writing the datasets.\n    \"\"\"\n    errors = {}\n\n    for path, data in data_dict.items():\n        try:\n            write_dataset(data, data_store, path, **kwargs)\n        except Exception as e:\n            errors[path] = str(e)\n\n    if errors:\n        error_msg = \"\\n\".join(f\"- {path}: {error}\" for path, error in errors.items())\n        raise ValueError(f\"Errors writing datasets:\\n{error_msg}\")\n</code></pre>"},{"location":"api/core/#gigaspatial.core.io.adls_data_store","title":"<code>adls_data_store</code>","text":""},{"location":"api/core/#gigaspatial.core.io.adls_data_store.ADLSDataStore","title":"<code>ADLSDataStore</code>","text":"<p>               Bases: <code>DataStore</code></p> <p>An implementation of DataStore for Azure Data Lake Storage.</p> Source code in <code>gigaspatial/core/io/adls_data_store.py</code> <pre><code>class ADLSDataStore(DataStore):\n    \"\"\"\n    An implementation of DataStore for Azure Data Lake Storage.\n    \"\"\"\n\n    def __init__(\n        self,\n        container: str = config.ADLS_CONTAINER_NAME,\n        connection_string: str = config.ADLS_CONNECTION_STRING,\n        account_url: str = config.ADLS_ACCOUNT_URL,\n        sas_token: str = config.ADLS_SAS_TOKEN,\n    ):\n        \"\"\"\n        Create a new instance of ADLSDataStore\n        :param container: The name of the container in ADLS to interact with.\n        \"\"\"\n        if connection_string:\n            self.blob_service_client = BlobServiceClient.from_connection_string(\n                connection_string\n            )\n        elif account_url and sas_token:\n            self.blob_service_client = BlobServiceClient(\n                account_url=account_url, credential=sas_token\n            )\n        else:\n            raise ValueError(\n                \"Either connection_string or account_url and sas_token must be provided.\"\n            )\n\n        self.container_client = self.blob_service_client.get_container_client(\n            container=container\n        )\n        self.container = container\n\n    def read_file(self, path: str, encoding: Optional[str] = None) -&gt; Union[str, bytes]:\n        \"\"\"\n        Read file with flexible encoding support.\n\n        :param path: Path to the file in blob storage\n        :param encoding: File encoding (optional)\n        :return: File contents as string or bytes\n        \"\"\"\n        try:\n            blob_client = self.container_client.get_blob_client(path)\n            blob_data = blob_client.download_blob().readall()\n\n            # If no encoding specified, return raw bytes\n            if encoding is None:\n                return blob_data\n\n            # If encoding is specified, decode the bytes\n            return blob_data.decode(encoding)\n\n        except Exception as e:\n            raise IOError(f\"Error reading file {path}: {e}\")\n\n    def write_file(self, path: str, data) -&gt; None:\n        \"\"\"\n        Write file with support for content type and improved type handling.\n\n        :param path: Destination path in blob storage\n        :param data: File contents\n        \"\"\"\n        blob_client = self.blob_service_client.get_blob_client(\n            container=self.container, blob=path, snapshot=None\n        )\n\n        if isinstance(data, str):\n            binary_data = data.encode()\n        elif isinstance(data, bytes):\n            binary_data = data\n        else:\n            raise Exception(f'Unsupported data type. Only \"bytes\" or \"string\" accepted')\n\n        blob_client.upload_blob(binary_data, overwrite=True)\n\n    def upload_file(self, file_path, blob_path):\n        \"\"\"Uploads a single file to Azure Blob Storage.\"\"\"\n        try:\n            blob_client = self.container_client.get_blob_client(blob_path)\n            with open(file_path, \"rb\") as data:\n                blob_client.upload_blob(data, overwrite=True)\n            print(f\"Uploaded {file_path} to {blob_path}\")\n        except Exception as e:\n            print(f\"Failed to upload {file_path}: {e}\")\n\n    def upload_directory(self, dir_path, blob_dir_path):\n        \"\"\"Uploads all files from a directory to Azure Blob Storage.\"\"\"\n        for root, dirs, files in os.walk(dir_path):\n            for file in files:\n                local_file_path = os.path.join(root, file)\n                relative_path = os.path.relpath(local_file_path, dir_path)\n                blob_file_path = os.path.join(blob_dir_path, relative_path).replace(\n                    \"\\\\\", \"/\"\n                )\n\n                self.upload_file(local_file_path, blob_file_path)\n\n    def download_directory(self, blob_dir_path: str, local_dir_path: str):\n        \"\"\"Downloads all files from a directory in Azure Blob Storage to a local directory.\"\"\"\n        try:\n            # Ensure the local directory exists\n            os.makedirs(local_dir_path, exist_ok=True)\n\n            # List all files in the blob directory\n            blob_items = self.container_client.list_blobs(\n                name_starts_with=blob_dir_path\n            )\n\n            for blob_item in blob_items:\n                # Get the relative path of the blob file\n                relative_path = os.path.relpath(blob_item.name, blob_dir_path)\n                # Construct the local file path\n                local_file_path = os.path.join(local_dir_path, relative_path)\n                # Create directories if needed\n                os.makedirs(os.path.dirname(local_file_path), exist_ok=True)\n\n                # Download the blob to the local file\n                blob_client = self.container_client.get_blob_client(blob_item.name)\n                with open(local_file_path, \"wb\") as file:\n                    file.write(blob_client.download_blob().readall())\n\n            print(f\"Downloaded directory {blob_dir_path} to {local_dir_path}\")\n        except Exception as e:\n            print(f\"Failed to download directory {blob_dir_path}: {e}\")\n\n    def copy_directory(self, source_dir: str, destination_dir: str):\n        \"\"\"\n        Copies all files from a source directory to a destination directory within the same container.\n\n        :param source_dir: The source directory path in the blob storage\n        :param destination_dir: The destination directory path in the blob storage\n        \"\"\"\n        try:\n            # Ensure source directory path ends with a trailing slash\n            source_dir = source_dir.rstrip(\"/\") + \"/\"\n            destination_dir = destination_dir.rstrip(\"/\") + \"/\"\n\n            # List all blobs in the source directory\n            source_blobs = self.container_client.list_blobs(name_starts_with=source_dir)\n\n            for blob in source_blobs:\n                # Get the relative path of the blob\n                relative_path = os.path.relpath(blob.name, source_dir)\n\n                # Construct the new blob path\n                new_blob_path = os.path.join(destination_dir, relative_path).replace(\n                    \"\\\\\", \"/\"\n                )\n\n                # Use copy_file method to copy each file\n                self.copy_file(blob.name, new_blob_path, overwrite=True)\n\n            print(f\"Copied directory from {source_dir} to {destination_dir}\")\n        except Exception as e:\n            print(f\"Failed to copy directory {source_dir}: {e}\")\n\n    def copy_file(\n        self, source_path: str, destination_path: str, overwrite: bool = False\n    ):\n        \"\"\"\n        Copies a single file from source to destination within the same container.\n\n        :param source_path: The source file path in the blob storage\n        :param destination_path: The destination file path in the blob storage\n        :param overwrite: If True, overwrite the destination file if it already exists\n        \"\"\"\n        try:\n            if not self.file_exists(source_path):\n                raise FileNotFoundError(f\"Source file not found: {source_path}\")\n\n            if self.file_exists(destination_path) and not overwrite:\n                raise FileExistsError(\n                    f\"Destination file already exists and overwrite is False: {destination_path}\"\n                )\n\n            # Create source and destination blob clients\n            source_blob_client = self.container_client.get_blob_client(source_path)\n            destination_blob_client = self.container_client.get_blob_client(\n                destination_path\n            )\n\n            # Start the server-side copy operation\n            destination_blob_client.start_copy_from_url(source_blob_client.url)\n\n            print(f\"Copied file from {source_path} to {destination_path}\")\n        except Exception as e:\n            print(f\"Failed to copy file {source_path}: {e}\")\n            raise\n\n    def exists(self, path: str) -&gt; bool:\n        blob_client = self.blob_service_client.get_blob_client(\n            container=self.container, blob=path, snapshot=None\n        )\n        return blob_client.exists()\n\n    def file_exists(self, path: str) -&gt; bool:\n        return self.exists(path) and not self.is_dir(path)\n\n    def file_size(self, path: str) -&gt; float:\n        blob_client = self.blob_service_client.get_blob_client(\n            container=self.container, blob=path, snapshot=None\n        )\n        properties = blob_client.get_blob_properties()\n\n        # The size is in bytes, convert it to kilobytes\n        size_in_bytes = properties.size\n        size_in_kb = size_in_bytes / 1024.0\n        return size_in_kb\n\n    def list_files(self, path: str):\n        blob_items = self.container_client.list_blobs(name_starts_with=path)\n        return [item[\"name\"] for item in blob_items]\n\n    def walk(self, top: str):\n        top = top.rstrip(\"/\") + \"/\"\n        blob_items = self.container_client.list_blobs(name_starts_with=top)\n        blobs = [item[\"name\"] for item in blob_items]\n        for blob in blobs:\n            dirpath, filename = os.path.split(blob)\n            yield (dirpath, [], [filename])\n\n    def list_directories(self, path: str) -&gt; list:\n        \"\"\"List only directory names (not files) from a given path in ADLS.\"\"\"\n        search_path = path.rstrip(\"/\") + \"/\" if path else \"\"\n\n        blob_items = self.container_client.list_blobs(name_starts_with=search_path)\n\n        directories = set()\n\n        for blob_item in blob_items:\n            # Get the relative path from the search path\n            relative_path = blob_item.name[len(search_path) :]\n\n            # Skip if it's empty (shouldn't happen but just in case)\n            if not relative_path:\n                continue\n\n            # If there's a \"/\" in the relative path, it means there's a subdirectory\n            if \"/\" in relative_path:\n                # Get the first directory name\n                dir_name = relative_path.split(\"/\")[0]\n                directories.add(dir_name)\n\n        return sorted(list(directories))\n\n    @contextlib.contextmanager\n    def open(self, path: str, mode: str = \"r\"):\n        \"\"\"\n        Context manager for file operations with enhanced mode support.\n\n        :param path: File path in blob storage\n        :param mode: File open mode (r, rb, w, wb)\n        \"\"\"\n        if mode == \"w\":\n            file = io.StringIO()\n            yield file\n            self.write_file(path, file.getvalue())\n\n        elif mode == \"wb\":\n            file = io.BytesIO()\n            yield file\n            self.write_file(path, file.getvalue())\n\n        elif mode == \"r\":\n            data = self.read_file(path, encoding=\"UTF-8\")\n            file = io.StringIO(data)\n            yield file\n\n        elif mode == \"rb\":\n            data = self.read_file(path)\n            file = io.BytesIO(data)\n            yield file\n\n    def get_file_metadata(self, path: str) -&gt; dict:\n        \"\"\"\n        Retrieve comprehensive file metadata.\n\n        :param path: File path in blob storage\n        :return: File metadata dictionary\n        \"\"\"\n        blob_client = self.container_client.get_blob_client(path)\n        properties = blob_client.get_blob_properties()\n\n        return {\n            \"name\": path,\n            \"size_bytes\": properties.size,\n            \"content_type\": properties.content_settings.content_type,\n            \"last_modified\": properties.last_modified,\n            \"etag\": properties.etag,\n        }\n\n    def is_file(self, path: str) -&gt; bool:\n        return self.file_exists(path)\n\n    def is_dir(self, path: str) -&gt; bool:\n        dir_path = path.rstrip(\"/\") + \"/\"\n\n        existing_blobs = self.list_files(dir_path)\n\n        if len(existing_blobs) &gt; 1:\n            return True\n        elif len(existing_blobs) == 1:\n            if existing_blobs[0] != path.rstrip(\"/\"):\n                return True\n\n        return False\n\n    def rmdir(self, dir: str) -&gt; None:\n        # Normalize directory path to ensure it targets all children\n        dir_path = dir.rstrip(\"/\") + \"/\"\n\n        # Azure Blob batch delete has a hard limit on number of sub-requests\n        # per batch (currently 256). Delete in chunks to avoid\n        # ExceedsMaxBatchRequestCount errors.\n        blobs = list(self.list_files(dir_path))\n        if not blobs:\n            return\n\n        BATCH_LIMIT = 256\n        for start_idx in range(0, len(blobs), BATCH_LIMIT):\n            batch = blobs[start_idx : start_idx + BATCH_LIMIT]\n            self.container_client.delete_blobs(*batch)\n\n    def mkdir(self, path: str, exist_ok: bool = False) -&gt; None:\n        \"\"\"\n        Create a directory in Azure Blob Storage.\n\n        In ADLS, directories are conceptual and created by adding a placeholder blob.\n\n        :param path: Path of the directory to create\n        :param exist_ok: If False, raise an error if the directory already exists\n        \"\"\"\n        dir_path = path.rstrip(\"/\") + \"/\"\n\n        existing_blobs = list(self.list_files(dir_path))\n\n        if existing_blobs and not exist_ok:\n            raise FileExistsError(f\"Directory {path} already exists\")\n\n        # Create a placeholder blob to represent the directory\n        placeholder_blob_path = os.path.join(dir_path, \".placeholder\")\n\n        # Only create placeholder if it doesn't already exist\n        if not self.file_exists(placeholder_blob_path):\n            placeholder_content = (\n                b\"This is a placeholder blob to represent a directory.\"\n            )\n            blob_client = self.blob_service_client.get_blob_client(\n                container=self.container, blob=placeholder_blob_path\n            )\n            blob_client.upload_blob(placeholder_content, overwrite=True)\n\n    def remove(self, path: str) -&gt; None:\n        blob_client = self.blob_service_client.get_blob_client(\n            container=self.container, blob=path, snapshot=None\n        )\n        if blob_client.exists():\n            blob_client.delete_blob()\n\n    def rename(\n        self,\n        source_path: str,\n        destination_path: str,\n        overwrite: bool = False,\n        delete_source: bool = True,\n        wait: bool = True,\n        timeout_seconds: int = 300,\n        poll_interval_seconds: int = 1,\n    ) -&gt; None:\n        \"\"\"\n        Rename (move) a single file by copying to the new path and deleting the source.\n\n        :param source_path: Existing blob path\n        :param destination_path: Target blob path\n        :param overwrite: Overwrite destination if it already exists\n        :param delete_source: Delete original after successful copy\n        :param wait: Wait for the copy operation to complete\n        :param timeout_seconds: Max time to wait for copy to succeed\n        :param poll_interval_seconds: Polling interval while waiting\n        \"\"\"\n\n        if not self.file_exists(source_path):\n            raise FileNotFoundError(f\"Source file not found: {source_path}\")\n\n        if self.file_exists(destination_path) and not overwrite:\n            raise FileExistsError(\n                f\"Destination already exists and overwrite is False: {destination_path}\"\n            )\n\n        # Use copy_file method to copy the file\n        self.copy_file(source_path, destination_path, overwrite=overwrite)\n\n        if wait:\n            # Wait for copy to complete if requested\n            dest_client = self.container_client.get_blob_client(destination_path)\n            deadline = time.time() + timeout_seconds\n            while True:\n                props = dest_client.get_blob_properties()\n                status = getattr(props.copy, \"status\", None)\n                if status == \"success\":\n                    break\n                if status in {\"aborted\", \"failed\"}:\n                    raise IOError(\n                        f\"Copy failed with status {status} from {source_path} to {destination_path}\"\n                    )\n                if time.time() &gt; deadline:\n                    raise TimeoutError(\n                        f\"Timed out waiting for copy to complete for {destination_path}\"\n                    )\n                time.sleep(poll_interval_seconds)\n\n        if delete_source:\n            self.remove(source_path)\n</code></pre>"},{"location":"api/core/#gigaspatial.core.io.adls_data_store.ADLSDataStore.__init__","title":"<code>__init__(container=config.ADLS_CONTAINER_NAME, connection_string=config.ADLS_CONNECTION_STRING, account_url=config.ADLS_ACCOUNT_URL, sas_token=config.ADLS_SAS_TOKEN)</code>","text":"<p>Create a new instance of ADLSDataStore :param container: The name of the container in ADLS to interact with.</p> Source code in <code>gigaspatial/core/io/adls_data_store.py</code> <pre><code>def __init__(\n    self,\n    container: str = config.ADLS_CONTAINER_NAME,\n    connection_string: str = config.ADLS_CONNECTION_STRING,\n    account_url: str = config.ADLS_ACCOUNT_URL,\n    sas_token: str = config.ADLS_SAS_TOKEN,\n):\n    \"\"\"\n    Create a new instance of ADLSDataStore\n    :param container: The name of the container in ADLS to interact with.\n    \"\"\"\n    if connection_string:\n        self.blob_service_client = BlobServiceClient.from_connection_string(\n            connection_string\n        )\n    elif account_url and sas_token:\n        self.blob_service_client = BlobServiceClient(\n            account_url=account_url, credential=sas_token\n        )\n    else:\n        raise ValueError(\n            \"Either connection_string or account_url and sas_token must be provided.\"\n        )\n\n    self.container_client = self.blob_service_client.get_container_client(\n        container=container\n    )\n    self.container = container\n</code></pre>"},{"location":"api/core/#gigaspatial.core.io.adls_data_store.ADLSDataStore.copy_directory","title":"<code>copy_directory(source_dir, destination_dir)</code>","text":"<p>Copies all files from a source directory to a destination directory within the same container.</p> <p>:param source_dir: The source directory path in the blob storage :param destination_dir: The destination directory path in the blob storage</p> Source code in <code>gigaspatial/core/io/adls_data_store.py</code> <pre><code>def copy_directory(self, source_dir: str, destination_dir: str):\n    \"\"\"\n    Copies all files from a source directory to a destination directory within the same container.\n\n    :param source_dir: The source directory path in the blob storage\n    :param destination_dir: The destination directory path in the blob storage\n    \"\"\"\n    try:\n        # Ensure source directory path ends with a trailing slash\n        source_dir = source_dir.rstrip(\"/\") + \"/\"\n        destination_dir = destination_dir.rstrip(\"/\") + \"/\"\n\n        # List all blobs in the source directory\n        source_blobs = self.container_client.list_blobs(name_starts_with=source_dir)\n\n        for blob in source_blobs:\n            # Get the relative path of the blob\n            relative_path = os.path.relpath(blob.name, source_dir)\n\n            # Construct the new blob path\n            new_blob_path = os.path.join(destination_dir, relative_path).replace(\n                \"\\\\\", \"/\"\n            )\n\n            # Use copy_file method to copy each file\n            self.copy_file(blob.name, new_blob_path, overwrite=True)\n\n        print(f\"Copied directory from {source_dir} to {destination_dir}\")\n    except Exception as e:\n        print(f\"Failed to copy directory {source_dir}: {e}\")\n</code></pre>"},{"location":"api/core/#gigaspatial.core.io.adls_data_store.ADLSDataStore.copy_file","title":"<code>copy_file(source_path, destination_path, overwrite=False)</code>","text":"<p>Copies a single file from source to destination within the same container.</p> <p>:param source_path: The source file path in the blob storage :param destination_path: The destination file path in the blob storage :param overwrite: If True, overwrite the destination file if it already exists</p> Source code in <code>gigaspatial/core/io/adls_data_store.py</code> <pre><code>def copy_file(\n    self, source_path: str, destination_path: str, overwrite: bool = False\n):\n    \"\"\"\n    Copies a single file from source to destination within the same container.\n\n    :param source_path: The source file path in the blob storage\n    :param destination_path: The destination file path in the blob storage\n    :param overwrite: If True, overwrite the destination file if it already exists\n    \"\"\"\n    try:\n        if not self.file_exists(source_path):\n            raise FileNotFoundError(f\"Source file not found: {source_path}\")\n\n        if self.file_exists(destination_path) and not overwrite:\n            raise FileExistsError(\n                f\"Destination file already exists and overwrite is False: {destination_path}\"\n            )\n\n        # Create source and destination blob clients\n        source_blob_client = self.container_client.get_blob_client(source_path)\n        destination_blob_client = self.container_client.get_blob_client(\n            destination_path\n        )\n\n        # Start the server-side copy operation\n        destination_blob_client.start_copy_from_url(source_blob_client.url)\n\n        print(f\"Copied file from {source_path} to {destination_path}\")\n    except Exception as e:\n        print(f\"Failed to copy file {source_path}: {e}\")\n        raise\n</code></pre>"},{"location":"api/core/#gigaspatial.core.io.adls_data_store.ADLSDataStore.download_directory","title":"<code>download_directory(blob_dir_path, local_dir_path)</code>","text":"<p>Downloads all files from a directory in Azure Blob Storage to a local directory.</p> Source code in <code>gigaspatial/core/io/adls_data_store.py</code> <pre><code>def download_directory(self, blob_dir_path: str, local_dir_path: str):\n    \"\"\"Downloads all files from a directory in Azure Blob Storage to a local directory.\"\"\"\n    try:\n        # Ensure the local directory exists\n        os.makedirs(local_dir_path, exist_ok=True)\n\n        # List all files in the blob directory\n        blob_items = self.container_client.list_blobs(\n            name_starts_with=blob_dir_path\n        )\n\n        for blob_item in blob_items:\n            # Get the relative path of the blob file\n            relative_path = os.path.relpath(blob_item.name, blob_dir_path)\n            # Construct the local file path\n            local_file_path = os.path.join(local_dir_path, relative_path)\n            # Create directories if needed\n            os.makedirs(os.path.dirname(local_file_path), exist_ok=True)\n\n            # Download the blob to the local file\n            blob_client = self.container_client.get_blob_client(blob_item.name)\n            with open(local_file_path, \"wb\") as file:\n                file.write(blob_client.download_blob().readall())\n\n        print(f\"Downloaded directory {blob_dir_path} to {local_dir_path}\")\n    except Exception as e:\n        print(f\"Failed to download directory {blob_dir_path}: {e}\")\n</code></pre>"},{"location":"api/core/#gigaspatial.core.io.adls_data_store.ADLSDataStore.get_file_metadata","title":"<code>get_file_metadata(path)</code>","text":"<p>Retrieve comprehensive file metadata.</p> <p>:param path: File path in blob storage :return: File metadata dictionary</p> Source code in <code>gigaspatial/core/io/adls_data_store.py</code> <pre><code>def get_file_metadata(self, path: str) -&gt; dict:\n    \"\"\"\n    Retrieve comprehensive file metadata.\n\n    :param path: File path in blob storage\n    :return: File metadata dictionary\n    \"\"\"\n    blob_client = self.container_client.get_blob_client(path)\n    properties = blob_client.get_blob_properties()\n\n    return {\n        \"name\": path,\n        \"size_bytes\": properties.size,\n        \"content_type\": properties.content_settings.content_type,\n        \"last_modified\": properties.last_modified,\n        \"etag\": properties.etag,\n    }\n</code></pre>"},{"location":"api/core/#gigaspatial.core.io.adls_data_store.ADLSDataStore.list_directories","title":"<code>list_directories(path)</code>","text":"<p>List only directory names (not files) from a given path in ADLS.</p> Source code in <code>gigaspatial/core/io/adls_data_store.py</code> <pre><code>def list_directories(self, path: str) -&gt; list:\n    \"\"\"List only directory names (not files) from a given path in ADLS.\"\"\"\n    search_path = path.rstrip(\"/\") + \"/\" if path else \"\"\n\n    blob_items = self.container_client.list_blobs(name_starts_with=search_path)\n\n    directories = set()\n\n    for blob_item in blob_items:\n        # Get the relative path from the search path\n        relative_path = blob_item.name[len(search_path) :]\n\n        # Skip if it's empty (shouldn't happen but just in case)\n        if not relative_path:\n            continue\n\n        # If there's a \"/\" in the relative path, it means there's a subdirectory\n        if \"/\" in relative_path:\n            # Get the first directory name\n            dir_name = relative_path.split(\"/\")[0]\n            directories.add(dir_name)\n\n    return sorted(list(directories))\n</code></pre>"},{"location":"api/core/#gigaspatial.core.io.adls_data_store.ADLSDataStore.mkdir","title":"<code>mkdir(path, exist_ok=False)</code>","text":"<p>Create a directory in Azure Blob Storage.</p> <p>In ADLS, directories are conceptual and created by adding a placeholder blob.</p> <p>:param path: Path of the directory to create :param exist_ok: If False, raise an error if the directory already exists</p> Source code in <code>gigaspatial/core/io/adls_data_store.py</code> <pre><code>def mkdir(self, path: str, exist_ok: bool = False) -&gt; None:\n    \"\"\"\n    Create a directory in Azure Blob Storage.\n\n    In ADLS, directories are conceptual and created by adding a placeholder blob.\n\n    :param path: Path of the directory to create\n    :param exist_ok: If False, raise an error if the directory already exists\n    \"\"\"\n    dir_path = path.rstrip(\"/\") + \"/\"\n\n    existing_blobs = list(self.list_files(dir_path))\n\n    if existing_blobs and not exist_ok:\n        raise FileExistsError(f\"Directory {path} already exists\")\n\n    # Create a placeholder blob to represent the directory\n    placeholder_blob_path = os.path.join(dir_path, \".placeholder\")\n\n    # Only create placeholder if it doesn't already exist\n    if not self.file_exists(placeholder_blob_path):\n        placeholder_content = (\n            b\"This is a placeholder blob to represent a directory.\"\n        )\n        blob_client = self.blob_service_client.get_blob_client(\n            container=self.container, blob=placeholder_blob_path\n        )\n        blob_client.upload_blob(placeholder_content, overwrite=True)\n</code></pre>"},{"location":"api/core/#gigaspatial.core.io.adls_data_store.ADLSDataStore.open","title":"<code>open(path, mode='r')</code>","text":"<p>Context manager for file operations with enhanced mode support.</p> <p>:param path: File path in blob storage :param mode: File open mode (r, rb, w, wb)</p> Source code in <code>gigaspatial/core/io/adls_data_store.py</code> <pre><code>@contextlib.contextmanager\ndef open(self, path: str, mode: str = \"r\"):\n    \"\"\"\n    Context manager for file operations with enhanced mode support.\n\n    :param path: File path in blob storage\n    :param mode: File open mode (r, rb, w, wb)\n    \"\"\"\n    if mode == \"w\":\n        file = io.StringIO()\n        yield file\n        self.write_file(path, file.getvalue())\n\n    elif mode == \"wb\":\n        file = io.BytesIO()\n        yield file\n        self.write_file(path, file.getvalue())\n\n    elif mode == \"r\":\n        data = self.read_file(path, encoding=\"UTF-8\")\n        file = io.StringIO(data)\n        yield file\n\n    elif mode == \"rb\":\n        data = self.read_file(path)\n        file = io.BytesIO(data)\n        yield file\n</code></pre>"},{"location":"api/core/#gigaspatial.core.io.adls_data_store.ADLSDataStore.read_file","title":"<code>read_file(path, encoding=None)</code>","text":"<p>Read file with flexible encoding support.</p> <p>:param path: Path to the file in blob storage :param encoding: File encoding (optional) :return: File contents as string or bytes</p> Source code in <code>gigaspatial/core/io/adls_data_store.py</code> <pre><code>def read_file(self, path: str, encoding: Optional[str] = None) -&gt; Union[str, bytes]:\n    \"\"\"\n    Read file with flexible encoding support.\n\n    :param path: Path to the file in blob storage\n    :param encoding: File encoding (optional)\n    :return: File contents as string or bytes\n    \"\"\"\n    try:\n        blob_client = self.container_client.get_blob_client(path)\n        blob_data = blob_client.download_blob().readall()\n\n        # If no encoding specified, return raw bytes\n        if encoding is None:\n            return blob_data\n\n        # If encoding is specified, decode the bytes\n        return blob_data.decode(encoding)\n\n    except Exception as e:\n        raise IOError(f\"Error reading file {path}: {e}\")\n</code></pre>"},{"location":"api/core/#gigaspatial.core.io.adls_data_store.ADLSDataStore.rename","title":"<code>rename(source_path, destination_path, overwrite=False, delete_source=True, wait=True, timeout_seconds=300, poll_interval_seconds=1)</code>","text":"<p>Rename (move) a single file by copying to the new path and deleting the source.</p> <p>:param source_path: Existing blob path :param destination_path: Target blob path :param overwrite: Overwrite destination if it already exists :param delete_source: Delete original after successful copy :param wait: Wait for the copy operation to complete :param timeout_seconds: Max time to wait for copy to succeed :param poll_interval_seconds: Polling interval while waiting</p> Source code in <code>gigaspatial/core/io/adls_data_store.py</code> <pre><code>def rename(\n    self,\n    source_path: str,\n    destination_path: str,\n    overwrite: bool = False,\n    delete_source: bool = True,\n    wait: bool = True,\n    timeout_seconds: int = 300,\n    poll_interval_seconds: int = 1,\n) -&gt; None:\n    \"\"\"\n    Rename (move) a single file by copying to the new path and deleting the source.\n\n    :param source_path: Existing blob path\n    :param destination_path: Target blob path\n    :param overwrite: Overwrite destination if it already exists\n    :param delete_source: Delete original after successful copy\n    :param wait: Wait for the copy operation to complete\n    :param timeout_seconds: Max time to wait for copy to succeed\n    :param poll_interval_seconds: Polling interval while waiting\n    \"\"\"\n\n    if not self.file_exists(source_path):\n        raise FileNotFoundError(f\"Source file not found: {source_path}\")\n\n    if self.file_exists(destination_path) and not overwrite:\n        raise FileExistsError(\n            f\"Destination already exists and overwrite is False: {destination_path}\"\n        )\n\n    # Use copy_file method to copy the file\n    self.copy_file(source_path, destination_path, overwrite=overwrite)\n\n    if wait:\n        # Wait for copy to complete if requested\n        dest_client = self.container_client.get_blob_client(destination_path)\n        deadline = time.time() + timeout_seconds\n        while True:\n            props = dest_client.get_blob_properties()\n            status = getattr(props.copy, \"status\", None)\n            if status == \"success\":\n                break\n            if status in {\"aborted\", \"failed\"}:\n                raise IOError(\n                    f\"Copy failed with status {status} from {source_path} to {destination_path}\"\n                )\n            if time.time() &gt; deadline:\n                raise TimeoutError(\n                    f\"Timed out waiting for copy to complete for {destination_path}\"\n                )\n            time.sleep(poll_interval_seconds)\n\n    if delete_source:\n        self.remove(source_path)\n</code></pre>"},{"location":"api/core/#gigaspatial.core.io.adls_data_store.ADLSDataStore.upload_directory","title":"<code>upload_directory(dir_path, blob_dir_path)</code>","text":"<p>Uploads all files from a directory to Azure Blob Storage.</p> Source code in <code>gigaspatial/core/io/adls_data_store.py</code> <pre><code>def upload_directory(self, dir_path, blob_dir_path):\n    \"\"\"Uploads all files from a directory to Azure Blob Storage.\"\"\"\n    for root, dirs, files in os.walk(dir_path):\n        for file in files:\n            local_file_path = os.path.join(root, file)\n            relative_path = os.path.relpath(local_file_path, dir_path)\n            blob_file_path = os.path.join(blob_dir_path, relative_path).replace(\n                \"\\\\\", \"/\"\n            )\n\n            self.upload_file(local_file_path, blob_file_path)\n</code></pre>"},{"location":"api/core/#gigaspatial.core.io.adls_data_store.ADLSDataStore.upload_file","title":"<code>upload_file(file_path, blob_path)</code>","text":"<p>Uploads a single file to Azure Blob Storage.</p> Source code in <code>gigaspatial/core/io/adls_data_store.py</code> <pre><code>def upload_file(self, file_path, blob_path):\n    \"\"\"Uploads a single file to Azure Blob Storage.\"\"\"\n    try:\n        blob_client = self.container_client.get_blob_client(blob_path)\n        with open(file_path, \"rb\") as data:\n            blob_client.upload_blob(data, overwrite=True)\n        print(f\"Uploaded {file_path} to {blob_path}\")\n    except Exception as e:\n        print(f\"Failed to upload {file_path}: {e}\")\n</code></pre>"},{"location":"api/core/#gigaspatial.core.io.adls_data_store.ADLSDataStore.write_file","title":"<code>write_file(path, data)</code>","text":"<p>Write file with support for content type and improved type handling.</p> <p>:param path: Destination path in blob storage :param data: File contents</p> Source code in <code>gigaspatial/core/io/adls_data_store.py</code> <pre><code>def write_file(self, path: str, data) -&gt; None:\n    \"\"\"\n    Write file with support for content type and improved type handling.\n\n    :param path: Destination path in blob storage\n    :param data: File contents\n    \"\"\"\n    blob_client = self.blob_service_client.get_blob_client(\n        container=self.container, blob=path, snapshot=None\n    )\n\n    if isinstance(data, str):\n        binary_data = data.encode()\n    elif isinstance(data, bytes):\n        binary_data = data\n    else:\n        raise Exception(f'Unsupported data type. Only \"bytes\" or \"string\" accepted')\n\n    blob_client.upload_blob(binary_data, overwrite=True)\n</code></pre>"},{"location":"api/core/#gigaspatial.core.io.data_api","title":"<code>data_api</code>","text":""},{"location":"api/core/#gigaspatial.core.io.data_api.GigaDataAPI","title":"<code>GigaDataAPI</code>","text":"Source code in <code>gigaspatial/core/io/data_api.py</code> <pre><code>class GigaDataAPI:\n\n    def __init__(\n        self,\n        profile_file: Union[str, Path] = config.API_PROFILE_FILE_PATH,\n        share_name: str = config.API_SHARE_NAME,\n        schema_name: str = config.API_SCHEMA_NAME,\n    ):\n        \"\"\"\n        Initialize the GigaDataAPI class with the profile file, share name, and schema name.\n\n        profile_file: Path to the delta-sharing profile file.\n        share_name: Name of the share (e.g., \"gold\").\n        schema_name: Name of the schema (e.g., \"school-master\").\n        \"\"\"\n        self.profile_file = profile_file\n        self.share_name = share_name\n        self.schema_name = schema_name\n        self.client = delta_sharing.SharingClient(profile_file)\n\n        self._cache = {}\n\n    def get_country_list(self, sort=True):\n        \"\"\"\n        Retrieve a list of available countries in the dataset.\n\n        :param sort: Whether to sort the country list alphabetically (default is True).\n        \"\"\"\n        country_list = [\n            t.name\n            for t in self.client.list_all_tables()\n            if t.schema == self.schema_name\n        ]\n        if sort:\n            country_list.sort()\n        return country_list\n\n    def load_country_data(self, country, filters=None, use_cache=True):\n        \"\"\"\n        Load the dataset for the specified country with optional filtering and caching.\n\n        country: The country code (e.g., \"MWI\").\n        filters: A dictionary with column names as keys and filter values as values.\n        use_cache: Whether to use cached data if available (default is True).\n        \"\"\"\n        # Check if data is cached\n        if use_cache and country in self._cache:\n            df_country = self._cache[country]\n        else:\n            # Load data from the API\n            table_url = (\n                f\"{self.profile_file}#{self.share_name}.{self.schema_name}.{country}\"\n            )\n            df_country = delta_sharing.load_as_pandas(table_url)\n            self._cache[country] = df_country  # Cache the data\n\n        # Apply filters if provided\n        if filters:\n            for column, value in filters.items():\n                df_country = df_country[df_country[column] == value]\n\n        return df_country\n\n    def load_multiple_countries(self, countries):\n        \"\"\"\n        Load data for multiple countries and combine them into a single DataFrame.\n\n        countries: A list of country codes.\n        \"\"\"\n        df_list = []\n        for country in countries:\n            df_list.append(self.load_country_data(country))\n        return pd.concat(df_list, ignore_index=True)\n\n    def get_country_metadata(self, country):\n        \"\"\"\n        Retrieve metadata (e.g., column names and data types) for a country's dataset.\n\n        country: The country code (e.g., \"MWI\").\n        \"\"\"\n        df_country = self.load_country_data(country)\n        metadata = {\n            \"columns\": df_country.columns.tolist(),\n            \"data_types\": df_country.dtypes.to_dict(),\n            \"num_records\": len(df_country),\n        }\n        return metadata\n\n    def get_all_cached_data_as_dict(self):\n        \"\"\"\n        Retrieve all cached data in a dictionary format, where each key is a country code,\n        and the value is the DataFrame of that country.\n        \"\"\"\n        return self._cache if self._cache else {}\n\n    def get_all_cached_data_as_json(self):\n        \"\"\"\n        Retrieve all cached data in a JSON-like format. Each country is represented as a key,\n        and the value is a list of records (i.e., the DataFrame's `to_dict(orient='records')` format).\n        \"\"\"\n        if not self._cache:\n            return {}\n\n        # Convert each DataFrame in the cache to a JSON-like format (list of records)\n        return {\n            country: df.to_dict(orient=\"records\") for country, df in self._cache.items()\n        }\n</code></pre>"},{"location":"api/core/#gigaspatial.core.io.data_api.GigaDataAPI.__init__","title":"<code>__init__(profile_file=config.API_PROFILE_FILE_PATH, share_name=config.API_SHARE_NAME, schema_name=config.API_SCHEMA_NAME)</code>","text":"<p>Initialize the GigaDataAPI class with the profile file, share name, and schema name.</p> <p>profile_file: Path to the delta-sharing profile file. share_name: Name of the share (e.g., \"gold\"). schema_name: Name of the schema (e.g., \"school-master\").</p> Source code in <code>gigaspatial/core/io/data_api.py</code> <pre><code>def __init__(\n    self,\n    profile_file: Union[str, Path] = config.API_PROFILE_FILE_PATH,\n    share_name: str = config.API_SHARE_NAME,\n    schema_name: str = config.API_SCHEMA_NAME,\n):\n    \"\"\"\n    Initialize the GigaDataAPI class with the profile file, share name, and schema name.\n\n    profile_file: Path to the delta-sharing profile file.\n    share_name: Name of the share (e.g., \"gold\").\n    schema_name: Name of the schema (e.g., \"school-master\").\n    \"\"\"\n    self.profile_file = profile_file\n    self.share_name = share_name\n    self.schema_name = schema_name\n    self.client = delta_sharing.SharingClient(profile_file)\n\n    self._cache = {}\n</code></pre>"},{"location":"api/core/#gigaspatial.core.io.data_api.GigaDataAPI.get_all_cached_data_as_dict","title":"<code>get_all_cached_data_as_dict()</code>","text":"<p>Retrieve all cached data in a dictionary format, where each key is a country code, and the value is the DataFrame of that country.</p> Source code in <code>gigaspatial/core/io/data_api.py</code> <pre><code>def get_all_cached_data_as_dict(self):\n    \"\"\"\n    Retrieve all cached data in a dictionary format, where each key is a country code,\n    and the value is the DataFrame of that country.\n    \"\"\"\n    return self._cache if self._cache else {}\n</code></pre>"},{"location":"api/core/#gigaspatial.core.io.data_api.GigaDataAPI.get_all_cached_data_as_json","title":"<code>get_all_cached_data_as_json()</code>","text":"<p>Retrieve all cached data in a JSON-like format. Each country is represented as a key, and the value is a list of records (i.e., the DataFrame's <code>to_dict(orient='records')</code> format).</p> Source code in <code>gigaspatial/core/io/data_api.py</code> <pre><code>def get_all_cached_data_as_json(self):\n    \"\"\"\n    Retrieve all cached data in a JSON-like format. Each country is represented as a key,\n    and the value is a list of records (i.e., the DataFrame's `to_dict(orient='records')` format).\n    \"\"\"\n    if not self._cache:\n        return {}\n\n    # Convert each DataFrame in the cache to a JSON-like format (list of records)\n    return {\n        country: df.to_dict(orient=\"records\") for country, df in self._cache.items()\n    }\n</code></pre>"},{"location":"api/core/#gigaspatial.core.io.data_api.GigaDataAPI.get_country_list","title":"<code>get_country_list(sort=True)</code>","text":"<p>Retrieve a list of available countries in the dataset.</p> <p>:param sort: Whether to sort the country list alphabetically (default is True).</p> Source code in <code>gigaspatial/core/io/data_api.py</code> <pre><code>def get_country_list(self, sort=True):\n    \"\"\"\n    Retrieve a list of available countries in the dataset.\n\n    :param sort: Whether to sort the country list alphabetically (default is True).\n    \"\"\"\n    country_list = [\n        t.name\n        for t in self.client.list_all_tables()\n        if t.schema == self.schema_name\n    ]\n    if sort:\n        country_list.sort()\n    return country_list\n</code></pre>"},{"location":"api/core/#gigaspatial.core.io.data_api.GigaDataAPI.get_country_metadata","title":"<code>get_country_metadata(country)</code>","text":"<p>Retrieve metadata (e.g., column names and data types) for a country's dataset.</p> <p>country: The country code (e.g., \"MWI\").</p> Source code in <code>gigaspatial/core/io/data_api.py</code> <pre><code>def get_country_metadata(self, country):\n    \"\"\"\n    Retrieve metadata (e.g., column names and data types) for a country's dataset.\n\n    country: The country code (e.g., \"MWI\").\n    \"\"\"\n    df_country = self.load_country_data(country)\n    metadata = {\n        \"columns\": df_country.columns.tolist(),\n        \"data_types\": df_country.dtypes.to_dict(),\n        \"num_records\": len(df_country),\n    }\n    return metadata\n</code></pre>"},{"location":"api/core/#gigaspatial.core.io.data_api.GigaDataAPI.load_country_data","title":"<code>load_country_data(country, filters=None, use_cache=True)</code>","text":"<p>Load the dataset for the specified country with optional filtering and caching.</p> <p>country: The country code (e.g., \"MWI\"). filters: A dictionary with column names as keys and filter values as values. use_cache: Whether to use cached data if available (default is True).</p> Source code in <code>gigaspatial/core/io/data_api.py</code> <pre><code>def load_country_data(self, country, filters=None, use_cache=True):\n    \"\"\"\n    Load the dataset for the specified country with optional filtering and caching.\n\n    country: The country code (e.g., \"MWI\").\n    filters: A dictionary with column names as keys and filter values as values.\n    use_cache: Whether to use cached data if available (default is True).\n    \"\"\"\n    # Check if data is cached\n    if use_cache and country in self._cache:\n        df_country = self._cache[country]\n    else:\n        # Load data from the API\n        table_url = (\n            f\"{self.profile_file}#{self.share_name}.{self.schema_name}.{country}\"\n        )\n        df_country = delta_sharing.load_as_pandas(table_url)\n        self._cache[country] = df_country  # Cache the data\n\n    # Apply filters if provided\n    if filters:\n        for column, value in filters.items():\n            df_country = df_country[df_country[column] == value]\n\n    return df_country\n</code></pre>"},{"location":"api/core/#gigaspatial.core.io.data_api.GigaDataAPI.load_multiple_countries","title":"<code>load_multiple_countries(countries)</code>","text":"<p>Load data for multiple countries and combine them into a single DataFrame.</p> <p>countries: A list of country codes.</p> Source code in <code>gigaspatial/core/io/data_api.py</code> <pre><code>def load_multiple_countries(self, countries):\n    \"\"\"\n    Load data for multiple countries and combine them into a single DataFrame.\n\n    countries: A list of country codes.\n    \"\"\"\n    df_list = []\n    for country in countries:\n        df_list.append(self.load_country_data(country))\n    return pd.concat(df_list, ignore_index=True)\n</code></pre>"},{"location":"api/core/#gigaspatial.core.io.data_store","title":"<code>data_store</code>","text":""},{"location":"api/core/#gigaspatial.core.io.data_store.DataStore","title":"<code>DataStore</code>","text":"<p>               Bases: <code>ABC</code></p> <p>Abstract base class defining the interface for data store implementations. This class serves as a parent for both local and cloud-based storage solutions.</p> Source code in <code>gigaspatial/core/io/data_store.py</code> <pre><code>class DataStore(ABC):\n    \"\"\"\n    Abstract base class defining the interface for data store implementations.\n    This class serves as a parent for both local and cloud-based storage solutions.\n    \"\"\"\n\n    @abstractmethod\n    def read_file(self, path: str) -&gt; Any:\n        \"\"\"\n        Read contents of a file from the data store.\n\n        Args:\n            path: Path to the file to read\n\n        Returns:\n            Contents of the file\n\n        Raises:\n            IOError: If file cannot be read\n        \"\"\"\n        pass\n\n    @abstractmethod\n    def write_file(self, path: str, data: Any) -&gt; None:\n        \"\"\"\n        Write data to a file in the data store.\n\n        Args:\n            path: Path where to write the file\n            data: Data to write to the file\n\n        Raises:\n            IOError: If file cannot be written\n        \"\"\"\n        pass\n\n    @abstractmethod\n    def file_exists(self, path: str) -&gt; bool:\n        \"\"\"\n        Check if a file exists in the data store.\n\n        Args:\n            path: Path to check\n\n        Returns:\n            True if file exists, False otherwise\n        \"\"\"\n        pass\n\n    @abstractmethod\n    def list_files(self, path: str) -&gt; List[str]:\n        \"\"\"\n        List all files in a directory.\n\n        Args:\n            path: Directory path to list\n\n        Returns:\n            List of file paths in the directory\n        \"\"\"\n        pass\n\n    @abstractmethod\n    def walk(self, top: str) -&gt; Generator:\n        \"\"\"\n        Walk through directory tree, similar to os.walk().\n\n        Args:\n            top: Starting directory for the walk\n\n        Returns:\n            Generator yielding tuples of (dirpath, dirnames, filenames)\n        \"\"\"\n        pass\n\n    @abstractmethod\n    def open(self, file: str, mode: str = \"r\") -&gt; Union[str, bytes]:\n        \"\"\"\n        Context manager for file operations.\n\n        Args:\n            file: Path to the file\n            mode: File mode ('r', 'w', 'rb', 'wb')\n\n        Yields:\n            File-like object\n\n        Raises:\n            IOError: If file cannot be opened\n        \"\"\"\n        pass\n\n    @abstractmethod\n    def is_file(self, path: str) -&gt; bool:\n        \"\"\"\n        Check if path points to a file.\n\n        Args:\n            path: Path to check\n\n        Returns:\n            True if path is a file, False otherwise\n        \"\"\"\n        pass\n\n    @abstractmethod\n    def is_dir(self, path: str) -&gt; bool:\n        \"\"\"\n        Check if path points to a directory.\n\n        Args:\n            path: Path to check\n\n        Returns:\n            True if path is a directory, False otherwise\n        \"\"\"\n        pass\n\n    @abstractmethod\n    def remove(self, path: str) -&gt; None:\n        \"\"\"\n        Remove a file.\n\n        Args:\n            path: Path to the file to remove\n\n        Raises:\n            IOError: If file cannot be removed\n        \"\"\"\n        pass\n\n    @abstractmethod\n    def rmdir(self, dir: str) -&gt; None:\n        \"\"\"\n        Remove a directory and all its contents.\n\n        Args:\n            dir: Path to the directory to remove\n\n        Raises:\n            IOError: If directory cannot be removed\n        \"\"\"\n        pass\n</code></pre>"},{"location":"api/core/#gigaspatial.core.io.data_store.DataStore.file_exists","title":"<code>file_exists(path)</code>  <code>abstractmethod</code>","text":"<p>Check if a file exists in the data store.</p> <p>Parameters:</p> Name Type Description Default <code>path</code> <code>str</code> <p>Path to check</p> required <p>Returns:</p> Type Description <code>bool</code> <p>True if file exists, False otherwise</p> Source code in <code>gigaspatial/core/io/data_store.py</code> <pre><code>@abstractmethod\ndef file_exists(self, path: str) -&gt; bool:\n    \"\"\"\n    Check if a file exists in the data store.\n\n    Args:\n        path: Path to check\n\n    Returns:\n        True if file exists, False otherwise\n    \"\"\"\n    pass\n</code></pre>"},{"location":"api/core/#gigaspatial.core.io.data_store.DataStore.is_dir","title":"<code>is_dir(path)</code>  <code>abstractmethod</code>","text":"<p>Check if path points to a directory.</p> <p>Parameters:</p> Name Type Description Default <code>path</code> <code>str</code> <p>Path to check</p> required <p>Returns:</p> Type Description <code>bool</code> <p>True if path is a directory, False otherwise</p> Source code in <code>gigaspatial/core/io/data_store.py</code> <pre><code>@abstractmethod\ndef is_dir(self, path: str) -&gt; bool:\n    \"\"\"\n    Check if path points to a directory.\n\n    Args:\n        path: Path to check\n\n    Returns:\n        True if path is a directory, False otherwise\n    \"\"\"\n    pass\n</code></pre>"},{"location":"api/core/#gigaspatial.core.io.data_store.DataStore.is_file","title":"<code>is_file(path)</code>  <code>abstractmethod</code>","text":"<p>Check if path points to a file.</p> <p>Parameters:</p> Name Type Description Default <code>path</code> <code>str</code> <p>Path to check</p> required <p>Returns:</p> Type Description <code>bool</code> <p>True if path is a file, False otherwise</p> Source code in <code>gigaspatial/core/io/data_store.py</code> <pre><code>@abstractmethod\ndef is_file(self, path: str) -&gt; bool:\n    \"\"\"\n    Check if path points to a file.\n\n    Args:\n        path: Path to check\n\n    Returns:\n        True if path is a file, False otherwise\n    \"\"\"\n    pass\n</code></pre>"},{"location":"api/core/#gigaspatial.core.io.data_store.DataStore.list_files","title":"<code>list_files(path)</code>  <code>abstractmethod</code>","text":"<p>List all files in a directory.</p> <p>Parameters:</p> Name Type Description Default <code>path</code> <code>str</code> <p>Directory path to list</p> required <p>Returns:</p> Type Description <code>List[str]</code> <p>List of file paths in the directory</p> Source code in <code>gigaspatial/core/io/data_store.py</code> <pre><code>@abstractmethod\ndef list_files(self, path: str) -&gt; List[str]:\n    \"\"\"\n    List all files in a directory.\n\n    Args:\n        path: Directory path to list\n\n    Returns:\n        List of file paths in the directory\n    \"\"\"\n    pass\n</code></pre>"},{"location":"api/core/#gigaspatial.core.io.data_store.DataStore.open","title":"<code>open(file, mode='r')</code>  <code>abstractmethod</code>","text":"<p>Context manager for file operations.</p> <p>Parameters:</p> Name Type Description Default <code>file</code> <code>str</code> <p>Path to the file</p> required <code>mode</code> <code>str</code> <p>File mode ('r', 'w', 'rb', 'wb')</p> <code>'r'</code> <p>Yields:</p> Type Description <code>Union[str, bytes]</code> <p>File-like object</p> <p>Raises:</p> Type Description <code>IOError</code> <p>If file cannot be opened</p> Source code in <code>gigaspatial/core/io/data_store.py</code> <pre><code>@abstractmethod\ndef open(self, file: str, mode: str = \"r\") -&gt; Union[str, bytes]:\n    \"\"\"\n    Context manager for file operations.\n\n    Args:\n        file: Path to the file\n        mode: File mode ('r', 'w', 'rb', 'wb')\n\n    Yields:\n        File-like object\n\n    Raises:\n        IOError: If file cannot be opened\n    \"\"\"\n    pass\n</code></pre>"},{"location":"api/core/#gigaspatial.core.io.data_store.DataStore.read_file","title":"<code>read_file(path)</code>  <code>abstractmethod</code>","text":"<p>Read contents of a file from the data store.</p> <p>Parameters:</p> Name Type Description Default <code>path</code> <code>str</code> <p>Path to the file to read</p> required <p>Returns:</p> Type Description <code>Any</code> <p>Contents of the file</p> <p>Raises:</p> Type Description <code>IOError</code> <p>If file cannot be read</p> Source code in <code>gigaspatial/core/io/data_store.py</code> <pre><code>@abstractmethod\ndef read_file(self, path: str) -&gt; Any:\n    \"\"\"\n    Read contents of a file from the data store.\n\n    Args:\n        path: Path to the file to read\n\n    Returns:\n        Contents of the file\n\n    Raises:\n        IOError: If file cannot be read\n    \"\"\"\n    pass\n</code></pre>"},{"location":"api/core/#gigaspatial.core.io.data_store.DataStore.remove","title":"<code>remove(path)</code>  <code>abstractmethod</code>","text":"<p>Remove a file.</p> <p>Parameters:</p> Name Type Description Default <code>path</code> <code>str</code> <p>Path to the file to remove</p> required <p>Raises:</p> Type Description <code>IOError</code> <p>If file cannot be removed</p> Source code in <code>gigaspatial/core/io/data_store.py</code> <pre><code>@abstractmethod\ndef remove(self, path: str) -&gt; None:\n    \"\"\"\n    Remove a file.\n\n    Args:\n        path: Path to the file to remove\n\n    Raises:\n        IOError: If file cannot be removed\n    \"\"\"\n    pass\n</code></pre>"},{"location":"api/core/#gigaspatial.core.io.data_store.DataStore.rmdir","title":"<code>rmdir(dir)</code>  <code>abstractmethod</code>","text":"<p>Remove a directory and all its contents.</p> <p>Parameters:</p> Name Type Description Default <code>dir</code> <code>str</code> <p>Path to the directory to remove</p> required <p>Raises:</p> Type Description <code>IOError</code> <p>If directory cannot be removed</p> Source code in <code>gigaspatial/core/io/data_store.py</code> <pre><code>@abstractmethod\ndef rmdir(self, dir: str) -&gt; None:\n    \"\"\"\n    Remove a directory and all its contents.\n\n    Args:\n        dir: Path to the directory to remove\n\n    Raises:\n        IOError: If directory cannot be removed\n    \"\"\"\n    pass\n</code></pre>"},{"location":"api/core/#gigaspatial.core.io.data_store.DataStore.walk","title":"<code>walk(top)</code>  <code>abstractmethod</code>","text":"<p>Walk through directory tree, similar to os.walk().</p> <p>Parameters:</p> Name Type Description Default <code>top</code> <code>str</code> <p>Starting directory for the walk</p> required <p>Returns:</p> Type Description <code>Generator</code> <p>Generator yielding tuples of (dirpath, dirnames, filenames)</p> Source code in <code>gigaspatial/core/io/data_store.py</code> <pre><code>@abstractmethod\ndef walk(self, top: str) -&gt; Generator:\n    \"\"\"\n    Walk through directory tree, similar to os.walk().\n\n    Args:\n        top: Starting directory for the walk\n\n    Returns:\n        Generator yielding tuples of (dirpath, dirnames, filenames)\n    \"\"\"\n    pass\n</code></pre>"},{"location":"api/core/#gigaspatial.core.io.data_store.DataStore.write_file","title":"<code>write_file(path, data)</code>  <code>abstractmethod</code>","text":"<p>Write data to a file in the data store.</p> <p>Parameters:</p> Name Type Description Default <code>path</code> <code>str</code> <p>Path where to write the file</p> required <code>data</code> <code>Any</code> <p>Data to write to the file</p> required <p>Raises:</p> Type Description <code>IOError</code> <p>If file cannot be written</p> Source code in <code>gigaspatial/core/io/data_store.py</code> <pre><code>@abstractmethod\ndef write_file(self, path: str, data: Any) -&gt; None:\n    \"\"\"\n    Write data to a file in the data store.\n\n    Args:\n        path: Path where to write the file\n        data: Data to write to the file\n\n    Raises:\n        IOError: If file cannot be written\n    \"\"\"\n    pass\n</code></pre>"},{"location":"api/core/#gigaspatial.core.io.database","title":"<code>database</code>","text":""},{"location":"api/core/#gigaspatial.core.io.database.DBConnection","title":"<code>DBConnection</code>","text":"<p>A unified database connection class supporting both Trino and PostgreSQL.</p> Source code in <code>gigaspatial/core/io/database.py</code> <pre><code>class DBConnection:\n    \"\"\"\n    A unified database connection class supporting both Trino and PostgreSQL.\n    \"\"\"\n\n    DB_CONFIG = global_config.DB_CONFIG or {}\n\n    def __init__(\n        self,\n        db_type: Literal[\"postgresql\", \"trino\"] = DB_CONFIG.get(\n            \"db_type\", \"postgresql\"\n        ),\n        host: Optional[str] = DB_CONFIG.get(\"host\", None),\n        port: Union[int, str] = DB_CONFIG.get(\"port\", None),  # type: ignore\n        user: Optional[str] = DB_CONFIG.get(\"user\", None),\n        password: Optional[str] = DB_CONFIG.get(\"password\", None),\n        catalog: Optional[str] = DB_CONFIG.get(\"catalog\", None),  # For Trino\n        database: Optional[str] = DB_CONFIG.get(\"database\", None),  # For PostgreSQL\n        schema: str = DB_CONFIG.get(\"schema\", \"public\"),  # Default for PostgreSQL\n        http_scheme: str = DB_CONFIG.get(\"http_scheme\", \"https\"),  # For Trino\n        sslmode: str = DB_CONFIG.get(\"sslmode\", \"require\"),  # For PostgreSQL\n        **kwargs,\n    ):\n        \"\"\"\n        Initialize a database connection for either Trino or PostgreSQL.\n\n        Args:\n            db_type: Either \"trino\" or \"postgresql\"\n            host: Database server host\n            port: Database server port\n            user: Username\n            password: Password\n            catalog: Trino catalog name\n            database: PostgreSQL database name\n            schema: Default schema name\n            http_scheme: For Trino (\"http\" or \"https\")\n            sslmode: For PostgreSQL (e.g., \"require\", \"verify-full\")\n            **kwargs: Additional connection parameters\n        \"\"\"\n        self.db_type = db_type.lower()\n        self.host = host\n        self.port = str(port) if port else None\n        self.user = user\n        self.password = quote_plus(password) if password else None\n        self.default_schema = schema\n\n        if self.db_type == \"trino\":\n            self.catalog = catalog\n            self.http_scheme = http_scheme\n            self.engine = self._create_trino_engine(**kwargs)\n        elif self.db_type == \"postgresql\":\n            self.database = database\n            self.sslmode = sslmode\n            self.engine = self._create_postgresql_engine(**kwargs)\n        else:\n            raise ValueError(f\"Unsupported database type: {db_type}\")\n\n        self._add_event_listener()\n\n    def _create_trino_engine(self, **kwargs) -&gt; Engine:\n        \"\"\"Create a Trino SQLAlchemy engine.\"\"\"\n        self._connection_string = (\n            f\"trino://{self.user}:{self.password}@{self.host}:{self.port}/\"\n            f\"{self.catalog}/{self.default_schema}\"\n        )\n        return create_engine(\n            self._connection_string,\n            connect_args={\"http_scheme\": self.http_scheme},\n            **kwargs,\n        )\n\n    def _create_postgresql_engine(self, **kwargs) -&gt; Engine:\n        \"\"\"Create a PostgreSQL SQLAlchemy engine.\"\"\"\n        self._connection_string = (\n            f\"postgresql://{self.user}:{self.password}@{self.host}:{self.port}/\"\n            f\"{self.database}?sslmode={self.sslmode}\"\n        )\n        return create_engine(self._connection_string, **kwargs)\n\n    def _add_event_listener(self):\n        \"\"\"Add event listeners for schema setting.\"\"\"\n        if self.db_type == \"trino\":\n\n            @event.listens_for(self.engine, \"connect\", insert=True)\n            def set_current_schema(dbapi_connection, connection_record):\n                cursor_obj = dbapi_connection.cursor()\n                try:\n                    cursor_obj.execute(f\"USE {self.default_schema}\")\n                except Exception as e:\n                    warnings.warn(f\"Could not set schema to {self.default_schema}: {e}\")\n                finally:\n                    cursor_obj.close()\n\n    def get_connection_string(self) -&gt; str:\n        \"\"\"\n        Returns the connection string used to create the engine.\n\n        Returns:\n            str: The connection string.\n        \"\"\"\n        return self._connection_string\n\n    def get_schema_names(self) -&gt; List[str]:\n        \"\"\"Get list of all schema names.\"\"\"\n        inspector = inspect(self.engine)\n        return inspector.get_schema_names()\n\n    def get_table_names(self, schema: Optional[str] = None) -&gt; List[str]:\n        \"\"\"Get list of table names in a schema.\"\"\"\n        schema = schema or self.default_schema\n        inspector = inspect(self.engine)\n        return inspector.get_table_names(schema=schema)\n\n    def get_view_names(self, schema: Optional[str] = None) -&gt; List[str]:\n        \"\"\"Get list of view names in a schema.\"\"\"\n        schema = schema or self.default_schema\n        inspector = inspect(self.engine)\n        return inspector.get_view_names(schema=schema)\n\n    def get_column_names(\n        self, table_name: str, schema: Optional[str] = None\n    ) -&gt; List[str]:\n        \"\"\"Get column names for a specific table.\"\"\"\n        if \".\" in table_name:\n            schema, table_name = table_name.split(\".\")\n        else:\n            schema = schema or self.default_schema\n\n        inspector = inspect(self.engine)\n        columns = inspector.get_columns(table_name, schema=schema)\n        return [col[\"name\"] for col in columns]\n\n    def get_table_info(\n        self, table_name: str, schema: Optional[str] = None\n    ) -&gt; List[Dict]:\n        \"\"\"Get detailed column information for a table.\"\"\"\n        if \".\" in table_name:\n            schema, table_name = table_name.split(\".\")\n        else:\n            schema = schema or self.default_schema\n\n        inspector = inspect(self.engine)\n        return inspector.get_columns(table_name, schema=schema)\n\n    def get_primary_keys(\n        self, table_name: str, schema: Optional[str] = None\n    ) -&gt; List[str]:\n        \"\"\"Get primary key columns for a table.\"\"\"\n        if \".\" in table_name:\n            schema, table_name = table_name.split(\".\")\n        else:\n            schema = schema or self.default_schema\n\n        inspector = inspect(self.engine)\n        try:\n            return inspector.get_pk_constraint(table_name, schema=schema)[\n                \"constrained_columns\"\n            ]\n        except:\n            return []  # Some databases may not support PK constraints\n\n    def table_exists(self, table_name: str, schema: Optional[str] = None) -&gt; bool:\n        \"\"\"Check if a table exists.\"\"\"\n        if \".\" in table_name:\n            schema, table_name = table_name.split(\".\")\n        else:\n            schema = schema or self.default_schema\n\n        return table_name in self.get_table_names(schema=schema)\n\n    # PostgreSQL-specific methods\n    def get_extensions(self) -&gt; List[str]:\n        \"\"\"Get list of installed PostgreSQL extensions (PostgreSQL only).\"\"\"\n        if self.db_type != \"postgresql\":\n            raise NotImplementedError(\n                \"This method is only available for PostgreSQL connections\"\n            )\n\n        with self.engine.connect() as conn:\n            result = conn.execute(\"SELECT extname FROM pg_extension\")\n            return [row[0] for row in result]\n\n    def execute_query(\n        self, query: str, fetch_results: bool = True, params: Optional[Dict] = None\n    ) -&gt; Union[List[tuple], None]:\n        \"\"\"\n        Executes a SQL query (works for both PostgreSQL and Trino).\n\n        Args:\n            query: SQL query to execute\n            fetch_results: Whether to fetch results\n            params: Parameters for parameterized queries\n\n        Returns:\n            Results as list of tuples or None\n        \"\"\"\n        try:\n            with self.engine.connect() as connection:\n                stmt = text(query)\n                result = (\n                    connection.execute(stmt, params)\n                    if params\n                    else connection.execute(stmt)\n                )\n\n                if fetch_results and result.returns_rows:\n                    return result.fetchall()\n                return None\n        except SQLAlchemyError as e:\n            print(f\"Error executing query: {e}\")\n            raise\n\n    def test_connection(self) -&gt; bool:\n        \"\"\"\n        Tests the database connection (works for both PostgreSQL and Trino).\n\n        Returns:\n            True if connection successful, False otherwise\n        \"\"\"\n        test_query = (\n            \"SELECT 1\"\n            if self.db_type == \"postgresql\"\n            else \"SELECT 1 AS connection_test\"\n        )\n\n        try:\n            print(\n                f\"Attempting to connect to {self.db_type} at {self.host}:{self.port}...\"\n            )\n            with self.engine.connect() as conn:\n                conn.execute(text(test_query))\n            print(f\"Successfully connected to {self.db_type.upper()}.\")\n            return True\n        except Exception as e:\n            print(f\"Failed to connect to {self.db_type.upper()}: {e}\")\n            return False\n\n    def read_sql_to_dataframe(\n        self, query: str, params: Optional[Dict] = None\n    ) -&gt; pd.DataFrame:\n        \"\"\"\n        Executes query and returns results as pandas DataFrame (works for both).\n\n        Args:\n            query: SQL query to execute\n            params: Parameters for parameterized queries\n\n        Returns:\n            pandas DataFrame with results\n        \"\"\"\n        try:\n            with self.engine.connect() as connection:\n                return pd.read_sql_query(text(query), connection, params=params)\n        except SQLAlchemyError as e:\n            print(f\"Error reading SQL to DataFrame: {e}\")\n            raise\n\n    def read_sql_to_dask_dataframe(\n        self,\n        table_name: str,\n        index_col: str,\n        columns: Optional[List[str]] = None,\n        limit: Optional[int] = None,\n        **kwargs,\n    ) -&gt; pd.DataFrame:\n        \"\"\"\n        Reads data to Dask DataFrame (works for both, but connection string differs).\n\n        Args:\n            table_name: Table name (schema.table or just table)\n            columns: List of columns to select\n            limit: Maximum rows to return\n            **kwargs: Additional arguments\n\n        Returns:\n            Dask DataFrame with results\n        \"\"\"\n        try:\n            connection_string = self.get_connection_string()\n\n            # Handle schema.table format\n            if \".\" in table_name:\n                schema, table = table_name.split(\".\")\n            else:\n                schema = self.default_schema\n                table = table_name\n\n            metadata = MetaData()\n            table_obj = Table(table, metadata, schema=schema, autoload_with=self.engine)\n\n            # Build query\n            query = (\n                select(*[table_obj.c[col] for col in columns])\n                if columns\n                else select(table_obj)\n            )\n            if limit:\n                query = query.limit(limit)\n\n            return dd.read_sql_query(\n                sql=query, con=connection_string, index_col=index_col, **kwargs\n            )\n        except Exception as e:\n            print(f\"Error reading SQL to Dask DataFrame: {e}\")\n            raise ValueError(f\"Failed to read SQL to Dask DataFrame: {e}\") from e\n</code></pre>"},{"location":"api/core/#gigaspatial.core.io.database.DBConnection.__init__","title":"<code>__init__(db_type=DB_CONFIG.get('db_type', 'postgresql'), host=DB_CONFIG.get('host', None), port=DB_CONFIG.get('port', None), user=DB_CONFIG.get('user', None), password=DB_CONFIG.get('password', None), catalog=DB_CONFIG.get('catalog', None), database=DB_CONFIG.get('database', None), schema=DB_CONFIG.get('schema', 'public'), http_scheme=DB_CONFIG.get('http_scheme', 'https'), sslmode=DB_CONFIG.get('sslmode', 'require'), **kwargs)</code>","text":"<p>Initialize a database connection for either Trino or PostgreSQL.</p> <p>Parameters:</p> Name Type Description Default <code>db_type</code> <code>Literal['postgresql', 'trino']</code> <p>Either \"trino\" or \"postgresql\"</p> <code>get('db_type', 'postgresql')</code> <code>host</code> <code>Optional[str]</code> <p>Database server host</p> <code>get('host', None)</code> <code>port</code> <code>Union[int, str]</code> <p>Database server port</p> <code>get('port', None)</code> <code>user</code> <code>Optional[str]</code> <p>Username</p> <code>get('user', None)</code> <code>password</code> <code>Optional[str]</code> <p>Password</p> <code>get('password', None)</code> <code>catalog</code> <code>Optional[str]</code> <p>Trino catalog name</p> <code>get('catalog', None)</code> <code>database</code> <code>Optional[str]</code> <p>PostgreSQL database name</p> <code>get('database', None)</code> <code>schema</code> <code>str</code> <p>Default schema name</p> <code>get('schema', 'public')</code> <code>http_scheme</code> <code>str</code> <p>For Trino (\"http\" or \"https\")</p> <code>get('http_scheme', 'https')</code> <code>sslmode</code> <code>str</code> <p>For PostgreSQL (e.g., \"require\", \"verify-full\")</p> <code>get('sslmode', 'require')</code> <code>**kwargs</code> <p>Additional connection parameters</p> <code>{}</code> Source code in <code>gigaspatial/core/io/database.py</code> <pre><code>def __init__(\n    self,\n    db_type: Literal[\"postgresql\", \"trino\"] = DB_CONFIG.get(\n        \"db_type\", \"postgresql\"\n    ),\n    host: Optional[str] = DB_CONFIG.get(\"host\", None),\n    port: Union[int, str] = DB_CONFIG.get(\"port\", None),  # type: ignore\n    user: Optional[str] = DB_CONFIG.get(\"user\", None),\n    password: Optional[str] = DB_CONFIG.get(\"password\", None),\n    catalog: Optional[str] = DB_CONFIG.get(\"catalog\", None),  # For Trino\n    database: Optional[str] = DB_CONFIG.get(\"database\", None),  # For PostgreSQL\n    schema: str = DB_CONFIG.get(\"schema\", \"public\"),  # Default for PostgreSQL\n    http_scheme: str = DB_CONFIG.get(\"http_scheme\", \"https\"),  # For Trino\n    sslmode: str = DB_CONFIG.get(\"sslmode\", \"require\"),  # For PostgreSQL\n    **kwargs,\n):\n    \"\"\"\n    Initialize a database connection for either Trino or PostgreSQL.\n\n    Args:\n        db_type: Either \"trino\" or \"postgresql\"\n        host: Database server host\n        port: Database server port\n        user: Username\n        password: Password\n        catalog: Trino catalog name\n        database: PostgreSQL database name\n        schema: Default schema name\n        http_scheme: For Trino (\"http\" or \"https\")\n        sslmode: For PostgreSQL (e.g., \"require\", \"verify-full\")\n        **kwargs: Additional connection parameters\n    \"\"\"\n    self.db_type = db_type.lower()\n    self.host = host\n    self.port = str(port) if port else None\n    self.user = user\n    self.password = quote_plus(password) if password else None\n    self.default_schema = schema\n\n    if self.db_type == \"trino\":\n        self.catalog = catalog\n        self.http_scheme = http_scheme\n        self.engine = self._create_trino_engine(**kwargs)\n    elif self.db_type == \"postgresql\":\n        self.database = database\n        self.sslmode = sslmode\n        self.engine = self._create_postgresql_engine(**kwargs)\n    else:\n        raise ValueError(f\"Unsupported database type: {db_type}\")\n\n    self._add_event_listener()\n</code></pre>"},{"location":"api/core/#gigaspatial.core.io.database.DBConnection.execute_query","title":"<code>execute_query(query, fetch_results=True, params=None)</code>","text":"<p>Executes a SQL query (works for both PostgreSQL and Trino).</p> <p>Parameters:</p> Name Type Description Default <code>query</code> <code>str</code> <p>SQL query to execute</p> required <code>fetch_results</code> <code>bool</code> <p>Whether to fetch results</p> <code>True</code> <code>params</code> <code>Optional[Dict]</code> <p>Parameters for parameterized queries</p> <code>None</code> <p>Returns:</p> Type Description <code>Union[List[tuple], None]</code> <p>Results as list of tuples or None</p> Source code in <code>gigaspatial/core/io/database.py</code> <pre><code>def execute_query(\n    self, query: str, fetch_results: bool = True, params: Optional[Dict] = None\n) -&gt; Union[List[tuple], None]:\n    \"\"\"\n    Executes a SQL query (works for both PostgreSQL and Trino).\n\n    Args:\n        query: SQL query to execute\n        fetch_results: Whether to fetch results\n        params: Parameters for parameterized queries\n\n    Returns:\n        Results as list of tuples or None\n    \"\"\"\n    try:\n        with self.engine.connect() as connection:\n            stmt = text(query)\n            result = (\n                connection.execute(stmt, params)\n                if params\n                else connection.execute(stmt)\n            )\n\n            if fetch_results and result.returns_rows:\n                return result.fetchall()\n            return None\n    except SQLAlchemyError as e:\n        print(f\"Error executing query: {e}\")\n        raise\n</code></pre>"},{"location":"api/core/#gigaspatial.core.io.database.DBConnection.get_column_names","title":"<code>get_column_names(table_name, schema=None)</code>","text":"<p>Get column names for a specific table.</p> Source code in <code>gigaspatial/core/io/database.py</code> <pre><code>def get_column_names(\n    self, table_name: str, schema: Optional[str] = None\n) -&gt; List[str]:\n    \"\"\"Get column names for a specific table.\"\"\"\n    if \".\" in table_name:\n        schema, table_name = table_name.split(\".\")\n    else:\n        schema = schema or self.default_schema\n\n    inspector = inspect(self.engine)\n    columns = inspector.get_columns(table_name, schema=schema)\n    return [col[\"name\"] for col in columns]\n</code></pre>"},{"location":"api/core/#gigaspatial.core.io.database.DBConnection.get_connection_string","title":"<code>get_connection_string()</code>","text":"<p>Returns the connection string used to create the engine.</p> <p>Returns:</p> Name Type Description <code>str</code> <code>str</code> <p>The connection string.</p> Source code in <code>gigaspatial/core/io/database.py</code> <pre><code>def get_connection_string(self) -&gt; str:\n    \"\"\"\n    Returns the connection string used to create the engine.\n\n    Returns:\n        str: The connection string.\n    \"\"\"\n    return self._connection_string\n</code></pre>"},{"location":"api/core/#gigaspatial.core.io.database.DBConnection.get_extensions","title":"<code>get_extensions()</code>","text":"<p>Get list of installed PostgreSQL extensions (PostgreSQL only).</p> Source code in <code>gigaspatial/core/io/database.py</code> <pre><code>def get_extensions(self) -&gt; List[str]:\n    \"\"\"Get list of installed PostgreSQL extensions (PostgreSQL only).\"\"\"\n    if self.db_type != \"postgresql\":\n        raise NotImplementedError(\n            \"This method is only available for PostgreSQL connections\"\n        )\n\n    with self.engine.connect() as conn:\n        result = conn.execute(\"SELECT extname FROM pg_extension\")\n        return [row[0] for row in result]\n</code></pre>"},{"location":"api/core/#gigaspatial.core.io.database.DBConnection.get_primary_keys","title":"<code>get_primary_keys(table_name, schema=None)</code>","text":"<p>Get primary key columns for a table.</p> Source code in <code>gigaspatial/core/io/database.py</code> <pre><code>def get_primary_keys(\n    self, table_name: str, schema: Optional[str] = None\n) -&gt; List[str]:\n    \"\"\"Get primary key columns for a table.\"\"\"\n    if \".\" in table_name:\n        schema, table_name = table_name.split(\".\")\n    else:\n        schema = schema or self.default_schema\n\n    inspector = inspect(self.engine)\n    try:\n        return inspector.get_pk_constraint(table_name, schema=schema)[\n            \"constrained_columns\"\n        ]\n    except:\n        return []  # Some databases may not support PK constraints\n</code></pre>"},{"location":"api/core/#gigaspatial.core.io.database.DBConnection.get_schema_names","title":"<code>get_schema_names()</code>","text":"<p>Get list of all schema names.</p> Source code in <code>gigaspatial/core/io/database.py</code> <pre><code>def get_schema_names(self) -&gt; List[str]:\n    \"\"\"Get list of all schema names.\"\"\"\n    inspector = inspect(self.engine)\n    return inspector.get_schema_names()\n</code></pre>"},{"location":"api/core/#gigaspatial.core.io.database.DBConnection.get_table_info","title":"<code>get_table_info(table_name, schema=None)</code>","text":"<p>Get detailed column information for a table.</p> Source code in <code>gigaspatial/core/io/database.py</code> <pre><code>def get_table_info(\n    self, table_name: str, schema: Optional[str] = None\n) -&gt; List[Dict]:\n    \"\"\"Get detailed column information for a table.\"\"\"\n    if \".\" in table_name:\n        schema, table_name = table_name.split(\".\")\n    else:\n        schema = schema or self.default_schema\n\n    inspector = inspect(self.engine)\n    return inspector.get_columns(table_name, schema=schema)\n</code></pre>"},{"location":"api/core/#gigaspatial.core.io.database.DBConnection.get_table_names","title":"<code>get_table_names(schema=None)</code>","text":"<p>Get list of table names in a schema.</p> Source code in <code>gigaspatial/core/io/database.py</code> <pre><code>def get_table_names(self, schema: Optional[str] = None) -&gt; List[str]:\n    \"\"\"Get list of table names in a schema.\"\"\"\n    schema = schema or self.default_schema\n    inspector = inspect(self.engine)\n    return inspector.get_table_names(schema=schema)\n</code></pre>"},{"location":"api/core/#gigaspatial.core.io.database.DBConnection.get_view_names","title":"<code>get_view_names(schema=None)</code>","text":"<p>Get list of view names in a schema.</p> Source code in <code>gigaspatial/core/io/database.py</code> <pre><code>def get_view_names(self, schema: Optional[str] = None) -&gt; List[str]:\n    \"\"\"Get list of view names in a schema.\"\"\"\n    schema = schema or self.default_schema\n    inspector = inspect(self.engine)\n    return inspector.get_view_names(schema=schema)\n</code></pre>"},{"location":"api/core/#gigaspatial.core.io.database.DBConnection.read_sql_to_dask_dataframe","title":"<code>read_sql_to_dask_dataframe(table_name, index_col, columns=None, limit=None, **kwargs)</code>","text":"<p>Reads data to Dask DataFrame (works for both, but connection string differs).</p> <p>Parameters:</p> Name Type Description Default <code>table_name</code> <code>str</code> <p>Table name (schema.table or just table)</p> required <code>columns</code> <code>Optional[List[str]]</code> <p>List of columns to select</p> <code>None</code> <code>limit</code> <code>Optional[int]</code> <p>Maximum rows to return</p> <code>None</code> <code>**kwargs</code> <p>Additional arguments</p> <code>{}</code> <p>Returns:</p> Type Description <code>DataFrame</code> <p>Dask DataFrame with results</p> Source code in <code>gigaspatial/core/io/database.py</code> <pre><code>def read_sql_to_dask_dataframe(\n    self,\n    table_name: str,\n    index_col: str,\n    columns: Optional[List[str]] = None,\n    limit: Optional[int] = None,\n    **kwargs,\n) -&gt; pd.DataFrame:\n    \"\"\"\n    Reads data to Dask DataFrame (works for both, but connection string differs).\n\n    Args:\n        table_name: Table name (schema.table or just table)\n        columns: List of columns to select\n        limit: Maximum rows to return\n        **kwargs: Additional arguments\n\n    Returns:\n        Dask DataFrame with results\n    \"\"\"\n    try:\n        connection_string = self.get_connection_string()\n\n        # Handle schema.table format\n        if \".\" in table_name:\n            schema, table = table_name.split(\".\")\n        else:\n            schema = self.default_schema\n            table = table_name\n\n        metadata = MetaData()\n        table_obj = Table(table, metadata, schema=schema, autoload_with=self.engine)\n\n        # Build query\n        query = (\n            select(*[table_obj.c[col] for col in columns])\n            if columns\n            else select(table_obj)\n        )\n        if limit:\n            query = query.limit(limit)\n\n        return dd.read_sql_query(\n            sql=query, con=connection_string, index_col=index_col, **kwargs\n        )\n    except Exception as e:\n        print(f\"Error reading SQL to Dask DataFrame: {e}\")\n        raise ValueError(f\"Failed to read SQL to Dask DataFrame: {e}\") from e\n</code></pre>"},{"location":"api/core/#gigaspatial.core.io.database.DBConnection.read_sql_to_dataframe","title":"<code>read_sql_to_dataframe(query, params=None)</code>","text":"<p>Executes query and returns results as pandas DataFrame (works for both).</p> <p>Parameters:</p> Name Type Description Default <code>query</code> <code>str</code> <p>SQL query to execute</p> required <code>params</code> <code>Optional[Dict]</code> <p>Parameters for parameterized queries</p> <code>None</code> <p>Returns:</p> Type Description <code>DataFrame</code> <p>pandas DataFrame with results</p> Source code in <code>gigaspatial/core/io/database.py</code> <pre><code>def read_sql_to_dataframe(\n    self, query: str, params: Optional[Dict] = None\n) -&gt; pd.DataFrame:\n    \"\"\"\n    Executes query and returns results as pandas DataFrame (works for both).\n\n    Args:\n        query: SQL query to execute\n        params: Parameters for parameterized queries\n\n    Returns:\n        pandas DataFrame with results\n    \"\"\"\n    try:\n        with self.engine.connect() as connection:\n            return pd.read_sql_query(text(query), connection, params=params)\n    except SQLAlchemyError as e:\n        print(f\"Error reading SQL to DataFrame: {e}\")\n        raise\n</code></pre>"},{"location":"api/core/#gigaspatial.core.io.database.DBConnection.table_exists","title":"<code>table_exists(table_name, schema=None)</code>","text":"<p>Check if a table exists.</p> Source code in <code>gigaspatial/core/io/database.py</code> <pre><code>def table_exists(self, table_name: str, schema: Optional[str] = None) -&gt; bool:\n    \"\"\"Check if a table exists.\"\"\"\n    if \".\" in table_name:\n        schema, table_name = table_name.split(\".\")\n    else:\n        schema = schema or self.default_schema\n\n    return table_name in self.get_table_names(schema=schema)\n</code></pre>"},{"location":"api/core/#gigaspatial.core.io.database.DBConnection.test_connection","title":"<code>test_connection()</code>","text":"<p>Tests the database connection (works for both PostgreSQL and Trino).</p> <p>Returns:</p> Type Description <code>bool</code> <p>True if connection successful, False otherwise</p> Source code in <code>gigaspatial/core/io/database.py</code> <pre><code>def test_connection(self) -&gt; bool:\n    \"\"\"\n    Tests the database connection (works for both PostgreSQL and Trino).\n\n    Returns:\n        True if connection successful, False otherwise\n    \"\"\"\n    test_query = (\n        \"SELECT 1\"\n        if self.db_type == \"postgresql\"\n        else \"SELECT 1 AS connection_test\"\n    )\n\n    try:\n        print(\n            f\"Attempting to connect to {self.db_type} at {self.host}:{self.port}...\"\n        )\n        with self.engine.connect() as conn:\n            conn.execute(text(test_query))\n        print(f\"Successfully connected to {self.db_type.upper()}.\")\n        return True\n    except Exception as e:\n        print(f\"Failed to connect to {self.db_type.upper()}: {e}\")\n        return False\n</code></pre>"},{"location":"api/core/#gigaspatial.core.io.local_data_store","title":"<code>local_data_store</code>","text":""},{"location":"api/core/#gigaspatial.core.io.local_data_store.LocalDataStore","title":"<code>LocalDataStore</code>","text":"<p>               Bases: <code>DataStore</code></p> <p>Implementation for local filesystem storage.</p> Source code in <code>gigaspatial/core/io/local_data_store.py</code> <pre><code>class LocalDataStore(DataStore):\n    \"\"\"Implementation for local filesystem storage.\"\"\"\n\n    def __init__(self, base_path: Union[str, Path] = \"\"):\n        super().__init__()\n        self.base_path = Path(base_path).resolve()\n\n    def _resolve_path(self, path: str) -&gt; Path:\n        \"\"\"Resolve path relative to base directory.\"\"\"\n        return self.base_path / path\n\n    def read_file(self, path: str) -&gt; bytes:\n        full_path = self._resolve_path(path)\n        with open(full_path, \"rb\") as f:\n            return f.read()\n\n    def write_file(self, path: str, data: Union[bytes, str]) -&gt; None:\n        full_path = self._resolve_path(path)\n        self.mkdir(str(full_path.parent), exist_ok=True)\n\n        if isinstance(data, str):\n            mode = \"w\"\n            encoding = \"utf-8\"\n        else:\n            mode = \"wb\"\n            encoding = None\n\n        with open(full_path, mode, encoding=encoding) as f:\n            f.write(data)\n\n    def file_exists(self, path: str) -&gt; bool:\n        return self._resolve_path(path).is_file()\n\n    def list_files(self, path: str) -&gt; List[str]:\n        full_path = self._resolve_path(path)\n        return [\n            str(f.relative_to(self.base_path))\n            for f in full_path.iterdir()\n            if f.is_file()\n        ]\n\n    def walk(self, top: str) -&gt; Generator[Tuple[str, List[str], List[str]], None, None]:\n        full_path = self._resolve_path(top)\n        for root, dirs, files in os.walk(full_path):\n            rel_root = str(Path(root).relative_to(self.base_path))\n            yield rel_root, dirs, files\n\n    def list_directories(self, path: str) -&gt; List[str]:\n        full_path = self._resolve_path(path)\n\n        if not full_path.exists():\n            return []\n\n        if not full_path.is_dir():\n            return []\n\n        return [d.name for d in full_path.iterdir() if d.is_dir()]\n\n    def open(self, path: str, mode: str = \"r\") -&gt; IO:\n        full_path = self._resolve_path(path)\n        self.mkdir(str(full_path.parent), exist_ok=True)\n        return open(full_path, mode)\n\n    def is_file(self, path: str) -&gt; bool:\n        return self._resolve_path(path).is_file()\n\n    def is_dir(self, path: str) -&gt; bool:\n        return self._resolve_path(path).is_dir()\n\n    def remove(self, path: str) -&gt; None:\n        full_path = self._resolve_path(path)\n        if full_path.is_file():\n            os.remove(full_path)\n\n    def copy_file(self, src: str, dst: str) -&gt; None:\n        \"\"\"Copy a file from src to dst.\"\"\"\n        src_path = self._resolve_path(src)\n        dst_path = self._resolve_path(dst)\n        self.mkdir(str(dst_path.parent), exist_ok=True)\n        shutil.copy2(src_path, dst_path)\n\n    def rmdir(self, directory: str) -&gt; None:\n        full_path = self._resolve_path(directory)\n        if full_path.is_dir():\n            os.rmdir(full_path)\n\n    def mkdir(self, path: str, exist_ok: bool = False) -&gt; None:\n        full_path = self._resolve_path(path)\n        full_path.mkdir(parents=True, exist_ok=exist_ok)\n\n    def exists(self, path: str) -&gt; bool:\n        return self._resolve_path(path).exists()\n</code></pre>"},{"location":"api/core/#gigaspatial.core.io.local_data_store.LocalDataStore.copy_file","title":"<code>copy_file(src, dst)</code>","text":"<p>Copy a file from src to dst.</p> Source code in <code>gigaspatial/core/io/local_data_store.py</code> <pre><code>def copy_file(self, src: str, dst: str) -&gt; None:\n    \"\"\"Copy a file from src to dst.\"\"\"\n    src_path = self._resolve_path(src)\n    dst_path = self._resolve_path(dst)\n    self.mkdir(str(dst_path.parent), exist_ok=True)\n    shutil.copy2(src_path, dst_path)\n</code></pre>"},{"location":"api/core/#gigaspatial.core.io.readers","title":"<code>readers</code>","text":""},{"location":"api/core/#gigaspatial.core.io.readers.read_dataset","title":"<code>read_dataset(data_store, path, compression=None, **kwargs)</code>","text":"<p>Read data from various file formats stored in both local and cloud-based storage.</p>"},{"location":"api/core/#gigaspatial.core.io.readers.read_dataset--parameters","title":"Parameters:","text":"<p>data_store : DataStore     Instance of DataStore for accessing data storage. path : str, Path     Path to the file in data storage. **kwargs : dict     Additional arguments passed to the specific reader function.</p>"},{"location":"api/core/#gigaspatial.core.io.readers.read_dataset--returns","title":"Returns:","text":"<p>pandas.DataFrame or geopandas.GeoDataFrame     The data read from the file.</p>"},{"location":"api/core/#gigaspatial.core.io.readers.read_dataset--raises","title":"Raises:","text":"<p>FileNotFoundError     If the file doesn't exist in blob storage. ValueError     If the file type is unsupported or if there's an error reading the file.</p> Source code in <code>gigaspatial/core/io/readers.py</code> <pre><code>def read_dataset(data_store: DataStore, path: str, compression: str = None, **kwargs):\n    \"\"\"\n    Read data from various file formats stored in both local and cloud-based storage.\n\n    Parameters:\n    ----------\n    data_store : DataStore\n        Instance of DataStore for accessing data storage.\n    path : str, Path\n        Path to the file in data storage.\n    **kwargs : dict\n        Additional arguments passed to the specific reader function.\n\n    Returns:\n    -------\n    pandas.DataFrame or geopandas.GeoDataFrame\n        The data read from the file.\n\n    Raises:\n    ------\n    FileNotFoundError\n        If the file doesn't exist in blob storage.\n    ValueError\n        If the file type is unsupported or if there's an error reading the file.\n    \"\"\"\n\n    # Define supported file formats and their readers\n    BINARY_FORMATS = {\n        \".shp\",\n        \".zip\",\n        \".parquet\",\n        \".gpkg\",\n        \".xlsx\",\n        \".xls\",\n        \".kmz\",\n        \".gz\",\n    }\n\n    PANDAS_READERS = {\n        \".csv\": pd.read_csv,\n        \".xlsx\": lambda f, **kw: pd.read_excel(f, engine=\"openpyxl\", **kw),\n        \".xls\": lambda f, **kw: pd.read_excel(f, engine=\"xlrd\", **kw),\n        \".json\": pd.read_json,\n        # \".gz\": lambda f, **kw: pd.read_csv(f, compression=\"gzip\", **kw),\n    }\n\n    GEO_READERS = {\n        \".shp\": gpd.read_file,\n        \".zip\": gpd.read_file,\n        \".geojson\": gpd.read_file,\n        \".gpkg\": gpd.read_file,\n        \".parquet\": gpd.read_parquet,\n        \".kmz\": read_kmz,\n    }\n\n    COMPRESSION_FORMATS = {\n        \".gz\": \"gzip\",\n        \".bz2\": \"bz2\",\n        \".zip\": \"zip\",\n        \".xz\": \"xz\",\n    }\n\n    try:\n        # Check if file exists\n        if not data_store.file_exists(path):\n            raise FileNotFoundError(f\"File '{path}' not found in blob storage\")\n\n        path_obj = Path(path)\n        suffixes = path_obj.suffixes\n        file_extension = suffixes[-1].lower() if suffixes else \"\"\n\n        if compression is None and file_extension in COMPRESSION_FORMATS:\n            compression_format = COMPRESSION_FORMATS[file_extension]\n\n            # if file has multiple extensions (e.g., .csv.gz), get the inner format\n            if len(suffixes) &gt; 1:\n                inner_extension = suffixes[-2].lower()\n\n                if inner_extension == \".tar\":\n                    raise ValueError(\n                        \"Tar archives (.tar.gz) are not directly supported\"\n                    )\n\n                if inner_extension in PANDAS_READERS:\n                    try:\n                        with data_store.open(path, \"rb\") as f:\n                            return PANDAS_READERS[inner_extension](\n                                f, compression=compression_format, **kwargs\n                            )\n                    except Exception as e:\n                        raise ValueError(f\"Error reading compressed file: {str(e)}\")\n                elif inner_extension in GEO_READERS:\n                    try:\n                        with data_store.open(path, \"rb\") as f:\n                            if compression_format == \"gzip\":\n                                import gzip\n\n                                decompressed_data = gzip.decompress(f.read())\n                                import io\n\n                                return GEO_READERS[inner_extension](\n                                    io.BytesIO(decompressed_data), **kwargs\n                                )\n                            else:\n                                raise ValueError(\n                                    f\"Compression format {compression_format} not supported for geo data\"\n                                )\n                    except Exception as e:\n                        raise ValueError(f\"Error reading compressed geo file: {str(e)}\")\n            else:\n                # if just .gz without clear inner type, assume csv\n                try:\n                    with data_store.open(path, \"rb\") as f:\n                        return pd.read_csv(f, compression=compression_format, **kwargs)\n                except Exception as e:\n                    raise ValueError(\n                        f\"Error reading compressed file as CSV: {str(e)}. \"\n                        f\"If not a CSV, specify the format in the filename (e.g., .json.gz)\"\n                    )\n\n        # Special handling for compressed files\n        if file_extension == \".zip\":\n            # For zip files, we need to use binary mode\n            with data_store.open(path, \"rb\") as f:\n                return gpd.read_file(f)\n\n        # Determine if we need binary mode based on file type\n        mode = \"rb\" if file_extension in BINARY_FORMATS else \"r\"\n\n        # Try reading with appropriate reader\n        if file_extension in PANDAS_READERS:\n            try:\n                with data_store.open(path, mode) as f:\n                    return PANDAS_READERS[file_extension](f, **kwargs)\n            except Exception as e:\n                raise ValueError(f\"Error reading file with pandas: {str(e)}\")\n\n        if file_extension in GEO_READERS:\n            try:\n                with data_store.open(path, \"rb\") as f:\n                    return GEO_READERS[file_extension](f, **kwargs)\n            except Exception as e:\n                # For parquet files, try pandas reader if geopandas fails\n                if file_extension == \".parquet\":\n                    try:\n                        with data_store.open(path, \"rb\") as f:\n                            return pd.read_parquet(f, **kwargs)\n                    except Exception as e2:\n                        raise ValueError(\n                            f\"Failed to read parquet with both geopandas ({str(e)}) \"\n                            f\"and pandas ({str(e2)})\"\n                        )\n                raise ValueError(f\"Error reading file with geopandas: {str(e)}\")\n\n        # If we get here, the file type is unsupported\n        supported_formats = sorted(set(PANDAS_READERS.keys()) | set(GEO_READERS.keys()))\n        supported_compressions = sorted(COMPRESSION_FORMATS.keys())\n        raise ValueError(\n            f\"Unsupported file type: {file_extension}\\n\"\n            f\"Supported formats: {', '.join(supported_formats)}\"\n            f\"Supported compressions: {', '.join(supported_compressions)}\"\n        )\n\n    except Exception as e:\n        if isinstance(e, (FileNotFoundError, ValueError)):\n            raise\n        raise RuntimeError(f\"Unexpected error reading dataset: {str(e)}\")\n</code></pre>"},{"location":"api/core/#gigaspatial.core.io.readers.read_datasets","title":"<code>read_datasets(data_store, paths, **kwargs)</code>","text":"<p>Read multiple datasets from data storage at once.</p>"},{"location":"api/core/#gigaspatial.core.io.readers.read_datasets--parameters","title":"Parameters:","text":"<p>data_store : DataStore     Instance of DataStore for accessing data storage. paths : list of str     Paths to files in data storage. **kwargs : dict     Additional arguments passed to read_dataset.</p>"},{"location":"api/core/#gigaspatial.core.io.readers.read_datasets--returns","title":"Returns:","text":"<p>dict     Dictionary mapping paths to their corresponding DataFrames/GeoDataFrames.</p> Source code in <code>gigaspatial/core/io/readers.py</code> <pre><code>def read_datasets(data_store: DataStore, paths, **kwargs):\n    \"\"\"\n    Read multiple datasets from data storage at once.\n\n    Parameters:\n    ----------\n    data_store : DataStore\n        Instance of DataStore for accessing data storage.\n    paths : list of str\n        Paths to files in data storage.\n    **kwargs : dict\n        Additional arguments passed to read_dataset.\n\n    Returns:\n    -------\n    dict\n        Dictionary mapping paths to their corresponding DataFrames/GeoDataFrames.\n    \"\"\"\n    results = {}\n    errors = {}\n\n    for path in paths:\n        try:\n            results[path] = read_dataset(data_store, path, **kwargs)\n        except Exception as e:\n            errors[path] = str(e)\n\n    if errors:\n        error_msg = \"\\n\".join(f\"- {path}: {error}\" for path, error in errors.items())\n        raise ValueError(f\"Errors reading datasets:\\n{error_msg}\")\n\n    return results\n</code></pre>"},{"location":"api/core/#gigaspatial.core.io.readers.read_gzipped_json_or_csv","title":"<code>read_gzipped_json_or_csv(file_path, data_store)</code>","text":"<p>Reads a gzipped file, attempting to parse it as JSON (lines=True) or CSV.</p> Source code in <code>gigaspatial/core/io/readers.py</code> <pre><code>def read_gzipped_json_or_csv(file_path, data_store):\n    \"\"\"Reads a gzipped file, attempting to parse it as JSON (lines=True) or CSV.\"\"\"\n\n    with data_store.open(file_path, \"rb\") as f:\n        g = gzip.GzipFile(fileobj=f)\n        text = g.read().decode(\"utf-8\")\n        try:\n            df = pd.read_json(io.StringIO(text), lines=True)\n            return df\n        except json.JSONDecodeError:\n            try:\n                df = pd.read_csv(io.StringIO(text))\n                return df\n            except pd.errors.ParserError:\n                print(f\"Error: Could not parse {file_path} as JSON or CSV.\")\n                return None\n</code></pre>"},{"location":"api/core/#gigaspatial.core.io.readers.read_kmz","title":"<code>read_kmz(file_obj, **kwargs)</code>","text":"<p>Helper function to read KMZ files and return a GeoDataFrame.</p> Source code in <code>gigaspatial/core/io/readers.py</code> <pre><code>def read_kmz(file_obj, **kwargs):\n    \"\"\"Helper function to read KMZ files and return a GeoDataFrame.\"\"\"\n    try:\n        with zipfile.ZipFile(file_obj) as kmz:\n            # Find the KML file in the archive (usually doc.kml)\n            kml_filename = next(\n                name for name in kmz.namelist() if name.endswith(\".kml\")\n            )\n\n            # Read the KML content\n            kml_content = io.BytesIO(kmz.read(kml_filename))\n\n            gdf = gpd.read_file(kml_content)\n\n            # Validate the GeoDataFrame\n            if gdf.empty:\n                raise ValueError(\n                    \"The KML file is empty or does not contain valid geospatial data.\"\n                )\n\n        return gdf\n\n    except zipfile.BadZipFile:\n        raise ValueError(\"The provided file is not a valid KMZ file.\")\n    except StopIteration:\n        raise ValueError(\"No KML file found in the KMZ archive.\")\n    except Exception as e:\n        raise RuntimeError(f\"An error occurred: {e}\")\n</code></pre>"},{"location":"api/core/#gigaspatial.core.io.snowflake_data_store","title":"<code>snowflake_data_store</code>","text":""},{"location":"api/core/#gigaspatial.core.io.snowflake_data_store.SnowflakeDataStore","title":"<code>SnowflakeDataStore</code>","text":"<p>               Bases: <code>DataStore</code></p> <p>An implementation of DataStore for Snowflake internal stages. Uses Snowflake stages for file storage and retrieval.</p> Source code in <code>gigaspatial/core/io/snowflake_data_store.py</code> <pre><code>class SnowflakeDataStore(DataStore):\n    \"\"\"\n    An implementation of DataStore for Snowflake internal stages.\n    Uses Snowflake stages for file storage and retrieval.\n    \"\"\"\n\n    def __init__(\n        self,\n        account: str = config.SNOWFLAKE_ACCOUNT,\n        user: str = config.SNOWFLAKE_USER,\n        password: str = config.SNOWFLAKE_PASSWORD,\n        warehouse: str = config.SNOWFLAKE_WAREHOUSE,\n        database: str = config.SNOWFLAKE_DATABASE,\n        schema: str = config.SNOWFLAKE_SCHEMA,\n        stage_name: str = config.SNOWFLAKE_STAGE_NAME,\n    ):\n        \"\"\"\n        Create a new instance of SnowflakeDataStore.\n\n        :param account: Snowflake account identifier\n        :param user: Snowflake username\n        :param password: Snowflake password\n        :param warehouse: Snowflake warehouse name\n        :param database: Snowflake database name\n        :param schema: Snowflake schema name\n        :param stage_name: Name of the Snowflake stage to use for file storage\n        \"\"\"\n        if not all([account, user, password, warehouse, database, schema, stage_name]):\n            raise ValueError(\n                \"Snowflake connection parameters (account, user, password, warehouse, \"\n                \"database, schema, stage_name) must be provided via config or constructor.\"\n            )\n\n        self.account = account\n        self.user = user\n        self.password = password\n        self.warehouse = warehouse\n        self.database = database\n        self.schema = schema\n        self.stage_name = stage_name\n\n        # Create connection\n        self.connection = self._create_connection()\n        self.logger = config.get_logger(self.__class__.__name__)\n\n        # Temporary directory for file operations\n        self._temp_dir = tempfile.mkdtemp()\n\n    def _create_connection(self):\n        \"\"\"Create and return a Snowflake connection.\"\"\"\n        conn_params = {\n            \"account\": self.account,\n            \"user\": self.user,\n            \"password\": self.password,\n            \"warehouse\": self.warehouse,\n            \"database\": self.database,\n            \"schema\": self.schema,\n        }\n\n        connection = snowflake.connector.connect(**conn_params)\n\n        # Explicitly set the database and schema context\n        # This ensures the session knows which database/schema to use\n        cursor = connection.cursor()\n        try:\n            # Use database first\n            cursor.execute(f'USE DATABASE \"{self.database}\"')\n            # Then use schema (don't need to specify database again)\n            cursor.execute(f'USE SCHEMA \"{self.schema}\"')\n            cursor.close()\n        except Exception as e:\n            cursor.close()\n            connection.close()\n            error_msg = (\n                f\"Failed to set database/schema context: {e}\\n\"\n                f\"Make sure the database '{self.database}' and schema '{self.schema}' exist.\\n\"\n                f\"You may need to run the setup_snowflake_test.sql script first.\\n\"\n                f\"Current config - Database: {self.database}, Schema: {self.schema}, Stage: {self.stage_name}\"\n            )\n            raise IOError(error_msg)\n\n        return connection\n\n    def _ensure_connection(self):\n        \"\"\"Ensure the connection is active, reconnect if needed.\"\"\"\n        try:\n            self.connection.cursor().execute(\"SELECT 1\")\n        except Exception:\n            self.connection = self._create_connection()\n\n    def _get_stage_path(self, path: str) -&gt; str:\n        \"\"\"Convert a file path to a Snowflake stage path.\"\"\"\n        # Remove leading/trailing slashes and normalize\n        path = path.strip(\"/\")\n        # Stage paths use forward slashes and @stage_name/ prefix\n        return f\"@{self.stage_name}/{path}\"\n\n    def _normalize_path(self, path: str) -&gt; str:\n        \"\"\"Normalize path for Snowflake stage operations.\"\"\"\n        return path.strip(\"/\").replace(\"\\\\\", \"/\")\n\n    def read_file(self, path: str, encoding: Optional[str] = None) -&gt; Union[str, bytes]:\n        \"\"\"\n        Read file from Snowflake stage.\n\n        :param path: Path to the file in the stage\n        :param encoding: File encoding (optional)\n        :return: File contents as string or bytes\n        \"\"\"\n        self._ensure_connection()\n        cursor = self.connection.cursor(DictCursor)\n\n        try:\n            normalized_path = self._normalize_path(path)\n            stage_path = self._get_stage_path(normalized_path)\n\n            # Create temporary directory for download\n            temp_download_dir = os.path.join(self._temp_dir, \"downloads\")\n            os.makedirs(temp_download_dir, exist_ok=True)\n\n            # Download file from stage using GET command\n            # GET command: GET &lt;stage_path&gt; file://&lt;local_path&gt;\n            temp_dir_normalized = temp_download_dir.replace(\"\\\\\", \"/\")\n            if not temp_dir_normalized.endswith(\"/\"):\n                temp_dir_normalized += \"/\"\n\n            get_command = f\"GET {stage_path} 'file://{temp_dir_normalized}'\"\n            cursor.execute(get_command)\n\n            # Find the downloaded file (Snowflake may add prefixes/suffixes or preserve structure)\n            downloaded_files = []\n            for root, dirs, files in os.walk(temp_download_dir):\n                for f in files:\n                    file_path = os.path.join(root, f)\n                    # Check if this file matches our expected filename\n                    if os.path.basename(normalized_path) in f or normalized_path.endswith(f):\n                        downloaded_files.append(file_path)\n\n            if not downloaded_files:\n                raise FileNotFoundError(f\"File not found in stage: {path}\")\n\n            # Read the first matching file\n            downloaded_path = downloaded_files[0]\n            with open(downloaded_path, \"rb\") as f:\n                data = f.read()\n\n            # Clean up\n            os.remove(downloaded_path)\n            # Clean up empty directories\n            try:\n                if os.path.exists(temp_download_dir) and not os.listdir(temp_download_dir):\n                    os.rmdir(temp_download_dir)\n            except OSError:\n                pass\n\n            # Decode if encoding is specified\n            if encoding:\n                return data.decode(encoding)\n            return data\n\n        except Exception as e:\n            raise IOError(f\"Error reading file {path} from Snowflake stage: {e}\")\n        finally:\n            cursor.close()\n\n    def write_file(self, path: str, data: Union[bytes, str]) -&gt; None:\n        \"\"\"\n        Write file to Snowflake stage.\n\n        :param path: Destination path in the stage\n        :param data: File contents\n        \"\"\"\n        self._ensure_connection()\n        cursor = self.connection.cursor()\n\n        try:\n            # Convert to bytes if string\n            if isinstance(data, str):\n                binary_data = data.encode(\"utf-8\")\n            elif isinstance(data, bytes):\n                binary_data = data\n            else:\n                raise ValueError('Unsupported data type. Only \"bytes\" or \"string\" accepted')\n\n            normalized_path = self._normalize_path(path)\n\n            # Write to temporary file first\n            # Use the full path structure for the temp file to preserve directory structure\n            temp_file_path = os.path.join(self._temp_dir, normalized_path)\n            os.makedirs(os.path.dirname(temp_file_path), exist_ok=True)\n\n            with open(temp_file_path, \"wb\") as f:\n                f.write(binary_data)\n\n            # Upload to stage using PUT command\n            # Snowflake PUT requires the local file path and the target stage path\n            # Convert Windows paths to Unix-style for Snowflake\n            temp_file_normalized = os.path.abspath(temp_file_path).replace(\"\\\\\", \"/\")\n\n            # PUT command: PUT 'file://&lt;absolute_local_path&gt;' @stage_name/&lt;path&gt;\n            # The file will be stored at the specified path in the stage\n            stage_target = f\"@{self.stage_name}/\"\n            if \"/\" in normalized_path:\n                # Include directory structure in stage path\n                dir_path = os.path.dirname(normalized_path)\n                stage_target = f\"@{self.stage_name}/{dir_path}/\"\n\n            # Snowflake PUT syntax: PUT 'file://&lt;path&gt;' @stage/path\n            put_command = f\"PUT 'file://{temp_file_normalized}' {stage_target} OVERWRITE=TRUE AUTO_COMPRESS=FALSE\"\n            cursor.execute(put_command)\n\n            # Clean up temp file\n            if os.path.exists(temp_file_path):\n                os.remove(temp_file_path)\n                # Clean up empty directories if they were created\n                try:\n                    temp_dir = os.path.dirname(temp_file_path)\n                    if temp_dir != self._temp_dir and os.path.exists(temp_dir):\n                        os.rmdir(temp_dir)\n                except OSError:\n                    pass  # Directory not empty or other error, ignore\n\n        except Exception as e:\n            raise IOError(f\"Error writing file {path} to Snowflake stage: {e}\")\n        finally:\n            cursor.close()\n\n    def upload_file(self, file_path: str, stage_path: str):\n        \"\"\"\n        Uploads a single file from local filesystem to Snowflake stage.\n\n        :param file_path: Local file path\n        :param stage_path: Destination path in the stage\n        \"\"\"\n        try:\n            if not os.path.exists(file_path):\n                raise FileNotFoundError(f\"Local file not found: {file_path}\")\n\n            # Read the file\n            with open(file_path, \"rb\") as f:\n                data = f.read()\n\n            # Write to stage using write_file\n            self.write_file(stage_path, data)\n            self.logger.info(f\"Uploaded {file_path} to {stage_path}\")\n        except Exception as e:\n            self.logger.error(f\"Failed to upload {file_path}: {e}\")\n            raise\n\n    def upload_directory(self, dir_path: str, stage_dir_path: str):\n        \"\"\"\n        Uploads all files from a local directory to Snowflake stage.\n\n        :param dir_path: Local directory path\n        :param stage_dir_path: Destination directory path in the stage\n        \"\"\"\n        if not os.path.isdir(dir_path):\n            raise NotADirectoryError(f\"Local directory not found: {dir_path}\")\n\n        for root, dirs, files in os.walk(dir_path):\n            for file in files:\n                local_file_path = os.path.join(root, file)\n                relative_path = os.path.relpath(local_file_path, dir_path)\n                # Normalize path separators for stage\n                stage_file_path = os.path.join(stage_dir_path, relative_path).replace(\"\\\\\", \"/\")\n\n                self.upload_file(local_file_path, stage_file_path)\n\n    def download_directory(self, stage_dir_path: str, local_dir_path: str):\n        \"\"\"\n        Downloads all files from a Snowflake stage directory to a local directory.\n\n        :param stage_dir_path: Source directory path in the stage\n        :param local_dir_path: Destination local directory path\n        \"\"\"\n        try:\n            # Ensure the local directory exists\n            os.makedirs(local_dir_path, exist_ok=True)\n\n            # List all files in the stage directory\n            files = self.list_files(stage_dir_path)\n\n            for file_path in files:\n                # Get the relative path from the stage directory\n                if stage_dir_path:\n                    if file_path.startswith(stage_dir_path):\n                        relative_path = file_path[len(stage_dir_path):].lstrip(\"/\")\n                    else:\n                        # If file_path doesn't start with stage_dir_path, use it as is\n                        relative_path = os.path.basename(file_path)\n                else:\n                    relative_path = file_path\n\n                # Construct the local file path\n                local_file_path = os.path.join(local_dir_path, relative_path)\n                # Create directories if needed\n                os.makedirs(os.path.dirname(local_file_path), exist_ok=True)\n\n                # Download the file\n                data = self.read_file(file_path)\n                with open(local_file_path, \"wb\") as f:\n                    if isinstance(data, str):\n                        f.write(data.encode(\"utf-8\"))\n                    else:\n                        f.write(data)\n\n            self.logger.info(f\"Downloaded directory {stage_dir_path} to {local_dir_path}\")\n        except Exception as e:\n            self.logger.error(f\"Failed to download directory {stage_dir_path}: {e}\")\n            raise\n\n    def copy_directory(self, source_dir: str, destination_dir: str):\n        \"\"\"\n        Copies all files from a source directory to a destination directory within the stage.\n\n        :param source_dir: Source directory path in the stage\n        :param destination_dir: Destination directory path in the stage\n        \"\"\"\n        try:\n            # Normalize directory paths\n            source_dir = source_dir.rstrip(\"/\")\n            destination_dir = destination_dir.rstrip(\"/\")\n\n            # List all files in the source directory\n            files = self.list_files(source_dir)\n\n            for file_path in files:\n                # Get relative path from source directory\n                if source_dir:\n                    if file_path.startswith(source_dir):\n                        relative_path = file_path[len(source_dir):].lstrip(\"/\")\n                    else:\n                        relative_path = os.path.basename(file_path)\n                else:\n                    relative_path = file_path\n\n                # Construct the destination file path\n                if destination_dir:\n                    dest_file_path = f\"{destination_dir}/{relative_path}\".replace(\"//\", \"/\")\n                else:\n                    dest_file_path = relative_path\n\n                # Copy each file\n                self.copy_file(file_path, dest_file_path, overwrite=True)\n\n            self.logger.info(f\"Copied directory from {source_dir} to {destination_dir}\")\n        except Exception as e:\n            self.logger.error(f\"Failed to copy directory {source_dir}: {e}\")\n            raise\n\n    def copy_file(\n            self, source_path: str, destination_path: str, overwrite: bool = False\n    ):\n        \"\"\"\n        Copies a single file within the Snowflake stage.\n\n        :param source_path: Source file path in the stage\n        :param destination_path: Destination file path in the stage\n        :param overwrite: If True, overwrite the destination file if it already exists\n        \"\"\"\n        try:\n            if not self.file_exists(source_path):\n                raise FileNotFoundError(f\"Source file not found: {source_path}\")\n\n            if self.file_exists(destination_path) and not overwrite:\n                raise FileExistsError(\n                    f\"Destination file already exists and overwrite is False: {destination_path}\"\n                )\n\n            # Read from source and write to destination\n            data = self.read_file(source_path)\n            self.write_file(destination_path, data)\n\n            self.logger.info(f\"Copied file from {source_path} to {destination_path}\")\n        except Exception as e:\n            self.logger.error(f\"Failed to copy file {source_path}: {e}\")\n            raise\n\n    def exists(self, path: str) -&gt; bool:\n        \"\"\"Check if a path exists (file or directory).\"\"\"\n        return self.file_exists(path) or self.is_dir(path)\n\n    def file_exists(self, path: str) -&gt; bool:\n        \"\"\"\n        Check if a file exists in the Snowflake stage.\n\n        :param path: Path to check\n        :return: True if file exists, False otherwise\n        \"\"\"\n        self._ensure_connection()\n        cursor = self.connection.cursor(DictCursor)\n\n        try:\n            normalized_path = self._normalize_path(path)\n            stage_path = self._get_stage_path(normalized_path)\n\n            # List files in stage with the given path pattern\n            list_command = f\"LIST {stage_path}\"\n            cursor.execute(list_command)\n            results = cursor.fetchall()\n\n            # Check if exact file exists\n            for result in results:\n                if result[\"name\"].endswith(normalized_path) or result[\"name\"] == stage_path:\n                    return True\n\n            return False\n\n        except Exception as e:\n            self.logger.warning(f\"Error checking file existence {path}: {e}\")\n            return False\n        finally:\n            cursor.close()\n\n    def file_size(self, path: str) -&gt; float:\n        \"\"\"\n        Get the size of a file in kilobytes.\n\n        :param path: File path in the stage\n        :return: File size in kilobytes\n        \"\"\"\n        self._ensure_connection()\n        cursor = self.connection.cursor(DictCursor)\n\n        try:\n            normalized_path = self._normalize_path(path)\n            stage_path = self._get_stage_path(normalized_path)\n\n            # LIST command returns file metadata including size\n            list_command = f\"LIST {stage_path}\"\n            cursor.execute(list_command)\n            results = cursor.fetchall()\n\n            # Find the matching file and get its size\n            for result in results:\n                file_path = result[\"name\"]\n                if normalized_path in file_path.lower() or file_path.endswith(normalized_path):\n                    # Size is in bytes, convert to kilobytes\n                    size_bytes = result.get(\"size\", 0)\n                    size_kb = size_bytes / 1024.0\n                    return size_kb\n\n            raise FileNotFoundError(f\"File not found: {path}\")\n        except Exception as e:\n            self.logger.error(f\"Error getting file size for {path}: {e}\")\n            raise\n        finally:\n            cursor.close()\n\n    def list_files(self, path: str) -&gt; List[str]:\n        \"\"\"\n        List all files in a directory within the Snowflake stage.\n\n        :param path: Directory path to list\n        :return: List of file paths\n        \"\"\"\n        self._ensure_connection()\n        cursor = self.connection.cursor(DictCursor)\n\n        try:\n            normalized_path = self._normalize_path(path)\n            stage_path = self._get_stage_path(normalized_path)\n\n            # List files in stage\n            list_command = f\"LIST {stage_path}\"\n            cursor.execute(list_command)\n            results = cursor.fetchall()\n\n            # Extract file paths relative to the base stage path\n            files = []\n            for result in results:\n                file_path = result[\"name\"]\n                # Snowflake LIST returns names in lowercase without @ symbol\n                # Remove stage prefix to get relative path\n                # Check both @stage_name/ and lowercase stage_name/ formats\n                stage_prefixes = [\n                    f\"@{self.stage_name}/\",\n                    f\"{self.stage_name.lower()}/\",\n                    f\"@{self.stage_name.lower()}/\",\n                ]\n\n                for prefix in stage_prefixes:\n                    if file_path.startswith(prefix):\n                        relative_path = file_path[len(prefix):]\n                        files.append(relative_path)\n                        break\n                else:\n                    # If no prefix matches, try to extract path after stage name\n                    # Sometimes stage name might be in different case\n                    stage_name_lower = self.stage_name.lower()\n                    if stage_name_lower in file_path.lower():\n                        # Find the position after the stage name\n                        idx = file_path.lower().find(stage_name_lower)\n                        if idx != -1:\n                            # Get everything after stage name and '/'\n                            after_stage = file_path[idx + len(stage_name_lower):].lstrip(\"/\")\n                            if after_stage.startswith(normalized_path):\n                                relative_path = after_stage\n                                files.append(relative_path)\n\n            return files\n\n        except Exception as e:\n            self.logger.warning(f\"Error listing files in {path}: {e}\")\n            return []\n        finally:\n            cursor.close()\n\n    def walk(self, top: str) -&gt; Generator[Tuple[str, List[str], List[str]], None, None]:\n        \"\"\"\n        Walk through directory tree in Snowflake stage, similar to os.walk().\n\n        :param top: Starting directory for the walk\n        :return: Generator yielding tuples of (dirpath, dirnames, filenames)\n        \"\"\"\n        try:\n            normalized_top = self._normalize_path(top)\n\n            # Use list_files to get all files (it handles path parsing correctly)\n            all_files = self.list_files(normalized_top)\n\n            # Organize into directory structure\n            dirs = {}\n\n            for file_path in all_files:\n                # Ensure we're working with paths relative to the top\n                if normalized_top and not file_path.startswith(normalized_top):\n                    continue\n\n                # Get relative path from top\n                if normalized_top and file_path.startswith(normalized_top):\n                    relative_path = file_path[len(normalized_top):].lstrip(\"/\")\n                else:\n                    relative_path = file_path\n\n                if not relative_path:\n                    continue\n\n                # Get directory and filename\n                if \"/\" in relative_path:\n                    dir_path, filename = os.path.split(relative_path)\n                    full_dir_path = f\"{normalized_top}/{dir_path}\" if normalized_top else dir_path\n                    if full_dir_path not in dirs:\n                        dirs[full_dir_path] = []\n                    dirs[full_dir_path].append(filename)\n                else:\n                    # File in root of the top directory\n                    if normalized_top not in dirs:\n                        dirs[normalized_top] = []\n                    dirs[normalized_top].append(relative_path)\n\n            # Yield results in os.walk format\n            for dir_path, files in dirs.items():\n                # Extract subdirectories (simplified - Snowflake stages are flat)\n                subdirs = []\n                yield (dir_path, subdirs, files)\n\n        except Exception as e:\n            self.logger.warning(f\"Error walking directory {top}: {e}\")\n            yield (top, [], [])\n\n    def list_directories(self, path: str) -&gt; List[str]:\n        \"\"\"\n        List only directory names (not files) from a given path in the stage.\n\n        :param path: Directory path to list\n        :return: List of directory names\n        \"\"\"\n        normalized_path = self._normalize_path(path)\n        files = self.list_files(normalized_path)\n\n        directories = set()\n\n        for file_path in files:\n            # Get relative path from the search path\n            if normalized_path:\n                if file_path.startswith(normalized_path):\n                    relative_path = file_path[len(normalized_path):].lstrip(\"/\")\n                else:\n                    continue\n            else:\n                relative_path = file_path\n\n            # Skip if empty\n            if not relative_path:\n                continue\n\n            # If there's a \"/\" in the relative path, it means there's a subdirectory\n            if \"/\" in relative_path:\n                # Get the first directory name\n                dir_name = relative_path.split(\"/\")[0]\n                directories.add(dir_name)\n\n        return sorted(list(directories))\n\n    @contextlib.contextmanager\n    def open(self, path: str, mode: str = \"r\"):\n        \"\"\"\n        Context manager for file operations.\n\n        :param path: File path in Snowflake stage\n        :param mode: File open mode (r, rb, w, wb)\n        \"\"\"\n        if mode == \"w\":\n            file = io.StringIO()\n            yield file\n            self.write_file(path, file.getvalue())\n\n        elif mode == \"wb\":\n            file = io.BytesIO()\n            yield file\n            self.write_file(path, file.getvalue())\n\n        elif mode == \"r\":\n            data = self.read_file(path, encoding=\"UTF-8\")\n            file = io.StringIO(data)\n            yield file\n\n        elif mode == \"rb\":\n            data = self.read_file(path)\n            file = io.BytesIO(data)\n            yield file\n\n        else:\n            raise ValueError(f\"Unsupported mode: {mode}\")\n\n    def get_file_metadata(self, path: str) -&gt; dict:\n        \"\"\"\n        Retrieve comprehensive file metadata from Snowflake stage.\n\n        :param path: File path in the stage\n        :return: File metadata dictionary\n        \"\"\"\n        self._ensure_connection()\n        cursor = self.connection.cursor(DictCursor)\n\n        try:\n            normalized_path = self._normalize_path(path)\n            stage_path = self._get_stage_path(normalized_path)\n\n            # LIST command returns file metadata\n            list_command = f\"LIST {stage_path}\"\n            cursor.execute(list_command)\n            results = cursor.fetchall()\n\n            # Find the matching file\n            for result in results:\n                file_path = result[\"name\"]\n                if normalized_path in file_path.lower() or file_path.endswith(normalized_path):\n                    return {\n                        \"name\": path,\n                        \"size_bytes\": result.get(\"size\", 0),\n                        \"last_modified\": result.get(\"last_modified\"),\n                        \"md5\": result.get(\"md5\"),\n                    }\n\n            raise FileNotFoundError(f\"File not found: {path}\")\n        except Exception as e:\n            self.logger.error(f\"Error getting file metadata for {path}: {e}\")\n            raise\n        finally:\n            cursor.close()\n\n    def is_file(self, path: str) -&gt; bool:\n        \"\"\"Check if path points to a file.\"\"\"\n        return self.file_exists(path)\n\n    def is_dir(self, path: str) -&gt; bool:\n        \"\"\"Check if path points to a directory.\"\"\"\n        # First check if it's actually a file (exact match)\n        if self.file_exists(path):\n            return False\n\n        # In Snowflake stages, directories are conceptual\n        # Check if there are files with this path prefix\n        normalized_path = self._normalize_path(path)\n        files = self.list_files(normalized_path)\n\n        # Filter out files that are exact matches (they're files, not directories)\n        exact_match = any(f == normalized_path or f == path for f in files)\n        if exact_match:\n            return False\n\n        return len(files) &gt; 0\n\n    def rmdir(self, dir: str) -&gt; None:\n        \"\"\"\n        Remove a directory and all its contents from the Snowflake stage.\n\n        :param dir: Path to the directory to remove\n        \"\"\"\n        self._ensure_connection()\n        cursor = self.connection.cursor()\n\n        try:\n            normalized_dir = self._normalize_path(dir)\n            stage_path = self._get_stage_path(normalized_dir)\n\n            # Remove all files in the directory\n            remove_command = f\"REMOVE {stage_path}\"\n            cursor.execute(remove_command)\n\n        except Exception as e:\n            raise IOError(f\"Error removing directory {dir}: {e}\")\n        finally:\n            cursor.close()\n\n    def mkdir(self, path: str, exist_ok: bool = False) -&gt; None:\n        \"\"\"\n        Create a directory in Snowflake stage.\n\n        In Snowflake stages, directories are created implicitly when files are uploaded.\n        This method creates a placeholder file if the directory doesn't exist.\n\n        :param path: Path of the directory to create\n        :param exist_ok: If False, raise an error if the directory already exists\n        \"\"\"\n        # Check if directory already exists\n        if self.is_dir(path) and not exist_ok:\n            raise FileExistsError(f\"Directory {path} already exists\")\n\n        # Create a placeholder file to ensure directory exists\n        placeholder_path = os.path.join(path, \".placeholder\").replace(\"\\\\\", \"/\")\n        if not self.file_exists(placeholder_path):\n            self.write_file(placeholder_path, b\"Placeholder file for directory\")\n\n    def remove(self, path: str) -&gt; None:\n        \"\"\"\n        Remove a file from the Snowflake stage.\n\n        :param path: Path to the file to remove\n        \"\"\"\n        self._ensure_connection()\n        cursor = self.connection.cursor()\n\n        try:\n            normalized_path = self._normalize_path(path)\n            stage_path = self._get_stage_path(normalized_path)\n\n            remove_command = f\"REMOVE {stage_path}\"\n            cursor.execute(remove_command)\n\n        except Exception as e:\n            raise IOError(f\"Error removing file {path}: {e}\")\n        finally:\n            cursor.close()\n\n    def rename(\n        self,\n        source_path: str,\n        destination_path: str,\n        overwrite: bool = False,\n        delete_source: bool = True,\n    ) -&gt; None:\n        \"\"\"\n        Rename (move) a single file by copying to the new path and deleting the source.\n\n        :param source_path: Existing file path in the stage\n        :param destination_path: Target file path in the stage\n        :param overwrite: Overwrite destination if it already exists\n        :param delete_source: Delete original after successful copy\n        \"\"\"\n        if not self.file_exists(source_path):\n            raise FileNotFoundError(f\"Source file not found: {source_path}\")\n\n        if self.file_exists(destination_path) and not overwrite:\n            raise FileExistsError(\n                f\"Destination already exists and overwrite is False: {destination_path}\"\n            )\n\n        # Copy file to new location\n        self.copy_file(source_path, destination_path, overwrite=overwrite)\n\n        # Delete source if requested\n        if delete_source:\n            self.remove(source_path)\n\n    def close(self):\n        \"\"\"Close the Snowflake connection.\"\"\"\n        if self.connection:\n            self.connection.close()\n\n    def __enter__(self):\n        \"\"\"Context manager entry.\"\"\"\n        return self\n\n    def __exit__(self, exc_type, exc_val, exc_tb):\n        \"\"\"Context manager exit.\"\"\"\n        self.close()\n</code></pre>"},{"location":"api/core/#gigaspatial.core.io.snowflake_data_store.SnowflakeDataStore.__enter__","title":"<code>__enter__()</code>","text":"<p>Context manager entry.</p> Source code in <code>gigaspatial/core/io/snowflake_data_store.py</code> <pre><code>def __enter__(self):\n    \"\"\"Context manager entry.\"\"\"\n    return self\n</code></pre>"},{"location":"api/core/#gigaspatial.core.io.snowflake_data_store.SnowflakeDataStore.__exit__","title":"<code>__exit__(exc_type, exc_val, exc_tb)</code>","text":"<p>Context manager exit.</p> Source code in <code>gigaspatial/core/io/snowflake_data_store.py</code> <pre><code>def __exit__(self, exc_type, exc_val, exc_tb):\n    \"\"\"Context manager exit.\"\"\"\n    self.close()\n</code></pre>"},{"location":"api/core/#gigaspatial.core.io.snowflake_data_store.SnowflakeDataStore.__init__","title":"<code>__init__(account=config.SNOWFLAKE_ACCOUNT, user=config.SNOWFLAKE_USER, password=config.SNOWFLAKE_PASSWORD, warehouse=config.SNOWFLAKE_WAREHOUSE, database=config.SNOWFLAKE_DATABASE, schema=config.SNOWFLAKE_SCHEMA, stage_name=config.SNOWFLAKE_STAGE_NAME)</code>","text":"<p>Create a new instance of SnowflakeDataStore.</p> <p>:param account: Snowflake account identifier :param user: Snowflake username :param password: Snowflake password :param warehouse: Snowflake warehouse name :param database: Snowflake database name :param schema: Snowflake schema name :param stage_name: Name of the Snowflake stage to use for file storage</p> Source code in <code>gigaspatial/core/io/snowflake_data_store.py</code> <pre><code>def __init__(\n    self,\n    account: str = config.SNOWFLAKE_ACCOUNT,\n    user: str = config.SNOWFLAKE_USER,\n    password: str = config.SNOWFLAKE_PASSWORD,\n    warehouse: str = config.SNOWFLAKE_WAREHOUSE,\n    database: str = config.SNOWFLAKE_DATABASE,\n    schema: str = config.SNOWFLAKE_SCHEMA,\n    stage_name: str = config.SNOWFLAKE_STAGE_NAME,\n):\n    \"\"\"\n    Create a new instance of SnowflakeDataStore.\n\n    :param account: Snowflake account identifier\n    :param user: Snowflake username\n    :param password: Snowflake password\n    :param warehouse: Snowflake warehouse name\n    :param database: Snowflake database name\n    :param schema: Snowflake schema name\n    :param stage_name: Name of the Snowflake stage to use for file storage\n    \"\"\"\n    if not all([account, user, password, warehouse, database, schema, stage_name]):\n        raise ValueError(\n            \"Snowflake connection parameters (account, user, password, warehouse, \"\n            \"database, schema, stage_name) must be provided via config or constructor.\"\n        )\n\n    self.account = account\n    self.user = user\n    self.password = password\n    self.warehouse = warehouse\n    self.database = database\n    self.schema = schema\n    self.stage_name = stage_name\n\n    # Create connection\n    self.connection = self._create_connection()\n    self.logger = config.get_logger(self.__class__.__name__)\n\n    # Temporary directory for file operations\n    self._temp_dir = tempfile.mkdtemp()\n</code></pre>"},{"location":"api/core/#gigaspatial.core.io.snowflake_data_store.SnowflakeDataStore.close","title":"<code>close()</code>","text":"<p>Close the Snowflake connection.</p> Source code in <code>gigaspatial/core/io/snowflake_data_store.py</code> <pre><code>def close(self):\n    \"\"\"Close the Snowflake connection.\"\"\"\n    if self.connection:\n        self.connection.close()\n</code></pre>"},{"location":"api/core/#gigaspatial.core.io.snowflake_data_store.SnowflakeDataStore.copy_directory","title":"<code>copy_directory(source_dir, destination_dir)</code>","text":"<p>Copies all files from a source directory to a destination directory within the stage.</p> <p>:param source_dir: Source directory path in the stage :param destination_dir: Destination directory path in the stage</p> Source code in <code>gigaspatial/core/io/snowflake_data_store.py</code> <pre><code>def copy_directory(self, source_dir: str, destination_dir: str):\n    \"\"\"\n    Copies all files from a source directory to a destination directory within the stage.\n\n    :param source_dir: Source directory path in the stage\n    :param destination_dir: Destination directory path in the stage\n    \"\"\"\n    try:\n        # Normalize directory paths\n        source_dir = source_dir.rstrip(\"/\")\n        destination_dir = destination_dir.rstrip(\"/\")\n\n        # List all files in the source directory\n        files = self.list_files(source_dir)\n\n        for file_path in files:\n            # Get relative path from source directory\n            if source_dir:\n                if file_path.startswith(source_dir):\n                    relative_path = file_path[len(source_dir):].lstrip(\"/\")\n                else:\n                    relative_path = os.path.basename(file_path)\n            else:\n                relative_path = file_path\n\n            # Construct the destination file path\n            if destination_dir:\n                dest_file_path = f\"{destination_dir}/{relative_path}\".replace(\"//\", \"/\")\n            else:\n                dest_file_path = relative_path\n\n            # Copy each file\n            self.copy_file(file_path, dest_file_path, overwrite=True)\n\n        self.logger.info(f\"Copied directory from {source_dir} to {destination_dir}\")\n    except Exception as e:\n        self.logger.error(f\"Failed to copy directory {source_dir}: {e}\")\n        raise\n</code></pre>"},{"location":"api/core/#gigaspatial.core.io.snowflake_data_store.SnowflakeDataStore.copy_file","title":"<code>copy_file(source_path, destination_path, overwrite=False)</code>","text":"<p>Copies a single file within the Snowflake stage.</p> <p>:param source_path: Source file path in the stage :param destination_path: Destination file path in the stage :param overwrite: If True, overwrite the destination file if it already exists</p> Source code in <code>gigaspatial/core/io/snowflake_data_store.py</code> <pre><code>def copy_file(\n        self, source_path: str, destination_path: str, overwrite: bool = False\n):\n    \"\"\"\n    Copies a single file within the Snowflake stage.\n\n    :param source_path: Source file path in the stage\n    :param destination_path: Destination file path in the stage\n    :param overwrite: If True, overwrite the destination file if it already exists\n    \"\"\"\n    try:\n        if not self.file_exists(source_path):\n            raise FileNotFoundError(f\"Source file not found: {source_path}\")\n\n        if self.file_exists(destination_path) and not overwrite:\n            raise FileExistsError(\n                f\"Destination file already exists and overwrite is False: {destination_path}\"\n            )\n\n        # Read from source and write to destination\n        data = self.read_file(source_path)\n        self.write_file(destination_path, data)\n\n        self.logger.info(f\"Copied file from {source_path} to {destination_path}\")\n    except Exception as e:\n        self.logger.error(f\"Failed to copy file {source_path}: {e}\")\n        raise\n</code></pre>"},{"location":"api/core/#gigaspatial.core.io.snowflake_data_store.SnowflakeDataStore.download_directory","title":"<code>download_directory(stage_dir_path, local_dir_path)</code>","text":"<p>Downloads all files from a Snowflake stage directory to a local directory.</p> <p>:param stage_dir_path: Source directory path in the stage :param local_dir_path: Destination local directory path</p> Source code in <code>gigaspatial/core/io/snowflake_data_store.py</code> <pre><code>def download_directory(self, stage_dir_path: str, local_dir_path: str):\n    \"\"\"\n    Downloads all files from a Snowflake stage directory to a local directory.\n\n    :param stage_dir_path: Source directory path in the stage\n    :param local_dir_path: Destination local directory path\n    \"\"\"\n    try:\n        # Ensure the local directory exists\n        os.makedirs(local_dir_path, exist_ok=True)\n\n        # List all files in the stage directory\n        files = self.list_files(stage_dir_path)\n\n        for file_path in files:\n            # Get the relative path from the stage directory\n            if stage_dir_path:\n                if file_path.startswith(stage_dir_path):\n                    relative_path = file_path[len(stage_dir_path):].lstrip(\"/\")\n                else:\n                    # If file_path doesn't start with stage_dir_path, use it as is\n                    relative_path = os.path.basename(file_path)\n            else:\n                relative_path = file_path\n\n            # Construct the local file path\n            local_file_path = os.path.join(local_dir_path, relative_path)\n            # Create directories if needed\n            os.makedirs(os.path.dirname(local_file_path), exist_ok=True)\n\n            # Download the file\n            data = self.read_file(file_path)\n            with open(local_file_path, \"wb\") as f:\n                if isinstance(data, str):\n                    f.write(data.encode(\"utf-8\"))\n                else:\n                    f.write(data)\n\n        self.logger.info(f\"Downloaded directory {stage_dir_path} to {local_dir_path}\")\n    except Exception as e:\n        self.logger.error(f\"Failed to download directory {stage_dir_path}: {e}\")\n        raise\n</code></pre>"},{"location":"api/core/#gigaspatial.core.io.snowflake_data_store.SnowflakeDataStore.exists","title":"<code>exists(path)</code>","text":"<p>Check if a path exists (file or directory).</p> Source code in <code>gigaspatial/core/io/snowflake_data_store.py</code> <pre><code>def exists(self, path: str) -&gt; bool:\n    \"\"\"Check if a path exists (file or directory).\"\"\"\n    return self.file_exists(path) or self.is_dir(path)\n</code></pre>"},{"location":"api/core/#gigaspatial.core.io.snowflake_data_store.SnowflakeDataStore.file_exists","title":"<code>file_exists(path)</code>","text":"<p>Check if a file exists in the Snowflake stage.</p> <p>:param path: Path to check :return: True if file exists, False otherwise</p> Source code in <code>gigaspatial/core/io/snowflake_data_store.py</code> <pre><code>def file_exists(self, path: str) -&gt; bool:\n    \"\"\"\n    Check if a file exists in the Snowflake stage.\n\n    :param path: Path to check\n    :return: True if file exists, False otherwise\n    \"\"\"\n    self._ensure_connection()\n    cursor = self.connection.cursor(DictCursor)\n\n    try:\n        normalized_path = self._normalize_path(path)\n        stage_path = self._get_stage_path(normalized_path)\n\n        # List files in stage with the given path pattern\n        list_command = f\"LIST {stage_path}\"\n        cursor.execute(list_command)\n        results = cursor.fetchall()\n\n        # Check if exact file exists\n        for result in results:\n            if result[\"name\"].endswith(normalized_path) or result[\"name\"] == stage_path:\n                return True\n\n        return False\n\n    except Exception as e:\n        self.logger.warning(f\"Error checking file existence {path}: {e}\")\n        return False\n    finally:\n        cursor.close()\n</code></pre>"},{"location":"api/core/#gigaspatial.core.io.snowflake_data_store.SnowflakeDataStore.file_size","title":"<code>file_size(path)</code>","text":"<p>Get the size of a file in kilobytes.</p> <p>:param path: File path in the stage :return: File size in kilobytes</p> Source code in <code>gigaspatial/core/io/snowflake_data_store.py</code> <pre><code>def file_size(self, path: str) -&gt; float:\n    \"\"\"\n    Get the size of a file in kilobytes.\n\n    :param path: File path in the stage\n    :return: File size in kilobytes\n    \"\"\"\n    self._ensure_connection()\n    cursor = self.connection.cursor(DictCursor)\n\n    try:\n        normalized_path = self._normalize_path(path)\n        stage_path = self._get_stage_path(normalized_path)\n\n        # LIST command returns file metadata including size\n        list_command = f\"LIST {stage_path}\"\n        cursor.execute(list_command)\n        results = cursor.fetchall()\n\n        # Find the matching file and get its size\n        for result in results:\n            file_path = result[\"name\"]\n            if normalized_path in file_path.lower() or file_path.endswith(normalized_path):\n                # Size is in bytes, convert to kilobytes\n                size_bytes = result.get(\"size\", 0)\n                size_kb = size_bytes / 1024.0\n                return size_kb\n\n        raise FileNotFoundError(f\"File not found: {path}\")\n    except Exception as e:\n        self.logger.error(f\"Error getting file size for {path}: {e}\")\n        raise\n    finally:\n        cursor.close()\n</code></pre>"},{"location":"api/core/#gigaspatial.core.io.snowflake_data_store.SnowflakeDataStore.get_file_metadata","title":"<code>get_file_metadata(path)</code>","text":"<p>Retrieve comprehensive file metadata from Snowflake stage.</p> <p>:param path: File path in the stage :return: File metadata dictionary</p> Source code in <code>gigaspatial/core/io/snowflake_data_store.py</code> <pre><code>def get_file_metadata(self, path: str) -&gt; dict:\n    \"\"\"\n    Retrieve comprehensive file metadata from Snowflake stage.\n\n    :param path: File path in the stage\n    :return: File metadata dictionary\n    \"\"\"\n    self._ensure_connection()\n    cursor = self.connection.cursor(DictCursor)\n\n    try:\n        normalized_path = self._normalize_path(path)\n        stage_path = self._get_stage_path(normalized_path)\n\n        # LIST command returns file metadata\n        list_command = f\"LIST {stage_path}\"\n        cursor.execute(list_command)\n        results = cursor.fetchall()\n\n        # Find the matching file\n        for result in results:\n            file_path = result[\"name\"]\n            if normalized_path in file_path.lower() or file_path.endswith(normalized_path):\n                return {\n                    \"name\": path,\n                    \"size_bytes\": result.get(\"size\", 0),\n                    \"last_modified\": result.get(\"last_modified\"),\n                    \"md5\": result.get(\"md5\"),\n                }\n\n        raise FileNotFoundError(f\"File not found: {path}\")\n    except Exception as e:\n        self.logger.error(f\"Error getting file metadata for {path}: {e}\")\n        raise\n    finally:\n        cursor.close()\n</code></pre>"},{"location":"api/core/#gigaspatial.core.io.snowflake_data_store.SnowflakeDataStore.is_dir","title":"<code>is_dir(path)</code>","text":"<p>Check if path points to a directory.</p> Source code in <code>gigaspatial/core/io/snowflake_data_store.py</code> <pre><code>def is_dir(self, path: str) -&gt; bool:\n    \"\"\"Check if path points to a directory.\"\"\"\n    # First check if it's actually a file (exact match)\n    if self.file_exists(path):\n        return False\n\n    # In Snowflake stages, directories are conceptual\n    # Check if there are files with this path prefix\n    normalized_path = self._normalize_path(path)\n    files = self.list_files(normalized_path)\n\n    # Filter out files that are exact matches (they're files, not directories)\n    exact_match = any(f == normalized_path or f == path for f in files)\n    if exact_match:\n        return False\n\n    return len(files) &gt; 0\n</code></pre>"},{"location":"api/core/#gigaspatial.core.io.snowflake_data_store.SnowflakeDataStore.is_file","title":"<code>is_file(path)</code>","text":"<p>Check if path points to a file.</p> Source code in <code>gigaspatial/core/io/snowflake_data_store.py</code> <pre><code>def is_file(self, path: str) -&gt; bool:\n    \"\"\"Check if path points to a file.\"\"\"\n    return self.file_exists(path)\n</code></pre>"},{"location":"api/core/#gigaspatial.core.io.snowflake_data_store.SnowflakeDataStore.list_directories","title":"<code>list_directories(path)</code>","text":"<p>List only directory names (not files) from a given path in the stage.</p> <p>:param path: Directory path to list :return: List of directory names</p> Source code in <code>gigaspatial/core/io/snowflake_data_store.py</code> <pre><code>def list_directories(self, path: str) -&gt; List[str]:\n    \"\"\"\n    List only directory names (not files) from a given path in the stage.\n\n    :param path: Directory path to list\n    :return: List of directory names\n    \"\"\"\n    normalized_path = self._normalize_path(path)\n    files = self.list_files(normalized_path)\n\n    directories = set()\n\n    for file_path in files:\n        # Get relative path from the search path\n        if normalized_path:\n            if file_path.startswith(normalized_path):\n                relative_path = file_path[len(normalized_path):].lstrip(\"/\")\n            else:\n                continue\n        else:\n            relative_path = file_path\n\n        # Skip if empty\n        if not relative_path:\n            continue\n\n        # If there's a \"/\" in the relative path, it means there's a subdirectory\n        if \"/\" in relative_path:\n            # Get the first directory name\n            dir_name = relative_path.split(\"/\")[0]\n            directories.add(dir_name)\n\n    return sorted(list(directories))\n</code></pre>"},{"location":"api/core/#gigaspatial.core.io.snowflake_data_store.SnowflakeDataStore.list_files","title":"<code>list_files(path)</code>","text":"<p>List all files in a directory within the Snowflake stage.</p> <p>:param path: Directory path to list :return: List of file paths</p> Source code in <code>gigaspatial/core/io/snowflake_data_store.py</code> <pre><code>def list_files(self, path: str) -&gt; List[str]:\n    \"\"\"\n    List all files in a directory within the Snowflake stage.\n\n    :param path: Directory path to list\n    :return: List of file paths\n    \"\"\"\n    self._ensure_connection()\n    cursor = self.connection.cursor(DictCursor)\n\n    try:\n        normalized_path = self._normalize_path(path)\n        stage_path = self._get_stage_path(normalized_path)\n\n        # List files in stage\n        list_command = f\"LIST {stage_path}\"\n        cursor.execute(list_command)\n        results = cursor.fetchall()\n\n        # Extract file paths relative to the base stage path\n        files = []\n        for result in results:\n            file_path = result[\"name\"]\n            # Snowflake LIST returns names in lowercase without @ symbol\n            # Remove stage prefix to get relative path\n            # Check both @stage_name/ and lowercase stage_name/ formats\n            stage_prefixes = [\n                f\"@{self.stage_name}/\",\n                f\"{self.stage_name.lower()}/\",\n                f\"@{self.stage_name.lower()}/\",\n            ]\n\n            for prefix in stage_prefixes:\n                if file_path.startswith(prefix):\n                    relative_path = file_path[len(prefix):]\n                    files.append(relative_path)\n                    break\n            else:\n                # If no prefix matches, try to extract path after stage name\n                # Sometimes stage name might be in different case\n                stage_name_lower = self.stage_name.lower()\n                if stage_name_lower in file_path.lower():\n                    # Find the position after the stage name\n                    idx = file_path.lower().find(stage_name_lower)\n                    if idx != -1:\n                        # Get everything after stage name and '/'\n                        after_stage = file_path[idx + len(stage_name_lower):].lstrip(\"/\")\n                        if after_stage.startswith(normalized_path):\n                            relative_path = after_stage\n                            files.append(relative_path)\n\n        return files\n\n    except Exception as e:\n        self.logger.warning(f\"Error listing files in {path}: {e}\")\n        return []\n    finally:\n        cursor.close()\n</code></pre>"},{"location":"api/core/#gigaspatial.core.io.snowflake_data_store.SnowflakeDataStore.mkdir","title":"<code>mkdir(path, exist_ok=False)</code>","text":"<p>Create a directory in Snowflake stage.</p> <p>In Snowflake stages, directories are created implicitly when files are uploaded. This method creates a placeholder file if the directory doesn't exist.</p> <p>:param path: Path of the directory to create :param exist_ok: If False, raise an error if the directory already exists</p> Source code in <code>gigaspatial/core/io/snowflake_data_store.py</code> <pre><code>def mkdir(self, path: str, exist_ok: bool = False) -&gt; None:\n    \"\"\"\n    Create a directory in Snowflake stage.\n\n    In Snowflake stages, directories are created implicitly when files are uploaded.\n    This method creates a placeholder file if the directory doesn't exist.\n\n    :param path: Path of the directory to create\n    :param exist_ok: If False, raise an error if the directory already exists\n    \"\"\"\n    # Check if directory already exists\n    if self.is_dir(path) and not exist_ok:\n        raise FileExistsError(f\"Directory {path} already exists\")\n\n    # Create a placeholder file to ensure directory exists\n    placeholder_path = os.path.join(path, \".placeholder\").replace(\"\\\\\", \"/\")\n    if not self.file_exists(placeholder_path):\n        self.write_file(placeholder_path, b\"Placeholder file for directory\")\n</code></pre>"},{"location":"api/core/#gigaspatial.core.io.snowflake_data_store.SnowflakeDataStore.open","title":"<code>open(path, mode='r')</code>","text":"<p>Context manager for file operations.</p> <p>:param path: File path in Snowflake stage :param mode: File open mode (r, rb, w, wb)</p> Source code in <code>gigaspatial/core/io/snowflake_data_store.py</code> <pre><code>@contextlib.contextmanager\ndef open(self, path: str, mode: str = \"r\"):\n    \"\"\"\n    Context manager for file operations.\n\n    :param path: File path in Snowflake stage\n    :param mode: File open mode (r, rb, w, wb)\n    \"\"\"\n    if mode == \"w\":\n        file = io.StringIO()\n        yield file\n        self.write_file(path, file.getvalue())\n\n    elif mode == \"wb\":\n        file = io.BytesIO()\n        yield file\n        self.write_file(path, file.getvalue())\n\n    elif mode == \"r\":\n        data = self.read_file(path, encoding=\"UTF-8\")\n        file = io.StringIO(data)\n        yield file\n\n    elif mode == \"rb\":\n        data = self.read_file(path)\n        file = io.BytesIO(data)\n        yield file\n\n    else:\n        raise ValueError(f\"Unsupported mode: {mode}\")\n</code></pre>"},{"location":"api/core/#gigaspatial.core.io.snowflake_data_store.SnowflakeDataStore.read_file","title":"<code>read_file(path, encoding=None)</code>","text":"<p>Read file from Snowflake stage.</p> <p>:param path: Path to the file in the stage :param encoding: File encoding (optional) :return: File contents as string or bytes</p> Source code in <code>gigaspatial/core/io/snowflake_data_store.py</code> <pre><code>def read_file(self, path: str, encoding: Optional[str] = None) -&gt; Union[str, bytes]:\n    \"\"\"\n    Read file from Snowflake stage.\n\n    :param path: Path to the file in the stage\n    :param encoding: File encoding (optional)\n    :return: File contents as string or bytes\n    \"\"\"\n    self._ensure_connection()\n    cursor = self.connection.cursor(DictCursor)\n\n    try:\n        normalized_path = self._normalize_path(path)\n        stage_path = self._get_stage_path(normalized_path)\n\n        # Create temporary directory for download\n        temp_download_dir = os.path.join(self._temp_dir, \"downloads\")\n        os.makedirs(temp_download_dir, exist_ok=True)\n\n        # Download file from stage using GET command\n        # GET command: GET &lt;stage_path&gt; file://&lt;local_path&gt;\n        temp_dir_normalized = temp_download_dir.replace(\"\\\\\", \"/\")\n        if not temp_dir_normalized.endswith(\"/\"):\n            temp_dir_normalized += \"/\"\n\n        get_command = f\"GET {stage_path} 'file://{temp_dir_normalized}'\"\n        cursor.execute(get_command)\n\n        # Find the downloaded file (Snowflake may add prefixes/suffixes or preserve structure)\n        downloaded_files = []\n        for root, dirs, files in os.walk(temp_download_dir):\n            for f in files:\n                file_path = os.path.join(root, f)\n                # Check if this file matches our expected filename\n                if os.path.basename(normalized_path) in f or normalized_path.endswith(f):\n                    downloaded_files.append(file_path)\n\n        if not downloaded_files:\n            raise FileNotFoundError(f\"File not found in stage: {path}\")\n\n        # Read the first matching file\n        downloaded_path = downloaded_files[0]\n        with open(downloaded_path, \"rb\") as f:\n            data = f.read()\n\n        # Clean up\n        os.remove(downloaded_path)\n        # Clean up empty directories\n        try:\n            if os.path.exists(temp_download_dir) and not os.listdir(temp_download_dir):\n                os.rmdir(temp_download_dir)\n        except OSError:\n            pass\n\n        # Decode if encoding is specified\n        if encoding:\n            return data.decode(encoding)\n        return data\n\n    except Exception as e:\n        raise IOError(f\"Error reading file {path} from Snowflake stage: {e}\")\n    finally:\n        cursor.close()\n</code></pre>"},{"location":"api/core/#gigaspatial.core.io.snowflake_data_store.SnowflakeDataStore.remove","title":"<code>remove(path)</code>","text":"<p>Remove a file from the Snowflake stage.</p> <p>:param path: Path to the file to remove</p> Source code in <code>gigaspatial/core/io/snowflake_data_store.py</code> <pre><code>def remove(self, path: str) -&gt; None:\n    \"\"\"\n    Remove a file from the Snowflake stage.\n\n    :param path: Path to the file to remove\n    \"\"\"\n    self._ensure_connection()\n    cursor = self.connection.cursor()\n\n    try:\n        normalized_path = self._normalize_path(path)\n        stage_path = self._get_stage_path(normalized_path)\n\n        remove_command = f\"REMOVE {stage_path}\"\n        cursor.execute(remove_command)\n\n    except Exception as e:\n        raise IOError(f\"Error removing file {path}: {e}\")\n    finally:\n        cursor.close()\n</code></pre>"},{"location":"api/core/#gigaspatial.core.io.snowflake_data_store.SnowflakeDataStore.rename","title":"<code>rename(source_path, destination_path, overwrite=False, delete_source=True)</code>","text":"<p>Rename (move) a single file by copying to the new path and deleting the source.</p> <p>:param source_path: Existing file path in the stage :param destination_path: Target file path in the stage :param overwrite: Overwrite destination if it already exists :param delete_source: Delete original after successful copy</p> Source code in <code>gigaspatial/core/io/snowflake_data_store.py</code> <pre><code>def rename(\n    self,\n    source_path: str,\n    destination_path: str,\n    overwrite: bool = False,\n    delete_source: bool = True,\n) -&gt; None:\n    \"\"\"\n    Rename (move) a single file by copying to the new path and deleting the source.\n\n    :param source_path: Existing file path in the stage\n    :param destination_path: Target file path in the stage\n    :param overwrite: Overwrite destination if it already exists\n    :param delete_source: Delete original after successful copy\n    \"\"\"\n    if not self.file_exists(source_path):\n        raise FileNotFoundError(f\"Source file not found: {source_path}\")\n\n    if self.file_exists(destination_path) and not overwrite:\n        raise FileExistsError(\n            f\"Destination already exists and overwrite is False: {destination_path}\"\n        )\n\n    # Copy file to new location\n    self.copy_file(source_path, destination_path, overwrite=overwrite)\n\n    # Delete source if requested\n    if delete_source:\n        self.remove(source_path)\n</code></pre>"},{"location":"api/core/#gigaspatial.core.io.snowflake_data_store.SnowflakeDataStore.rmdir","title":"<code>rmdir(dir)</code>","text":"<p>Remove a directory and all its contents from the Snowflake stage.</p> <p>:param dir: Path to the directory to remove</p> Source code in <code>gigaspatial/core/io/snowflake_data_store.py</code> <pre><code>def rmdir(self, dir: str) -&gt; None:\n    \"\"\"\n    Remove a directory and all its contents from the Snowflake stage.\n\n    :param dir: Path to the directory to remove\n    \"\"\"\n    self._ensure_connection()\n    cursor = self.connection.cursor()\n\n    try:\n        normalized_dir = self._normalize_path(dir)\n        stage_path = self._get_stage_path(normalized_dir)\n\n        # Remove all files in the directory\n        remove_command = f\"REMOVE {stage_path}\"\n        cursor.execute(remove_command)\n\n    except Exception as e:\n        raise IOError(f\"Error removing directory {dir}: {e}\")\n    finally:\n        cursor.close()\n</code></pre>"},{"location":"api/core/#gigaspatial.core.io.snowflake_data_store.SnowflakeDataStore.upload_directory","title":"<code>upload_directory(dir_path, stage_dir_path)</code>","text":"<p>Uploads all files from a local directory to Snowflake stage.</p> <p>:param dir_path: Local directory path :param stage_dir_path: Destination directory path in the stage</p> Source code in <code>gigaspatial/core/io/snowflake_data_store.py</code> <pre><code>def upload_directory(self, dir_path: str, stage_dir_path: str):\n    \"\"\"\n    Uploads all files from a local directory to Snowflake stage.\n\n    :param dir_path: Local directory path\n    :param stage_dir_path: Destination directory path in the stage\n    \"\"\"\n    if not os.path.isdir(dir_path):\n        raise NotADirectoryError(f\"Local directory not found: {dir_path}\")\n\n    for root, dirs, files in os.walk(dir_path):\n        for file in files:\n            local_file_path = os.path.join(root, file)\n            relative_path = os.path.relpath(local_file_path, dir_path)\n            # Normalize path separators for stage\n            stage_file_path = os.path.join(stage_dir_path, relative_path).replace(\"\\\\\", \"/\")\n\n            self.upload_file(local_file_path, stage_file_path)\n</code></pre>"},{"location":"api/core/#gigaspatial.core.io.snowflake_data_store.SnowflakeDataStore.upload_file","title":"<code>upload_file(file_path, stage_path)</code>","text":"<p>Uploads a single file from local filesystem to Snowflake stage.</p> <p>:param file_path: Local file path :param stage_path: Destination path in the stage</p> Source code in <code>gigaspatial/core/io/snowflake_data_store.py</code> <pre><code>def upload_file(self, file_path: str, stage_path: str):\n    \"\"\"\n    Uploads a single file from local filesystem to Snowflake stage.\n\n    :param file_path: Local file path\n    :param stage_path: Destination path in the stage\n    \"\"\"\n    try:\n        if not os.path.exists(file_path):\n            raise FileNotFoundError(f\"Local file not found: {file_path}\")\n\n        # Read the file\n        with open(file_path, \"rb\") as f:\n            data = f.read()\n\n        # Write to stage using write_file\n        self.write_file(stage_path, data)\n        self.logger.info(f\"Uploaded {file_path} to {stage_path}\")\n    except Exception as e:\n        self.logger.error(f\"Failed to upload {file_path}: {e}\")\n        raise\n</code></pre>"},{"location":"api/core/#gigaspatial.core.io.snowflake_data_store.SnowflakeDataStore.walk","title":"<code>walk(top)</code>","text":"<p>Walk through directory tree in Snowflake stage, similar to os.walk().</p> <p>:param top: Starting directory for the walk :return: Generator yielding tuples of (dirpath, dirnames, filenames)</p> Source code in <code>gigaspatial/core/io/snowflake_data_store.py</code> <pre><code>def walk(self, top: str) -&gt; Generator[Tuple[str, List[str], List[str]], None, None]:\n    \"\"\"\n    Walk through directory tree in Snowflake stage, similar to os.walk().\n\n    :param top: Starting directory for the walk\n    :return: Generator yielding tuples of (dirpath, dirnames, filenames)\n    \"\"\"\n    try:\n        normalized_top = self._normalize_path(top)\n\n        # Use list_files to get all files (it handles path parsing correctly)\n        all_files = self.list_files(normalized_top)\n\n        # Organize into directory structure\n        dirs = {}\n\n        for file_path in all_files:\n            # Ensure we're working with paths relative to the top\n            if normalized_top and not file_path.startswith(normalized_top):\n                continue\n\n            # Get relative path from top\n            if normalized_top and file_path.startswith(normalized_top):\n                relative_path = file_path[len(normalized_top):].lstrip(\"/\")\n            else:\n                relative_path = file_path\n\n            if not relative_path:\n                continue\n\n            # Get directory and filename\n            if \"/\" in relative_path:\n                dir_path, filename = os.path.split(relative_path)\n                full_dir_path = f\"{normalized_top}/{dir_path}\" if normalized_top else dir_path\n                if full_dir_path not in dirs:\n                    dirs[full_dir_path] = []\n                dirs[full_dir_path].append(filename)\n            else:\n                # File in root of the top directory\n                if normalized_top not in dirs:\n                    dirs[normalized_top] = []\n                dirs[normalized_top].append(relative_path)\n\n        # Yield results in os.walk format\n        for dir_path, files in dirs.items():\n            # Extract subdirectories (simplified - Snowflake stages are flat)\n            subdirs = []\n            yield (dir_path, subdirs, files)\n\n    except Exception as e:\n        self.logger.warning(f\"Error walking directory {top}: {e}\")\n        yield (top, [], [])\n</code></pre>"},{"location":"api/core/#gigaspatial.core.io.snowflake_data_store.SnowflakeDataStore.write_file","title":"<code>write_file(path, data)</code>","text":"<p>Write file to Snowflake stage.</p> <p>:param path: Destination path in the stage :param data: File contents</p> Source code in <code>gigaspatial/core/io/snowflake_data_store.py</code> <pre><code>def write_file(self, path: str, data: Union[bytes, str]) -&gt; None:\n    \"\"\"\n    Write file to Snowflake stage.\n\n    :param path: Destination path in the stage\n    :param data: File contents\n    \"\"\"\n    self._ensure_connection()\n    cursor = self.connection.cursor()\n\n    try:\n        # Convert to bytes if string\n        if isinstance(data, str):\n            binary_data = data.encode(\"utf-8\")\n        elif isinstance(data, bytes):\n            binary_data = data\n        else:\n            raise ValueError('Unsupported data type. Only \"bytes\" or \"string\" accepted')\n\n        normalized_path = self._normalize_path(path)\n\n        # Write to temporary file first\n        # Use the full path structure for the temp file to preserve directory structure\n        temp_file_path = os.path.join(self._temp_dir, normalized_path)\n        os.makedirs(os.path.dirname(temp_file_path), exist_ok=True)\n\n        with open(temp_file_path, \"wb\") as f:\n            f.write(binary_data)\n\n        # Upload to stage using PUT command\n        # Snowflake PUT requires the local file path and the target stage path\n        # Convert Windows paths to Unix-style for Snowflake\n        temp_file_normalized = os.path.abspath(temp_file_path).replace(\"\\\\\", \"/\")\n\n        # PUT command: PUT 'file://&lt;absolute_local_path&gt;' @stage_name/&lt;path&gt;\n        # The file will be stored at the specified path in the stage\n        stage_target = f\"@{self.stage_name}/\"\n        if \"/\" in normalized_path:\n            # Include directory structure in stage path\n            dir_path = os.path.dirname(normalized_path)\n            stage_target = f\"@{self.stage_name}/{dir_path}/\"\n\n        # Snowflake PUT syntax: PUT 'file://&lt;path&gt;' @stage/path\n        put_command = f\"PUT 'file://{temp_file_normalized}' {stage_target} OVERWRITE=TRUE AUTO_COMPRESS=FALSE\"\n        cursor.execute(put_command)\n\n        # Clean up temp file\n        if os.path.exists(temp_file_path):\n            os.remove(temp_file_path)\n            # Clean up empty directories if they were created\n            try:\n                temp_dir = os.path.dirname(temp_file_path)\n                if temp_dir != self._temp_dir and os.path.exists(temp_dir):\n                    os.rmdir(temp_dir)\n            except OSError:\n                pass  # Directory not empty or other error, ignore\n\n    except Exception as e:\n        raise IOError(f\"Error writing file {path} to Snowflake stage: {e}\")\n    finally:\n        cursor.close()\n</code></pre>"},{"location":"api/core/#gigaspatial.core.io.writers","title":"<code>writers</code>","text":""},{"location":"api/core/#gigaspatial.core.io.writers.write_dataset","title":"<code>write_dataset(data, data_store, path, **kwargs)</code>","text":"<p>Write DataFrame, GeoDataFrame, or a generic object (for JSON) to various file formats in DataStore.</p>"},{"location":"api/core/#gigaspatial.core.io.writers.write_dataset--parameters","title":"Parameters:","text":"<p>data : pandas.DataFrame, geopandas.GeoDataFrame, or any object     The data to write to data storage. data_store : DataStore     Instance of DataStore for accessing data storage. path : str     Path where the file will be written in data storage. **kwargs : dict     Additional arguments passed to the specific writer function.</p>"},{"location":"api/core/#gigaspatial.core.io.writers.write_dataset--raises","title":"Raises:","text":"<p>ValueError     If the file type is unsupported or if there's an error writing the file. TypeError         If input data is not a DataFrame, GeoDataFrame, AND not a generic object         intended for a .json file.</p> Source code in <code>gigaspatial/core/io/writers.py</code> <pre><code>def write_dataset(data, data_store: DataStore, path, **kwargs):\n    \"\"\"\n    Write DataFrame, GeoDataFrame, or a generic object (for JSON)\n    to various file formats in DataStore.\n\n    Parameters:\n    ----------\n    data : pandas.DataFrame, geopandas.GeoDataFrame, or any object\n        The data to write to data storage.\n    data_store : DataStore\n        Instance of DataStore for accessing data storage.\n    path : str\n        Path where the file will be written in data storage.\n    **kwargs : dict\n        Additional arguments passed to the specific writer function.\n\n    Raises:\n    ------\n    ValueError\n        If the file type is unsupported or if there's an error writing the file.\n    TypeError\n            If input data is not a DataFrame, GeoDataFrame, AND not a generic object\n            intended for a .json file.\n    \"\"\"\n\n    # Define supported file formats and their writers\n    BINARY_FORMATS = {\".shp\", \".zip\", \".parquet\", \".gpkg\", \".xlsx\", \".xls\"}\n\n    PANDAS_WRITERS = {\n        \".csv\": lambda df, buf, **kw: df.to_csv(buf, **kw),\n        \".xlsx\": lambda df, buf, **kw: df.to_excel(buf, engine=\"openpyxl\", **kw),\n        \".json\": lambda df, buf, **kw: df.to_json(buf, **kw),\n        \".parquet\": lambda df, buf, **kw: df.to_parquet(buf, **kw),\n    }\n\n    GEO_WRITERS = {\n        \".geojson\": lambda gdf, buf, **kw: gdf.to_file(buf, driver=\"GeoJSON\", **kw),\n        \".gpkg\": lambda gdf, buf, **kw: gdf.to_file(buf, driver=\"GPKG\", **kw),\n        \".parquet\": lambda gdf, buf, **kw: gdf.to_parquet(buf, **kw),\n    }\n\n    try:\n        # Get file suffix and ensure it's lowercase\n        suffix = Path(path).suffix.lower()\n\n        # 1. Handle generic JSON data\n        is_dataframe_like = isinstance(data, (pd.DataFrame, gpd.GeoDataFrame))\n        if not is_dataframe_like:\n            if suffix == \".json\":\n                try:\n                    # Pass generic data directly to the write_json function\n                    write_json(data, data_store, path, **kwargs)\n                    return  # Successfully wrote JSON, so exit\n                except Exception as e:\n                    raise ValueError(f\"Error writing generic JSON data: {str(e)}\")\n            else:\n                # Raise an error if it's not a DataFrame/GeoDataFrame and not a .json file\n                raise TypeError(\n                    \"Input data must be a pandas DataFrame or GeoDataFrame, \"\n                    \"or a generic object destined for a '.json' file.\"\n                )\n\n        # 2. Handle DataFrame/GeoDataFrame\n        # Determine if we need binary mode based on file type\n        mode = \"wb\" if suffix in BINARY_FORMATS else \"w\"\n\n        # Handle different data types and formats\n        if isinstance(data, gpd.GeoDataFrame):\n            if suffix not in GEO_WRITERS:\n                supported_formats = sorted(GEO_WRITERS.keys())\n                raise ValueError(\n                    f\"Unsupported file type for GeoDataFrame: {suffix}\\n\"\n                    f\"Supported formats: {', '.join(supported_formats)}\"\n                )\n\n            try:\n                with data_store.open(path, \"wb\") as f:\n                    GEO_WRITERS[suffix](data, f, **kwargs)\n            except Exception as e:\n                raise ValueError(f\"Error writing GeoDataFrame: {str(e)}\")\n\n        else:  # pandas DataFrame\n            if suffix not in PANDAS_WRITERS:\n                supported_formats = sorted(PANDAS_WRITERS.keys())\n                raise ValueError(\n                    f\"Unsupported file type for DataFrame: {suffix}\\n\"\n                    f\"Supported formats: {', '.join(supported_formats)}\"\n                )\n\n            try:\n                with data_store.open(path, mode) as f:\n                    PANDAS_WRITERS[suffix](data, f, **kwargs)\n            except Exception as e:\n                raise ValueError(f\"Error writing DataFrame: {str(e)}\")\n\n    except Exception as e:\n        if isinstance(e, (TypeError, ValueError)):\n            raise\n        raise RuntimeError(f\"Unexpected error writing dataset: {str(e)}\")\n</code></pre>"},{"location":"api/core/#gigaspatial.core.io.writers.write_datasets","title":"<code>write_datasets(data_dict, data_store, **kwargs)</code>","text":"<p>Write multiple datasets to data storage at once.</p>"},{"location":"api/core/#gigaspatial.core.io.writers.write_datasets--parameters","title":"Parameters:","text":"<p>data_dict : dict     Dictionary mapping paths to DataFrames/GeoDataFrames. data_store : DataStore     Instance of DataStore for accessing data storage. **kwargs : dict     Additional arguments passed to write_dataset.</p>"},{"location":"api/core/#gigaspatial.core.io.writers.write_datasets--raises","title":"Raises:","text":"<p>ValueError     If there are any errors writing the datasets.</p> Source code in <code>gigaspatial/core/io/writers.py</code> <pre><code>def write_datasets(data_dict, data_store: DataStore, **kwargs):\n    \"\"\"\n    Write multiple datasets to data storage at once.\n\n    Parameters:\n    ----------\n    data_dict : dict\n        Dictionary mapping paths to DataFrames/GeoDataFrames.\n    data_store : DataStore\n        Instance of DataStore for accessing data storage.\n    **kwargs : dict\n        Additional arguments passed to write_dataset.\n\n    Raises:\n    ------\n    ValueError\n        If there are any errors writing the datasets.\n    \"\"\"\n    errors = {}\n\n    for path, data in data_dict.items():\n        try:\n            write_dataset(data, data_store, path, **kwargs)\n        except Exception as e:\n            errors[path] = str(e)\n\n    if errors:\n        error_msg = \"\\n\".join(f\"- {path}: {error}\" for path, error in errors.items())\n        raise ValueError(f\"Errors writing datasets:\\n{error_msg}\")\n</code></pre>"},{"location":"api/core/#gigaspatial.core.schemas","title":"<code>schemas</code>","text":""},{"location":"api/core/#gigaspatial.core.schemas.entity","title":"<code>entity</code>","text":""},{"location":"api/core/#gigaspatial.core.schemas.entity.BaseGigaEntity","title":"<code>BaseGigaEntity</code>","text":"<p>               Bases: <code>BaseModel</code></p> <p>Base class for all Giga entities with common fields.</p> Source code in <code>gigaspatial/core/schemas/entity.py</code> <pre><code>class BaseGigaEntity(BaseModel):\n    \"\"\"Base class for all Giga entities with common fields.\"\"\"\n\n    source: Optional[str] = Field(None, max_length=100, description=\"Source reference\")\n    source_detail: Optional[str] = None\n\n    @property\n    def id(self) -&gt; str:\n        \"\"\"Abstract property that must be implemented by subclasses.\"\"\"\n        raise NotImplementedError(\"Subclasses must implement id property\")\n</code></pre>"},{"location":"api/core/#gigaspatial.core.schemas.entity.BaseGigaEntity.id","title":"<code>id: str</code>  <code>property</code>","text":"<p>Abstract property that must be implemented by subclasses.</p>"},{"location":"api/core/#gigaspatial.core.schemas.entity.EntityTable","title":"<code>EntityTable</code>","text":"<p>               Bases: <code>BaseModel</code>, <code>Generic[E]</code></p> Source code in <code>gigaspatial/core/schemas/entity.py</code> <pre><code>class EntityTable(BaseModel, Generic[E]):\n    entities: List[E] = Field(default_factory=list)\n    _cached_kdtree: Optional[cKDTree] = PrivateAttr(\n        default=None\n    )  # Internal cache for the KDTree\n\n    @classmethod\n    def from_file(\n        cls: Type[\"EntityTable\"],\n        file_path: Union[str, Path],\n        entity_class: Type[E],\n        data_store: Optional[DataStore] = None,\n        **kwargs,\n    ) -&gt; \"EntityTable\":\n        \"\"\"\n        Create an EntityTable instance from a file.\n\n        Args:\n            file_path: Path to the dataset file\n            entity_class: The entity class for validation\n\n        Returns:\n            EntityTable instance\n\n        Raises:\n            ValidationError: If any row fails validation\n            FileNotFoundError: If the file doesn't exist\n        \"\"\"\n        data_store = data_store or LocalDataStore()\n        file_path = Path(file_path)\n        if not file_path.exists():\n            raise FileNotFoundError(f\"File not found: {file_path}\")\n\n        df = read_dataset(data_store, file_path, **kwargs)\n        try:\n            entities = [entity_class(**row) for row in df.to_dict(orient=\"records\")]\n            return cls(entities=entities)\n        except ValidationError as e:\n            raise ValueError(f\"Validation error in input data: {e}\")\n        except Exception as e:\n            raise ValueError(f\"Error reading or processing the file: {e}\")\n\n    def _check_has_location(self, method_name: str) -&gt; bool:\n        \"\"\"Helper method to check if entities have location data.\"\"\"\n        if not self.entities:\n            return False\n        if not isinstance(self.entities[0], GigaEntity):\n            raise ValueError(\n                f\"Cannot perform {method_name}: entities of type {type(self.entities[0]).__name__} \"\n                \"do not have location data (latitude/longitude)\"\n            )\n        return True\n\n    def to_dataframe(self) -&gt; pd.DataFrame:\n        \"\"\"Convert the entity table to a pandas DataFrame.\"\"\"\n        return pd.DataFrame([e.model_dump() for e in self.entities])\n\n    def to_geodataframe(self) -&gt; gpd.GeoDataFrame:\n        \"\"\"Convert the entity table to a GeoDataFrame.\"\"\"\n        if not self._check_has_location(\"to_geodataframe\"):\n            raise ValueError(\"Cannot create GeoDataFrame: no entities available\")\n        df = self.to_dataframe()\n        return gpd.GeoDataFrame(\n            df,\n            geometry=gpd.points_from_xy(df[\"longitude\"], df[\"latitude\"]),\n            crs=\"EPSG:4326\",\n        )\n\n    def to_coordinate_vector(self) -&gt; np.ndarray:\n        \"\"\"Transforms the entity table into a numpy vector of coordinates\"\"\"\n        if not self.entities:\n            return np.zeros((0, 2))\n\n        if not self._check_has_location(\"to_coordinate_vector\"):\n            return np.zeros((0, 2))\n\n        return np.array([[e.latitude, e.longitude] for e in self.entities])\n\n    def get_lat_array(self) -&gt; np.ndarray:\n        \"\"\"Get an array of latitude values.\"\"\"\n        if not self._check_has_location(\"get_lat_array\"):\n            return np.array([])\n        return np.array([e.latitude for e in self.entities])\n\n    def get_lon_array(self) -&gt; np.ndarray:\n        \"\"\"Get an array of longitude values.\"\"\"\n        if not self._check_has_location(\"get_lon_array\"):\n            return np.array([])\n        return np.array([e.longitude for e in self.entities])\n\n    def filter_by_admin1(self, admin1_id_giga: str) -&gt; \"EntityTable[E]\":\n        \"\"\"Filter entities by primary administrative division.\"\"\"\n        return self.__class__(\n            entities=[e for e in self.entities if e.admin1_id_giga == admin1_id_giga]\n        )\n\n    def filter_by_admin2(self, admin2_id_giga: str) -&gt; \"EntityTable[E]\":\n        \"\"\"Filter entities by secondary administrative division.\"\"\"\n        return self.__class__(\n            entities=[e for e in self.entities if e.admin2_id_giga == admin2_id_giga]\n        )\n\n    def filter_by_polygon(self, polygon: Polygon) -&gt; \"EntityTable[E]\":\n        \"\"\"Filter entities within a polygon\"\"\"\n        if not self._check_has_location(\"filter_by_polygon\"):\n            return self.__class__(entities=[])\n\n        filtered = [\n            e for e in self.entities if polygon.contains(Point(e.longitude, e.latitude))\n        ]\n        return self.__class__(entities=filtered)\n\n    def filter_by_bounds(\n        self, min_lat: float, max_lat: float, min_lon: float, max_lon: float\n    ) -&gt; \"EntityTable[E]\":\n        \"\"\"Filter entities whose coordinates fall within the given bounds.\"\"\"\n        if not self._check_has_location(\"filter_by_bounds\"):\n            return self.__class__(entities=[])\n\n        filtered = [\n            e\n            for e in self.entities\n            if min_lat &lt;= e.latitude &lt;= max_lat and min_lon &lt;= e.longitude &lt;= max_lon\n        ]\n        return self.__class__(entities=filtered)\n\n    def get_nearest_neighbors(\n        self, lat: float, lon: float, k: int = 5\n    ) -&gt; \"EntityTable[E]\":\n        \"\"\"Find k nearest neighbors to a point using a cached KDTree.\"\"\"\n        if not self._check_has_location(\"get_nearest_neighbors\"):\n            return self.__class__(entities=[])\n\n        if not self._cached_kdtree:\n            self._build_kdtree()  # Build the KDTree if not already cached\n\n        if not self._cached_kdtree:  # If still None after building\n            return self.__class__(entities=[])\n\n        _, indices = self._cached_kdtree.query([[lat, lon]], k=k)\n        return self.__class__(entities=[self.entities[i] for i in indices[0]])\n\n    def _build_kdtree(self):\n        \"\"\"Builds and caches the KDTree.\"\"\"\n        if not self._check_has_location(\"_build_kdtree\"):\n            self._cached_kdtree = None\n            return\n        coords = self.to_coordinate_vector()\n        if coords:\n            self._cached_kdtree = cKDTree(coords)\n\n    def clear_cache(self):\n        \"\"\"Clears the KDTree cache.\"\"\"\n        self._cached_kdtree = None\n\n    def to_file(\n        self,\n        file_path: Union[str, Path],\n        data_store: Optional[DataStore] = None,\n        **kwargs,\n    ) -&gt; None:\n        \"\"\"\n        Save the entity data to a file.\n\n        Args:\n            file_path: Path to save the file\n        \"\"\"\n        if not self.entities:\n            raise ValueError(\"Cannot write to a file: no entities available.\")\n\n        data_store = data_store or LocalDataStore()\n\n        write_dataset(self.to_dataframe(), data_store, file_path, **kwargs)\n\n    def __len__(self) -&gt; int:\n        return len(self.entities)\n\n    def __iter__(self):\n        return iter(self.entities)\n</code></pre>"},{"location":"api/core/#gigaspatial.core.schemas.entity.EntityTable.clear_cache","title":"<code>clear_cache()</code>","text":"<p>Clears the KDTree cache.</p> Source code in <code>gigaspatial/core/schemas/entity.py</code> <pre><code>def clear_cache(self):\n    \"\"\"Clears the KDTree cache.\"\"\"\n    self._cached_kdtree = None\n</code></pre>"},{"location":"api/core/#gigaspatial.core.schemas.entity.EntityTable.filter_by_admin1","title":"<code>filter_by_admin1(admin1_id_giga)</code>","text":"<p>Filter entities by primary administrative division.</p> Source code in <code>gigaspatial/core/schemas/entity.py</code> <pre><code>def filter_by_admin1(self, admin1_id_giga: str) -&gt; \"EntityTable[E]\":\n    \"\"\"Filter entities by primary administrative division.\"\"\"\n    return self.__class__(\n        entities=[e for e in self.entities if e.admin1_id_giga == admin1_id_giga]\n    )\n</code></pre>"},{"location":"api/core/#gigaspatial.core.schemas.entity.EntityTable.filter_by_admin2","title":"<code>filter_by_admin2(admin2_id_giga)</code>","text":"<p>Filter entities by secondary administrative division.</p> Source code in <code>gigaspatial/core/schemas/entity.py</code> <pre><code>def filter_by_admin2(self, admin2_id_giga: str) -&gt; \"EntityTable[E]\":\n    \"\"\"Filter entities by secondary administrative division.\"\"\"\n    return self.__class__(\n        entities=[e for e in self.entities if e.admin2_id_giga == admin2_id_giga]\n    )\n</code></pre>"},{"location":"api/core/#gigaspatial.core.schemas.entity.EntityTable.filter_by_bounds","title":"<code>filter_by_bounds(min_lat, max_lat, min_lon, max_lon)</code>","text":"<p>Filter entities whose coordinates fall within the given bounds.</p> Source code in <code>gigaspatial/core/schemas/entity.py</code> <pre><code>def filter_by_bounds(\n    self, min_lat: float, max_lat: float, min_lon: float, max_lon: float\n) -&gt; \"EntityTable[E]\":\n    \"\"\"Filter entities whose coordinates fall within the given bounds.\"\"\"\n    if not self._check_has_location(\"filter_by_bounds\"):\n        return self.__class__(entities=[])\n\n    filtered = [\n        e\n        for e in self.entities\n        if min_lat &lt;= e.latitude &lt;= max_lat and min_lon &lt;= e.longitude &lt;= max_lon\n    ]\n    return self.__class__(entities=filtered)\n</code></pre>"},{"location":"api/core/#gigaspatial.core.schemas.entity.EntityTable.filter_by_polygon","title":"<code>filter_by_polygon(polygon)</code>","text":"<p>Filter entities within a polygon</p> Source code in <code>gigaspatial/core/schemas/entity.py</code> <pre><code>def filter_by_polygon(self, polygon: Polygon) -&gt; \"EntityTable[E]\":\n    \"\"\"Filter entities within a polygon\"\"\"\n    if not self._check_has_location(\"filter_by_polygon\"):\n        return self.__class__(entities=[])\n\n    filtered = [\n        e for e in self.entities if polygon.contains(Point(e.longitude, e.latitude))\n    ]\n    return self.__class__(entities=filtered)\n</code></pre>"},{"location":"api/core/#gigaspatial.core.schemas.entity.EntityTable.from_file","title":"<code>from_file(file_path, entity_class, data_store=None, **kwargs)</code>  <code>classmethod</code>","text":"<p>Create an EntityTable instance from a file.</p> <p>Parameters:</p> Name Type Description Default <code>file_path</code> <code>Union[str, Path]</code> <p>Path to the dataset file</p> required <code>entity_class</code> <code>Type[E]</code> <p>The entity class for validation</p> required <p>Returns:</p> Type Description <code>EntityTable</code> <p>EntityTable instance</p> <p>Raises:</p> Type Description <code>ValidationError</code> <p>If any row fails validation</p> <code>FileNotFoundError</code> <p>If the file doesn't exist</p> Source code in <code>gigaspatial/core/schemas/entity.py</code> <pre><code>@classmethod\ndef from_file(\n    cls: Type[\"EntityTable\"],\n    file_path: Union[str, Path],\n    entity_class: Type[E],\n    data_store: Optional[DataStore] = None,\n    **kwargs,\n) -&gt; \"EntityTable\":\n    \"\"\"\n    Create an EntityTable instance from a file.\n\n    Args:\n        file_path: Path to the dataset file\n        entity_class: The entity class for validation\n\n    Returns:\n        EntityTable instance\n\n    Raises:\n        ValidationError: If any row fails validation\n        FileNotFoundError: If the file doesn't exist\n    \"\"\"\n    data_store = data_store or LocalDataStore()\n    file_path = Path(file_path)\n    if not file_path.exists():\n        raise FileNotFoundError(f\"File not found: {file_path}\")\n\n    df = read_dataset(data_store, file_path, **kwargs)\n    try:\n        entities = [entity_class(**row) for row in df.to_dict(orient=\"records\")]\n        return cls(entities=entities)\n    except ValidationError as e:\n        raise ValueError(f\"Validation error in input data: {e}\")\n    except Exception as e:\n        raise ValueError(f\"Error reading or processing the file: {e}\")\n</code></pre>"},{"location":"api/core/#gigaspatial.core.schemas.entity.EntityTable.get_lat_array","title":"<code>get_lat_array()</code>","text":"<p>Get an array of latitude values.</p> Source code in <code>gigaspatial/core/schemas/entity.py</code> <pre><code>def get_lat_array(self) -&gt; np.ndarray:\n    \"\"\"Get an array of latitude values.\"\"\"\n    if not self._check_has_location(\"get_lat_array\"):\n        return np.array([])\n    return np.array([e.latitude for e in self.entities])\n</code></pre>"},{"location":"api/core/#gigaspatial.core.schemas.entity.EntityTable.get_lon_array","title":"<code>get_lon_array()</code>","text":"<p>Get an array of longitude values.</p> Source code in <code>gigaspatial/core/schemas/entity.py</code> <pre><code>def get_lon_array(self) -&gt; np.ndarray:\n    \"\"\"Get an array of longitude values.\"\"\"\n    if not self._check_has_location(\"get_lon_array\"):\n        return np.array([])\n    return np.array([e.longitude for e in self.entities])\n</code></pre>"},{"location":"api/core/#gigaspatial.core.schemas.entity.EntityTable.get_nearest_neighbors","title":"<code>get_nearest_neighbors(lat, lon, k=5)</code>","text":"<p>Find k nearest neighbors to a point using a cached KDTree.</p> Source code in <code>gigaspatial/core/schemas/entity.py</code> <pre><code>def get_nearest_neighbors(\n    self, lat: float, lon: float, k: int = 5\n) -&gt; \"EntityTable[E]\":\n    \"\"\"Find k nearest neighbors to a point using a cached KDTree.\"\"\"\n    if not self._check_has_location(\"get_nearest_neighbors\"):\n        return self.__class__(entities=[])\n\n    if not self._cached_kdtree:\n        self._build_kdtree()  # Build the KDTree if not already cached\n\n    if not self._cached_kdtree:  # If still None after building\n        return self.__class__(entities=[])\n\n    _, indices = self._cached_kdtree.query([[lat, lon]], k=k)\n    return self.__class__(entities=[self.entities[i] for i in indices[0]])\n</code></pre>"},{"location":"api/core/#gigaspatial.core.schemas.entity.EntityTable.to_coordinate_vector","title":"<code>to_coordinate_vector()</code>","text":"<p>Transforms the entity table into a numpy vector of coordinates</p> Source code in <code>gigaspatial/core/schemas/entity.py</code> <pre><code>def to_coordinate_vector(self) -&gt; np.ndarray:\n    \"\"\"Transforms the entity table into a numpy vector of coordinates\"\"\"\n    if not self.entities:\n        return np.zeros((0, 2))\n\n    if not self._check_has_location(\"to_coordinate_vector\"):\n        return np.zeros((0, 2))\n\n    return np.array([[e.latitude, e.longitude] for e in self.entities])\n</code></pre>"},{"location":"api/core/#gigaspatial.core.schemas.entity.EntityTable.to_dataframe","title":"<code>to_dataframe()</code>","text":"<p>Convert the entity table to a pandas DataFrame.</p> Source code in <code>gigaspatial/core/schemas/entity.py</code> <pre><code>def to_dataframe(self) -&gt; pd.DataFrame:\n    \"\"\"Convert the entity table to a pandas DataFrame.\"\"\"\n    return pd.DataFrame([e.model_dump() for e in self.entities])\n</code></pre>"},{"location":"api/core/#gigaspatial.core.schemas.entity.EntityTable.to_file","title":"<code>to_file(file_path, data_store=None, **kwargs)</code>","text":"<p>Save the entity data to a file.</p> <p>Parameters:</p> Name Type Description Default <code>file_path</code> <code>Union[str, Path]</code> <p>Path to save the file</p> required Source code in <code>gigaspatial/core/schemas/entity.py</code> <pre><code>def to_file(\n    self,\n    file_path: Union[str, Path],\n    data_store: Optional[DataStore] = None,\n    **kwargs,\n) -&gt; None:\n    \"\"\"\n    Save the entity data to a file.\n\n    Args:\n        file_path: Path to save the file\n    \"\"\"\n    if not self.entities:\n        raise ValueError(\"Cannot write to a file: no entities available.\")\n\n    data_store = data_store or LocalDataStore()\n\n    write_dataset(self.to_dataframe(), data_store, file_path, **kwargs)\n</code></pre>"},{"location":"api/core/#gigaspatial.core.schemas.entity.EntityTable.to_geodataframe","title":"<code>to_geodataframe()</code>","text":"<p>Convert the entity table to a GeoDataFrame.</p> Source code in <code>gigaspatial/core/schemas/entity.py</code> <pre><code>def to_geodataframe(self) -&gt; gpd.GeoDataFrame:\n    \"\"\"Convert the entity table to a GeoDataFrame.\"\"\"\n    if not self._check_has_location(\"to_geodataframe\"):\n        raise ValueError(\"Cannot create GeoDataFrame: no entities available\")\n    df = self.to_dataframe()\n    return gpd.GeoDataFrame(\n        df,\n        geometry=gpd.points_from_xy(df[\"longitude\"], df[\"latitude\"]),\n        crs=\"EPSG:4326\",\n    )\n</code></pre>"},{"location":"api/core/#gigaspatial.core.schemas.entity.GigaEntity","title":"<code>GigaEntity</code>","text":"<p>               Bases: <code>BaseGigaEntity</code></p> <p>Entity with location data.</p> Source code in <code>gigaspatial/core/schemas/entity.py</code> <pre><code>class GigaEntity(BaseGigaEntity):\n    \"\"\"Entity with location data.\"\"\"\n\n    latitude: float = Field(\n        ..., ge=-90, le=90, description=\"Latitude coordinate of the entity\"\n    )\n    longitude: float = Field(\n        ..., ge=-180, le=180, description=\"Longitude coordinate of the entity\"\n    )\n    admin1: Optional[str] = Field(\n        \"Unknown\", max_length=100, description=\"Primary administrative division\"\n    )\n    admin1_id_giga: Optional[str] = Field(\n        None,\n        max_length=50,\n        description=\"Unique identifier for the primary administrative division\",\n    )\n    admin2: Optional[str] = Field(\n        \"Unknown\", max_length=100, description=\"Secondary administrative division\"\n    )\n    admin2_id_giga: Optional[str] = Field(\n        None,\n        max_length=50,\n        description=\"Unique identifier for the secondary administrative division\",\n    )\n</code></pre>"},{"location":"api/core/#gigaspatial.core.schemas.entity.GigaEntityNoLocation","title":"<code>GigaEntityNoLocation</code>","text":"<p>               Bases: <code>BaseGigaEntity</code></p> <p>Entity without location data.</p> Source code in <code>gigaspatial/core/schemas/entity.py</code> <pre><code>class GigaEntityNoLocation(BaseGigaEntity):\n    \"\"\"Entity without location data.\"\"\"\n\n    pass\n</code></pre>"},{"location":"api/generators/","title":"Generators Module","text":""},{"location":"api/generators/#gigaspatial.generators","title":"<code>gigaspatial.generators</code>","text":""},{"location":"api/generators/#gigaspatial.generators.poi","title":"<code>poi</code>","text":""},{"location":"api/generators/#gigaspatial.generators.poi.PoiViewGenerator","title":"<code>PoiViewGenerator</code>","text":"<p>POI View Generator for integrating various geospatial datasets such as Google Open Buildings, Microsoft Global Buildings, GHSL Built Surface, and GHSL Settlement Model (SMOD) data with Points of Interest (POIs).</p> <p>This class provides methods to load, process, and map external geospatial data to a given set of POIs, enriching them with relevant attributes. It leverages handler/reader classes for efficient data access and processing.</p> <p>The POIs can be initialized from a list of (latitude, longitude) tuples, a list of dictionaries, a pandas DataFrame, or a geopandas GeoDataFrame.</p> Source code in <code>gigaspatial/generators/poi.py</code> <pre><code>class PoiViewGenerator:\n    \"\"\"\n    POI View Generator for integrating various geospatial datasets\n    such as Google Open Buildings, Microsoft Global Buildings, GHSL Built Surface,\n    and GHSL Settlement Model (SMOD) data with Points of Interest (POIs).\n\n    This class provides methods to load, process, and map external geospatial\n    data to a given set of POIs, enriching them with relevant attributes.\n    It leverages handler/reader classes for efficient data access and processing.\n\n    The POIs can be initialized from a list of (latitude, longitude) tuples,\n    a list of dictionaries, a pandas DataFrame, or a geopandas GeoDataFrame.\n    \"\"\"\n\n    def __init__(\n        self,\n        points: Union[\n            List[Tuple[float, float]], List[dict], pd.DataFrame, gpd.GeoDataFrame\n        ],\n        poi_id_column: str = \"poi_id\",\n        config: Optional[PoiViewGeneratorConfig] = None,\n        data_store: Optional[DataStore] = None,\n        logger: logging.Logger = None,\n    ):\n        \"\"\"\n        Initializes the PoiViewGenerator with the input points and configurations.\n\n        The input `points` are converted into an internal GeoDataFrame\n        (`_points_gdf`) for consistent geospatial operations.\n\n        Args:\n            points (Union[List[Tuple[float, float]], List[dict], pd.DataFrame, gpd.GeoDataFrame]):\n                The input points of interest. Can be:\n                - A list of (latitude, longitude) tuples.\n                - A list of dictionaries, where each dict must contain 'latitude' and 'longitude' keys.\n                - A pandas DataFrame with 'latitude' and 'longitude' columns.\n                - A geopandas GeoDataFrame (expected to have a 'geometry' column representing points).\n            generator_config (Optional[PoiViewGeneratorConfig]):\n                Configuration for the POI view generation process. If None, a\n                default `PoiViewGeneratorConfig` will be used.\n            data_store (Optional[DataStore]):\n                An instance of a data store for managing data access (e.g., LocalDataStore).\n                If None, a default `LocalDataStore` will be used.\n        \"\"\"\n        if hasattr(points, \"__len__\") and len(points) == 0:\n            raise ValueError(\"Points input cannot be empty\")\n\n        self.config = config or PoiViewGeneratorConfig()\n        self.data_store = data_store or LocalDataStore()\n        self.logger = logger or global_config.get_logger(self.__class__.__name__)\n        self._points_gdf = self._init_points_gdf(points, poi_id_column)\n        self._view: pd.DataFrame = self._points_gdf.drop(columns=[\"geometry\"])\n\n    @staticmethod\n    def _init_points_gdf(\n        points: Union[\n            List[Tuple[float, float]], List[dict], pd.DataFrame, gpd.GeoDataFrame\n        ],\n        poi_id_column: str,\n    ) -&gt; gpd.GeoDataFrame:\n        \"\"\"\n        Internal static method to convert various point input formats into a GeoDataFrame.\n\n        This method standardizes coordinate column names to 'latitude' and 'longitude'\n        for consistent internal representation. It also ensures each point has a unique\n        identifier in the 'poi_id' column.\n\n        Args:\n            points: Input points in various formats:\n                - List of (latitude, longitude) tuples\n                - List of dictionaries with coordinate keys\n                - DataFrame with coordinate columns\n                - GeoDataFrame with point geometries\n\n        Returns:\n            gpd.GeoDataFrame: Standardized GeoDataFrame with 'latitude', 'longitude',\n                             and 'poi_id' columns\n\n        Raises:\n            ValueError: If points format is not supported or coordinate columns cannot be detected\n        \"\"\"\n        if isinstance(points, gpd.GeoDataFrame):\n            # Convert geometry to lat/lon if needed\n            if points.geometry.name == \"geometry\":\n                points = points.copy()\n                points[\"latitude\"] = points.geometry.y\n                points[\"longitude\"] = points.geometry.x\n            if poi_id_column not in points.columns:\n                points[\"poi_id\"] = [f\"poi_{i}\" for i in range(len(points))]\n            else:\n                points = points.rename(\n                    columns={poi_id_column: \"poi_id\"},\n                )\n                if points[\"poi_id\"].duplicated().any():\n                    raise ValueError(\n                        f\"Column '{poi_id_column}' provided as 'poi_id_column' contains duplicate values.\"\n                    )\n\n            if points.crs != \"EPSG:4326\":\n                points = points.to_crs(\"EPSG:4326\")\n            return points\n\n        elif isinstance(points, pd.DataFrame):\n            # Detect and standardize coordinate columns\n            try:\n                lat_col, lon_col = detect_coordinate_columns(points)\n                points = points.copy()\n                points[\"latitude\"] = points[lat_col]\n                points[\"longitude\"] = points[lon_col]\n                if poi_id_column not in points.columns:\n                    points[\"poi_id\"] = [f\"poi_{i}\" for i in range(len(points))]\n                else:\n                    points = points.rename(\n                        columns={poi_id_column: \"poi_id\"},\n                    )\n                    if points[\"poi_id\"].duplicated().any():\n                        raise ValueError(\n                            f\"Column '{poi_id_column}' provided as 'poi_id_column' contains duplicate values.\"\n                        )\n                return convert_to_geodataframe(\n                    points, lat_col=\"latitude\", lon_col=\"longitude\"\n                )\n            except ValueError as e:\n                raise ValueError(\n                    f\"Could not detect coordinate columns in DataFrame: {str(e)}\"\n                )\n\n        elif isinstance(points, list):\n            if len(points) == 0:\n                return gpd.GeoDataFrame(\n                    columns=[\"latitude\", \"longitude\", \"poi_id\", \"geometry\"],\n                    geometry=\"geometry\",\n                    crs=\"EPSG:4326\",\n                )\n\n            if isinstance(points[0], tuple) and len(points[0]) == 2:\n                # List of (lat, lon) tuples\n                df = pd.DataFrame(points, columns=[\"latitude\", \"longitude\"])\n                df[\"poi_id\"] = [f\"poi_{i}\" for i in range(len(points))]\n                return convert_to_geodataframe(df)\n\n            elif isinstance(points[0], dict):\n                # List of dictionaries\n                df = pd.DataFrame(points)\n                try:\n                    lat_col, lon_col = detect_coordinate_columns(df)\n                    df[\"latitude\"] = df[lat_col]\n                    df[\"longitude\"] = df[lon_col]\n                    if poi_id_column not in df.columns:\n                        df[\"poi_id\"] = [f\"poi_{i}\" for i in range(len(points))]\n                    else:\n                        df = df.rename(\n                            columns={poi_id_column: \"poi_id\"},\n                        )\n                        if df[\"poi_id\"].duplicated().any():\n                            raise ValueError(\n                                f\"Column '{poi_id_column}' provided as 'poi_id_column' contains duplicate values.\"\n                            )\n                    return convert_to_geodataframe(\n                        df, lat_col=\"latitude\", lon_col=\"longitude\"\n                    )\n                except ValueError as e:\n                    raise ValueError(\n                        f\"Could not detect coordinate columns in dictionary list: {str(e)}\"\n                    )\n\n        raise ValueError(\"Unsupported points input type for PoiViewGenerator.\")\n\n    @property\n    def points_gdf(self) -&gt; gpd.GeoDataFrame:\n        \"\"\"Gets the internal GeoDataFrame of points of interest.\"\"\"\n        return self._points_gdf\n\n    @property\n    def view(self) -&gt; pd.DataFrame:\n        \"\"\"The DataFrame representing the current point of interest view.\"\"\"\n        return self._view\n\n    def _update_view(self, new_data: pd.DataFrame) -&gt; None:\n        \"\"\"\n        Internal helper to update the main view DataFrame with new columns.\n        This method is designed to be called by map_* methods.\n\n        Args:\n            new_data (pd.DataFrame): A DataFrame containing 'poi_id' and new columns\n                                     to be merged into the main view.\n        \"\"\"\n        if \"poi_id\" not in new_data.columns:\n            available_cols = list(new_data.columns)\n            raise ValueError(\n                f\"new_data DataFrame must contain 'poi_id' column. \"\n                f\"Available columns: {available_cols}\"\n            )\n\n        # Check for poi_id mismatches\n        original_poi_ids = set(self._view[\"poi_id\"])\n        new_poi_ids = set(new_data[\"poi_id\"])\n        missing_pois = original_poi_ids - new_poi_ids\n\n        if missing_pois:\n            self.logger.warning(\n                f\"{len(missing_pois)} POIs will have NaN values for new columns\"\n            )\n\n        # Ensure poi_id is the index for efficient merging\n        # Create a copy to avoid SettingWithCopyWarning if new_data is a slice\n        new_data_indexed = new_data.set_index(\"poi_id\").copy()\n\n        # Merge on 'poi_id' (which is now the index of self._view and new_data_indexed)\n        # Using left join to keep all POIs from the original view\n        self._view = (\n            self._view.set_index(\"poi_id\")\n            .join(new_data_indexed, how=\"left\")\n            .reset_index()\n        )\n\n        self.logger.debug(\n            f\"View updated with columns: {list(new_data_indexed.columns)}\"\n        )\n\n    def map_nearest_points(\n        self,\n        points_df: Union[pd.DataFrame, gpd.GeoDataFrame],\n        id_column: str,\n        lat_column: Optional[str] = None,\n        lon_column: Optional[str] = None,\n        output_prefix: str = \"nearest\",\n        **kwargs,\n    ) -&gt; pd.DataFrame:\n        \"\"\"\n        Maps nearest points from a given DataFrame to the POIs.\n\n        Enriches the `points_gdf` with the ID and distance to the nearest point\n        from the input DataFrame for each POI.\n\n        Args:\n            points_df (Union[pd.DataFrame, gpd.GeoDataFrame]):\n                DataFrame containing points to find nearest neighbors from.\n                Must have latitude and longitude columns or point geometries.\n            id_column (str):\n                Name of the column containing unique identifiers for each point.\n            lat_column (str, optional):\n                Name of the latitude column in points_df. If None, will attempt to detect it\n                or extract from geometry if points_df is a GeoDataFrame.\n            lon_column (str, optional):\n                Name of the longitude column in points_df. If None, will attempt to detect it\n                or extract from geometry if points_df is a GeoDataFrame.\n            output_prefix (str, optional):\n                Prefix for the output column names. Defaults to \"nearest\".\n            **kwargs:\n                Additional keyword arguments passed to the data reader (if applicable).\n\n        Returns:\n            pd.DataFrame: The updated GeoDataFrame with new columns:\n                          '{output_prefix}_id' and '{output_prefix}_distance'.\n                          Returns a copy of the current `points_gdf` if no points are found.\n\n        Raises:\n            ValueError: If required columns are missing from points_df or if coordinate\n                       columns cannot be detected or extracted from geometry.\n        \"\"\"\n        self.logger.info(\n            f\"Mapping nearest points from {points_df.__class__.__name__} to POIs\"\n        )\n\n        # Validate input DataFrame\n        if points_df.empty:\n            self.logger.info(\"No points found in the input DataFrame\")\n            return self.view\n\n        # Handle GeoDataFrame\n        if isinstance(points_df, gpd.GeoDataFrame):\n            points_df = points_df.copy()\n            if points_df.geometry.name == \"geometry\":\n                points_df[\"latitude\"] = points_df.geometry.y\n                points_df[\"longitude\"] = points_df.geometry.x\n                lat_column = \"latitude\"\n                lon_column = \"longitude\"\n                self.logger.info(\"Extracted coordinates from geometry\")\n\n        # Detect coordinate columns if not provided\n        if lat_column is None or lon_column is None:\n            try:\n                detected_lat, detected_lon = detect_coordinate_columns(points_df)\n                lat_column = lat_column or detected_lat\n                lon_column = lon_column or detected_lon\n                self.logger.info(\n                    f\"Detected coordinate columns: {lat_column}, {lon_column}\"\n                )\n            except ValueError as e:\n                raise ValueError(f\"Could not detect coordinate columns: {str(e)}\")\n\n        # Validate required columns\n        required_columns = [lat_column, lon_column, id_column]\n        missing_columns = [\n            col for col in required_columns if col not in points_df.columns\n        ]\n        if missing_columns:\n            raise ValueError(\n                f\"Missing required columns in points_df: {missing_columns}\"\n            )\n\n        from gigaspatial.processing.geo import calculate_distance\n\n        self.logger.info(\"Calculating nearest points for each POI\")\n        tree = cKDTree(points_df[[lat_column, lon_column]])\n        points_df_poi = self.points_gdf.copy()\n        _, idx = tree.query(points_df_poi[[\"latitude\", \"longitude\"]], k=1)\n        df_nearest = points_df.iloc[idx]\n        dist = calculate_distance(\n            lat1=points_df_poi.latitude,\n            lon1=points_df_poi.longitude,\n            lat2=df_nearest[lat_column],\n            lon2=df_nearest[lon_column],\n        )\n        # Create a temporary DataFrame to hold the results for merging\n        temp_result_df = pd.DataFrame(\n            {\n                \"poi_id\": points_df_poi[\"poi_id\"],\n                f\"{output_prefix}_id\": points_df.iloc[idx][id_column].values,\n                f\"{output_prefix}_distance\": dist,\n            }\n        )\n        # self._update_view(temp_result_df) # Removed direct view update\n        self.logger.info(\n            f\"Nearest points mapping complete with prefix '{output_prefix}'\"\n        )\n        return temp_result_df  # Return the DataFrame\n\n    def map_google_buildings(\n        self,\n        handler: Optional[GoogleOpenBuildingsHandler] = None,\n        **kwargs,\n    ) -&gt; pd.DataFrame:\n        \"\"\"\n        Maps Google Open Buildings data to the POIs by finding the nearest building.\n\n        Enriches the `points_gdf` with the ID and distance to the nearest\n        Google Open Building for each POI.\n\n        Args:\n            data_config (Optional[GoogleOpenBuildingsConfig]):\n                Configuration for accessing Google Open Buildings data. If None, a\n                default `GoogleOpenBuildingsConfig` will be used.\n            **kwargs:\n                Additional keyword arguments passed to the data reader (if applicable).\n\n        Returns:\n            pd.DataFrame: The updated GeoDataFrame with new columns:\n                          'nearest_google_building_id' and 'nearest_google_building_distance'.\n                          Returns a copy of the current `points_gdf` if no buildings are found.\n        \"\"\"\n        self.logger.info(\"Mapping Google Open Buildings data to POIs\")\n        handler = handler or GoogleOpenBuildingsHandler(data_store=self.data_store)\n\n        self.logger.info(\"Loading Google Buildings point data\")\n        buildings_df = handler.load_points(\n            self.points_gdf, ensure_available=self.config.ensure_available, **kwargs\n        )\n        if buildings_df is None or len(buildings_df) == 0:\n            self.logger.info(\"No Google buildings data found for the provided POIs\")\n            return self.view\n\n        mapped_data = self.map_nearest_points(\n            points_df=buildings_df,\n            id_column=\"full_plus_code\",\n            output_prefix=\"nearest_google_building\",\n            **kwargs,\n        )\n        self._update_view(mapped_data)\n        return self.view\n\n    def map_ms_buildings(\n        self,\n        handler: Optional[MSBuildingsHandler] = None,\n        **kwargs,\n    ) -&gt; pd.DataFrame:\n        \"\"\"\n        Maps Microsoft Global Buildings data to the POIs by finding the nearest building.\n\n        Enriches the `points_gdf` with the ID and distance to the nearest\n        Microsoft Global Building for each POI. If buildings don't have an ID column,\n        creates a unique ID using the building's coordinates.\n\n        Args:\n            data_config (Optional[MSBuildingsConfig]):\n                Configuration for accessing Microsoft Global Buildings data. If None, a\n                default `MSBuildingsConfig` will be used.\n            **kwargs:\n                Additional keyword arguments passed to the data reader (if applicable).\n\n        Returns:\n            pd.DataFrame: The updated GeoDataFrame with new columns:\n                          'nearest_ms_building_id' and 'nearest_ms_building_distance'.\n                          Returns a copy of the current `points_gdf` if no buildings are found.\n        \"\"\"\n        self.logger.info(\"Mapping Microsoft Global Buildings data to POIs\")\n        handler = handler or MSBuildingsHandler(data_store=self.data_store)\n        self.logger.info(\"Loading Microsoft Buildings polygon data\")\n        buildings_gdf = handler.load_data(\n            self.points_gdf, ensure_available=self.config.ensure_available\n        )\n        if buildings_gdf is None or len(buildings_gdf) == 0:\n            self.logger.info(\"No Microsoft buildings data found for the provided POIs\")\n            return self.points_gdf.copy()\n\n        building_centroids = get_centroids(buildings_gdf)\n\n        if \"building_id\" not in buildings_gdf:\n            self.logger.info(\"Creating building IDs from coordinates\")\n            building_centroids[\"building_id\"] = building_centroids.apply(\n                lambda row: f\"{row.geometry.y:.6f}_{row.geometry.x:.6f}\",\n                axis=1,\n            )\n\n        mapped_data = self.map_nearest_points(\n            points_df=building_centroids,\n            id_column=\"building_id\",\n            output_prefix=\"nearest_ms_building\",\n            **kwargs,\n        )\n        self._update_view(mapped_data)\n        return self.view\n\n    def map_zonal_stats(\n        self,\n        data: Union[List[TifProcessor], gpd.GeoDataFrame],\n        stat: str = \"mean\",\n        map_radius_meters: Optional[float] = None,\n        output_column: str = \"zonal_stat\",\n        value_column: Optional[str] = None,\n        predicate: Literal[\"intersects\", \"within\", \"fractional\"] = \"intersects\",\n        **kwargs,\n    ) -&gt; pd.DataFrame:\n        \"\"\"\n        Maps zonal statistics from raster or polygon data to POIs.\n\n        Can operate in three modes:\n        1. Raster point sampling: Directly samples raster values at POI locations\n        2. Raster zonal statistics: Creates buffers around POIs and calculates statistics within them\n        3. Polygon aggregation: Aggregates polygon data to POI buffers with optional area weighting\n\n        Args:\n            data (Union[TifProcessor, List[TifProcessor], gpd.GeoDataFrame]):\n                Either a TifProcessor object, a list of TifProcessor objects (which will be merged\n                into a single TifProcessor for processing), or a GeoDataFrame containing polygon\n                data to aggregate.\n            stat (str, optional):\n                For raster data: Statistic to calculate (\"sum\", \"mean\", \"median\", \"min\", \"max\").\n                For polygon data: Aggregation method to use.\n                Defaults to \"mean\".\n            map_radius_meters (float, optional):\n                If provided, creates circular buffers of this radius around each POI\n                and calculates statistics within the buffers. If None, samples directly\n                at POI locations (only for raster data).\n            output_column (str, optional):\n                Name of the output column to store the results. Defaults to \"zonal_stat\".\n            value_column (str, optional):\n                For polygon data: Name of the column to aggregate. Required for polygon data.\n                Not used for raster data.\n            predicate (Literal[\"intersects\", \"within\", \"fractional\"], optional):\n                The spatial relationship to use for aggregation. Defaults to \"intersects\".\n            **kwargs:\n                Additional keyword arguments passed to the sampling/aggregation functions.\n\n        Returns:\n            pd.DataFrame: The updated GeoDataFrame with a new column containing the\n                          calculated statistics. Returns a copy of the current `points_gdf`\n                          if no valid data is found.\n\n        Raises:\n            ValueError: If no valid data is provided, if parameters are incompatible,\n                      or if required parameters (value_column) are missing for polygon data.\n        \"\"\"\n\n        raster_processor: Optional[TifProcessor] = None\n\n        if isinstance(data, TifProcessor):\n            raster_processor = data\n        elif isinstance(data, list) and all(isinstance(x, TifProcessor) for x in data):\n            if not data:\n                self.logger.info(\"No valid raster data provided\")\n                return self.view\n\n            if len(data) &gt; 1:\n                all_source_paths = [tp.dataset_path for tp in data]\n\n                self.logger.info(\n                    f\"Merging {len(all_source_paths)} rasters into a single TifProcessor for zonal statistics.\"\n                )\n                raster_processor = TifProcessor(\n                    dataset_path=all_source_paths,\n                    data_store=self.data_store,\n                    **kwargs,\n                )\n            else:\n                raster_processor = data[0]\n\n        if raster_processor:\n            results_df = pd.DataFrame({\"poi_id\": self.points_gdf[\"poi_id\"]})\n            raster_crs = raster_processor.crs\n\n            if map_radius_meters is not None:\n                self.logger.info(\n                    f\"Calculating {stat} within {map_radius_meters}m buffers around POIs\"\n                )\n                # Create buffers around POIs\n                buffers_gdf = buffer_geodataframe(\n                    self.points_gdf,\n                    buffer_distance_meters=map_radius_meters,\n                    cap_style=\"round\",\n                )\n\n                # Calculate zonal statistics\n                sampled_values = raster_processor.sample_by_polygons(\n                    polygon_list=buffers_gdf.to_crs(raster_crs).geometry,\n                    stat=stat,\n                )\n            else:\n                self.logger.info(f\"Sampling {stat} at POI locations\")\n                # Sample directly at POI locations\n                coord_list = (\n                    self.points_gdf.to_crs(raster_crs).get_coordinates().to_numpy()\n                )\n                sampled_values = raster_processor.sample_by_coordinates(\n                    coordinate_list=coord_list, **kwargs\n                )\n\n            results_df[output_column] = sampled_values\n\n        elif isinstance(data, gpd.GeoDataFrame):\n            # Handle polygon data\n            if data.empty:\n                self.logger.info(\"No valid GeoDataFrame data provided\")\n                return pd.DataFrame(\n                    columns=[\"poi_id\", output_column]\n                )  # Return empty DataFrame\n\n            if map_radius_meters is None:\n                raise ValueError(\n                    \"map_radius_meters must be provided for for GeoDataFrame data\"\n                )\n\n            # Create buffers around POIs\n            buffer_gdf = buffer_geodataframe(\n                self.points_gdf,\n                buffer_distance_meters=map_radius_meters,\n                cap_style=\"round\",\n            )\n\n            if any(data.geom_type.isin([\"MultiPoint\", \"Point\"])):\n\n                self.logger.info(\n                    f\"Aggregating point data within {map_radius_meters}m buffers around POIs using predicate '{predicate}'\"\n                )\n\n                # If no value_column, default to 'count'\n                if value_column is None:\n                    actual_stat = \"count\"\n                    self.logger.warning(\n                        \"No value_column provided for point data. Defaulting to 'count' aggregation.\"\n                    )\n                else:\n                    actual_stat = stat\n                    if value_column not in data.columns:\n                        raise ValueError(\n                            f\"Value column '{value_column}' not found in input GeoDataFrame.\"\n                        )\n\n                aggregation_result_gdf = aggregate_points_to_zones(\n                    points=data,\n                    zones=buffer_gdf,\n                    value_columns=value_column,\n                    aggregation=actual_stat,\n                    point_zone_predicate=predicate,  # can't be `fractional``\n                    zone_id_column=\"poi_id\",\n                    output_suffix=\"\",\n                    drop_geometry=True,\n                    **kwargs,\n                )\n\n                output_col_from_agg = (\n                    f\"{value_column}_{actual_stat}\" if value_column else \"point_count\"\n                )\n                results_df = aggregation_result_gdf[[\"poi_id\", output_col_from_agg]]\n\n                if output_column != \"zonal_stat\":\n                    results_df = results_df.rename(\n                        columns={output_col_from_agg: output_column}\n                    )\n\n            else:\n                if value_column is None:\n                    raise ValueError(\n                        \"value_column must be provided for polygon data aggregation.\"\n                    )\n                if value_column not in data.columns:\n                    raise ValueError(\n                        f\"Value column '{value_column}' not found in input GeoDataFrame.\"\n                    )\n                self.logger.info(\n                    f\"Aggregating polygon data within {map_radius_meters}m buffers around POIs using predicate '{predicate}'\"\n                )\n\n                # Aggregate polygons to buffers\n                aggregation_result_gdf = aggregate_polygons_to_zones(\n                    polygons=data,\n                    zones=buffer_gdf,\n                    value_columns=value_column,\n                    aggregation=stat,\n                    predicate=predicate,\n                    zone_id_column=\"poi_id\",\n                    output_suffix=\"\",\n                    drop_geometry=True,\n                    **kwargs,\n                )\n\n                output_col_from_agg = value_column\n\n            results_df = aggregation_result_gdf[[\"poi_id\", output_col_from_agg]]\n\n            if output_column != \"zonal_stat\":\n                results_df = results_df.rename(\n                    columns={output_col_from_agg: output_column}\n                )\n\n        else:\n            raise ValueError(\n                \"data must be either a list of TifProcessor objects or a GeoDataFrame\"\n            )\n\n        # self._update_view(results_df) # Removed direct view update\n        self.logger.info(\n            f\"Zonal statistics mapping complete for column(s) derived from '{output_column}' or '{value_column}'\"\n        )\n        return results_df  # Return the DataFrame\n\n    def map_built_s(\n        self,\n        map_radius_meters: float = 150,\n        stat: str = \"sum\",\n        dataset_year=2020,\n        dataset_resolution=100,\n        output_column=\"built_surface_m2\",\n        **kwargs,\n    ) -&gt; pd.DataFrame:\n        \"\"\"\n        Maps GHSL Built Surface (GHS_BUILT_S) data to the POIs.\n\n        Calculates the sum of built surface area within a specified buffer\n        radius around each POI. Enriches `points_gdf` with the 'built_surface_m2' column.\n\n        Args:\n            data_config (Optional[GHSLDataConfig]):\n                Configuration for accessing GHSL Built Surface data. If None, a\n                default `GHSLDataConfig` for 'GHS_BUILT_S' will be used.\n            map_radius_meters (float):\n                The buffer distance in meters around each POI to calculate\n                zonal statistics for built surface. Defaults to 150 meters.\n            **kwargs:\n                Additional keyword arguments passed to the data reader (if applicable).\n\n        Returns:\n            pd.DataFrame: The updated GeoDataFrame with a new column:\n                          'built_surface_m2'. Returns a copy of the current\n                          `points_gdf` if no GHSL Built Surface data is found.\n        \"\"\"\n        self.logger.info(\"Mapping GHSL Built Surface data to POIs\")\n        handler = GHSLDataHandler(\n            product=\"GHS_BUILT_S\",\n            year=dataset_year,\n            resolution=dataset_resolution,\n            data_store=self.data_store,\n            **kwargs,\n        )\n        self.logger.info(\"Loading GHSL Built Surface raster tiles\")\n        tif_processors = handler.load_data(\n            self.points_gdf.copy(),\n            ensure_available=self.config.ensure_available,\n            merge_rasters=True,\n            **kwargs,\n        )\n\n        mapped_data = self.map_zonal_stats(\n            data=tif_processors,\n            stat=stat,\n            map_radius_meters=map_radius_meters,\n            output_column=output_column,\n            **kwargs,\n        )\n        self._update_view(mapped_data)\n        return self.view\n\n    def map_smod(\n        self,\n        dataset_year=2020,\n        dataset_resolution=1000,\n        output_column=\"smod_class\",\n        **kwargs,\n    ) -&gt; pd.DataFrame:\n        \"\"\"\n        Maps GHSL Settlement Model (SMOD) data to the POIs.\n\n        Samples the SMOD class value at each POI's location. Enriches `points_gdf`\n        with the 'smod_class' column.\n\n        Args:\n            data_config (Optional[GHSLDataConfig]):\n                Configuration for accessing GHSL SMOD data. If None, a\n                default `GHSLDataConfig` for 'GHS_SMOD' will be used.\n            **kwargs:\n                Additional keyword arguments passed to the data reader (if applicable).\n\n        Returns:\n            pd.DataFrame: The updated GeoDataFrame with a new column:\n                          'smod_class'. Returns a copy of the current\n                          `points_gdf` if no GHSL SMOD data is found.\n        \"\"\"\n        self.logger.info(\"Mapping GHSL Settlement Model (SMOD) data to POIs\")\n        handler = GHSLDataHandler(\n            product=\"GHS_SMOD\",\n            year=dataset_year,\n            resolution=dataset_resolution,\n            data_store=self.data_store,\n            coord_system=54009,\n            **kwargs,\n        )\n\n        self.logger.info(\"Loading GHSL SMOD raster tiles\")\n        tif_processors = handler.load_data(\n            self.points_gdf.copy(),\n            ensure_available=self.config.ensure_available,\n            merge_rasters=True,\n            **kwargs,\n        )\n\n        mapped_data = self.map_zonal_stats(\n            data=tif_processors,\n            output_column=output_column,\n            **kwargs,\n        )\n        self._update_view(mapped_data)\n        return self.view\n\n    def map_wp_pop(\n        self,\n        country: Union[str, List[str]],\n        map_radius_meters: float,\n        resolution=1000,\n        predicate: Literal[\n            \"centroid_within\", \"intersects\", \"fractional\", \"within\"\n        ] = \"intersects\",\n        output_column: str = \"population\",\n        **kwargs,\n    ):\n        # Ensure country is always a list for consistent handling\n        countries_list = [country] if isinstance(country, str) else country\n\n        handler = WPPopulationHandler(\n            resolution=resolution,\n            data_store=self.data_store,\n            **kwargs,\n        )\n\n        # Restrict to single country for age_structures project\n        if handler.config.project == \"age_structures\" and len(countries_list) &gt; 1:\n            raise ValueError(\n                \"For 'age_structures' project, only a single country can be processed at a time.\"\n            )\n\n        self.logger.info(\n            f\"Mapping WorldPop Population data (year: {handler.config.year}, resolution: {handler.config.resolution}m)\"\n        )\n\n        if predicate == \"fractional\" and resolution == 100:\n            self.logger.warning(\n                \"Fractional aggregations only supported for datasets with 1000m resolution. Using `intersects` as predicate\"\n            )\n            predicate = \"intersects\"\n\n        data_to_process: Union[List[TifProcessor], gpd.GeoDataFrame, pd.DataFrame]\n\n        if predicate == \"centroid_within\":\n            if handler.config.project == \"age_structures\":\n                # Load individual tif processors for the single country\n                all_tif_processors = handler.load_data(\n                    countries_list[0],\n                    ensure_available=self.config.ensure_available,\n                    **kwargs,\n                )\n\n                # Sum results from each tif_processor separately\n                summed_results_by_poi = {\n                    poi_id: 0.0 for poi_id in self.points_gdf[\"poi_id\"].unique()\n                }\n\n                self.logger.info(\n                    f\"Sampling individual age_structures rasters using 'sum' statistic and summing per POI.\"\n                )\n                for tif_processor in all_tif_processors:\n                    single_raster_df = self.map_zonal_stats(\n                        data=tif_processor,\n                        stat=\"sum\",\n                        map_radius_meters=map_radius_meters,\n                        value_column=\"pixel_value\",\n                        predicate=predicate,\n                        output_column=output_column,  # This output_column will be in the temporary DF\n                        **kwargs,\n                    )\n                    # Add values from this single raster to the cumulative sum\n                    for _, row in single_raster_df.iterrows():\n                        summed_results_by_poi[row[\"poi_id\"]] += row[output_column]\n\n                # Convert the summed dictionary back to a DataFrame\n                data_to_process = pd.DataFrame(\n                    list(summed_results_by_poi.items()),\n                    columns=[\"poi_id\", output_column],\n                )\n\n            else:\n                # Existing behavior for non-age_structures projects or if merging is fine\n                # 'data_to_process' will be a list of TifProcessor objects, which map_zonal_stats will merge\n                data_to_process = []\n                for c in countries_list:\n                    data_to_process.extend(\n                        handler.load_data(\n                            c, ensure_available=self.config.ensure_available, **kwargs\n                        )\n                    )\n        else:\n            # 'data_to_process' will be a GeoDataFrame\n            data_to_process = pd.concat(\n                [\n                    handler.load_into_geodataframe(\n                        c, ensure_available=self.config.ensure_available, **kwargs\n                    )\n                    for c in countries_list  # Original iteration over countries_list\n                ],\n                ignore_index=True,\n            )\n\n        self.logger.info(\n            f\"Mapping WorldPop Population data into {map_radius_meters}m zones around POIs using 'sum' statistic\"\n        )\n\n        final_mapped_df: pd.DataFrame\n\n        # If 'data_to_process' is already the summed DataFrame (from age_structures/centroid_within branch),\n        # use it directly.\n        if (\n            isinstance(data_to_process, pd.DataFrame)\n            and output_column in data_to_process.columns\n            and \"poi_id\" in data_to_process.columns\n        ):\n            final_mapped_df = data_to_process\n        else:\n            # For other cases, proceed with the original call to map_zonal_stats\n            final_mapped_df = self.map_zonal_stats(\n                data=data_to_process,\n                stat=\"sum\",\n                map_radius_meters=map_radius_meters,\n                value_column=\"pixel_value\",\n                predicate=predicate,\n                output_column=output_column,\n                **kwargs,\n            )\n        self._update_view(\n            final_mapped_df\n        )  # Update the view with the final mapped DataFrame\n        return self.view\n\n    def save_view(\n        self,\n        name: str,\n        output_format: Optional[str] = None,\n    ) -&gt; Path:\n        \"\"\"\n        Saves the current POI view (the enriched DataFrame) to a file.\n\n        The output path and format are determined by the `config`\n        or overridden by the `output_format` parameter.\n\n        Args:\n            name (str): The base name for the output file (without extension).\n            output_format (Optional[str]):\n                The desired output format (e.g., \"csv\", \"geojson\"). If None,\n                the `output_format` from `config` will be used.\n\n        Returns:\n            Path: The full path to the saved output file.\n        \"\"\"\n        format_to_use = output_format or self.config.output_format\n        output_path = self.config.base_path / f\"{name}.{format_to_use}\"\n\n        self.logger.info(f\"Saving POI view to {output_path}\")\n        # Save the current view, which is a pandas DataFrame, not a GeoDataFrame\n        # GeoJSON/Shapefile formats would require converting back to GeoDataFrame first.\n        # For CSV, Parquet, Feather, this is fine.\n        if format_to_use in [\"geojson\", \"shp\", \"gpkg\"]:\n            self.logger.warning(\n                f\"Saving to {format_to_use} requires converting back to GeoDataFrame. Geometry column will be re-added.\"\n            )\n            # Re-add geometry for saving to geospatial formats\n            view_to_save_gdf = self.view.merge(\n                self.points_gdf[[\"poi_id\", \"geometry\"]], on=\"poi_id\", how=\"left\"\n            )\n            write_dataset(\n                data=view_to_save_gdf,\n                path=str(output_path),\n                data_store=self.data_store,\n            )\n        else:\n            write_dataset(\n                data=self.view,  # Use the internal _view DataFrame\n                path=str(output_path),\n                data_store=self.data_store,\n            )\n\n        return output_path\n\n    def to_dataframe(self) -&gt; pd.DataFrame:\n        \"\"\"\n        Returns the current POI view as a DataFrame.\n\n        This method combines all accumulated variables in the view\n\n        Returns:\n            pd.DataFrame: The current view.\n        \"\"\"\n        return self.view\n\n    def to_geodataframe(self) -&gt; gpd.GeoDataFrame:\n        \"\"\"\n        Returns the current POI view merged with the original point geometries as a GeoDataFrame.\n\n        This method combines all accumulated variables in the view with the corresponding\n        point geometries, providing a spatially-enabled DataFrame for further analysis or export.\n\n        Returns:\n            gpd.GeoDataFrame: The current view merged with point geometries.\n        \"\"\"\n        return gpd.GeoDataFrame(\n            self.view.merge(\n                self.points_gdf[[\"poi_id\", \"geometry\"]], on=\"poi_id\", how=\"left\"\n            ),\n            crs=\"EPSG:4326\",\n        )\n\n    def chain_operations(self, operations: List[dict]) -&gt; \"PoiViewGenerator\":\n        \"\"\"\n        Chain multiple mapping operations for fluent interface.\n\n        Args:\n            operations: List of dicts with 'method' and 'kwargs' keys\n\n        Example:\n            generator.chain_operations([\n                {'method': 'map_google_buildings', 'kwargs': {}},\n                {'method': 'map_built_s', 'kwargs': {'map_radius_meters': 200}},\n            ])\n        \"\"\"\n        for op in operations:\n            method_name = op[\"method\"]\n            kwargs = op.get(\"kwargs\", {})\n            if hasattr(self, method_name):\n                getattr(self, method_name)(**kwargs)\n            else:\n                raise AttributeError(f\"Method {method_name} not found\")\n        return self\n\n    def validate_data_coverage(self, data_bounds: gpd.GeoDataFrame) -&gt; dict:\n        \"\"\"\n        Validate how many POIs fall within the data coverage area.\n\n        Returns:\n            dict: Coverage statistics\n        \"\"\"\n        poi_within = self.points_gdf.within(data_bounds.union_all())\n        coverage_stats = {\n            \"total_pois\": len(self.points_gdf),\n            \"covered_pois\": poi_within.sum(),\n            \"coverage_percentage\": (poi_within.sum() / len(self.points_gdf)) * 100,\n            \"uncovered_pois\": (~poi_within).sum(),\n        }\n        return coverage_stats\n</code></pre>"},{"location":"api/generators/#gigaspatial.generators.poi.PoiViewGenerator.points_gdf","title":"<code>points_gdf: gpd.GeoDataFrame</code>  <code>property</code>","text":"<p>Gets the internal GeoDataFrame of points of interest.</p>"},{"location":"api/generators/#gigaspatial.generators.poi.PoiViewGenerator.view","title":"<code>view: pd.DataFrame</code>  <code>property</code>","text":"<p>The DataFrame representing the current point of interest view.</p>"},{"location":"api/generators/#gigaspatial.generators.poi.PoiViewGenerator.__init__","title":"<code>__init__(points, poi_id_column='poi_id', config=None, data_store=None, logger=None)</code>","text":"<p>Initializes the PoiViewGenerator with the input points and configurations.</p> <p>The input <code>points</code> are converted into an internal GeoDataFrame (<code>_points_gdf</code>) for consistent geospatial operations.</p> <p>Parameters:</p> Name Type Description Default <code>points</code> <code>Union[List[Tuple[float, float]], List[dict], DataFrame, GeoDataFrame]</code> <p>The input points of interest. Can be: - A list of (latitude, longitude) tuples. - A list of dictionaries, where each dict must contain 'latitude' and 'longitude' keys. - A pandas DataFrame with 'latitude' and 'longitude' columns. - A geopandas GeoDataFrame (expected to have a 'geometry' column representing points).</p> required <code>generator_config</code> <code>Optional[PoiViewGeneratorConfig]</code> <p>Configuration for the POI view generation process. If None, a default <code>PoiViewGeneratorConfig</code> will be used.</p> required <code>data_store</code> <code>Optional[DataStore]</code> <p>An instance of a data store for managing data access (e.g., LocalDataStore). If None, a default <code>LocalDataStore</code> will be used.</p> <code>None</code> Source code in <code>gigaspatial/generators/poi.py</code> <pre><code>def __init__(\n    self,\n    points: Union[\n        List[Tuple[float, float]], List[dict], pd.DataFrame, gpd.GeoDataFrame\n    ],\n    poi_id_column: str = \"poi_id\",\n    config: Optional[PoiViewGeneratorConfig] = None,\n    data_store: Optional[DataStore] = None,\n    logger: logging.Logger = None,\n):\n    \"\"\"\n    Initializes the PoiViewGenerator with the input points and configurations.\n\n    The input `points` are converted into an internal GeoDataFrame\n    (`_points_gdf`) for consistent geospatial operations.\n\n    Args:\n        points (Union[List[Tuple[float, float]], List[dict], pd.DataFrame, gpd.GeoDataFrame]):\n            The input points of interest. Can be:\n            - A list of (latitude, longitude) tuples.\n            - A list of dictionaries, where each dict must contain 'latitude' and 'longitude' keys.\n            - A pandas DataFrame with 'latitude' and 'longitude' columns.\n            - A geopandas GeoDataFrame (expected to have a 'geometry' column representing points).\n        generator_config (Optional[PoiViewGeneratorConfig]):\n            Configuration for the POI view generation process. If None, a\n            default `PoiViewGeneratorConfig` will be used.\n        data_store (Optional[DataStore]):\n            An instance of a data store for managing data access (e.g., LocalDataStore).\n            If None, a default `LocalDataStore` will be used.\n    \"\"\"\n    if hasattr(points, \"__len__\") and len(points) == 0:\n        raise ValueError(\"Points input cannot be empty\")\n\n    self.config = config or PoiViewGeneratorConfig()\n    self.data_store = data_store or LocalDataStore()\n    self.logger = logger or global_config.get_logger(self.__class__.__name__)\n    self._points_gdf = self._init_points_gdf(points, poi_id_column)\n    self._view: pd.DataFrame = self._points_gdf.drop(columns=[\"geometry\"])\n</code></pre>"},{"location":"api/generators/#gigaspatial.generators.poi.PoiViewGenerator.chain_operations","title":"<code>chain_operations(operations)</code>","text":"<p>Chain multiple mapping operations for fluent interface.</p> <p>Parameters:</p> Name Type Description Default <code>operations</code> <code>List[dict]</code> <p>List of dicts with 'method' and 'kwargs' keys</p> required Example <p>generator.chain_operations([     {'method': 'map_google_buildings', 'kwargs': {}},     {'method': 'map_built_s', 'kwargs': {'map_radius_meters': 200}}, ])</p> Source code in <code>gigaspatial/generators/poi.py</code> <pre><code>def chain_operations(self, operations: List[dict]) -&gt; \"PoiViewGenerator\":\n    \"\"\"\n    Chain multiple mapping operations for fluent interface.\n\n    Args:\n        operations: List of dicts with 'method' and 'kwargs' keys\n\n    Example:\n        generator.chain_operations([\n            {'method': 'map_google_buildings', 'kwargs': {}},\n            {'method': 'map_built_s', 'kwargs': {'map_radius_meters': 200}},\n        ])\n    \"\"\"\n    for op in operations:\n        method_name = op[\"method\"]\n        kwargs = op.get(\"kwargs\", {})\n        if hasattr(self, method_name):\n            getattr(self, method_name)(**kwargs)\n        else:\n            raise AttributeError(f\"Method {method_name} not found\")\n    return self\n</code></pre>"},{"location":"api/generators/#gigaspatial.generators.poi.PoiViewGenerator.map_built_s","title":"<code>map_built_s(map_radius_meters=150, stat='sum', dataset_year=2020, dataset_resolution=100, output_column='built_surface_m2', **kwargs)</code>","text":"<p>Maps GHSL Built Surface (GHS_BUILT_S) data to the POIs.</p> <p>Calculates the sum of built surface area within a specified buffer radius around each POI. Enriches <code>points_gdf</code> with the 'built_surface_m2' column.</p> <p>Parameters:</p> Name Type Description Default <code>data_config</code> <code>Optional[GHSLDataConfig]</code> <p>Configuration for accessing GHSL Built Surface data. If None, a default <code>GHSLDataConfig</code> for 'GHS_BUILT_S' will be used.</p> required <code>map_radius_meters</code> <code>float</code> <p>The buffer distance in meters around each POI to calculate zonal statistics for built surface. Defaults to 150 meters.</p> <code>150</code> <code>**kwargs</code> <p>Additional keyword arguments passed to the data reader (if applicable).</p> <code>{}</code> <p>Returns:</p> Type Description <code>DataFrame</code> <p>pd.DataFrame: The updated GeoDataFrame with a new column:           'built_surface_m2'. Returns a copy of the current           <code>points_gdf</code> if no GHSL Built Surface data is found.</p> Source code in <code>gigaspatial/generators/poi.py</code> <pre><code>def map_built_s(\n    self,\n    map_radius_meters: float = 150,\n    stat: str = \"sum\",\n    dataset_year=2020,\n    dataset_resolution=100,\n    output_column=\"built_surface_m2\",\n    **kwargs,\n) -&gt; pd.DataFrame:\n    \"\"\"\n    Maps GHSL Built Surface (GHS_BUILT_S) data to the POIs.\n\n    Calculates the sum of built surface area within a specified buffer\n    radius around each POI. Enriches `points_gdf` with the 'built_surface_m2' column.\n\n    Args:\n        data_config (Optional[GHSLDataConfig]):\n            Configuration for accessing GHSL Built Surface data. If None, a\n            default `GHSLDataConfig` for 'GHS_BUILT_S' will be used.\n        map_radius_meters (float):\n            The buffer distance in meters around each POI to calculate\n            zonal statistics for built surface. Defaults to 150 meters.\n        **kwargs:\n            Additional keyword arguments passed to the data reader (if applicable).\n\n    Returns:\n        pd.DataFrame: The updated GeoDataFrame with a new column:\n                      'built_surface_m2'. Returns a copy of the current\n                      `points_gdf` if no GHSL Built Surface data is found.\n    \"\"\"\n    self.logger.info(\"Mapping GHSL Built Surface data to POIs\")\n    handler = GHSLDataHandler(\n        product=\"GHS_BUILT_S\",\n        year=dataset_year,\n        resolution=dataset_resolution,\n        data_store=self.data_store,\n        **kwargs,\n    )\n    self.logger.info(\"Loading GHSL Built Surface raster tiles\")\n    tif_processors = handler.load_data(\n        self.points_gdf.copy(),\n        ensure_available=self.config.ensure_available,\n        merge_rasters=True,\n        **kwargs,\n    )\n\n    mapped_data = self.map_zonal_stats(\n        data=tif_processors,\n        stat=stat,\n        map_radius_meters=map_radius_meters,\n        output_column=output_column,\n        **kwargs,\n    )\n    self._update_view(mapped_data)\n    return self.view\n</code></pre>"},{"location":"api/generators/#gigaspatial.generators.poi.PoiViewGenerator.map_google_buildings","title":"<code>map_google_buildings(handler=None, **kwargs)</code>","text":"<p>Maps Google Open Buildings data to the POIs by finding the nearest building.</p> <p>Enriches the <code>points_gdf</code> with the ID and distance to the nearest Google Open Building for each POI.</p> <p>Parameters:</p> Name Type Description Default <code>data_config</code> <code>Optional[GoogleOpenBuildingsConfig]</code> <p>Configuration for accessing Google Open Buildings data. If None, a default <code>GoogleOpenBuildingsConfig</code> will be used.</p> required <code>**kwargs</code> <p>Additional keyword arguments passed to the data reader (if applicable).</p> <code>{}</code> <p>Returns:</p> Type Description <code>DataFrame</code> <p>pd.DataFrame: The updated GeoDataFrame with new columns:           'nearest_google_building_id' and 'nearest_google_building_distance'.           Returns a copy of the current <code>points_gdf</code> if no buildings are found.</p> Source code in <code>gigaspatial/generators/poi.py</code> <pre><code>def map_google_buildings(\n    self,\n    handler: Optional[GoogleOpenBuildingsHandler] = None,\n    **kwargs,\n) -&gt; pd.DataFrame:\n    \"\"\"\n    Maps Google Open Buildings data to the POIs by finding the nearest building.\n\n    Enriches the `points_gdf` with the ID and distance to the nearest\n    Google Open Building for each POI.\n\n    Args:\n        data_config (Optional[GoogleOpenBuildingsConfig]):\n            Configuration for accessing Google Open Buildings data. If None, a\n            default `GoogleOpenBuildingsConfig` will be used.\n        **kwargs:\n            Additional keyword arguments passed to the data reader (if applicable).\n\n    Returns:\n        pd.DataFrame: The updated GeoDataFrame with new columns:\n                      'nearest_google_building_id' and 'nearest_google_building_distance'.\n                      Returns a copy of the current `points_gdf` if no buildings are found.\n    \"\"\"\n    self.logger.info(\"Mapping Google Open Buildings data to POIs\")\n    handler = handler or GoogleOpenBuildingsHandler(data_store=self.data_store)\n\n    self.logger.info(\"Loading Google Buildings point data\")\n    buildings_df = handler.load_points(\n        self.points_gdf, ensure_available=self.config.ensure_available, **kwargs\n    )\n    if buildings_df is None or len(buildings_df) == 0:\n        self.logger.info(\"No Google buildings data found for the provided POIs\")\n        return self.view\n\n    mapped_data = self.map_nearest_points(\n        points_df=buildings_df,\n        id_column=\"full_plus_code\",\n        output_prefix=\"nearest_google_building\",\n        **kwargs,\n    )\n    self._update_view(mapped_data)\n    return self.view\n</code></pre>"},{"location":"api/generators/#gigaspatial.generators.poi.PoiViewGenerator.map_ms_buildings","title":"<code>map_ms_buildings(handler=None, **kwargs)</code>","text":"<p>Maps Microsoft Global Buildings data to the POIs by finding the nearest building.</p> <p>Enriches the <code>points_gdf</code> with the ID and distance to the nearest Microsoft Global Building for each POI. If buildings don't have an ID column, creates a unique ID using the building's coordinates.</p> <p>Parameters:</p> Name Type Description Default <code>data_config</code> <code>Optional[MSBuildingsConfig]</code> <p>Configuration for accessing Microsoft Global Buildings data. If None, a default <code>MSBuildingsConfig</code> will be used.</p> required <code>**kwargs</code> <p>Additional keyword arguments passed to the data reader (if applicable).</p> <code>{}</code> <p>Returns:</p> Type Description <code>DataFrame</code> <p>pd.DataFrame: The updated GeoDataFrame with new columns:           'nearest_ms_building_id' and 'nearest_ms_building_distance'.           Returns a copy of the current <code>points_gdf</code> if no buildings are found.</p> Source code in <code>gigaspatial/generators/poi.py</code> <pre><code>def map_ms_buildings(\n    self,\n    handler: Optional[MSBuildingsHandler] = None,\n    **kwargs,\n) -&gt; pd.DataFrame:\n    \"\"\"\n    Maps Microsoft Global Buildings data to the POIs by finding the nearest building.\n\n    Enriches the `points_gdf` with the ID and distance to the nearest\n    Microsoft Global Building for each POI. If buildings don't have an ID column,\n    creates a unique ID using the building's coordinates.\n\n    Args:\n        data_config (Optional[MSBuildingsConfig]):\n            Configuration for accessing Microsoft Global Buildings data. If None, a\n            default `MSBuildingsConfig` will be used.\n        **kwargs:\n            Additional keyword arguments passed to the data reader (if applicable).\n\n    Returns:\n        pd.DataFrame: The updated GeoDataFrame with new columns:\n                      'nearest_ms_building_id' and 'nearest_ms_building_distance'.\n                      Returns a copy of the current `points_gdf` if no buildings are found.\n    \"\"\"\n    self.logger.info(\"Mapping Microsoft Global Buildings data to POIs\")\n    handler = handler or MSBuildingsHandler(data_store=self.data_store)\n    self.logger.info(\"Loading Microsoft Buildings polygon data\")\n    buildings_gdf = handler.load_data(\n        self.points_gdf, ensure_available=self.config.ensure_available\n    )\n    if buildings_gdf is None or len(buildings_gdf) == 0:\n        self.logger.info(\"No Microsoft buildings data found for the provided POIs\")\n        return self.points_gdf.copy()\n\n    building_centroids = get_centroids(buildings_gdf)\n\n    if \"building_id\" not in buildings_gdf:\n        self.logger.info(\"Creating building IDs from coordinates\")\n        building_centroids[\"building_id\"] = building_centroids.apply(\n            lambda row: f\"{row.geometry.y:.6f}_{row.geometry.x:.6f}\",\n            axis=1,\n        )\n\n    mapped_data = self.map_nearest_points(\n        points_df=building_centroids,\n        id_column=\"building_id\",\n        output_prefix=\"nearest_ms_building\",\n        **kwargs,\n    )\n    self._update_view(mapped_data)\n    return self.view\n</code></pre>"},{"location":"api/generators/#gigaspatial.generators.poi.PoiViewGenerator.map_nearest_points","title":"<code>map_nearest_points(points_df, id_column, lat_column=None, lon_column=None, output_prefix='nearest', **kwargs)</code>","text":"<p>Maps nearest points from a given DataFrame to the POIs.</p> <p>Enriches the <code>points_gdf</code> with the ID and distance to the nearest point from the input DataFrame for each POI.</p> <p>Parameters:</p> Name Type Description Default <code>points_df</code> <code>Union[DataFrame, GeoDataFrame]</code> <p>DataFrame containing points to find nearest neighbors from. Must have latitude and longitude columns or point geometries.</p> required <code>id_column</code> <code>str</code> <p>Name of the column containing unique identifiers for each point.</p> required <code>lat_column</code> <code>str</code> <p>Name of the latitude column in points_df. If None, will attempt to detect it or extract from geometry if points_df is a GeoDataFrame.</p> <code>None</code> <code>lon_column</code> <code>str</code> <p>Name of the longitude column in points_df. If None, will attempt to detect it or extract from geometry if points_df is a GeoDataFrame.</p> <code>None</code> <code>output_prefix</code> <code>str</code> <p>Prefix for the output column names. Defaults to \"nearest\".</p> <code>'nearest'</code> <code>**kwargs</code> <p>Additional keyword arguments passed to the data reader (if applicable).</p> <code>{}</code> <p>Returns:</p> Type Description <code>DataFrame</code> <p>pd.DataFrame: The updated GeoDataFrame with new columns:           '{output_prefix}_id' and '{output_prefix}_distance'.           Returns a copy of the current <code>points_gdf</code> if no points are found.</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If required columns are missing from points_df or if coordinate        columns cannot be detected or extracted from geometry.</p> Source code in <code>gigaspatial/generators/poi.py</code> <pre><code>def map_nearest_points(\n    self,\n    points_df: Union[pd.DataFrame, gpd.GeoDataFrame],\n    id_column: str,\n    lat_column: Optional[str] = None,\n    lon_column: Optional[str] = None,\n    output_prefix: str = \"nearest\",\n    **kwargs,\n) -&gt; pd.DataFrame:\n    \"\"\"\n    Maps nearest points from a given DataFrame to the POIs.\n\n    Enriches the `points_gdf` with the ID and distance to the nearest point\n    from the input DataFrame for each POI.\n\n    Args:\n        points_df (Union[pd.DataFrame, gpd.GeoDataFrame]):\n            DataFrame containing points to find nearest neighbors from.\n            Must have latitude and longitude columns or point geometries.\n        id_column (str):\n            Name of the column containing unique identifiers for each point.\n        lat_column (str, optional):\n            Name of the latitude column in points_df. If None, will attempt to detect it\n            or extract from geometry if points_df is a GeoDataFrame.\n        lon_column (str, optional):\n            Name of the longitude column in points_df. If None, will attempt to detect it\n            or extract from geometry if points_df is a GeoDataFrame.\n        output_prefix (str, optional):\n            Prefix for the output column names. Defaults to \"nearest\".\n        **kwargs:\n            Additional keyword arguments passed to the data reader (if applicable).\n\n    Returns:\n        pd.DataFrame: The updated GeoDataFrame with new columns:\n                      '{output_prefix}_id' and '{output_prefix}_distance'.\n                      Returns a copy of the current `points_gdf` if no points are found.\n\n    Raises:\n        ValueError: If required columns are missing from points_df or if coordinate\n                   columns cannot be detected or extracted from geometry.\n    \"\"\"\n    self.logger.info(\n        f\"Mapping nearest points from {points_df.__class__.__name__} to POIs\"\n    )\n\n    # Validate input DataFrame\n    if points_df.empty:\n        self.logger.info(\"No points found in the input DataFrame\")\n        return self.view\n\n    # Handle GeoDataFrame\n    if isinstance(points_df, gpd.GeoDataFrame):\n        points_df = points_df.copy()\n        if points_df.geometry.name == \"geometry\":\n            points_df[\"latitude\"] = points_df.geometry.y\n            points_df[\"longitude\"] = points_df.geometry.x\n            lat_column = \"latitude\"\n            lon_column = \"longitude\"\n            self.logger.info(\"Extracted coordinates from geometry\")\n\n    # Detect coordinate columns if not provided\n    if lat_column is None or lon_column is None:\n        try:\n            detected_lat, detected_lon = detect_coordinate_columns(points_df)\n            lat_column = lat_column or detected_lat\n            lon_column = lon_column or detected_lon\n            self.logger.info(\n                f\"Detected coordinate columns: {lat_column}, {lon_column}\"\n            )\n        except ValueError as e:\n            raise ValueError(f\"Could not detect coordinate columns: {str(e)}\")\n\n    # Validate required columns\n    required_columns = [lat_column, lon_column, id_column]\n    missing_columns = [\n        col for col in required_columns if col not in points_df.columns\n    ]\n    if missing_columns:\n        raise ValueError(\n            f\"Missing required columns in points_df: {missing_columns}\"\n        )\n\n    from gigaspatial.processing.geo import calculate_distance\n\n    self.logger.info(\"Calculating nearest points for each POI\")\n    tree = cKDTree(points_df[[lat_column, lon_column]])\n    points_df_poi = self.points_gdf.copy()\n    _, idx = tree.query(points_df_poi[[\"latitude\", \"longitude\"]], k=1)\n    df_nearest = points_df.iloc[idx]\n    dist = calculate_distance(\n        lat1=points_df_poi.latitude,\n        lon1=points_df_poi.longitude,\n        lat2=df_nearest[lat_column],\n        lon2=df_nearest[lon_column],\n    )\n    # Create a temporary DataFrame to hold the results for merging\n    temp_result_df = pd.DataFrame(\n        {\n            \"poi_id\": points_df_poi[\"poi_id\"],\n            f\"{output_prefix}_id\": points_df.iloc[idx][id_column].values,\n            f\"{output_prefix}_distance\": dist,\n        }\n    )\n    # self._update_view(temp_result_df) # Removed direct view update\n    self.logger.info(\n        f\"Nearest points mapping complete with prefix '{output_prefix}'\"\n    )\n    return temp_result_df  # Return the DataFrame\n</code></pre>"},{"location":"api/generators/#gigaspatial.generators.poi.PoiViewGenerator.map_smod","title":"<code>map_smod(dataset_year=2020, dataset_resolution=1000, output_column='smod_class', **kwargs)</code>","text":"<p>Maps GHSL Settlement Model (SMOD) data to the POIs.</p> <p>Samples the SMOD class value at each POI's location. Enriches <code>points_gdf</code> with the 'smod_class' column.</p> <p>Parameters:</p> Name Type Description Default <code>data_config</code> <code>Optional[GHSLDataConfig]</code> <p>Configuration for accessing GHSL SMOD data. If None, a default <code>GHSLDataConfig</code> for 'GHS_SMOD' will be used.</p> required <code>**kwargs</code> <p>Additional keyword arguments passed to the data reader (if applicable).</p> <code>{}</code> <p>Returns:</p> Type Description <code>DataFrame</code> <p>pd.DataFrame: The updated GeoDataFrame with a new column:           'smod_class'. Returns a copy of the current           <code>points_gdf</code> if no GHSL SMOD data is found.</p> Source code in <code>gigaspatial/generators/poi.py</code> <pre><code>def map_smod(\n    self,\n    dataset_year=2020,\n    dataset_resolution=1000,\n    output_column=\"smod_class\",\n    **kwargs,\n) -&gt; pd.DataFrame:\n    \"\"\"\n    Maps GHSL Settlement Model (SMOD) data to the POIs.\n\n    Samples the SMOD class value at each POI's location. Enriches `points_gdf`\n    with the 'smod_class' column.\n\n    Args:\n        data_config (Optional[GHSLDataConfig]):\n            Configuration for accessing GHSL SMOD data. If None, a\n            default `GHSLDataConfig` for 'GHS_SMOD' will be used.\n        **kwargs:\n            Additional keyword arguments passed to the data reader (if applicable).\n\n    Returns:\n        pd.DataFrame: The updated GeoDataFrame with a new column:\n                      'smod_class'. Returns a copy of the current\n                      `points_gdf` if no GHSL SMOD data is found.\n    \"\"\"\n    self.logger.info(\"Mapping GHSL Settlement Model (SMOD) data to POIs\")\n    handler = GHSLDataHandler(\n        product=\"GHS_SMOD\",\n        year=dataset_year,\n        resolution=dataset_resolution,\n        data_store=self.data_store,\n        coord_system=54009,\n        **kwargs,\n    )\n\n    self.logger.info(\"Loading GHSL SMOD raster tiles\")\n    tif_processors = handler.load_data(\n        self.points_gdf.copy(),\n        ensure_available=self.config.ensure_available,\n        merge_rasters=True,\n        **kwargs,\n    )\n\n    mapped_data = self.map_zonal_stats(\n        data=tif_processors,\n        output_column=output_column,\n        **kwargs,\n    )\n    self._update_view(mapped_data)\n    return self.view\n</code></pre>"},{"location":"api/generators/#gigaspatial.generators.poi.PoiViewGenerator.map_zonal_stats","title":"<code>map_zonal_stats(data, stat='mean', map_radius_meters=None, output_column='zonal_stat', value_column=None, predicate='intersects', **kwargs)</code>","text":"<p>Maps zonal statistics from raster or polygon data to POIs.</p> <p>Can operate in three modes: 1. Raster point sampling: Directly samples raster values at POI locations 2. Raster zonal statistics: Creates buffers around POIs and calculates statistics within them 3. Polygon aggregation: Aggregates polygon data to POI buffers with optional area weighting</p> <p>Parameters:</p> Name Type Description Default <code>data</code> <code>Union[TifProcessor, List[TifProcessor], GeoDataFrame]</code> <p>Either a TifProcessor object, a list of TifProcessor objects (which will be merged into a single TifProcessor for processing), or a GeoDataFrame containing polygon data to aggregate.</p> required <code>stat</code> <code>str</code> <p>For raster data: Statistic to calculate (\"sum\", \"mean\", \"median\", \"min\", \"max\"). For polygon data: Aggregation method to use. Defaults to \"mean\".</p> <code>'mean'</code> <code>map_radius_meters</code> <code>float</code> <p>If provided, creates circular buffers of this radius around each POI and calculates statistics within the buffers. If None, samples directly at POI locations (only for raster data).</p> <code>None</code> <code>output_column</code> <code>str</code> <p>Name of the output column to store the results. Defaults to \"zonal_stat\".</p> <code>'zonal_stat'</code> <code>value_column</code> <code>str</code> <p>For polygon data: Name of the column to aggregate. Required for polygon data. Not used for raster data.</p> <code>None</code> <code>predicate</code> <code>Literal['intersects', 'within', 'fractional']</code> <p>The spatial relationship to use for aggregation. Defaults to \"intersects\".</p> <code>'intersects'</code> <code>**kwargs</code> <p>Additional keyword arguments passed to the sampling/aggregation functions.</p> <code>{}</code> <p>Returns:</p> Type Description <code>DataFrame</code> <p>pd.DataFrame: The updated GeoDataFrame with a new column containing the           calculated statistics. Returns a copy of the current <code>points_gdf</code>           if no valid data is found.</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If no valid data is provided, if parameters are incompatible,       or if required parameters (value_column) are missing for polygon data.</p> Source code in <code>gigaspatial/generators/poi.py</code> <pre><code>def map_zonal_stats(\n    self,\n    data: Union[List[TifProcessor], gpd.GeoDataFrame],\n    stat: str = \"mean\",\n    map_radius_meters: Optional[float] = None,\n    output_column: str = \"zonal_stat\",\n    value_column: Optional[str] = None,\n    predicate: Literal[\"intersects\", \"within\", \"fractional\"] = \"intersects\",\n    **kwargs,\n) -&gt; pd.DataFrame:\n    \"\"\"\n    Maps zonal statistics from raster or polygon data to POIs.\n\n    Can operate in three modes:\n    1. Raster point sampling: Directly samples raster values at POI locations\n    2. Raster zonal statistics: Creates buffers around POIs and calculates statistics within them\n    3. Polygon aggregation: Aggregates polygon data to POI buffers with optional area weighting\n\n    Args:\n        data (Union[TifProcessor, List[TifProcessor], gpd.GeoDataFrame]):\n            Either a TifProcessor object, a list of TifProcessor objects (which will be merged\n            into a single TifProcessor for processing), or a GeoDataFrame containing polygon\n            data to aggregate.\n        stat (str, optional):\n            For raster data: Statistic to calculate (\"sum\", \"mean\", \"median\", \"min\", \"max\").\n            For polygon data: Aggregation method to use.\n            Defaults to \"mean\".\n        map_radius_meters (float, optional):\n            If provided, creates circular buffers of this radius around each POI\n            and calculates statistics within the buffers. If None, samples directly\n            at POI locations (only for raster data).\n        output_column (str, optional):\n            Name of the output column to store the results. Defaults to \"zonal_stat\".\n        value_column (str, optional):\n            For polygon data: Name of the column to aggregate. Required for polygon data.\n            Not used for raster data.\n        predicate (Literal[\"intersects\", \"within\", \"fractional\"], optional):\n            The spatial relationship to use for aggregation. Defaults to \"intersects\".\n        **kwargs:\n            Additional keyword arguments passed to the sampling/aggregation functions.\n\n    Returns:\n        pd.DataFrame: The updated GeoDataFrame with a new column containing the\n                      calculated statistics. Returns a copy of the current `points_gdf`\n                      if no valid data is found.\n\n    Raises:\n        ValueError: If no valid data is provided, if parameters are incompatible,\n                  or if required parameters (value_column) are missing for polygon data.\n    \"\"\"\n\n    raster_processor: Optional[TifProcessor] = None\n\n    if isinstance(data, TifProcessor):\n        raster_processor = data\n    elif isinstance(data, list) and all(isinstance(x, TifProcessor) for x in data):\n        if not data:\n            self.logger.info(\"No valid raster data provided\")\n            return self.view\n\n        if len(data) &gt; 1:\n            all_source_paths = [tp.dataset_path for tp in data]\n\n            self.logger.info(\n                f\"Merging {len(all_source_paths)} rasters into a single TifProcessor for zonal statistics.\"\n            )\n            raster_processor = TifProcessor(\n                dataset_path=all_source_paths,\n                data_store=self.data_store,\n                **kwargs,\n            )\n        else:\n            raster_processor = data[0]\n\n    if raster_processor:\n        results_df = pd.DataFrame({\"poi_id\": self.points_gdf[\"poi_id\"]})\n        raster_crs = raster_processor.crs\n\n        if map_radius_meters is not None:\n            self.logger.info(\n                f\"Calculating {stat} within {map_radius_meters}m buffers around POIs\"\n            )\n            # Create buffers around POIs\n            buffers_gdf = buffer_geodataframe(\n                self.points_gdf,\n                buffer_distance_meters=map_radius_meters,\n                cap_style=\"round\",\n            )\n\n            # Calculate zonal statistics\n            sampled_values = raster_processor.sample_by_polygons(\n                polygon_list=buffers_gdf.to_crs(raster_crs).geometry,\n                stat=stat,\n            )\n        else:\n            self.logger.info(f\"Sampling {stat} at POI locations\")\n            # Sample directly at POI locations\n            coord_list = (\n                self.points_gdf.to_crs(raster_crs).get_coordinates().to_numpy()\n            )\n            sampled_values = raster_processor.sample_by_coordinates(\n                coordinate_list=coord_list, **kwargs\n            )\n\n        results_df[output_column] = sampled_values\n\n    elif isinstance(data, gpd.GeoDataFrame):\n        # Handle polygon data\n        if data.empty:\n            self.logger.info(\"No valid GeoDataFrame data provided\")\n            return pd.DataFrame(\n                columns=[\"poi_id\", output_column]\n            )  # Return empty DataFrame\n\n        if map_radius_meters is None:\n            raise ValueError(\n                \"map_radius_meters must be provided for for GeoDataFrame data\"\n            )\n\n        # Create buffers around POIs\n        buffer_gdf = buffer_geodataframe(\n            self.points_gdf,\n            buffer_distance_meters=map_radius_meters,\n            cap_style=\"round\",\n        )\n\n        if any(data.geom_type.isin([\"MultiPoint\", \"Point\"])):\n\n            self.logger.info(\n                f\"Aggregating point data within {map_radius_meters}m buffers around POIs using predicate '{predicate}'\"\n            )\n\n            # If no value_column, default to 'count'\n            if value_column is None:\n                actual_stat = \"count\"\n                self.logger.warning(\n                    \"No value_column provided for point data. Defaulting to 'count' aggregation.\"\n                )\n            else:\n                actual_stat = stat\n                if value_column not in data.columns:\n                    raise ValueError(\n                        f\"Value column '{value_column}' not found in input GeoDataFrame.\"\n                    )\n\n            aggregation_result_gdf = aggregate_points_to_zones(\n                points=data,\n                zones=buffer_gdf,\n                value_columns=value_column,\n                aggregation=actual_stat,\n                point_zone_predicate=predicate,  # can't be `fractional``\n                zone_id_column=\"poi_id\",\n                output_suffix=\"\",\n                drop_geometry=True,\n                **kwargs,\n            )\n\n            output_col_from_agg = (\n                f\"{value_column}_{actual_stat}\" if value_column else \"point_count\"\n            )\n            results_df = aggregation_result_gdf[[\"poi_id\", output_col_from_agg]]\n\n            if output_column != \"zonal_stat\":\n                results_df = results_df.rename(\n                    columns={output_col_from_agg: output_column}\n                )\n\n        else:\n            if value_column is None:\n                raise ValueError(\n                    \"value_column must be provided for polygon data aggregation.\"\n                )\n            if value_column not in data.columns:\n                raise ValueError(\n                    f\"Value column '{value_column}' not found in input GeoDataFrame.\"\n                )\n            self.logger.info(\n                f\"Aggregating polygon data within {map_radius_meters}m buffers around POIs using predicate '{predicate}'\"\n            )\n\n            # Aggregate polygons to buffers\n            aggregation_result_gdf = aggregate_polygons_to_zones(\n                polygons=data,\n                zones=buffer_gdf,\n                value_columns=value_column,\n                aggregation=stat,\n                predicate=predicate,\n                zone_id_column=\"poi_id\",\n                output_suffix=\"\",\n                drop_geometry=True,\n                **kwargs,\n            )\n\n            output_col_from_agg = value_column\n\n        results_df = aggregation_result_gdf[[\"poi_id\", output_col_from_agg]]\n\n        if output_column != \"zonal_stat\":\n            results_df = results_df.rename(\n                columns={output_col_from_agg: output_column}\n            )\n\n    else:\n        raise ValueError(\n            \"data must be either a list of TifProcessor objects or a GeoDataFrame\"\n        )\n\n    # self._update_view(results_df) # Removed direct view update\n    self.logger.info(\n        f\"Zonal statistics mapping complete for column(s) derived from '{output_column}' or '{value_column}'\"\n    )\n    return results_df  # Return the DataFrame\n</code></pre>"},{"location":"api/generators/#gigaspatial.generators.poi.PoiViewGenerator.save_view","title":"<code>save_view(name, output_format=None)</code>","text":"<p>Saves the current POI view (the enriched DataFrame) to a file.</p> <p>The output path and format are determined by the <code>config</code> or overridden by the <code>output_format</code> parameter.</p> <p>Parameters:</p> Name Type Description Default <code>name</code> <code>str</code> <p>The base name for the output file (without extension).</p> required <code>output_format</code> <code>Optional[str]</code> <p>The desired output format (e.g., \"csv\", \"geojson\"). If None, the <code>output_format</code> from <code>config</code> will be used.</p> <code>None</code> <p>Returns:</p> Name Type Description <code>Path</code> <code>Path</code> <p>The full path to the saved output file.</p> Source code in <code>gigaspatial/generators/poi.py</code> <pre><code>def save_view(\n    self,\n    name: str,\n    output_format: Optional[str] = None,\n) -&gt; Path:\n    \"\"\"\n    Saves the current POI view (the enriched DataFrame) to a file.\n\n    The output path and format are determined by the `config`\n    or overridden by the `output_format` parameter.\n\n    Args:\n        name (str): The base name for the output file (without extension).\n        output_format (Optional[str]):\n            The desired output format (e.g., \"csv\", \"geojson\"). If None,\n            the `output_format` from `config` will be used.\n\n    Returns:\n        Path: The full path to the saved output file.\n    \"\"\"\n    format_to_use = output_format or self.config.output_format\n    output_path = self.config.base_path / f\"{name}.{format_to_use}\"\n\n    self.logger.info(f\"Saving POI view to {output_path}\")\n    # Save the current view, which is a pandas DataFrame, not a GeoDataFrame\n    # GeoJSON/Shapefile formats would require converting back to GeoDataFrame first.\n    # For CSV, Parquet, Feather, this is fine.\n    if format_to_use in [\"geojson\", \"shp\", \"gpkg\"]:\n        self.logger.warning(\n            f\"Saving to {format_to_use} requires converting back to GeoDataFrame. Geometry column will be re-added.\"\n        )\n        # Re-add geometry for saving to geospatial formats\n        view_to_save_gdf = self.view.merge(\n            self.points_gdf[[\"poi_id\", \"geometry\"]], on=\"poi_id\", how=\"left\"\n        )\n        write_dataset(\n            data=view_to_save_gdf,\n            path=str(output_path),\n            data_store=self.data_store,\n        )\n    else:\n        write_dataset(\n            data=self.view,  # Use the internal _view DataFrame\n            path=str(output_path),\n            data_store=self.data_store,\n        )\n\n    return output_path\n</code></pre>"},{"location":"api/generators/#gigaspatial.generators.poi.PoiViewGenerator.to_dataframe","title":"<code>to_dataframe()</code>","text":"<p>Returns the current POI view as a DataFrame.</p> <p>This method combines all accumulated variables in the view</p> <p>Returns:</p> Type Description <code>DataFrame</code> <p>pd.DataFrame: The current view.</p> Source code in <code>gigaspatial/generators/poi.py</code> <pre><code>def to_dataframe(self) -&gt; pd.DataFrame:\n    \"\"\"\n    Returns the current POI view as a DataFrame.\n\n    This method combines all accumulated variables in the view\n\n    Returns:\n        pd.DataFrame: The current view.\n    \"\"\"\n    return self.view\n</code></pre>"},{"location":"api/generators/#gigaspatial.generators.poi.PoiViewGenerator.to_geodataframe","title":"<code>to_geodataframe()</code>","text":"<p>Returns the current POI view merged with the original point geometries as a GeoDataFrame.</p> <p>This method combines all accumulated variables in the view with the corresponding point geometries, providing a spatially-enabled DataFrame for further analysis or export.</p> <p>Returns:</p> Type Description <code>GeoDataFrame</code> <p>gpd.GeoDataFrame: The current view merged with point geometries.</p> Source code in <code>gigaspatial/generators/poi.py</code> <pre><code>def to_geodataframe(self) -&gt; gpd.GeoDataFrame:\n    \"\"\"\n    Returns the current POI view merged with the original point geometries as a GeoDataFrame.\n\n    This method combines all accumulated variables in the view with the corresponding\n    point geometries, providing a spatially-enabled DataFrame for further analysis or export.\n\n    Returns:\n        gpd.GeoDataFrame: The current view merged with point geometries.\n    \"\"\"\n    return gpd.GeoDataFrame(\n        self.view.merge(\n            self.points_gdf[[\"poi_id\", \"geometry\"]], on=\"poi_id\", how=\"left\"\n        ),\n        crs=\"EPSG:4326\",\n    )\n</code></pre>"},{"location":"api/generators/#gigaspatial.generators.poi.PoiViewGenerator.validate_data_coverage","title":"<code>validate_data_coverage(data_bounds)</code>","text":"<p>Validate how many POIs fall within the data coverage area.</p> <p>Returns:</p> Name Type Description <code>dict</code> <code>dict</code> <p>Coverage statistics</p> Source code in <code>gigaspatial/generators/poi.py</code> <pre><code>def validate_data_coverage(self, data_bounds: gpd.GeoDataFrame) -&gt; dict:\n    \"\"\"\n    Validate how many POIs fall within the data coverage area.\n\n    Returns:\n        dict: Coverage statistics\n    \"\"\"\n    poi_within = self.points_gdf.within(data_bounds.union_all())\n    coverage_stats = {\n        \"total_pois\": len(self.points_gdf),\n        \"covered_pois\": poi_within.sum(),\n        \"coverage_percentage\": (poi_within.sum() / len(self.points_gdf)) * 100,\n        \"uncovered_pois\": (~poi_within).sum(),\n    }\n    return coverage_stats\n</code></pre>"},{"location":"api/generators/#gigaspatial.generators.poi.PoiViewGeneratorConfig","title":"<code>PoiViewGeneratorConfig</code>","text":"<p>Configuration for POI (Point of Interest) view generation.</p> <p>Attributes:</p> Name Type Description <code>base_path</code> <code>Path</code> <p>The base directory where generated POI views will be saved.               Defaults to a path retrieved from <code>config</code>.</p> <code>output_format</code> <code>str</code> <p>The default format for saving output files (e.g., \"csv\", \"geojson\").                  Defaults to \"csv\".</p> Source code in <code>gigaspatial/generators/poi.py</code> <pre><code>@dataclass\nclass PoiViewGeneratorConfig:\n    \"\"\"\n    Configuration for POI (Point of Interest) view generation.\n\n    Attributes:\n        base_path (Path): The base directory where generated POI views will be saved.\n                          Defaults to a path retrieved from `config`.\n        output_format (str): The default format for saving output files (e.g., \"csv\", \"geojson\").\n                             Defaults to \"csv\".\n    \"\"\"\n\n    base_path: Path = Field(default=global_config.get_path(\"poi\", \"views\"))\n    output_format: str = \"csv\"\n    ensure_available: bool = True\n</code></pre>"},{"location":"api/generators/#gigaspatial.generators.zonal","title":"<code>zonal</code>","text":""},{"location":"api/generators/#gigaspatial.generators.zonal.admin","title":"<code>admin</code>","text":""},{"location":"api/generators/#gigaspatial.generators.zonal.admin.AdminBoundariesViewGenerator","title":"<code>AdminBoundariesViewGenerator</code>","text":"<p>               Bases: <code>GeometryBasedZonalViewGenerator[T]</code></p> <p>Generates zonal views using administrative boundaries as the zones.</p> <p>This class specializes in creating zonal views where the zones are defined by administrative boundaries (e.g., countries, states, districts) at a specified administrative level. It extends the <code>GeometryBasedZonalViewGenerator</code> and leverages the <code>AdminBoundaries</code> handler to load the necessary geographical data.</p> <p>The administrative boundaries serve as the base geometries to which other geospatial data (points, polygons, rasters) can be mapped and aggregated.</p> <p>Attributes:</p> Name Type Description <code>country</code> <code>str</code> <p>The name or code of the country for which to load administrative boundaries.</p> <code>admin_level</code> <code>int</code> <p>The administrative level to load (e.g., 0 for country, 1 for states/provinces).</p> <code>admin_path</code> <code>Union[str, Path]</code> <p>Optional path to a local GeoJSON/Shapefile containing the administrative boundaries. If provided, this local file will be used instead of downloading.</p> <code>config</code> <code>Optional[ZonalViewGeneratorConfig]</code> <p>Configuration for the zonal view generation process.</p> <code>data_store</code> <code>Optional[DataStore]</code> <p>A DataStore instance for accessing data.</p> <code>logger</code> <code>Optional[Logger]</code> <p>A logger instance for logging messages.</p> Source code in <code>gigaspatial/generators/zonal/admin.py</code> <pre><code>class AdminBoundariesViewGenerator(GeometryBasedZonalViewGenerator[T]):\n    \"\"\"\n    Generates zonal views using administrative boundaries as the zones.\n\n    This class specializes in creating zonal views where the zones are defined by\n    administrative boundaries (e.g., countries, states, districts) at a specified\n    administrative level. It extends the `GeometryBasedZonalViewGenerator` and\n    leverages the `AdminBoundaries` handler to load the necessary geographical data.\n\n    The administrative boundaries serve as the base geometries to which other\n    geospatial data (points, polygons, rasters) can be mapped and aggregated.\n\n    Attributes:\n        country (str): The name or code of the country for which to load administrative boundaries.\n        admin_level (int): The administrative level to load (e.g., 0 for country, 1 for states/provinces).\n        admin_path (Union[str, Path], optional): Optional path to a local GeoJSON/Shapefile\n            containing the administrative boundaries. If provided, this local file will be\n            used instead of downloading.\n        config (Optional[ZonalViewGeneratorConfig]): Configuration for the zonal view generation process.\n        data_store (Optional[DataStore]): A DataStore instance for accessing data.\n        logger (Optional[logging.Logger]): A logger instance for logging messages.\n    \"\"\"\n\n    def __init__(\n        self,\n        country: str,\n        admin_level: int,\n        data_store: Optional[DataStore] = None,\n        admin_path: Optional[Union[str, Path]] = None,\n        config: Optional[ZonalViewGeneratorConfig] = None,\n        logger: logging.Logger = None,\n    ):\n        \"\"\"\n        Initializes the AdminBoundariesViewGenerator.\n\n        Args:\n            country (str): The name or code of the country (e.g., \"USA\", \"Germany\").\n            admin_level (int): The administrative level to load (e.g., 0 for country, 1 for states, 2 for districts).\n            admin_path (Union[str, Path], optional): Path to a local administrative boundaries file (GeoJSON, Shapefile).\n                                                     If provided, overrides default data loading.\n            config (Optional[ZonalViewGeneratorConfig]): Configuration for the zonal view generator.\n                                                         If None, a default config will be used.\n            data_store (Optional[DataStore]): Data storage interface. If None, LocalDataStore is used.\n            logger (Optional[logging.Logger]): Custom logger instance. If None, a default logger is used.\n        \"\"\"\n\n        super().__init__(\n            zone_data=self._init_zone_data(\n                country, admin_level, data_store, admin_path\n            ),\n            zone_id_column=\"id\",\n            config=config,\n            data_store=data_store,\n            logger=logger,\n        )\n        self._country = country\n        self.logger.info(\n            f\"Initialized AdminBoundariesViewGenerator for {country} (level {admin_level})\"\n        )\n\n    def _init_zone_data(\n        self,\n        country,\n        admin_level,\n        data_store: Optional[DataStore] = None,\n        admin_path: Optional[Union[str, Path]] = None,\n    ):\n        gdf_boundaries = AdminBoundaries.create(\n            country, admin_level, data_store, admin_path\n        ).to_geodataframe()\n        return gdf_boundaries\n\n    def map_wp_pop(\n        self,\n        country=None,\n        resolution=1000,\n        predicate=\"intersects\",\n        output_column=\"population\",\n        **kwargs,\n    ):\n        country = self._country if country is None else country\n        return super().map_wp_pop(\n            country, resolution, predicate, output_column, **kwargs\n        )\n</code></pre>"},{"location":"api/generators/#gigaspatial.generators.zonal.admin.AdminBoundariesViewGenerator.__init__","title":"<code>__init__(country, admin_level, data_store=None, admin_path=None, config=None, logger=None)</code>","text":"<p>Initializes the AdminBoundariesViewGenerator.</p> <p>Parameters:</p> Name Type Description Default <code>country</code> <code>str</code> <p>The name or code of the country (e.g., \"USA\", \"Germany\").</p> required <code>admin_level</code> <code>int</code> <p>The administrative level to load (e.g., 0 for country, 1 for states, 2 for districts).</p> required <code>admin_path</code> <code>Union[str, Path]</code> <p>Path to a local administrative boundaries file (GeoJSON, Shapefile).                                      If provided, overrides default data loading.</p> <code>None</code> <code>config</code> <code>Optional[ZonalViewGeneratorConfig]</code> <p>Configuration for the zonal view generator.                                          If None, a default config will be used.</p> <code>None</code> <code>data_store</code> <code>Optional[DataStore]</code> <p>Data storage interface. If None, LocalDataStore is used.</p> <code>None</code> <code>logger</code> <code>Optional[Logger]</code> <p>Custom logger instance. If None, a default logger is used.</p> <code>None</code> Source code in <code>gigaspatial/generators/zonal/admin.py</code> <pre><code>def __init__(\n    self,\n    country: str,\n    admin_level: int,\n    data_store: Optional[DataStore] = None,\n    admin_path: Optional[Union[str, Path]] = None,\n    config: Optional[ZonalViewGeneratorConfig] = None,\n    logger: logging.Logger = None,\n):\n    \"\"\"\n    Initializes the AdminBoundariesViewGenerator.\n\n    Args:\n        country (str): The name or code of the country (e.g., \"USA\", \"Germany\").\n        admin_level (int): The administrative level to load (e.g., 0 for country, 1 for states, 2 for districts).\n        admin_path (Union[str, Path], optional): Path to a local administrative boundaries file (GeoJSON, Shapefile).\n                                                 If provided, overrides default data loading.\n        config (Optional[ZonalViewGeneratorConfig]): Configuration for the zonal view generator.\n                                                     If None, a default config will be used.\n        data_store (Optional[DataStore]): Data storage interface. If None, LocalDataStore is used.\n        logger (Optional[logging.Logger]): Custom logger instance. If None, a default logger is used.\n    \"\"\"\n\n    super().__init__(\n        zone_data=self._init_zone_data(\n            country, admin_level, data_store, admin_path\n        ),\n        zone_id_column=\"id\",\n        config=config,\n        data_store=data_store,\n        logger=logger,\n    )\n    self._country = country\n    self.logger.info(\n        f\"Initialized AdminBoundariesViewGenerator for {country} (level {admin_level})\"\n    )\n</code></pre>"},{"location":"api/generators/#gigaspatial.generators.zonal.base","title":"<code>base</code>","text":""},{"location":"api/generators/#gigaspatial.generators.zonal.base.ZonalViewGenerator","title":"<code>ZonalViewGenerator</code>","text":"<p>               Bases: <code>ABC</code>, <code>Generic[T]</code></p> <p>Base class for mapping data to zonal datasets.</p> <p>This class provides the framework for mapping various data sources (points, polygons, rasters) to zonal geometries like grid tiles or catchment areas. It serves as an abstract base class that must be subclassed to implement specific zonal systems.</p> <p>The class supports three main types of data mapping: - Point data aggregation to zones - Polygon data aggregation with optional area weighting - Raster data sampling and statistics</p> <p>Attributes:</p> Name Type Description <code>data_store</code> <code>DataStore</code> <p>The data store for accessing input data.</p> <code>generator_config</code> <code>ZonalViewGeneratorConfig</code> <p>Configuration for the generator.</p> <code>logger</code> <p>Logger instance for this class.</p> Source code in <code>gigaspatial/generators/zonal/base.py</code> <pre><code>class ZonalViewGenerator(ABC, Generic[T]):\n    \"\"\"Base class for mapping data to zonal datasets.\n\n    This class provides the framework for mapping various data sources (points, polygons, rasters)\n    to zonal geometries like grid tiles or catchment areas. It serves as an abstract base class\n    that must be subclassed to implement specific zonal systems.\n\n    The class supports three main types of data mapping:\n    - Point data aggregation to zones\n    - Polygon data aggregation with optional area weighting\n    - Raster data sampling and statistics\n\n    Attributes:\n        data_store (DataStore): The data store for accessing input data.\n        generator_config (ZonalViewGeneratorConfig): Configuration for the generator.\n        logger: Logger instance for this class.\n    \"\"\"\n\n    def __init__(\n        self,\n        config: Optional[ZonalViewGeneratorConfig] = None,\n        data_store: Optional[DataStore] = None,\n        logger: logging.Logger = None,\n    ):\n        \"\"\"Initialize the ZonalViewGenerator.\n\n        Args:\n            generator_config (ZonalViewGeneratorConfig, optional): Configuration for the generator.\n                If None, uses default configuration.\n            data_store (DataStore, optional): The data store for accessing input data.\n                If None, uses LocalDataStore.\n        \"\"\"\n        self.config = config or ZonalViewGeneratorConfig()\n        self.data_store = data_store or LocalDataStore()\n        self.logger = logger or global_config.get_logger(self.__class__.__name__)\n        self._view: Optional[pd.DataFrame] = None\n\n    @abstractmethod\n    def get_zonal_geometries(self) -&gt; List[Polygon]:\n        \"\"\"Get the geometries of the zones.\n\n        This method must be implemented by subclasses to return the actual geometric\n        shapes of the zones (e.g., grid tiles, catchment boundaries, administrative areas).\n\n        Returns:\n            List[Polygon]: A list of Shapely Polygon objects representing zone geometries.\n        \"\"\"\n        pass\n\n    @abstractmethod\n    def get_zone_identifiers(self) -&gt; List[T]:\n        \"\"\"Get unique identifiers for each zone.\n\n        This method must be implemented by subclasses to return identifiers that\n        correspond one-to-one with the geometries returned by get_zonal_geometries().\n\n        Returns:\n            List[T]: A list of zone identifiers (e.g., quadkeys, H3 indices, tile IDs).\n                The type T is determined by the specific zonal system implementation.\n        \"\"\"\n        pass\n\n    def get_zone_geodataframe(self) -&gt; gpd.GeoDataFrame:\n        \"\"\"Convert zones to a GeoDataFrame.\n\n        Creates a GeoDataFrame containing zone identifiers and their corresponding\n        geometries in WGS84 (EPSG:4326) coordinate reference system.\n\n        Returns:\n            gpd.GeoDataFrame: A GeoDataFrame with 'zone_id' and 'geometry' columns,\n                where zone_id contains the identifiers and geometry contains the\n                corresponding Polygon objects.\n        \"\"\"\n        return gpd.GeoDataFrame(\n            {\n                \"zone_id\": self.get_zone_identifiers(),\n                \"geometry\": self.get_zonal_geometries(),\n            },\n            crs=\"EPSG:4326\",\n        )\n\n    @property\n    def zone_gdf(self) -&gt; gpd.GeoDataFrame:\n        \"\"\"Cached GeoDataFrame of zones.\n\n        Returns:\n            gpd.GeoDataFrame: Lazily-computed and cached GeoDataFrame of zone geometries\n                and identifiers.\n        \"\"\"\n        if not hasattr(self, \"_zone_gdf\"):\n            self._zone_gdf = self.get_zone_geodataframe()\n        return self._zone_gdf\n\n    @property\n    def view(self) -&gt; pd.DataFrame:\n        \"\"\"The DataFrame representing the current zonal view.\n\n        Returns:\n            pd.DataFrame: The DataFrame containing zone IDs, and\n                              any added variables. If no variables have been added,\n                              it returns the base `zone_gdf` without geometries.\n        \"\"\"\n        if self._view is None:\n            self._view = self.zone_gdf.drop(columns=\"geometry\")\n        return self._view\n\n    def add_variable_to_view(self, data_dict: Dict, column_name: str) -&gt; None:\n        \"\"\"\n        Adds a new variable (column) to the zonal view GeoDataFrame.\n\n        This method takes a dictionary (typically the result of map_points or map_polygons)\n        and adds its values as a new column to the internal `_view` (or `zone_gdf` if not yet initialized).\n        The dictionary keys are expected to be the `zone_id` values.\n\n        Args:\n            data_dict (Dict): A dictionary where keys are `zone_id`s and values are\n                              the data to be added.\n            column_name (str): The name of the new column to be added to the GeoDataFrame.\n        Raises:\n            ValueError: If the `data_dict` keys do not match the `zone_id`s in the zonal view.\n                        If the `column_name` already exists in the zonal view.\n        \"\"\"\n        if self._view is None:\n            self._view = self.zone_gdf.drop(columns=\"geometry\")\n\n        if column_name in self._view.columns:\n            raise ValueError(\n                f\"Column '{column_name}' already exists in the zonal view.\"\n            )\n\n        # Create a pandas Series from the dictionary, aligning by index (zone_id)\n        new_series = pd.Series(data_dict, name=column_name)\n\n        # Before merging, ensure the zone_ids in data_dict match those in _view\n        missing_zones_in_data = set(self._view[\"zone_id\"]) - set(new_series.index)\n        extra_zones_in_data = set(new_series.index) - set(self._view[\"zone_id\"])\n\n        if missing_zones_in_data:\n            self.logger.warning(\n                f\"Warning: {len(missing_zones_in_data)} zone(s) from the zonal view \"\n                f\"are missing in the provided data_dict for column '{column_name}'. \"\n                f\"These zones will have NaN values for '{column_name}'. Missing: {list(missing_zones_in_data)[:5]}...\"\n            )\n        if extra_zones_in_data:\n            self.logger.warning(\n                f\"Warning: {len(extra_zones_in_data)} zone(s) in the provided data_dict \"\n                f\"are not present in the zonal view for column '{column_name}'. \"\n                f\"These will be ignored. Extra: {list(extra_zones_in_data)[:5]}...\"\n            )\n\n        # Merge the new series with the _view based on 'zone_id'\n        # Using .set_index() for efficient alignment\n        original_index_name = self._view.index.name\n        self._view = self._view.set_index(\"zone_id\").join(new_series).reset_index()\n        if original_index_name:  # Restore original index name if it existed\n            self._view.index.name = original_index_name\n        else:  # If it was a default integer index, ensure it's not named 'index'\n            self._view.index.name = None\n\n        self.logger.info(f\"Added variable '{column_name}' to the zonal view.\")\n\n    def map_points(\n        self,\n        points: Union[pd.DataFrame, gpd.GeoDataFrame],\n        value_columns: Optional[Union[str, List[str]]] = None,\n        aggregation: Union[str, Dict[str, str]] = \"count\",\n        predicate: str = \"within\",\n        output_suffix: str = \"\",\n    ) -&gt; Dict:\n        \"\"\"Map point data to zones with spatial aggregation.\n\n        Aggregates point data to zones using spatial relationships. Points can be\n        counted or have their attribute values aggregated using various statistical methods.\n\n        Args:\n            points (Union[pd.DataFrame, gpd.GeoDataFrame]): The point data to map.\n                Must contain geometry information if DataFrame.\n            value_columns (Union[str, List[str]], optional): Column name(s) containing\n                values to aggregate. If None, only point counts are performed.\n            aggregation (Union[str, Dict[str, str]]): Aggregation method(s) to use.\n                Can be a single string (\"count\", \"mean\", \"sum\", \"min\", \"max\", etc.)\n                or a dictionary mapping column names to aggregation methods.\n            predicate (str): Spatial predicate for point-to-zone relationship.\n                Options include \"within\", \"intersects\", \"contains\". Defaults to \"within\".\n            output_suffix (str): Suffix to add to output column names. Defaults to empty string.\n\n        Returns:\n            Dict: Dictionary with zone IDs as keys and aggregated values as values.\n                If value_columns is None, returns point counts per zone.\n                If value_columns is specified, returns aggregated values per zone.\n        \"\"\"\n\n        self.logger.warning(\n            \"Using default points mapping implementation. Consider creating a specialized mapping function.\"\n        )\n        result = aggregate_points_to_zones(\n            points=points,\n            zones=self.zone_gdf,\n            value_columns=value_columns,\n            aggregation=aggregation,\n            point_zone_predicate=predicate,\n            zone_id_column=\"zone_id\",\n            output_suffix=output_suffix,\n        )\n\n        if isinstance(value_columns, str):\n            return result.set_index(\"zone_id\")[value_columns].to_dict()\n        elif isinstance(value_columns, list):\n            # If multiple value columns, return a dictionary of dictionaries\n            # Or, if preferred, a dictionary where values are lists/tuples of results\n            # For now, let's return a dict of series, which is common.\n            # The previous version implied a single dictionary result from map_points/polygons\n            # but with multiple columns, it's usually {zone_id: {col1: val1, col2: val2}}\n            # or {col_name: {zone_id: val}}\n            # In this version, it'll return a dictionary for each column.\n            return {\n                col: result.set_index(\"zone_id\")[col].to_dict() for col in value_columns\n            }\n        else:  # If value_columns is None, it should return point_count\n            self.logger.warning(\n                \"No `value_columns` provided. Mapping point counts. Consider passing `value_columns` and `aggregation` or `mapping_function`.\"\n            )\n            return result.set_index(\"zone_id\")[\"point_count\"].to_dict()\n\n    def map_polygons(\n        self,\n        polygons,\n        value_columns: Optional[Union[str, List[str]]] = None,\n        aggregation: Union[str, Dict[str, str]] = \"count\",\n        predicate: str = \"intersects\",\n        **kwargs,\n    ) -&gt; Dict:\n        \"\"\"\n        Maps polygon data to the instance's zones and aggregates values.\n\n        This method leverages `aggregate_polygons_to_zones` to perform a spatial\n        aggregation of polygon data onto the zones stored within this object instance.\n        It can count polygons, or aggregate their values, based on different spatial\n        relationships defined by the `predicate`.\n\n        Args:\n            polygons (Union[pd.DataFrame, gpd.GeoDataFrame]):\n                The polygon data to map. Must contain geometry information if a\n                DataFrame.\n            value_columns (Union[str, List[str]], optional):\n                The column name(s) from the `polygons` data to aggregate. If `None`,\n                the method will automatically count the number of polygons that\n                match the given `predicate` for each zone.\n            aggregation (Union[str, Dict[str, str]], optional):\n                The aggregation method(s) to use. Can be a single string (e.g., \"sum\",\n                \"mean\", \"max\") or a dictionary mapping column names to specific\n                aggregation methods. This is ignored and set to \"count\" if\n                `value_columns` is `None`. Defaults to \"count\".\n            predicate (Literal[\"intersects\", \"within\", \"fractional\"], optional):\n                The spatial relationship to use for aggregation:\n                - \"intersects\": Counts or aggregates values for any polygon that\n                  intersects a zone.\n                - \"within\": Counts or aggregates values for polygons that are\n                  entirely contained within a zone.\n                - \"fractional\": Performs area-weighted aggregation. The value of a\n                  polygon is distributed proportionally to the area of its overlap\n                  with each zone.\n                Defaults to \"intersects\".\n            **kwargs:\n                Additional keyword arguments to be passed to the underlying\n                `aggregate_polygons_to_zones_new` function.\n\n        Returns:\n            Dict:\n                A dictionary or a nested dictionary containing the aggregated values,\n                with zone IDs as keys. If `value_columns` is a single string, the\n                return value is a dictionary mapping zone ID to the aggregated value.\n                If `value_columns` is a list, the return value is a nested dictionary\n                mapping each column name to its own dictionary of aggregated values.\n\n        Raises:\n            ValueError: If `value_columns` is of an unexpected type after processing.\n\n        Example:\n            &gt;&gt;&gt; # Assuming 'self' is an object with a 'zone_gdf' attribute\n            &gt;&gt;&gt; # Count all land parcels that intersect each zone\n            &gt;&gt;&gt; parcel_counts = self.map_polygons(landuse_polygons)\n            &gt;&gt;&gt;\n            &gt;&gt;&gt; # Aggregate total population within zones using area weighting\n            &gt;&gt;&gt; population_by_zone = self.map_polygons(\n            ...     landuse_polygons,\n            ...     value_columns=\"population\",\n            ...     predicate=\"fractional\",\n            ...     aggregation=\"sum\"\n            ... )\n            &gt;&gt;&gt;\n            &gt;&gt;&gt; # Get the sum of residential area and count of buildings within each zone\n            &gt;&gt;&gt; residential_stats = self.map_polygons(\n            ...     building_polygons,\n            ...     value_columns=[\"residential_area_sqm\", \"building_id\"],\n            ...     aggregation={\"residential_area_sqm\": \"sum\", \"building_id\": \"count\"},\n            ...     predicate=\"intersects\"\n            ... )\n        \"\"\"\n\n        if value_columns is None:\n            self.logger.warning(\n                f\"No value_columns specified. Defaulting to counting polygons with {predicate} predicate.\"\n            )\n            temp_value_col = \"_temp_polygon_count_dummy\"\n            polygons[temp_value_col] = 1\n            actual_value_columns = temp_value_col\n            aggregation = \"count\"  # Force count if no value columns\n        else:\n            actual_value_columns = value_columns\n\n        result = aggregate_polygons_to_zones(\n            polygons=polygons,\n            zones=self.zone_gdf,\n            value_columns=actual_value_columns,\n            aggregation=aggregation,\n            predicate=predicate,\n            zone_id_column=\"zone_id\",\n        )\n\n        # Convert the result GeoDataFrame to the expected dictionary format\n        if isinstance(actual_value_columns, str):\n            return result.set_index(\"zone_id\")[actual_value_columns].to_dict()\n        elif isinstance(actual_value_columns, list):\n            return {\n                col: result.set_index(\"zone_id\")[col].to_dict()\n                for col in actual_value_columns\n            }\n        else:\n            raise ValueError(\"Unexpected type for actual_value_columns.\")\n\n    def map_rasters(\n        self,\n        raster_data: Union[TifProcessor, List[TifProcessor]],\n        stat: str = \"mean\",\n        **kwargs,\n    ) -&gt; Dict:\n        \"\"\"Map raster data to zones using zonal statistics.\n\n        Samples raster values within each zone and computes statistics. Automatically\n        handles coordinate reference system transformations between raster and zone data.\n\n        Args:\n            raster_data (Union[TifProcessor, List[TifProcessor]]):\n                Either a TifProcessor object or a list of TifProcessor objects (which will be merged\n                into a single TifProcessor for processing).\n            mapping_function (Callable, optional): Custom function for mapping rasters\n                to zones. If provided, signature should be mapping_function(self, tif_processors, **mapping_kwargs).\n                When used, stat and other parameters except mapping_kwargs are ignored.\n            stat (str): Statistic to calculate when aggregating raster values within\n                each zone. Options include \"mean\", \"sum\", \"min\", \"max\", \"std\", etc.\n                Defaults to \"mean\".\n            **mapping_kwargs: Additional keyword arguments for raster data.\n\n        Returns:\n            Dict: By default, returns a dictionary of sampled values\n                with zone IDs as keys.\n\n        Note:\n            If the coordinate reference system of the rasters differs from the zones,\n            the zone geometries will be automatically transformed to match the raster CRS.\n        \"\"\"\n        raster_processor: Optional[TifProcessor] = None\n\n        if isinstance(raster_data, TifProcessor):\n            raster_processor = raster_data\n        elif isinstance(raster_data, list) and all(\n            isinstance(x, TifProcessor) for x in raster_data\n        ):\n            if not raster_data:\n                self.logger.info(\"No valid raster data provided\")\n                return self.view\n\n            if len(raster_data) &gt; 1:\n                all_source_paths = [tp.dataset_path for tp in raster_data]\n\n                self.logger.info(\n                    f\"Merging {len(all_source_paths)} rasters into a single TifProcessor for zonal statistics.\"\n                )\n                raster_processor = TifProcessor(\n                    dataset_path=all_source_paths, data_store=self.data_store, **kwargs\n                )\n            else:\n                raster_processor = raster_data[0]\n        else:\n            raise ValueError(\n                \"raster_data must be a TifProcessor object or a list of TifProcessor objects.\"\n            )\n\n        raster_crs = raster_processor.crs\n\n        if raster_crs != self.zone_gdf.crs:\n            self.logger.info(f\"Projecting zones to raster CRS: {raster_crs}\")\n            zone_geoms = self._get_transformed_geometries(raster_crs)\n        else:\n            zone_geoms = self.get_zonal_geometries()\n\n        # Sample raster values\n        sampled_values = raster_processor.sample_by_polygons(\n            polygon_list=zone_geoms, stat=stat\n        )\n\n        zone_ids = self.get_zone_identifiers()\n\n        return {zone_id: value for zone_id, value in zip(zone_ids, sampled_values)}\n\n    @lru_cache(maxsize=32)\n    def _get_transformed_geometries(self, target_crs):\n        \"\"\"Get zone geometries transformed to target coordinate reference system.\n\n        This method is cached to avoid repeated coordinate transformations for\n        the same target CRS.\n\n        Args:\n            target_crs: Target coordinate reference system for transformation.\n\n        Returns:\n            List[Polygon]: List of zone geometries transformed to the target CRS.\n        \"\"\"\n        return self.zone_gdf.to_crs(target_crs).geometry.tolist()\n\n    def save_view(\n        self,\n        name: str,\n        output_format: Optional[str] = None,\n    ) -&gt; Path:\n        \"\"\"Save the generated zonal view to disk.\n\n        Args:\n            name (str): Base name for the output file (without extension).\n            output_format (str, optional): File format to save in (e.g., \"parquet\",\n                \"geojson\", \"shp\"). If None, uses the format specified in config.\n\n        Returns:\n            Path: The full path where the view was saved.\n\n        Note:\n            The output directory is determined by the config.base_path setting.\n            The file extension is automatically added based on the output format.\n            This method now saves the internal `self.view`.\n        \"\"\"\n        if self._view is None:\n            self.logger.warning(\n                \"No variables have been added to the zonal view. Saving the base zone_gdf.\"\n            )\n            view_to_save = self.zone_gdf\n        else:\n            view_to_save = self._view\n\n        format_to_use = output_format or self.config.output_format\n        output_path = self.config.base_path / f\"{name}.{format_to_use}\"\n\n        self.logger.info(f\"Saving zonal view to {output_path}\")\n\n        if format_to_use in [\"geojson\", \"shp\", \"gpkg\"]:\n            self.logger.warning(\n                f\"Saving to {format_to_use} requires converting back to GeoDataFrame. Geometry column will be re-added.\"\n            )\n            # Re-add geometry for saving to geospatial formats\n            view_to_save = self.view.merge(\n                self.zone_gdf[[\"zone_id\", \"geometry\"]], on=\"zone_id\", how=\"left\"\n            )\n\n        write_dataset(\n            data=view_to_save,\n            path=str(output_path),\n            data_store=self.data_store,\n        )\n\n        return output_path\n\n    def to_dataframe(self) -&gt; pd.DataFrame:\n        \"\"\"\n        Returns the current zonal view as a DataFrame.\n\n        This method combines all accumulated variables in the view\n\n        Returns:\n            pd.DataFrame: The current view.\n        \"\"\"\n        return self.view\n\n    def to_geodataframe(self) -&gt; gpd.GeoDataFrame:\n        \"\"\"\n        Returns the current zonal view merged with zone geometries as a GeoDataFrame.\n\n        This method combines all accumulated variables in the view with the corresponding\n        zone geometries, providing a spatially-enabled DataFrame for further analysis or export.\n\n        Returns:\n            gpd.GeoDataFrame: The current view merged with zone geometries.\n        \"\"\"\n        return gpd.GeoDataFrame(\n            (self.view).merge(\n                self.zone_gdf[[\"zone_id\", \"geometry\"]], on=\"zone_id\", how=\"left\"\n            ),\n            crs=self.zone_gdf.crs,\n        )\n</code></pre>"},{"location":"api/generators/#gigaspatial.generators.zonal.base.ZonalViewGenerator.view","title":"<code>view: pd.DataFrame</code>  <code>property</code>","text":"<p>The DataFrame representing the current zonal view.</p> <p>Returns:</p> Type Description <code>DataFrame</code> <p>pd.DataFrame: The DataFrame containing zone IDs, and               any added variables. If no variables have been added,               it returns the base <code>zone_gdf</code> without geometries.</p>"},{"location":"api/generators/#gigaspatial.generators.zonal.base.ZonalViewGenerator.zone_gdf","title":"<code>zone_gdf: gpd.GeoDataFrame</code>  <code>property</code>","text":"<p>Cached GeoDataFrame of zones.</p> <p>Returns:</p> Type Description <code>GeoDataFrame</code> <p>gpd.GeoDataFrame: Lazily-computed and cached GeoDataFrame of zone geometries and identifiers.</p>"},{"location":"api/generators/#gigaspatial.generators.zonal.base.ZonalViewGenerator.__init__","title":"<code>__init__(config=None, data_store=None, logger=None)</code>","text":"<p>Initialize the ZonalViewGenerator.</p> <p>Parameters:</p> Name Type Description Default <code>generator_config</code> <code>ZonalViewGeneratorConfig</code> <p>Configuration for the generator. If None, uses default configuration.</p> required <code>data_store</code> <code>DataStore</code> <p>The data store for accessing input data. If None, uses LocalDataStore.</p> <code>None</code> Source code in <code>gigaspatial/generators/zonal/base.py</code> <pre><code>def __init__(\n    self,\n    config: Optional[ZonalViewGeneratorConfig] = None,\n    data_store: Optional[DataStore] = None,\n    logger: logging.Logger = None,\n):\n    \"\"\"Initialize the ZonalViewGenerator.\n\n    Args:\n        generator_config (ZonalViewGeneratorConfig, optional): Configuration for the generator.\n            If None, uses default configuration.\n        data_store (DataStore, optional): The data store for accessing input data.\n            If None, uses LocalDataStore.\n    \"\"\"\n    self.config = config or ZonalViewGeneratorConfig()\n    self.data_store = data_store or LocalDataStore()\n    self.logger = logger or global_config.get_logger(self.__class__.__name__)\n    self._view: Optional[pd.DataFrame] = None\n</code></pre>"},{"location":"api/generators/#gigaspatial.generators.zonal.base.ZonalViewGenerator.add_variable_to_view","title":"<code>add_variable_to_view(data_dict, column_name)</code>","text":"<p>Adds a new variable (column) to the zonal view GeoDataFrame.</p> <p>This method takes a dictionary (typically the result of map_points or map_polygons) and adds its values as a new column to the internal <code>_view</code> (or <code>zone_gdf</code> if not yet initialized). The dictionary keys are expected to be the <code>zone_id</code> values.</p> <p>Parameters:</p> Name Type Description Default <code>data_dict</code> <code>Dict</code> <p>A dictionary where keys are <code>zone_id</code>s and values are               the data to be added.</p> required <code>column_name</code> <code>str</code> <p>The name of the new column to be added to the GeoDataFrame.</p> required Source code in <code>gigaspatial/generators/zonal/base.py</code> <pre><code>def add_variable_to_view(self, data_dict: Dict, column_name: str) -&gt; None:\n    \"\"\"\n    Adds a new variable (column) to the zonal view GeoDataFrame.\n\n    This method takes a dictionary (typically the result of map_points or map_polygons)\n    and adds its values as a new column to the internal `_view` (or `zone_gdf` if not yet initialized).\n    The dictionary keys are expected to be the `zone_id` values.\n\n    Args:\n        data_dict (Dict): A dictionary where keys are `zone_id`s and values are\n                          the data to be added.\n        column_name (str): The name of the new column to be added to the GeoDataFrame.\n    Raises:\n        ValueError: If the `data_dict` keys do not match the `zone_id`s in the zonal view.\n                    If the `column_name` already exists in the zonal view.\n    \"\"\"\n    if self._view is None:\n        self._view = self.zone_gdf.drop(columns=\"geometry\")\n\n    if column_name in self._view.columns:\n        raise ValueError(\n            f\"Column '{column_name}' already exists in the zonal view.\"\n        )\n\n    # Create a pandas Series from the dictionary, aligning by index (zone_id)\n    new_series = pd.Series(data_dict, name=column_name)\n\n    # Before merging, ensure the zone_ids in data_dict match those in _view\n    missing_zones_in_data = set(self._view[\"zone_id\"]) - set(new_series.index)\n    extra_zones_in_data = set(new_series.index) - set(self._view[\"zone_id\"])\n\n    if missing_zones_in_data:\n        self.logger.warning(\n            f\"Warning: {len(missing_zones_in_data)} zone(s) from the zonal view \"\n            f\"are missing in the provided data_dict for column '{column_name}'. \"\n            f\"These zones will have NaN values for '{column_name}'. Missing: {list(missing_zones_in_data)[:5]}...\"\n        )\n    if extra_zones_in_data:\n        self.logger.warning(\n            f\"Warning: {len(extra_zones_in_data)} zone(s) in the provided data_dict \"\n            f\"are not present in the zonal view for column '{column_name}'. \"\n            f\"These will be ignored. Extra: {list(extra_zones_in_data)[:5]}...\"\n        )\n\n    # Merge the new series with the _view based on 'zone_id'\n    # Using .set_index() for efficient alignment\n    original_index_name = self._view.index.name\n    self._view = self._view.set_index(\"zone_id\").join(new_series).reset_index()\n    if original_index_name:  # Restore original index name if it existed\n        self._view.index.name = original_index_name\n    else:  # If it was a default integer index, ensure it's not named 'index'\n        self._view.index.name = None\n\n    self.logger.info(f\"Added variable '{column_name}' to the zonal view.\")\n</code></pre>"},{"location":"api/generators/#gigaspatial.generators.zonal.base.ZonalViewGenerator.get_zonal_geometries","title":"<code>get_zonal_geometries()</code>  <code>abstractmethod</code>","text":"<p>Get the geometries of the zones.</p> <p>This method must be implemented by subclasses to return the actual geometric shapes of the zones (e.g., grid tiles, catchment boundaries, administrative areas).</p> <p>Returns:</p> Type Description <code>List[Polygon]</code> <p>List[Polygon]: A list of Shapely Polygon objects representing zone geometries.</p> Source code in <code>gigaspatial/generators/zonal/base.py</code> <pre><code>@abstractmethod\ndef get_zonal_geometries(self) -&gt; List[Polygon]:\n    \"\"\"Get the geometries of the zones.\n\n    This method must be implemented by subclasses to return the actual geometric\n    shapes of the zones (e.g., grid tiles, catchment boundaries, administrative areas).\n\n    Returns:\n        List[Polygon]: A list of Shapely Polygon objects representing zone geometries.\n    \"\"\"\n    pass\n</code></pre>"},{"location":"api/generators/#gigaspatial.generators.zonal.base.ZonalViewGenerator.get_zone_geodataframe","title":"<code>get_zone_geodataframe()</code>","text":"<p>Convert zones to a GeoDataFrame.</p> <p>Creates a GeoDataFrame containing zone identifiers and their corresponding geometries in WGS84 (EPSG:4326) coordinate reference system.</p> <p>Returns:</p> Type Description <code>GeoDataFrame</code> <p>gpd.GeoDataFrame: A GeoDataFrame with 'zone_id' and 'geometry' columns, where zone_id contains the identifiers and geometry contains the corresponding Polygon objects.</p> Source code in <code>gigaspatial/generators/zonal/base.py</code> <pre><code>def get_zone_geodataframe(self) -&gt; gpd.GeoDataFrame:\n    \"\"\"Convert zones to a GeoDataFrame.\n\n    Creates a GeoDataFrame containing zone identifiers and their corresponding\n    geometries in WGS84 (EPSG:4326) coordinate reference system.\n\n    Returns:\n        gpd.GeoDataFrame: A GeoDataFrame with 'zone_id' and 'geometry' columns,\n            where zone_id contains the identifiers and geometry contains the\n            corresponding Polygon objects.\n    \"\"\"\n    return gpd.GeoDataFrame(\n        {\n            \"zone_id\": self.get_zone_identifiers(),\n            \"geometry\": self.get_zonal_geometries(),\n        },\n        crs=\"EPSG:4326\",\n    )\n</code></pre>"},{"location":"api/generators/#gigaspatial.generators.zonal.base.ZonalViewGenerator.get_zone_identifiers","title":"<code>get_zone_identifiers()</code>  <code>abstractmethod</code>","text":"<p>Get unique identifiers for each zone.</p> <p>This method must be implemented by subclasses to return identifiers that correspond one-to-one with the geometries returned by get_zonal_geometries().</p> <p>Returns:</p> Type Description <code>List[T]</code> <p>List[T]: A list of zone identifiers (e.g., quadkeys, H3 indices, tile IDs). The type T is determined by the specific zonal system implementation.</p> Source code in <code>gigaspatial/generators/zonal/base.py</code> <pre><code>@abstractmethod\ndef get_zone_identifiers(self) -&gt; List[T]:\n    \"\"\"Get unique identifiers for each zone.\n\n    This method must be implemented by subclasses to return identifiers that\n    correspond one-to-one with the geometries returned by get_zonal_geometries().\n\n    Returns:\n        List[T]: A list of zone identifiers (e.g., quadkeys, H3 indices, tile IDs).\n            The type T is determined by the specific zonal system implementation.\n    \"\"\"\n    pass\n</code></pre>"},{"location":"api/generators/#gigaspatial.generators.zonal.base.ZonalViewGenerator.map_points","title":"<code>map_points(points, value_columns=None, aggregation='count', predicate='within', output_suffix='')</code>","text":"<p>Map point data to zones with spatial aggregation.</p> <p>Aggregates point data to zones using spatial relationships. Points can be counted or have their attribute values aggregated using various statistical methods.</p> <p>Parameters:</p> Name Type Description Default <code>points</code> <code>Union[DataFrame, GeoDataFrame]</code> <p>The point data to map. Must contain geometry information if DataFrame.</p> required <code>value_columns</code> <code>Union[str, List[str]]</code> <p>Column name(s) containing values to aggregate. If None, only point counts are performed.</p> <code>None</code> <code>aggregation</code> <code>Union[str, Dict[str, str]]</code> <p>Aggregation method(s) to use. Can be a single string (\"count\", \"mean\", \"sum\", \"min\", \"max\", etc.) or a dictionary mapping column names to aggregation methods.</p> <code>'count'</code> <code>predicate</code> <code>str</code> <p>Spatial predicate for point-to-zone relationship. Options include \"within\", \"intersects\", \"contains\". Defaults to \"within\".</p> <code>'within'</code> <code>output_suffix</code> <code>str</code> <p>Suffix to add to output column names. Defaults to empty string.</p> <code>''</code> <p>Returns:</p> Name Type Description <code>Dict</code> <code>Dict</code> <p>Dictionary with zone IDs as keys and aggregated values as values. If value_columns is None, returns point counts per zone. If value_columns is specified, returns aggregated values per zone.</p> Source code in <code>gigaspatial/generators/zonal/base.py</code> <pre><code>def map_points(\n    self,\n    points: Union[pd.DataFrame, gpd.GeoDataFrame],\n    value_columns: Optional[Union[str, List[str]]] = None,\n    aggregation: Union[str, Dict[str, str]] = \"count\",\n    predicate: str = \"within\",\n    output_suffix: str = \"\",\n) -&gt; Dict:\n    \"\"\"Map point data to zones with spatial aggregation.\n\n    Aggregates point data to zones using spatial relationships. Points can be\n    counted or have their attribute values aggregated using various statistical methods.\n\n    Args:\n        points (Union[pd.DataFrame, gpd.GeoDataFrame]): The point data to map.\n            Must contain geometry information if DataFrame.\n        value_columns (Union[str, List[str]], optional): Column name(s) containing\n            values to aggregate. If None, only point counts are performed.\n        aggregation (Union[str, Dict[str, str]]): Aggregation method(s) to use.\n            Can be a single string (\"count\", \"mean\", \"sum\", \"min\", \"max\", etc.)\n            or a dictionary mapping column names to aggregation methods.\n        predicate (str): Spatial predicate for point-to-zone relationship.\n            Options include \"within\", \"intersects\", \"contains\". Defaults to \"within\".\n        output_suffix (str): Suffix to add to output column names. Defaults to empty string.\n\n    Returns:\n        Dict: Dictionary with zone IDs as keys and aggregated values as values.\n            If value_columns is None, returns point counts per zone.\n            If value_columns is specified, returns aggregated values per zone.\n    \"\"\"\n\n    self.logger.warning(\n        \"Using default points mapping implementation. Consider creating a specialized mapping function.\"\n    )\n    result = aggregate_points_to_zones(\n        points=points,\n        zones=self.zone_gdf,\n        value_columns=value_columns,\n        aggregation=aggregation,\n        point_zone_predicate=predicate,\n        zone_id_column=\"zone_id\",\n        output_suffix=output_suffix,\n    )\n\n    if isinstance(value_columns, str):\n        return result.set_index(\"zone_id\")[value_columns].to_dict()\n    elif isinstance(value_columns, list):\n        # If multiple value columns, return a dictionary of dictionaries\n        # Or, if preferred, a dictionary where values are lists/tuples of results\n        # For now, let's return a dict of series, which is common.\n        # The previous version implied a single dictionary result from map_points/polygons\n        # but with multiple columns, it's usually {zone_id: {col1: val1, col2: val2}}\n        # or {col_name: {zone_id: val}}\n        # In this version, it'll return a dictionary for each column.\n        return {\n            col: result.set_index(\"zone_id\")[col].to_dict() for col in value_columns\n        }\n    else:  # If value_columns is None, it should return point_count\n        self.logger.warning(\n            \"No `value_columns` provided. Mapping point counts. Consider passing `value_columns` and `aggregation` or `mapping_function`.\"\n        )\n        return result.set_index(\"zone_id\")[\"point_count\"].to_dict()\n</code></pre>"},{"location":"api/generators/#gigaspatial.generators.zonal.base.ZonalViewGenerator.map_polygons","title":"<code>map_polygons(polygons, value_columns=None, aggregation='count', predicate='intersects', **kwargs)</code>","text":"<p>Maps polygon data to the instance's zones and aggregates values.</p> <p>This method leverages <code>aggregate_polygons_to_zones</code> to perform a spatial aggregation of polygon data onto the zones stored within this object instance. It can count polygons, or aggregate their values, based on different spatial relationships defined by the <code>predicate</code>.</p> <p>Parameters:</p> Name Type Description Default <code>polygons</code> <code>Union[DataFrame, GeoDataFrame]</code> <p>The polygon data to map. Must contain geometry information if a DataFrame.</p> required <code>value_columns</code> <code>Union[str, List[str]]</code> <p>The column name(s) from the <code>polygons</code> data to aggregate. If <code>None</code>, the method will automatically count the number of polygons that match the given <code>predicate</code> for each zone.</p> <code>None</code> <code>aggregation</code> <code>Union[str, Dict[str, str]]</code> <p>The aggregation method(s) to use. Can be a single string (e.g., \"sum\", \"mean\", \"max\") or a dictionary mapping column names to specific aggregation methods. This is ignored and set to \"count\" if <code>value_columns</code> is <code>None</code>. Defaults to \"count\".</p> <code>'count'</code> <code>predicate</code> <code>Literal[intersects, within, fractional]</code> <p>The spatial relationship to use for aggregation: - \"intersects\": Counts or aggregates values for any polygon that   intersects a zone. - \"within\": Counts or aggregates values for polygons that are   entirely contained within a zone. - \"fractional\": Performs area-weighted aggregation. The value of a   polygon is distributed proportionally to the area of its overlap   with each zone. Defaults to \"intersects\".</p> <code>'intersects'</code> <code>**kwargs</code> <p>Additional keyword arguments to be passed to the underlying <code>aggregate_polygons_to_zones_new</code> function.</p> <code>{}</code> <p>Returns:</p> Name Type Description <code>Dict</code> <code>Dict</code> <p>A dictionary or a nested dictionary containing the aggregated values, with zone IDs as keys. If <code>value_columns</code> is a single string, the return value is a dictionary mapping zone ID to the aggregated value. If <code>value_columns</code> is a list, the return value is a nested dictionary mapping each column name to its own dictionary of aggregated values.</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If <code>value_columns</code> is of an unexpected type after processing.</p> Example Source code in <code>gigaspatial/generators/zonal/base.py</code> <pre><code>def map_polygons(\n    self,\n    polygons,\n    value_columns: Optional[Union[str, List[str]]] = None,\n    aggregation: Union[str, Dict[str, str]] = \"count\",\n    predicate: str = \"intersects\",\n    **kwargs,\n) -&gt; Dict:\n    \"\"\"\n    Maps polygon data to the instance's zones and aggregates values.\n\n    This method leverages `aggregate_polygons_to_zones` to perform a spatial\n    aggregation of polygon data onto the zones stored within this object instance.\n    It can count polygons, or aggregate their values, based on different spatial\n    relationships defined by the `predicate`.\n\n    Args:\n        polygons (Union[pd.DataFrame, gpd.GeoDataFrame]):\n            The polygon data to map. Must contain geometry information if a\n            DataFrame.\n        value_columns (Union[str, List[str]], optional):\n            The column name(s) from the `polygons` data to aggregate. If `None`,\n            the method will automatically count the number of polygons that\n            match the given `predicate` for each zone.\n        aggregation (Union[str, Dict[str, str]], optional):\n            The aggregation method(s) to use. Can be a single string (e.g., \"sum\",\n            \"mean\", \"max\") or a dictionary mapping column names to specific\n            aggregation methods. This is ignored and set to \"count\" if\n            `value_columns` is `None`. Defaults to \"count\".\n        predicate (Literal[\"intersects\", \"within\", \"fractional\"], optional):\n            The spatial relationship to use for aggregation:\n            - \"intersects\": Counts or aggregates values for any polygon that\n              intersects a zone.\n            - \"within\": Counts or aggregates values for polygons that are\n              entirely contained within a zone.\n            - \"fractional\": Performs area-weighted aggregation. The value of a\n              polygon is distributed proportionally to the area of its overlap\n              with each zone.\n            Defaults to \"intersects\".\n        **kwargs:\n            Additional keyword arguments to be passed to the underlying\n            `aggregate_polygons_to_zones_new` function.\n\n    Returns:\n        Dict:\n            A dictionary or a nested dictionary containing the aggregated values,\n            with zone IDs as keys. If `value_columns` is a single string, the\n            return value is a dictionary mapping zone ID to the aggregated value.\n            If `value_columns` is a list, the return value is a nested dictionary\n            mapping each column name to its own dictionary of aggregated values.\n\n    Raises:\n        ValueError: If `value_columns` is of an unexpected type after processing.\n\n    Example:\n        &gt;&gt;&gt; # Assuming 'self' is an object with a 'zone_gdf' attribute\n        &gt;&gt;&gt; # Count all land parcels that intersect each zone\n        &gt;&gt;&gt; parcel_counts = self.map_polygons(landuse_polygons)\n        &gt;&gt;&gt;\n        &gt;&gt;&gt; # Aggregate total population within zones using area weighting\n        &gt;&gt;&gt; population_by_zone = self.map_polygons(\n        ...     landuse_polygons,\n        ...     value_columns=\"population\",\n        ...     predicate=\"fractional\",\n        ...     aggregation=\"sum\"\n        ... )\n        &gt;&gt;&gt;\n        &gt;&gt;&gt; # Get the sum of residential area and count of buildings within each zone\n        &gt;&gt;&gt; residential_stats = self.map_polygons(\n        ...     building_polygons,\n        ...     value_columns=[\"residential_area_sqm\", \"building_id\"],\n        ...     aggregation={\"residential_area_sqm\": \"sum\", \"building_id\": \"count\"},\n        ...     predicate=\"intersects\"\n        ... )\n    \"\"\"\n\n    if value_columns is None:\n        self.logger.warning(\n            f\"No value_columns specified. Defaulting to counting polygons with {predicate} predicate.\"\n        )\n        temp_value_col = \"_temp_polygon_count_dummy\"\n        polygons[temp_value_col] = 1\n        actual_value_columns = temp_value_col\n        aggregation = \"count\"  # Force count if no value columns\n    else:\n        actual_value_columns = value_columns\n\n    result = aggregate_polygons_to_zones(\n        polygons=polygons,\n        zones=self.zone_gdf,\n        value_columns=actual_value_columns,\n        aggregation=aggregation,\n        predicate=predicate,\n        zone_id_column=\"zone_id\",\n    )\n\n    # Convert the result GeoDataFrame to the expected dictionary format\n    if isinstance(actual_value_columns, str):\n        return result.set_index(\"zone_id\")[actual_value_columns].to_dict()\n    elif isinstance(actual_value_columns, list):\n        return {\n            col: result.set_index(\"zone_id\")[col].to_dict()\n            for col in actual_value_columns\n        }\n    else:\n        raise ValueError(\"Unexpected type for actual_value_columns.\")\n</code></pre>"},{"location":"api/generators/#gigaspatial.generators.zonal.base.ZonalViewGenerator.map_polygons--assuming-self-is-an-object-with-a-zone_gdf-attribute","title":"Assuming 'self' is an object with a 'zone_gdf' attribute","text":""},{"location":"api/generators/#gigaspatial.generators.zonal.base.ZonalViewGenerator.map_polygons--count-all-land-parcels-that-intersect-each-zone","title":"Count all land parcels that intersect each zone","text":"<p>parcel_counts = self.map_polygons(landuse_polygons)</p>"},{"location":"api/generators/#gigaspatial.generators.zonal.base.ZonalViewGenerator.map_polygons--aggregate-total-population-within-zones-using-area-weighting","title":"Aggregate total population within zones using area weighting","text":"<p>population_by_zone = self.map_polygons( ...     landuse_polygons, ...     value_columns=\"population\", ...     predicate=\"fractional\", ...     aggregation=\"sum\" ... )</p>"},{"location":"api/generators/#gigaspatial.generators.zonal.base.ZonalViewGenerator.map_polygons--get-the-sum-of-residential-area-and-count-of-buildings-within-each-zone","title":"Get the sum of residential area and count of buildings within each zone","text":"<p>residential_stats = self.map_polygons( ...     building_polygons, ...     value_columns=[\"residential_area_sqm\", \"building_id\"], ...     aggregation={\"residential_area_sqm\": \"sum\", \"building_id\": \"count\"}, ...     predicate=\"intersects\" ... )</p>"},{"location":"api/generators/#gigaspatial.generators.zonal.base.ZonalViewGenerator.map_rasters","title":"<code>map_rasters(raster_data, stat='mean', **kwargs)</code>","text":"<p>Map raster data to zones using zonal statistics.</p> <p>Samples raster values within each zone and computes statistics. Automatically handles coordinate reference system transformations between raster and zone data.</p> <p>Parameters:</p> Name Type Description Default <code>raster_data</code> <code>Union[TifProcessor, List[TifProcessor]]</code> <p>Either a TifProcessor object or a list of TifProcessor objects (which will be merged into a single TifProcessor for processing).</p> required <code>mapping_function</code> <code>Callable</code> <p>Custom function for mapping rasters to zones. If provided, signature should be mapping_function(self, tif_processors, **mapping_kwargs). When used, stat and other parameters except mapping_kwargs are ignored.</p> required <code>stat</code> <code>str</code> <p>Statistic to calculate when aggregating raster values within each zone. Options include \"mean\", \"sum\", \"min\", \"max\", \"std\", etc. Defaults to \"mean\".</p> <code>'mean'</code> <code>**mapping_kwargs</code> <p>Additional keyword arguments for raster data.</p> required <p>Returns:</p> Name Type Description <code>Dict</code> <code>Dict</code> <p>By default, returns a dictionary of sampled values with zone IDs as keys.</p> Note <p>If the coordinate reference system of the rasters differs from the zones, the zone geometries will be automatically transformed to match the raster CRS.</p> Source code in <code>gigaspatial/generators/zonal/base.py</code> <pre><code>def map_rasters(\n    self,\n    raster_data: Union[TifProcessor, List[TifProcessor]],\n    stat: str = \"mean\",\n    **kwargs,\n) -&gt; Dict:\n    \"\"\"Map raster data to zones using zonal statistics.\n\n    Samples raster values within each zone and computes statistics. Automatically\n    handles coordinate reference system transformations between raster and zone data.\n\n    Args:\n        raster_data (Union[TifProcessor, List[TifProcessor]]):\n            Either a TifProcessor object or a list of TifProcessor objects (which will be merged\n            into a single TifProcessor for processing).\n        mapping_function (Callable, optional): Custom function for mapping rasters\n            to zones. If provided, signature should be mapping_function(self, tif_processors, **mapping_kwargs).\n            When used, stat and other parameters except mapping_kwargs are ignored.\n        stat (str): Statistic to calculate when aggregating raster values within\n            each zone. Options include \"mean\", \"sum\", \"min\", \"max\", \"std\", etc.\n            Defaults to \"mean\".\n        **mapping_kwargs: Additional keyword arguments for raster data.\n\n    Returns:\n        Dict: By default, returns a dictionary of sampled values\n            with zone IDs as keys.\n\n    Note:\n        If the coordinate reference system of the rasters differs from the zones,\n        the zone geometries will be automatically transformed to match the raster CRS.\n    \"\"\"\n    raster_processor: Optional[TifProcessor] = None\n\n    if isinstance(raster_data, TifProcessor):\n        raster_processor = raster_data\n    elif isinstance(raster_data, list) and all(\n        isinstance(x, TifProcessor) for x in raster_data\n    ):\n        if not raster_data:\n            self.logger.info(\"No valid raster data provided\")\n            return self.view\n\n        if len(raster_data) &gt; 1:\n            all_source_paths = [tp.dataset_path for tp in raster_data]\n\n            self.logger.info(\n                f\"Merging {len(all_source_paths)} rasters into a single TifProcessor for zonal statistics.\"\n            )\n            raster_processor = TifProcessor(\n                dataset_path=all_source_paths, data_store=self.data_store, **kwargs\n            )\n        else:\n            raster_processor = raster_data[0]\n    else:\n        raise ValueError(\n            \"raster_data must be a TifProcessor object or a list of TifProcessor objects.\"\n        )\n\n    raster_crs = raster_processor.crs\n\n    if raster_crs != self.zone_gdf.crs:\n        self.logger.info(f\"Projecting zones to raster CRS: {raster_crs}\")\n        zone_geoms = self._get_transformed_geometries(raster_crs)\n    else:\n        zone_geoms = self.get_zonal_geometries()\n\n    # Sample raster values\n    sampled_values = raster_processor.sample_by_polygons(\n        polygon_list=zone_geoms, stat=stat\n    )\n\n    zone_ids = self.get_zone_identifiers()\n\n    return {zone_id: value for zone_id, value in zip(zone_ids, sampled_values)}\n</code></pre>"},{"location":"api/generators/#gigaspatial.generators.zonal.base.ZonalViewGenerator.save_view","title":"<code>save_view(name, output_format=None)</code>","text":"<p>Save the generated zonal view to disk.</p> <p>Parameters:</p> Name Type Description Default <code>name</code> <code>str</code> <p>Base name for the output file (without extension).</p> required <code>output_format</code> <code>str</code> <p>File format to save in (e.g., \"parquet\", \"geojson\", \"shp\"). If None, uses the format specified in config.</p> <code>None</code> <p>Returns:</p> Name Type Description <code>Path</code> <code>Path</code> <p>The full path where the view was saved.</p> Note <p>The output directory is determined by the config.base_path setting. The file extension is automatically added based on the output format. This method now saves the internal <code>self.view</code>.</p> Source code in <code>gigaspatial/generators/zonal/base.py</code> <pre><code>def save_view(\n    self,\n    name: str,\n    output_format: Optional[str] = None,\n) -&gt; Path:\n    \"\"\"Save the generated zonal view to disk.\n\n    Args:\n        name (str): Base name for the output file (without extension).\n        output_format (str, optional): File format to save in (e.g., \"parquet\",\n            \"geojson\", \"shp\"). If None, uses the format specified in config.\n\n    Returns:\n        Path: The full path where the view was saved.\n\n    Note:\n        The output directory is determined by the config.base_path setting.\n        The file extension is automatically added based on the output format.\n        This method now saves the internal `self.view`.\n    \"\"\"\n    if self._view is None:\n        self.logger.warning(\n            \"No variables have been added to the zonal view. Saving the base zone_gdf.\"\n        )\n        view_to_save = self.zone_gdf\n    else:\n        view_to_save = self._view\n\n    format_to_use = output_format or self.config.output_format\n    output_path = self.config.base_path / f\"{name}.{format_to_use}\"\n\n    self.logger.info(f\"Saving zonal view to {output_path}\")\n\n    if format_to_use in [\"geojson\", \"shp\", \"gpkg\"]:\n        self.logger.warning(\n            f\"Saving to {format_to_use} requires converting back to GeoDataFrame. Geometry column will be re-added.\"\n        )\n        # Re-add geometry for saving to geospatial formats\n        view_to_save = self.view.merge(\n            self.zone_gdf[[\"zone_id\", \"geometry\"]], on=\"zone_id\", how=\"left\"\n        )\n\n    write_dataset(\n        data=view_to_save,\n        path=str(output_path),\n        data_store=self.data_store,\n    )\n\n    return output_path\n</code></pre>"},{"location":"api/generators/#gigaspatial.generators.zonal.base.ZonalViewGenerator.to_dataframe","title":"<code>to_dataframe()</code>","text":"<p>Returns the current zonal view as a DataFrame.</p> <p>This method combines all accumulated variables in the view</p> <p>Returns:</p> Type Description <code>DataFrame</code> <p>pd.DataFrame: The current view.</p> Source code in <code>gigaspatial/generators/zonal/base.py</code> <pre><code>def to_dataframe(self) -&gt; pd.DataFrame:\n    \"\"\"\n    Returns the current zonal view as a DataFrame.\n\n    This method combines all accumulated variables in the view\n\n    Returns:\n        pd.DataFrame: The current view.\n    \"\"\"\n    return self.view\n</code></pre>"},{"location":"api/generators/#gigaspatial.generators.zonal.base.ZonalViewGenerator.to_geodataframe","title":"<code>to_geodataframe()</code>","text":"<p>Returns the current zonal view merged with zone geometries as a GeoDataFrame.</p> <p>This method combines all accumulated variables in the view with the corresponding zone geometries, providing a spatially-enabled DataFrame for further analysis or export.</p> <p>Returns:</p> Type Description <code>GeoDataFrame</code> <p>gpd.GeoDataFrame: The current view merged with zone geometries.</p> Source code in <code>gigaspatial/generators/zonal/base.py</code> <pre><code>def to_geodataframe(self) -&gt; gpd.GeoDataFrame:\n    \"\"\"\n    Returns the current zonal view merged with zone geometries as a GeoDataFrame.\n\n    This method combines all accumulated variables in the view with the corresponding\n    zone geometries, providing a spatially-enabled DataFrame for further analysis or export.\n\n    Returns:\n        gpd.GeoDataFrame: The current view merged with zone geometries.\n    \"\"\"\n    return gpd.GeoDataFrame(\n        (self.view).merge(\n            self.zone_gdf[[\"zone_id\", \"geometry\"]], on=\"zone_id\", how=\"left\"\n        ),\n        crs=self.zone_gdf.crs,\n    )\n</code></pre>"},{"location":"api/generators/#gigaspatial.generators.zonal.base.ZonalViewGeneratorConfig","title":"<code>ZonalViewGeneratorConfig</code>","text":"<p>               Bases: <code>BaseModel</code></p> <p>Configuration for zonal view generation.</p> <p>Attributes:</p> Name Type Description <code>base_path</code> <code>Path</code> <p>Base directory path for storing zonal views. Defaults to configured zonal views path.</p> <code>output_format</code> <code>str</code> <p>Default output format for saved views. Defaults to \"parquet\".</p> Source code in <code>gigaspatial/generators/zonal/base.py</code> <pre><code>class ZonalViewGeneratorConfig(BaseModel):\n    \"\"\"Configuration for zonal view generation.\n\n    Attributes:\n        base_path (Path): Base directory path for storing zonal views. Defaults to\n            configured zonal views path.\n        output_format (str): Default output format for saved views. Defaults to \"parquet\".\n    \"\"\"\n\n    base_path: Path = Field(default=global_config.get_path(\"zonal\", \"views\"))\n    output_format: str = \"parquet\"\n    ensure_available: bool = True\n</code></pre>"},{"location":"api/generators/#gigaspatial.generators.zonal.geometry","title":"<code>geometry</code>","text":""},{"location":"api/generators/#gigaspatial.generators.zonal.geometry.GeometryBasedZonalViewGenerator","title":"<code>GeometryBasedZonalViewGenerator</code>","text":"<p>               Bases: <code>ZonalViewGenerator[T]</code></p> <p>Mid-level class for zonal view generation based on geometries with identifiers.</p> <p>This class serves as an intermediate between the abstract ZonalViewGenerator and specific implementations like MercatorViewGenerator or H3ViewGenerator. It handles the common case where zones are defined by a mapping between zone identifiers and geometries, either provided as a dictionary or as a GeoDataFrame.</p> <p>The class extends the base functionality with methods for mapping common geospatial datasets including GHSL (Global Human Settlement Layer), Google Open Buildings, and Microsoft Global Buildings data.</p> <p>Attributes:</p> Name Type Description <code>zone_dict</code> <code>Dict[T, Polygon]</code> <p>Mapping of zone identifiers to geometries.</p> <code>zone_id_column</code> <code>str</code> <p>Name of the column containing zone identifiers.</p> <code>zone_data_crs</code> <code>str</code> <p>Coordinate reference system of the zone data.</p> <code>_zone_gdf</code> <code>GeoDataFrame</code> <p>Cached GeoDataFrame representation of zones.</p> <code>data_store</code> <code>DataStore</code> <p>For accessing input data.</p> <code>config</code> <code>ZonalViewGeneratorConfig</code> <p>Configuration for view generation.</p> <code>logger</code> <code>ZonalViewGeneratorConfig</code> <p>Logger instance for this class.</p> Source code in <code>gigaspatial/generators/zonal/geometry.py</code> <pre><code>class GeometryBasedZonalViewGenerator(ZonalViewGenerator[T]):\n    \"\"\"Mid-level class for zonal view generation based on geometries with identifiers.\n\n    This class serves as an intermediate between the abstract ZonalViewGenerator and specific\n    implementations like MercatorViewGenerator or H3ViewGenerator. It handles the common case\n    where zones are defined by a mapping between zone identifiers and geometries, either\n    provided as a dictionary or as a GeoDataFrame.\n\n    The class extends the base functionality with methods for mapping common geospatial\n    datasets including GHSL (Global Human Settlement Layer), Google Open Buildings,\n    and Microsoft Global Buildings data.\n\n    Attributes:\n        zone_dict (Dict[T, Polygon]): Mapping of zone identifiers to geometries.\n        zone_id_column (str): Name of the column containing zone identifiers.\n        zone_data_crs (str): Coordinate reference system of the zone data.\n        _zone_gdf (gpd.GeoDataFrame): Cached GeoDataFrame representation of zones.\n        data_store (DataStore): For accessing input data.\n        config (ZonalViewGeneratorConfig): Configuration for view generation.\n        logger: Logger instance for this class.\n    \"\"\"\n\n    def __init__(\n        self,\n        zone_data: Union[Dict[T, Polygon], gpd.GeoDataFrame],\n        zone_id_column: str = \"zone_id\",\n        zone_data_crs: str = \"EPSG:4326\",\n        config: Optional[ZonalViewGeneratorConfig] = None,\n        data_store: Optional[DataStore] = None,\n        logger: logging.Logger = None,\n    ):\n        \"\"\"Initialize with zone geometries and identifiers.\n\n        Args:\n            zone_data (Union[Dict[T, Polygon], gpd.GeoDataFrame]): Zone definitions.\n                Either a dictionary mapping zone identifiers to Polygon/MultiPolygon geometries,\n                or a GeoDataFrame with geometries and a zone identifier column.\n            zone_id_column (str): Name of the column containing zone identifiers.\n                Only used if zone_data is a GeoDataFrame. Defaults to \"zone_id\".\n            zone_data_crs (str): Coordinate reference system of the zone data.\n                Defaults to \"EPSG:4326\" (WGS84).\n            config (ZonalViewGeneratorConfig, optional): Generator configuration.\n                If None, uses default configuration.\n            data_store (DataStore, optional): Data store for accessing input data.\n                If None, uses LocalDataStore.\n\n        Raises:\n            TypeError: If zone_data is not a dictionary or GeoDataFrame, or if dictionary\n                values are not Polygon/MultiPolygon geometries.\n            ValueError: If zone_id_column is not found in GeoDataFrame, or if the provided\n                CRS doesn't match the GeoDataFrame's CRS.\n        \"\"\"\n        super().__init__(config=config, data_store=data_store, logger=logger)\n\n        self.zone_id_column = zone_id_column\n        self.zone_data_crs = zone_data_crs\n\n        # Store zone data based on input type\n        if isinstance(zone_data, dict):\n            for zone_id, geom in zone_data.items():\n                if not isinstance(geom, (Polygon, MultiPolygon)):\n                    raise TypeError(\n                        f\"Zone {zone_id}: Expected (Multi)Polygon, got {type(geom).__name__}\"\n                    )\n\n            # Store the original dictionary\n            self.zone_dict = zone_data\n\n            # Also create a GeoDataFrame for consistent access\n            self._zone_gdf = gpd.GeoDataFrame(\n                {\n                    \"zone_id\": list(zone_data.keys()),\n                    \"geometry\": list(zone_data.values()),\n                },\n                crs=zone_data_crs,\n            )\n            self.zone_id_column = \"zone_id\"\n        else:\n            if not isinstance(zone_data, gpd.GeoDataFrame):\n                raise TypeError(\n                    \"zone_data must be either a Dict[T, Polygon] or a GeoDataFrame\"\n                )\n\n            if zone_id_column not in zone_data.columns:\n                raise ValueError(\n                    f\"Zone ID column '{zone_id_column}' not found in GeoDataFrame\"\n                )\n\n            if zone_data_crs != zone_data.crs:\n                raise ValueError(\n                    f\"Provided data crs '{zone_data_crs}' does not match to the crs of the data '{zone_data.crs}'\"\n                )\n\n            # Store the GeoDataFrame\n            self._zone_gdf = zone_data.rename(columns={zone_id_column: \"zone_id\"})\n\n            # Also create a dictionary for fast lookups\n            self.zone_dict = dict(zip(zone_data[zone_id_column], zone_data.geometry))\n\n    def get_zonal_geometries(self) -&gt; List[Polygon]:\n        \"\"\"Get the geometry of each zone.\n\n        Returns:\n            List[Polygon]: A list of zone geometries in the order they appear in the\n                underlying GeoDataFrame.\n        \"\"\"\n        return self._zone_gdf.geometry.tolist()\n\n    def get_zone_identifiers(self) -&gt; List[T]:\n        \"\"\"Get the identifier for each zone.\n\n        Returns:\n            List[T]: A list of zone identifiers in the order they appear in the\n                underlying GeoDataFrame.\n        \"\"\"\n        return self._zone_gdf.zone_id.tolist()\n\n    def get_zone_geodataframe(self) -&gt; gpd.GeoDataFrame:\n        \"\"\"Convert zones to a GeoDataFrame with standardized column names.\n\n        Returns:\n            gpd.GeoDataFrame: A GeoDataFrame with 'zone_id' and 'geometry' columns.\n                The zone_id column is renamed from the original zone_id_column if different.\n        \"\"\"\n        # Since _zone_gdf is already created with 'zone_id' column in the constructor,\n        # we just need to return a copy of it\n        return self._zone_gdf.copy()\n\n    @property\n    def zone_gdf(self) -&gt; gpd.GeoDataFrame:\n        \"\"\"Override the base class zone_gdf property to ensure correct column names.\n\n        Returns:\n            gpd.GeoDataFrame: A GeoDataFrame with 'zone_id' and 'geometry' columns.\n        \"\"\"\n        return self._zone_gdf.copy()\n\n    def map_built_s(\n        self,\n        year=2020,\n        resolution=100,\n        stat: str = \"sum\",\n        output_column: str = \"built_surface_m2\",\n        **kwargs,\n    ) -&gt; pd.DataFrame:\n        \"\"\"Map GHSL Built-up Surface data to zones.\n\n        Convenience method for mapping Global Human Settlement Layer Built-up Surface\n        data using appropriate default parameters for built surface analysis.\n\n        Args:\n            year: The year of the data (default: 2020)\n            resolution: The resolution in meters (default: 100)\n            stat (str): Statistic to calculate for built surface values within each zone.\n                Defaults to \"sum\" which gives total built surface area.\n            output_column (str): The output column name. Defaults to \"built_surface_m2\".\n        Returns:\n            pd.DataFrame: Updated view DataFrame and settlement classification.\n                Adds a column with `output_column` containing the aggregated values.\n        \"\"\"\n        handler = GHSLDataHandler(\n            product=\"GHS_BUILT_S\",\n            year=year,\n            resolution=resolution,\n            data_store=self.data_store,\n            **kwargs,\n        )\n\n        return self.map_ghsl(\n            handler=handler, stat=stat, output_column=output_column, **kwargs\n        )\n\n    def map_smod(\n        self,\n        year=2020,\n        resolution=1000,\n        stat: str = \"median\",\n        output_column: str = \"smod_class\",\n        **kwargs,\n    ) -&gt; pd.DataFrame:\n        \"\"\"Map GHSL Settlement Model data to zones.\n\n        Convenience method for mapping Global Human Settlement Layer Settlement Model\n        data using appropriate default parameters for settlement classification analysis.\n\n        Args:\n            year: The year of the data (default: 2020)\n            resolution: The resolution in meters (default: 1000)\n            stat (str): Statistic to calculate for settlement class values within each zone.\n                Defaults to \"median\" which gives the predominant settlement class.\n            output_column (str): The output column name. Defaults to \"smod_class\".\n        Returns:\n            pd.DataFrame: Updated view DataFrame and settlement classification.\n                Adds a column with `output_column` containing the aggregated values.\n        \"\"\"\n        handler = GHSLDataHandler(\n            product=\"GHS_SMOD\",\n            year=year,\n            resolution=resolution,\n            data_store=self.data_store,\n            coord_system=54009,\n            **kwargs,\n        )\n\n        return self.map_ghsl(\n            handler=handler, stat=stat, output_column=output_column, **kwargs\n        )\n\n    def map_ghsl(\n        self,\n        handler: GHSLDataHandler,\n        stat: str,\n        output_column: Optional[str] = None,\n        **kwargs,\n    ) -&gt; pd.DataFrame:\n        \"\"\"Map Global Human Settlement Layer data to zones.\n\n        Loads and processes GHSL raster data for the intersecting tiles, then samples\n        the raster values within each zone using the specified statistic.\n\n        Args:\n            hander (GHSLDataHandler): Handler for the GHSL data.\n            stat (str): Statistic to calculate for raster values within each zone.\n                Common options: \"mean\", \"sum\", \"median\", \"min\", \"max\".\n            output_column (str): The output column name.\n                If None, uses the GHSL product name in lowercase followed by underscore.\n\n        Returns:\n            pd.DataFrame: Updated DataFrame with GHSL metrics.\n                Adds a column named as `output_column` containing the sampled values.\n\n        Note:\n            The method automatically determines which GHSL tiles intersect with the zones\n            and loads only the necessary data for efficient processing.\n        \"\"\"\n        handler = handler or GHSLDataHandler(data_store=self.data_store, **kwargs)\n        self.logger.info(\n            f\"Mapping {handler.config.product} data (year: {handler.config.year}, resolution: {handler.config.resolution}m)\"\n        )\n        tif_processors = handler.load_data(\n            self.zone_gdf,\n            ensure_available=self.config.ensure_available,\n            merge_rasters=True,\n            **kwargs,\n        )\n\n        self.logger.info(\n            f\"Sampling {handler.config.product} data using '{stat}' statistic\"\n        )\n        sampled_values = self.map_rasters(raster_data=tif_processors, stat=stat)\n\n        column_name = (\n            output_column\n            if output_column\n            else f\"{handler.config.product.lower()}_{stat}\"\n        )\n\n        self.add_variable_to_view(sampled_values, column_name)\n\n        return self.view\n\n    def map_google_buildings(\n        self,\n        handler: Optional[GoogleOpenBuildingsHandler] = None,\n        use_polygons: bool = False,\n        **kwargs,\n    ) -&gt; pd.DataFrame:\n        \"\"\"Map Google Open Buildings data to zones.\n\n        Processes Google Open Buildings dataset to calculate building counts and total\n        building area within each zone. Can use either point centroids (faster) or\n        polygon geometries (more accurate) for spatial operations.\n\n        Args:\n            google_open_buildings_config (GoogleOpenBuildingsConfig): Configuration\n                for accessing Google Open Buildings data. Uses default configuration if not provided.\n            use_polygons (bool): Whether to use polygon geometries for buildings.\n                If True, uses actual building polygons for more accurate area calculations\n                but with slower performance. If False, uses building centroids with\n                area values from attributes for faster processing. Defaults to False.\n\n        Returns:\n            pd.DataFrame: Updated DataFrame with building metrics.\n                Adds columns:\n                - 'google_buildings_count': Number of buildings in each zone\n                - 'google_buildings_area_in_meters': Total building area in square meters\n\n        Note:\n            If no Google Buildings data is found for the zones, returns the original\n            GeoDataFrame unchanged with a warning logged.\n        \"\"\"\n        self.logger.info(\n            f\"Mapping Google Open Buildings data (use_polygons={use_polygons})\"\n        )\n\n        self.logger.info(\"Loading Google Buildings point data\")\n        handler = handler or GoogleOpenBuildingsHandler(data_store=self.data_store)\n        buildings_df = handler.load_points(\n            self.zone_gdf, ensure_available=self.config.ensure_available, **kwargs\n        )\n\n        if buildings_df.empty:\n            self.logger.warning(\"No Google buildings data found for the provided zones\")\n            return self._zone_gdf.copy()\n\n        if not use_polygons:\n            self.logger.info(\"Aggregating building data using points with attributes\")\n            result = self.map_points(\n                points=buildings_df,\n                value_columns=[\"full_plus_code\", \"area_in_meters\"],\n                aggregation={\"full_plus_code\": \"count\", \"area_in_meters\": \"sum\"},\n                predicate=\"within\",\n            )\n\n            count_result = result[\"full_plus_code\"]\n            area_result = result[\"area_in_meters\"]\n\n        else:\n            self.logger.info(\n                \"Loading Google Buildings polygon data for more accurate mapping\"\n            )\n            buildings_gdf = handler.load_polygons(\n                self.zone_gdf, ensure_available=self.config.ensure_available, **kwargs\n            )\n\n            self.logger.info(\n                \"Calculating building areas with area-weighted aggregation\"\n            )\n            area_result = self.map_polygons(\n                buildings_gdf,\n                value_columns=\"area_in_meters\",\n                aggregation=\"sum\",\n                predicate=\"fractional\",\n            )\n\n            self.logger.info(\"Counting buildings using points data\")\n            count_result = self.map_points(points=buildings_df, predicate=\"within\")\n\n        self.add_variable_to_view(count_result, \"google_buildings_count\")\n        self.add_variable_to_view(area_result, \"google_buildings_area_in_meters\")\n\n        return self.view\n\n    def map_ms_buildings(\n        self,\n        handler: Optional[MSBuildingsHandler] = None,\n        use_polygons: bool = False,\n    ) -&gt; gpd.GeoDataFrame:\n        \"\"\"Map Microsoft Global Buildings data to zones.\n\n        Processes Microsoft Global Buildings dataset to calculate building counts and\n        total building area within each zone. Can use either centroid points (faster)\n        or polygon geometries (more accurate) for spatial operations.\n\n        Args:\n            ms_buildings_config (MSBuildingsConfig, optional): Configuration for\n                accessing Microsoft Global Buildings data. If None, uses default configuration.\n            use_polygons (bool): Whether to use polygon geometries for buildings.\n                If True, uses actual building polygons for more accurate area calculations\n                but with slower performance. If False, uses building centroids with\n                area values from attributes for faster processing. Defaults to False.\n\n        Returns:\n            gpd.GeoDataFrame: Updated GeoDataFrame with zones and building metrics.\n                Adds columns:\n                - 'ms_buildings_count': Number of buildings in each zone\n                - 'ms_buildings_area_in_meters': Total building area in square meters\n\n        Note:\n            If no Microsoft Buildings data is found for the zones, returns the original\n            GeoDataFrame unchanged with a warning logged. Building areas are calculated\n            in meters using appropriate UTM projections.\n        \"\"\"\n        self.logger.info(\"Mapping Microsoft Global Buildings data\")\n\n        self.logger.info(\"Loading Microsoft Buildings polygon data\")\n        handler = MSBuildingsHandler(data_store=self.data_store)\n        buildings_gdf = handler.load_data(\n            self.zone_gdf, ensure_available=self.config.ensure_available\n        )\n\n        # Check if we found any buildings\n        if buildings_gdf.empty:\n            self.logger.warning(\n                \"No Microsoft buildings data found for the provided zones\"\n            )\n            return self._zone_gdf.copy()\n\n        buildings_gdf = add_area_in_meters(\n            buildings_gdf, area_column_name=\"area_in_meters\"\n        )\n\n        building_centroids = get_centroids(buildings_gdf)\n\n        if not use_polygons:\n            self.logger.info(\"Aggregating building data using points with attributes\")\n\n            result = self.map_points(\n                points=building_centroids,\n                value_columns=[\"type\", \"area_in_meters\"],\n                aggregation={\"type\": \"count\", \"area_in_meters\": \"sum\"},\n                predicate=\"within\",\n            )\n\n            count_result = result[\"type\"]\n            area_result = result[\"area_in_meters\"]\n        else:\n\n            self.logger.info(\n                \"Calculating building areas with area-weighted aggregation\"\n            )\n            area_result = self.map_polygons(\n                buildings_gdf,\n                value_columns=\"area_in_meters\",\n                aggregation=\"sum\",\n                predicate=\"fractional\",\n            )\n\n            self.logger.info(\"Counting Microsoft buildings per zone\")\n\n            count_result = self.map_points(\n                points=building_centroids, predicate=\"within\"\n            )\n\n        self.add_variable_to_view(count_result, \"ms_buildings_count\")\n        self.add_variable_to_view(area_result, \"ms_buildings_area_in_meters\")\n\n        return self.view\n\n    def map_ghsl_pop(\n        self,\n        resolution=100,\n        stat: str = \"sum\",\n        output_column: str = \"ghsl_pop\",\n        predicate: Literal[\"intersects\", \"fractional\"] = \"intersects\",\n        **kwargs,\n    ):\n        handler = GHSLDataHandler(\n            product=\"GHS_POP\",\n            resolution=resolution,\n            data_store=self.data_store,\n            **kwargs,\n        )\n\n        if predicate == \"fractional\":\n            if resolution == 100:\n                self.logger.warning(\n                    \"Fractional aggregations only supported for datasets with 1000m resolution. Using `intersects` as predicate\"\n                )\n                predicate = \"intersects\"\n            else:\n                gdf_pop = handler.load_into_geodataframe(self.zone_gdf, **kwargs)\n\n                result = self.map_polygons(\n                    gdf_pop,\n                    value_columns=\"pixel_value\",\n                    aggregation=\"sum\",\n                    predicate=\"fractional\",\n                )\n\n                self.add_variable_to_view(result, output_column)\n                return self.view\n\n        return self.map_ghsl(\n            handler=handler, stat=stat, output_column=output_column, **kwargs\n        )\n\n    def map_wp_pop(\n        self,\n        country: Union[str, List[str]],\n        resolution=1000,\n        predicate: Literal[\n            \"centroid_within\", \"intersects\", \"fractional\"\n        ] = \"intersects\",\n        output_column: str = \"population\",\n        **kwargs,\n    ):\n\n        # Ensure country is always a list for consistent handling\n        countries_list = [country] if isinstance(country, str) else country\n\n        handler = WPPopulationHandler(\n            resolution=resolution,\n            data_store=self.data_store,\n            **kwargs,\n        )\n\n        # Restrict to single country for age_structures project\n        if handler.config.project == \"age_structures\" and len(countries_list) &gt; 1:\n            raise ValueError(\n                \"For 'age_structures' project, only a single country can be processed at a time.\"\n            )\n\n        self.logger.info(\n            f\"Mapping WorldPop Population data (year: {handler.config.year}, resolution: {handler.config.resolution}m)\"\n        )\n\n        if predicate == \"fractional\" and resolution == 100:\n            self.logger.warning(\n                \"Fractional aggregations only supported for datasets with 1000m resolution. Using `intersects` as predicate\"\n            )\n            predicate = \"intersects\"\n\n        if predicate == \"centroid_within\":\n            if handler.config.project == \"age_structures\":\n                # Load individual tif processors for the single country\n                all_tif_processors = handler.load_data(\n                    countries_list[0],\n                    ensure_available=self.config.ensure_available,\n                    **kwargs,\n                )\n\n                # Sum results from each tif_processor separately\n                all_results_by_zone = {\n                    zone_id: 0 for zone_id in self.get_zone_identifiers()\n                }\n                self.logger.info(\n                    f\"Sampling individual age_structures rasters using 'sum' statistic and summing per zone.\"\n                )\n                for tif_processor in all_tif_processors:\n                    single_raster_result = self.map_rasters(\n                        raster_data=tif_processor, stat=\"sum\"\n                    )\n                    for zone_id, value in single_raster_result.items():\n                        all_results_by_zone[zone_id] += value\n                result = all_results_by_zone\n            else:\n                # Existing behavior for non-age_structures projects or if merging is fine\n                tif_processors = []\n                for c in countries_list:\n                    tif_processors.extend(\n                        handler.load_data(\n                            c,\n                            ensure_available=self.config.ensure_available,\n                            **kwargs,\n                        )\n                    )\n                self.logger.info(\n                    f\"Sampling WorldPop Population data using 'sum' statistic\"\n                )\n                result = self.map_rasters(raster_data=tif_processors, stat=\"sum\")\n        else:\n            gdf_pop = pd.concat(\n                [\n                    handler.load_into_geodataframe(\n                        c,\n                        ensure_available=self.config.ensure_available,\n                        **kwargs,\n                    )\n                    for c in countries_list\n                ],\n                ignore_index=True,\n            )\n\n            self.logger.info(f\"Aggregating WorldPop Population data to the zones.\")\n            result = self.map_polygons(\n                gdf_pop,\n                value_columns=\"pixel_value\",\n                aggregation=\"sum\",\n                predicate=predicate,\n            )\n\n        self.add_variable_to_view(result, output_column)\n\n        return self.view\n</code></pre>"},{"location":"api/generators/#gigaspatial.generators.zonal.geometry.GeometryBasedZonalViewGenerator.zone_gdf","title":"<code>zone_gdf: gpd.GeoDataFrame</code>  <code>property</code>","text":"<p>Override the base class zone_gdf property to ensure correct column names.</p> <p>Returns:</p> Type Description <code>GeoDataFrame</code> <p>gpd.GeoDataFrame: A GeoDataFrame with 'zone_id' and 'geometry' columns.</p>"},{"location":"api/generators/#gigaspatial.generators.zonal.geometry.GeometryBasedZonalViewGenerator.__init__","title":"<code>__init__(zone_data, zone_id_column='zone_id', zone_data_crs='EPSG:4326', config=None, data_store=None, logger=None)</code>","text":"<p>Initialize with zone geometries and identifiers.</p> <p>Parameters:</p> Name Type Description Default <code>zone_data</code> <code>Union[Dict[T, Polygon], GeoDataFrame]</code> <p>Zone definitions. Either a dictionary mapping zone identifiers to Polygon/MultiPolygon geometries, or a GeoDataFrame with geometries and a zone identifier column.</p> required <code>zone_id_column</code> <code>str</code> <p>Name of the column containing zone identifiers. Only used if zone_data is a GeoDataFrame. Defaults to \"zone_id\".</p> <code>'zone_id'</code> <code>zone_data_crs</code> <code>str</code> <p>Coordinate reference system of the zone data. Defaults to \"EPSG:4326\" (WGS84).</p> <code>'EPSG:4326'</code> <code>config</code> <code>ZonalViewGeneratorConfig</code> <p>Generator configuration. If None, uses default configuration.</p> <code>None</code> <code>data_store</code> <code>DataStore</code> <p>Data store for accessing input data. If None, uses LocalDataStore.</p> <code>None</code> <p>Raises:</p> Type Description <code>TypeError</code> <p>If zone_data is not a dictionary or GeoDataFrame, or if dictionary values are not Polygon/MultiPolygon geometries.</p> <code>ValueError</code> <p>If zone_id_column is not found in GeoDataFrame, or if the provided CRS doesn't match the GeoDataFrame's CRS.</p> Source code in <code>gigaspatial/generators/zonal/geometry.py</code> <pre><code>def __init__(\n    self,\n    zone_data: Union[Dict[T, Polygon], gpd.GeoDataFrame],\n    zone_id_column: str = \"zone_id\",\n    zone_data_crs: str = \"EPSG:4326\",\n    config: Optional[ZonalViewGeneratorConfig] = None,\n    data_store: Optional[DataStore] = None,\n    logger: logging.Logger = None,\n):\n    \"\"\"Initialize with zone geometries and identifiers.\n\n    Args:\n        zone_data (Union[Dict[T, Polygon], gpd.GeoDataFrame]): Zone definitions.\n            Either a dictionary mapping zone identifiers to Polygon/MultiPolygon geometries,\n            or a GeoDataFrame with geometries and a zone identifier column.\n        zone_id_column (str): Name of the column containing zone identifiers.\n            Only used if zone_data is a GeoDataFrame. Defaults to \"zone_id\".\n        zone_data_crs (str): Coordinate reference system of the zone data.\n            Defaults to \"EPSG:4326\" (WGS84).\n        config (ZonalViewGeneratorConfig, optional): Generator configuration.\n            If None, uses default configuration.\n        data_store (DataStore, optional): Data store for accessing input data.\n            If None, uses LocalDataStore.\n\n    Raises:\n        TypeError: If zone_data is not a dictionary or GeoDataFrame, or if dictionary\n            values are not Polygon/MultiPolygon geometries.\n        ValueError: If zone_id_column is not found in GeoDataFrame, or if the provided\n            CRS doesn't match the GeoDataFrame's CRS.\n    \"\"\"\n    super().__init__(config=config, data_store=data_store, logger=logger)\n\n    self.zone_id_column = zone_id_column\n    self.zone_data_crs = zone_data_crs\n\n    # Store zone data based on input type\n    if isinstance(zone_data, dict):\n        for zone_id, geom in zone_data.items():\n            if not isinstance(geom, (Polygon, MultiPolygon)):\n                raise TypeError(\n                    f\"Zone {zone_id}: Expected (Multi)Polygon, got {type(geom).__name__}\"\n                )\n\n        # Store the original dictionary\n        self.zone_dict = zone_data\n\n        # Also create a GeoDataFrame for consistent access\n        self._zone_gdf = gpd.GeoDataFrame(\n            {\n                \"zone_id\": list(zone_data.keys()),\n                \"geometry\": list(zone_data.values()),\n            },\n            crs=zone_data_crs,\n        )\n        self.zone_id_column = \"zone_id\"\n    else:\n        if not isinstance(zone_data, gpd.GeoDataFrame):\n            raise TypeError(\n                \"zone_data must be either a Dict[T, Polygon] or a GeoDataFrame\"\n            )\n\n        if zone_id_column not in zone_data.columns:\n            raise ValueError(\n                f\"Zone ID column '{zone_id_column}' not found in GeoDataFrame\"\n            )\n\n        if zone_data_crs != zone_data.crs:\n            raise ValueError(\n                f\"Provided data crs '{zone_data_crs}' does not match to the crs of the data '{zone_data.crs}'\"\n            )\n\n        # Store the GeoDataFrame\n        self._zone_gdf = zone_data.rename(columns={zone_id_column: \"zone_id\"})\n\n        # Also create a dictionary for fast lookups\n        self.zone_dict = dict(zip(zone_data[zone_id_column], zone_data.geometry))\n</code></pre>"},{"location":"api/generators/#gigaspatial.generators.zonal.geometry.GeometryBasedZonalViewGenerator.get_zonal_geometries","title":"<code>get_zonal_geometries()</code>","text":"<p>Get the geometry of each zone.</p> <p>Returns:</p> Type Description <code>List[Polygon]</code> <p>List[Polygon]: A list of zone geometries in the order they appear in the underlying GeoDataFrame.</p> Source code in <code>gigaspatial/generators/zonal/geometry.py</code> <pre><code>def get_zonal_geometries(self) -&gt; List[Polygon]:\n    \"\"\"Get the geometry of each zone.\n\n    Returns:\n        List[Polygon]: A list of zone geometries in the order they appear in the\n            underlying GeoDataFrame.\n    \"\"\"\n    return self._zone_gdf.geometry.tolist()\n</code></pre>"},{"location":"api/generators/#gigaspatial.generators.zonal.geometry.GeometryBasedZonalViewGenerator.get_zone_geodataframe","title":"<code>get_zone_geodataframe()</code>","text":"<p>Convert zones to a GeoDataFrame with standardized column names.</p> <p>Returns:</p> Type Description <code>GeoDataFrame</code> <p>gpd.GeoDataFrame: A GeoDataFrame with 'zone_id' and 'geometry' columns. The zone_id column is renamed from the original zone_id_column if different.</p> Source code in <code>gigaspatial/generators/zonal/geometry.py</code> <pre><code>def get_zone_geodataframe(self) -&gt; gpd.GeoDataFrame:\n    \"\"\"Convert zones to a GeoDataFrame with standardized column names.\n\n    Returns:\n        gpd.GeoDataFrame: A GeoDataFrame with 'zone_id' and 'geometry' columns.\n            The zone_id column is renamed from the original zone_id_column if different.\n    \"\"\"\n    # Since _zone_gdf is already created with 'zone_id' column in the constructor,\n    # we just need to return a copy of it\n    return self._zone_gdf.copy()\n</code></pre>"},{"location":"api/generators/#gigaspatial.generators.zonal.geometry.GeometryBasedZonalViewGenerator.get_zone_identifiers","title":"<code>get_zone_identifiers()</code>","text":"<p>Get the identifier for each zone.</p> <p>Returns:</p> Type Description <code>List[T]</code> <p>List[T]: A list of zone identifiers in the order they appear in the underlying GeoDataFrame.</p> Source code in <code>gigaspatial/generators/zonal/geometry.py</code> <pre><code>def get_zone_identifiers(self) -&gt; List[T]:\n    \"\"\"Get the identifier for each zone.\n\n    Returns:\n        List[T]: A list of zone identifiers in the order they appear in the\n            underlying GeoDataFrame.\n    \"\"\"\n    return self._zone_gdf.zone_id.tolist()\n</code></pre>"},{"location":"api/generators/#gigaspatial.generators.zonal.geometry.GeometryBasedZonalViewGenerator.map_built_s","title":"<code>map_built_s(year=2020, resolution=100, stat='sum', output_column='built_surface_m2', **kwargs)</code>","text":"<p>Map GHSL Built-up Surface data to zones.</p> <p>Convenience method for mapping Global Human Settlement Layer Built-up Surface data using appropriate default parameters for built surface analysis.</p> <p>Parameters:</p> Name Type Description Default <code>year</code> <p>The year of the data (default: 2020)</p> <code>2020</code> <code>resolution</code> <p>The resolution in meters (default: 100)</p> <code>100</code> <code>stat</code> <code>str</code> <p>Statistic to calculate for built surface values within each zone. Defaults to \"sum\" which gives total built surface area.</p> <code>'sum'</code> <code>output_column</code> <code>str</code> <p>The output column name. Defaults to \"built_surface_m2\".</p> <code>'built_surface_m2'</code> Source code in <code>gigaspatial/generators/zonal/geometry.py</code> <pre><code>def map_built_s(\n    self,\n    year=2020,\n    resolution=100,\n    stat: str = \"sum\",\n    output_column: str = \"built_surface_m2\",\n    **kwargs,\n) -&gt; pd.DataFrame:\n    \"\"\"Map GHSL Built-up Surface data to zones.\n\n    Convenience method for mapping Global Human Settlement Layer Built-up Surface\n    data using appropriate default parameters for built surface analysis.\n\n    Args:\n        year: The year of the data (default: 2020)\n        resolution: The resolution in meters (default: 100)\n        stat (str): Statistic to calculate for built surface values within each zone.\n            Defaults to \"sum\" which gives total built surface area.\n        output_column (str): The output column name. Defaults to \"built_surface_m2\".\n    Returns:\n        pd.DataFrame: Updated view DataFrame and settlement classification.\n            Adds a column with `output_column` containing the aggregated values.\n    \"\"\"\n    handler = GHSLDataHandler(\n        product=\"GHS_BUILT_S\",\n        year=year,\n        resolution=resolution,\n        data_store=self.data_store,\n        **kwargs,\n    )\n\n    return self.map_ghsl(\n        handler=handler, stat=stat, output_column=output_column, **kwargs\n    )\n</code></pre>"},{"location":"api/generators/#gigaspatial.generators.zonal.geometry.GeometryBasedZonalViewGenerator.map_ghsl","title":"<code>map_ghsl(handler, stat, output_column=None, **kwargs)</code>","text":"<p>Map Global Human Settlement Layer data to zones.</p> <p>Loads and processes GHSL raster data for the intersecting tiles, then samples the raster values within each zone using the specified statistic.</p> <p>Parameters:</p> Name Type Description Default <code>hander</code> <code>GHSLDataHandler</code> <p>Handler for the GHSL data.</p> required <code>stat</code> <code>str</code> <p>Statistic to calculate for raster values within each zone. Common options: \"mean\", \"sum\", \"median\", \"min\", \"max\".</p> required <code>output_column</code> <code>str</code> <p>The output column name. If None, uses the GHSL product name in lowercase followed by underscore.</p> <code>None</code> <p>Returns:</p> Type Description <code>DataFrame</code> <p>pd.DataFrame: Updated DataFrame with GHSL metrics. Adds a column named as <code>output_column</code> containing the sampled values.</p> Note <p>The method automatically determines which GHSL tiles intersect with the zones and loads only the necessary data for efficient processing.</p> Source code in <code>gigaspatial/generators/zonal/geometry.py</code> <pre><code>def map_ghsl(\n    self,\n    handler: GHSLDataHandler,\n    stat: str,\n    output_column: Optional[str] = None,\n    **kwargs,\n) -&gt; pd.DataFrame:\n    \"\"\"Map Global Human Settlement Layer data to zones.\n\n    Loads and processes GHSL raster data for the intersecting tiles, then samples\n    the raster values within each zone using the specified statistic.\n\n    Args:\n        hander (GHSLDataHandler): Handler for the GHSL data.\n        stat (str): Statistic to calculate for raster values within each zone.\n            Common options: \"mean\", \"sum\", \"median\", \"min\", \"max\".\n        output_column (str): The output column name.\n            If None, uses the GHSL product name in lowercase followed by underscore.\n\n    Returns:\n        pd.DataFrame: Updated DataFrame with GHSL metrics.\n            Adds a column named as `output_column` containing the sampled values.\n\n    Note:\n        The method automatically determines which GHSL tiles intersect with the zones\n        and loads only the necessary data for efficient processing.\n    \"\"\"\n    handler = handler or GHSLDataHandler(data_store=self.data_store, **kwargs)\n    self.logger.info(\n        f\"Mapping {handler.config.product} data (year: {handler.config.year}, resolution: {handler.config.resolution}m)\"\n    )\n    tif_processors = handler.load_data(\n        self.zone_gdf,\n        ensure_available=self.config.ensure_available,\n        merge_rasters=True,\n        **kwargs,\n    )\n\n    self.logger.info(\n        f\"Sampling {handler.config.product} data using '{stat}' statistic\"\n    )\n    sampled_values = self.map_rasters(raster_data=tif_processors, stat=stat)\n\n    column_name = (\n        output_column\n        if output_column\n        else f\"{handler.config.product.lower()}_{stat}\"\n    )\n\n    self.add_variable_to_view(sampled_values, column_name)\n\n    return self.view\n</code></pre>"},{"location":"api/generators/#gigaspatial.generators.zonal.geometry.GeometryBasedZonalViewGenerator.map_google_buildings","title":"<code>map_google_buildings(handler=None, use_polygons=False, **kwargs)</code>","text":"<p>Map Google Open Buildings data to zones.</p> <p>Processes Google Open Buildings dataset to calculate building counts and total building area within each zone. Can use either point centroids (faster) or polygon geometries (more accurate) for spatial operations.</p> <p>Parameters:</p> Name Type Description Default <code>google_open_buildings_config</code> <code>GoogleOpenBuildingsConfig</code> <p>Configuration for accessing Google Open Buildings data. Uses default configuration if not provided.</p> required <code>use_polygons</code> <code>bool</code> <p>Whether to use polygon geometries for buildings. If True, uses actual building polygons for more accurate area calculations but with slower performance. If False, uses building centroids with area values from attributes for faster processing. Defaults to False.</p> <code>False</code> <p>Returns:</p> Type Description <code>DataFrame</code> <p>pd.DataFrame: Updated DataFrame with building metrics. Adds columns: - 'google_buildings_count': Number of buildings in each zone - 'google_buildings_area_in_meters': Total building area in square meters</p> Note <p>If no Google Buildings data is found for the zones, returns the original GeoDataFrame unchanged with a warning logged.</p> Source code in <code>gigaspatial/generators/zonal/geometry.py</code> <pre><code>def map_google_buildings(\n    self,\n    handler: Optional[GoogleOpenBuildingsHandler] = None,\n    use_polygons: bool = False,\n    **kwargs,\n) -&gt; pd.DataFrame:\n    \"\"\"Map Google Open Buildings data to zones.\n\n    Processes Google Open Buildings dataset to calculate building counts and total\n    building area within each zone. Can use either point centroids (faster) or\n    polygon geometries (more accurate) for spatial operations.\n\n    Args:\n        google_open_buildings_config (GoogleOpenBuildingsConfig): Configuration\n            for accessing Google Open Buildings data. Uses default configuration if not provided.\n        use_polygons (bool): Whether to use polygon geometries for buildings.\n            If True, uses actual building polygons for more accurate area calculations\n            but with slower performance. If False, uses building centroids with\n            area values from attributes for faster processing. Defaults to False.\n\n    Returns:\n        pd.DataFrame: Updated DataFrame with building metrics.\n            Adds columns:\n            - 'google_buildings_count': Number of buildings in each zone\n            - 'google_buildings_area_in_meters': Total building area in square meters\n\n    Note:\n        If no Google Buildings data is found for the zones, returns the original\n        GeoDataFrame unchanged with a warning logged.\n    \"\"\"\n    self.logger.info(\n        f\"Mapping Google Open Buildings data (use_polygons={use_polygons})\"\n    )\n\n    self.logger.info(\"Loading Google Buildings point data\")\n    handler = handler or GoogleOpenBuildingsHandler(data_store=self.data_store)\n    buildings_df = handler.load_points(\n        self.zone_gdf, ensure_available=self.config.ensure_available, **kwargs\n    )\n\n    if buildings_df.empty:\n        self.logger.warning(\"No Google buildings data found for the provided zones\")\n        return self._zone_gdf.copy()\n\n    if not use_polygons:\n        self.logger.info(\"Aggregating building data using points with attributes\")\n        result = self.map_points(\n            points=buildings_df,\n            value_columns=[\"full_plus_code\", \"area_in_meters\"],\n            aggregation={\"full_plus_code\": \"count\", \"area_in_meters\": \"sum\"},\n            predicate=\"within\",\n        )\n\n        count_result = result[\"full_plus_code\"]\n        area_result = result[\"area_in_meters\"]\n\n    else:\n        self.logger.info(\n            \"Loading Google Buildings polygon data for more accurate mapping\"\n        )\n        buildings_gdf = handler.load_polygons(\n            self.zone_gdf, ensure_available=self.config.ensure_available, **kwargs\n        )\n\n        self.logger.info(\n            \"Calculating building areas with area-weighted aggregation\"\n        )\n        area_result = self.map_polygons(\n            buildings_gdf,\n            value_columns=\"area_in_meters\",\n            aggregation=\"sum\",\n            predicate=\"fractional\",\n        )\n\n        self.logger.info(\"Counting buildings using points data\")\n        count_result = self.map_points(points=buildings_df, predicate=\"within\")\n\n    self.add_variable_to_view(count_result, \"google_buildings_count\")\n    self.add_variable_to_view(area_result, \"google_buildings_area_in_meters\")\n\n    return self.view\n</code></pre>"},{"location":"api/generators/#gigaspatial.generators.zonal.geometry.GeometryBasedZonalViewGenerator.map_ms_buildings","title":"<code>map_ms_buildings(handler=None, use_polygons=False)</code>","text":"<p>Map Microsoft Global Buildings data to zones.</p> <p>Processes Microsoft Global Buildings dataset to calculate building counts and total building area within each zone. Can use either centroid points (faster) or polygon geometries (more accurate) for spatial operations.</p> <p>Parameters:</p> Name Type Description Default <code>ms_buildings_config</code> <code>MSBuildingsConfig</code> <p>Configuration for accessing Microsoft Global Buildings data. If None, uses default configuration.</p> required <code>use_polygons</code> <code>bool</code> <p>Whether to use polygon geometries for buildings. If True, uses actual building polygons for more accurate area calculations but with slower performance. If False, uses building centroids with area values from attributes for faster processing. Defaults to False.</p> <code>False</code> <p>Returns:</p> Type Description <code>GeoDataFrame</code> <p>gpd.GeoDataFrame: Updated GeoDataFrame with zones and building metrics. Adds columns: - 'ms_buildings_count': Number of buildings in each zone - 'ms_buildings_area_in_meters': Total building area in square meters</p> Note <p>If no Microsoft Buildings data is found for the zones, returns the original GeoDataFrame unchanged with a warning logged. Building areas are calculated in meters using appropriate UTM projections.</p> Source code in <code>gigaspatial/generators/zonal/geometry.py</code> <pre><code>def map_ms_buildings(\n    self,\n    handler: Optional[MSBuildingsHandler] = None,\n    use_polygons: bool = False,\n) -&gt; gpd.GeoDataFrame:\n    \"\"\"Map Microsoft Global Buildings data to zones.\n\n    Processes Microsoft Global Buildings dataset to calculate building counts and\n    total building area within each zone. Can use either centroid points (faster)\n    or polygon geometries (more accurate) for spatial operations.\n\n    Args:\n        ms_buildings_config (MSBuildingsConfig, optional): Configuration for\n            accessing Microsoft Global Buildings data. If None, uses default configuration.\n        use_polygons (bool): Whether to use polygon geometries for buildings.\n            If True, uses actual building polygons for more accurate area calculations\n            but with slower performance. If False, uses building centroids with\n            area values from attributes for faster processing. Defaults to False.\n\n    Returns:\n        gpd.GeoDataFrame: Updated GeoDataFrame with zones and building metrics.\n            Adds columns:\n            - 'ms_buildings_count': Number of buildings in each zone\n            - 'ms_buildings_area_in_meters': Total building area in square meters\n\n    Note:\n        If no Microsoft Buildings data is found for the zones, returns the original\n        GeoDataFrame unchanged with a warning logged. Building areas are calculated\n        in meters using appropriate UTM projections.\n    \"\"\"\n    self.logger.info(\"Mapping Microsoft Global Buildings data\")\n\n    self.logger.info(\"Loading Microsoft Buildings polygon data\")\n    handler = MSBuildingsHandler(data_store=self.data_store)\n    buildings_gdf = handler.load_data(\n        self.zone_gdf, ensure_available=self.config.ensure_available\n    )\n\n    # Check if we found any buildings\n    if buildings_gdf.empty:\n        self.logger.warning(\n            \"No Microsoft buildings data found for the provided zones\"\n        )\n        return self._zone_gdf.copy()\n\n    buildings_gdf = add_area_in_meters(\n        buildings_gdf, area_column_name=\"area_in_meters\"\n    )\n\n    building_centroids = get_centroids(buildings_gdf)\n\n    if not use_polygons:\n        self.logger.info(\"Aggregating building data using points with attributes\")\n\n        result = self.map_points(\n            points=building_centroids,\n            value_columns=[\"type\", \"area_in_meters\"],\n            aggregation={\"type\": \"count\", \"area_in_meters\": \"sum\"},\n            predicate=\"within\",\n        )\n\n        count_result = result[\"type\"]\n        area_result = result[\"area_in_meters\"]\n    else:\n\n        self.logger.info(\n            \"Calculating building areas with area-weighted aggregation\"\n        )\n        area_result = self.map_polygons(\n            buildings_gdf,\n            value_columns=\"area_in_meters\",\n            aggregation=\"sum\",\n            predicate=\"fractional\",\n        )\n\n        self.logger.info(\"Counting Microsoft buildings per zone\")\n\n        count_result = self.map_points(\n            points=building_centroids, predicate=\"within\"\n        )\n\n    self.add_variable_to_view(count_result, \"ms_buildings_count\")\n    self.add_variable_to_view(area_result, \"ms_buildings_area_in_meters\")\n\n    return self.view\n</code></pre>"},{"location":"api/generators/#gigaspatial.generators.zonal.geometry.GeometryBasedZonalViewGenerator.map_smod","title":"<code>map_smod(year=2020, resolution=1000, stat='median', output_column='smod_class', **kwargs)</code>","text":"<p>Map GHSL Settlement Model data to zones.</p> <p>Convenience method for mapping Global Human Settlement Layer Settlement Model data using appropriate default parameters for settlement classification analysis.</p> <p>Parameters:</p> Name Type Description Default <code>year</code> <p>The year of the data (default: 2020)</p> <code>2020</code> <code>resolution</code> <p>The resolution in meters (default: 1000)</p> <code>1000</code> <code>stat</code> <code>str</code> <p>Statistic to calculate for settlement class values within each zone. Defaults to \"median\" which gives the predominant settlement class.</p> <code>'median'</code> <code>output_column</code> <code>str</code> <p>The output column name. Defaults to \"smod_class\".</p> <code>'smod_class'</code> Source code in <code>gigaspatial/generators/zonal/geometry.py</code> <pre><code>def map_smod(\n    self,\n    year=2020,\n    resolution=1000,\n    stat: str = \"median\",\n    output_column: str = \"smod_class\",\n    **kwargs,\n) -&gt; pd.DataFrame:\n    \"\"\"Map GHSL Settlement Model data to zones.\n\n    Convenience method for mapping Global Human Settlement Layer Settlement Model\n    data using appropriate default parameters for settlement classification analysis.\n\n    Args:\n        year: The year of the data (default: 2020)\n        resolution: The resolution in meters (default: 1000)\n        stat (str): Statistic to calculate for settlement class values within each zone.\n            Defaults to \"median\" which gives the predominant settlement class.\n        output_column (str): The output column name. Defaults to \"smod_class\".\n    Returns:\n        pd.DataFrame: Updated view DataFrame and settlement classification.\n            Adds a column with `output_column` containing the aggregated values.\n    \"\"\"\n    handler = GHSLDataHandler(\n        product=\"GHS_SMOD\",\n        year=year,\n        resolution=resolution,\n        data_store=self.data_store,\n        coord_system=54009,\n        **kwargs,\n    )\n\n    return self.map_ghsl(\n        handler=handler, stat=stat, output_column=output_column, **kwargs\n    )\n</code></pre>"},{"location":"api/generators/#gigaspatial.generators.zonal.h3","title":"<code>h3</code>","text":""},{"location":"api/generators/#gigaspatial.generators.zonal.h3.H3ViewGenerator","title":"<code>H3ViewGenerator</code>","text":"<p>               Bases: <code>GeometryBasedZonalViewGenerator[T]</code></p> <p>Generates zonal views using H3 hexagons as the zones.</p> <p>Mirrors <code>MercatorViewGenerator</code>/<code>S2ViewGenerator</code> but uses H3 cells (resolutions 0-15). The input <code>source</code> defines the area/cells and <code>resolution</code> determines the granularity.</p> <p>Supported sources: - Country string \u2192 <code>CountryH3Hexagons.create</code> - Shapely geometry or GeoDataFrame \u2192 <code>H3Hexagons.from_spatial</code> - List of points (shapely Points or (lon, lat) tuples) \u2192 <code>from_spatial</code> - List of H3 indexes (strings) \u2192 <code>H3Hexagons.from_hexagons</code></p> Source code in <code>gigaspatial/generators/zonal/h3.py</code> <pre><code>class H3ViewGenerator(GeometryBasedZonalViewGenerator[T]):\n    \"\"\"\n    Generates zonal views using H3 hexagons as the zones.\n\n    Mirrors `MercatorViewGenerator`/`S2ViewGenerator` but uses H3 cells\n    (resolutions 0-15). The input `source` defines the area/cells and\n    `resolution` determines the granularity.\n\n    Supported sources:\n    - Country string \u2192 `CountryH3Hexagons.create`\n    - Shapely geometry or GeoDataFrame \u2192 `H3Hexagons.from_spatial`\n    - List of points (shapely Points or (lon, lat) tuples) \u2192 `from_spatial`\n    - List of H3 indexes (strings) \u2192 `H3Hexagons.from_hexagons`\n    \"\"\"\n\n    def __init__(\n        self,\n        source: Union[\n            str,  # country\n            BaseGeometry,  # shapely geom\n            gpd.GeoDataFrame,\n            List[Union[Point, Tuple[float, float]]],  # points\n            List[str],  # h3 indexes\n        ],\n        resolution: int,\n        contain: str = \"overlap\",\n        config: Optional[ZonalViewGeneratorConfig] = None,\n        data_store: Optional[DataStore] = None,\n        logger: logging.Logger = None,\n    ):\n\n        super().__init__(\n            zone_data=self._init_zone_data(\n                source, resolution, contain=contain, data_store=data_store\n            ),\n            zone_id_column=\"h3\",\n            config=config,\n            data_store=data_store,\n            logger=logger,\n        )\n        self.logger.info(\"Initialized H3ViewGenerator\")\n\n    def _init_zone_data(\n        self,\n        source,\n        resolution: int,\n        contain: str = \"overlap\",\n        data_store: Optional[DataStore] = None,\n    ):\n        if isinstance(source, str):\n            hexes = CountryH3Hexagons.create(\n                country=source,\n                resolution=resolution,\n                contain=contain,\n                data_store=data_store,\n            )\n            self._country = source\n        elif isinstance(source, (BaseGeometry, gpd.GeoDataFrame, Iterable)):\n            if isinstance(source, Iterable) and all(isinstance(h, str) for h in source):\n                hexes = H3Hexagons.from_hexagons(list(source))\n            else:\n                hexes = H3Hexagons.from_spatial(\n                    source=source, resolution=resolution, contain=contain\n                )\n        else:\n            raise TypeError(\n                \"Unsupported source type for H3ViewGenerator. 'source' must be \"\n                \"a country name (str), a Shapely geometry, a GeoDataFrame, \"\n                \"a list of H3 indexes (str), or a list of (lon, lat) tuples/Shapely Point objects. \"\n                f\"Received type: {type(source)}.\"\n            )\n\n        return hexes.to_geodataframe()\n\n    def map_wp_pop(\n        self,\n        country=None,\n        resolution=1000,\n        predicate=\"intersects\",\n        output_column=\"population\",\n        **kwargs,\n    ):\n        if hasattr(self, \"_country\") and country is None:\n            country = self._country\n\n        return super().map_wp_pop(\n            country, resolution, predicate, output_column, **kwargs\n        )\n</code></pre>"},{"location":"api/generators/#gigaspatial.generators.zonal.mercator","title":"<code>mercator</code>","text":""},{"location":"api/generators/#gigaspatial.generators.zonal.mercator.MercatorViewGenerator","title":"<code>MercatorViewGenerator</code>","text":"<p>               Bases: <code>GeometryBasedZonalViewGenerator[T]</code></p> <p>Generates zonal views using Mercator tiles as the zones.</p> <p>This class specializes in creating zonal views where the zones are defined by Mercator tiles. It extends the <code>GeometryBasedZonalViewGenerator</code> and leverages the <code>MercatorTiles</code> and <code>CountryMercatorTiles</code> classes to generate tiles based on various input sources.</p> <p>The primary input source defines the geographical area of interest. This can be a country, a specific geometry, a set of points, or even a list of predefined quadkeys. The <code>zoom_level</code> determines the granularity of the Mercator tiles.</p> <p>Attributes:</p> Name Type Description <code>source</code> <code>Union[str, BaseGeometry, GeoDataFrame, List[Union[Point, Tuple[float, float]]], List[str]]</code> <p>Specifies the geographic area or specific tiles to use. Can be: - A country name (str): Uses <code>CountryMercatorTiles</code> to generate tiles covering the country. - A Shapely geometry (BaseGeometry):  Uses <code>MercatorTiles.from_spatial</code> to create tiles intersecting the geometry. - A GeoDataFrame (gpd.GeoDataFrame): Uses <code>MercatorTiles.from_spatial</code> to create tiles intersecting the geometries. - A list of points (List[Union[Point, Tuple[float, float]]]):  Uses <code>MercatorTiles.from_spatial</code> to create tiles containing the points. - A list of quadkeys (List[str]): Uses <code>MercatorTiles.from_quadkeys</code> to use the specified tiles directly.</p> <code>zoom_level</code> <code>int</code> <p>The zoom level of the Mercator tiles. Higher zoom levels result in smaller, more detailed tiles.</p> <code>predicate</code> <code>str</code> <p>The spatial predicate used when filtering tiles based on a spatial source (e.g., \"intersects\", \"contains\"). Defaults to \"intersects\".</p> <code>config</code> <code>Optional[ZonalViewGeneratorConfig]</code> <p>Configuration for the zonal view generation process.</p> <code>data_store</code> <code>Optional[DataStore]</code> <p>A DataStore instance for accessing data.</p> <code>logger</code> <code>Optional[Logger]</code> <p>A logger instance for logging.</p> <p>Methods:</p> Name Description <code>_init_zone_data</code> <p>Initializes the Mercator tile GeoDataFrame based on the input source.</p> <code># Inherits other methods from GeometryBasedZonalViewGenerator, such as</code> Example Source code in <code>gigaspatial/generators/zonal/mercator.py</code> <pre><code>class MercatorViewGenerator(GeometryBasedZonalViewGenerator[T]):\n    \"\"\"\n    Generates zonal views using Mercator tiles as the zones.\n\n    This class specializes in creating zonal views where the zones are defined by\n    Mercator tiles. It extends the `GeometryBasedZonalViewGenerator` and leverages\n    the `MercatorTiles` and `CountryMercatorTiles` classes to generate tiles based on\n    various input sources.\n\n    The primary input source defines the geographical area of interest. This can be\n    a country, a specific geometry, a set of points, or even a list of predefined\n    quadkeys. The `zoom_level` determines the granularity of the Mercator tiles.\n\n    Attributes:\n        source (Union[str, BaseGeometry, gpd.GeoDataFrame, List[Union[Point, Tuple[float, float]]], List[str]]):\n            Specifies the geographic area or specific tiles to use. Can be:\n            - A country name (str): Uses `CountryMercatorTiles` to generate tiles covering the country.\n            - A Shapely geometry (BaseGeometry):  Uses `MercatorTiles.from_spatial` to create tiles intersecting the geometry.\n            - A GeoDataFrame (gpd.GeoDataFrame): Uses `MercatorTiles.from_spatial` to create tiles intersecting the geometries.\n            - A list of points (List[Union[Point, Tuple[float, float]]]):  Uses `MercatorTiles.from_spatial` to create tiles containing the points.\n            - A list of quadkeys (List[str]): Uses `MercatorTiles.from_quadkeys` to use the specified tiles directly.\n        zoom_level (int): The zoom level of the Mercator tiles. Higher zoom levels result in smaller, more detailed tiles.\n        predicate (str):  The spatial predicate used when filtering tiles based on a spatial source (e.g., \"intersects\", \"contains\"). Defaults to \"intersects\".\n        config (Optional[ZonalViewGeneratorConfig]): Configuration for the zonal view generation process.\n        data_store (Optional[DataStore]):  A DataStore instance for accessing data.\n        logger (Optional[logging.Logger]):  A logger instance for logging.\n\n    Methods:\n        _init_zone_data(source, zoom_level, predicate):  Initializes the Mercator tile GeoDataFrame based on the input source.\n        # Inherits other methods from GeometryBasedZonalViewGenerator, such as:\n        # map_ghsl(), map_google_buildings(), map_ms_buildings(), aggregate_data(), save_view()\n\n    Example:\n        # Create a MercatorViewGenerator for tiles covering Germany at zoom level 6\n        generator = MercatorViewGenerator(source=\"Germany\", zoom_level=6)\n\n        # Create a MercatorViewGenerator for tiles intersecting a specific polygon\n        polygon = ... # Define a Shapely Polygon\n        generator = MercatorViewGenerator(source=polygon, zoom_level=8)\n\n        # Create a MercatorViewGenerator from a list of quadkeys\n        quadkeys = [\"0020023131023032\", \"0020023131023033\"]\n        generator = MercatorViewGenerator(source=quadkeys, zoom_level=12)\n    \"\"\"\n\n    def __init__(\n        self,\n        source: Union[\n            str,  # country\n            BaseGeometry,  # shapely geom\n            gpd.GeoDataFrame,\n            List[Union[Point, Tuple[float, float]]],  # points\n            List[str],  # quadkeys\n        ],\n        zoom_level: int,\n        predicate=\"intersects\",\n        config: Optional[ZonalViewGeneratorConfig] = None,\n        data_store: Optional[DataStore] = None,\n        logger: logging.Logger = None,\n    ):\n\n        super().__init__(\n            zone_data=self._init_zone_data(source, zoom_level, predicate, data_store),\n            zone_id_column=\"quadkey\",\n            config=config,\n            data_store=data_store,\n            logger=logger,\n        )\n        self.logger.info(f\"Initialized MercatorViewGenerator\")\n\n    def _init_zone_data(self, source, zoom_level, predicate, data_store=None):\n        if isinstance(source, str):\n            tiles = CountryMercatorTiles.create(\n                country=source, zoom_level=zoom_level, data_store=data_store\n            )\n            self._country = source\n        elif isinstance(source, (BaseGeometry, Iterable)):\n            if isinstance(source, Iterable) and all(\n                isinstance(qk, str) for qk in source\n            ):\n                tiles = MercatorTiles.from_quadkeys(source)\n            else:\n                tiles = MercatorTiles.from_spatial(\n                    source=source, zoom_level=zoom_level, predicate=predicate\n                )\n        else:\n            raise TypeError(\n                f\"Unsupported source type for MercatorViewGenerator. 'source' must be \"\n                f\"a country name (str), a Shapely geometry, a GeoDataFrame, \"\n                f\"a list of quadkeys (str), or a list of (lon, lat) tuples/Shapely Point objects. \"\n                f\"Received type: {type(source)}.\"\n            )\n\n        return tiles.to_geodataframe()\n\n    def map_wp_pop(\n        self,\n        country=None,\n        resolution=1000,\n        predicate=\"intersects\",\n        output_column=\"population\",\n        **kwargs,\n    ):\n        if hasattr(self, \"_country\") and country is None:\n            country = self._country\n\n        return super().map_wp_pop(\n            country, resolution, predicate, output_column, **kwargs\n        )\n</code></pre>"},{"location":"api/generators/#gigaspatial.generators.zonal.mercator.MercatorViewGenerator--create-a-mercatorviewgenerator-for-tiles-covering-germany-at-zoom-level-6","title":"Create a MercatorViewGenerator for tiles covering Germany at zoom level 6","text":"<p>generator = MercatorViewGenerator(source=\"Germany\", zoom_level=6)</p>"},{"location":"api/generators/#gigaspatial.generators.zonal.mercator.MercatorViewGenerator--create-a-mercatorviewgenerator-for-tiles-intersecting-a-specific-polygon","title":"Create a MercatorViewGenerator for tiles intersecting a specific polygon","text":"<p>polygon = ... # Define a Shapely Polygon generator = MercatorViewGenerator(source=polygon, zoom_level=8)</p>"},{"location":"api/generators/#gigaspatial.generators.zonal.mercator.MercatorViewGenerator--create-a-mercatorviewgenerator-from-a-list-of-quadkeys","title":"Create a MercatorViewGenerator from a list of quadkeys","text":"<p>quadkeys = [\"0020023131023032\", \"0020023131023033\"] generator = MercatorViewGenerator(source=quadkeys, zoom_level=12)</p>"},{"location":"api/generators/#gigaspatial.generators.zonal.s2","title":"<code>s2</code>","text":""},{"location":"api/generators/#gigaspatial.generators.zonal.s2.S2ViewGenerator","title":"<code>S2ViewGenerator</code>","text":"<p>               Bases: <code>GeometryBasedZonalViewGenerator[T]</code></p> <p>Generates zonal views using Google S2 cells as the zones.</p> <p>This mirrors the <code>MercatorViewGenerator</code> but uses S2 cells (levels 0-30) as zone units. The primary input source defines the geographic area of interest and the <code>level</code> determines the granularity of S2 cells.</p> <p>Attributes:</p> Name Type Description <code>source</code> <code>Union[str, BaseGeometry, GeoDataFrame, List[Union[Point, Tuple[float, float]]], List[Union[int, str]]]</code> <p>Specifies the geographic area or specific cells to use. Can be: - A country name (str): Uses <code>CountryS2Cells</code> to generate cells covering the country. - A Shapely geometry (BaseGeometry): Uses <code>S2Cells.from_spatial</code> to create cells intersecting the geometry. - A GeoDataFrame (gpd.GeoDataFrame): Uses <code>S2Cells.from_spatial</code> to create cells intersecting geometries or from points. - A list of points (List[Union[Point, Tuple[float, float]]]): Uses <code>S2Cells.from_points</code> via <code>from_spatial</code>. - A list of cell identifiers (List[Union[int, str]]): Uses <code>S2Cells.from_cells</code> (accepts integer IDs or token strings).</p> <code>level</code> <code>int</code> <p>The S2 level (0-30). Higher levels produce smaller cells.</p> <code>config</code> <code>Optional[ZonalViewGeneratorConfig]</code> <p>Configuration for generation.</p> <code>data_store</code> <code>Optional[DataStore]</code> <p>Optional data store instance.</p> <code>logger</code> <code>Optional[Logger]</code> <p>Optional logger.</p> Source code in <code>gigaspatial/generators/zonal/s2.py</code> <pre><code>class S2ViewGenerator(GeometryBasedZonalViewGenerator[T]):\n    \"\"\"\n    Generates zonal views using Google S2 cells as the zones.\n\n    This mirrors the `MercatorViewGenerator` but uses S2 cells (levels 0-30)\n    as zone units. The primary input source defines the geographic area of\n    interest and the `level` determines the granularity of S2 cells.\n\n    Attributes:\n        source (Union[str, BaseGeometry, gpd.GeoDataFrame, List[Union[Point, Tuple[float, float]]], List[Union[int, str]]]):\n            Specifies the geographic area or specific cells to use. Can be:\n            - A country name (str): Uses `CountryS2Cells` to generate cells covering the country.\n            - A Shapely geometry (BaseGeometry): Uses `S2Cells.from_spatial` to create cells intersecting the geometry.\n            - A GeoDataFrame (gpd.GeoDataFrame): Uses `S2Cells.from_spatial` to create cells intersecting geometries or from points.\n            - A list of points (List[Union[Point, Tuple[float, float]]]): Uses `S2Cells.from_points` via `from_spatial`.\n            - A list of cell identifiers (List[Union[int, str]]): Uses `S2Cells.from_cells` (accepts integer IDs or token strings).\n        level (int): The S2 level (0-30). Higher levels produce smaller cells.\n        config (Optional[ZonalViewGeneratorConfig]): Configuration for generation.\n        data_store (Optional[DataStore]): Optional data store instance.\n        logger (Optional[logging.Logger]): Optional logger.\n    \"\"\"\n\n    def __init__(\n        self,\n        source: Union[\n            str,  # country\n            BaseGeometry,  # shapely geom\n            gpd.GeoDataFrame,\n            List[Union[Point, Tuple[float, float]]],  # points\n            List[Union[int, str]],  # cell ids or tokens\n        ],\n        level: int,\n        config: Optional[ZonalViewGeneratorConfig] = None,\n        data_store: Optional[DataStore] = None,\n        logger: logging.Logger = None,\n        max_cells: int = 1000,\n    ):\n\n        super().__init__(\n            zone_data=self._init_zone_data(\n                source, level, data_store=data_store, max_cells=max_cells\n            ),\n            zone_id_column=\"cell_token\",\n            config=config,\n            data_store=data_store,\n            logger=logger,\n        )\n        self.logger.info(\"Initialized S2ViewGenerator\")\n\n    def _init_zone_data(\n        self,\n        source,\n        level: int,\n        data_store: Optional[DataStore] = None,\n        max_cells: int = 1000,\n    ):\n        if isinstance(source, str):\n            cells = CountryS2Cells.create(\n                country=source, level=level, data_store=data_store, max_cells=max_cells\n            )\n            self._country = source\n        elif isinstance(source, (BaseGeometry, gpd.GeoDataFrame, Iterable)):\n            # If it's an explicit cells list of ids/tokens\n            if isinstance(source, Iterable) and all(\n                isinstance(c, (int, str)) for c in source\n            ):\n                cells = S2Cells.from_cells(list(source))\n            else:\n                # Spatial extraction from geometry/points/gdf\n                cells = S2Cells.from_spatial(\n                    source=source, level=level, max_cells=max_cells\n                )\n        else:\n            raise TypeError(\n                \"Unsupported source type for S2ViewGenerator. 'source' must be \"\n                \"a country name (str), a Shapely geometry, a GeoDataFrame, \"\n                \"a list of S2 cell ids/tokens (int/str), or a list of (lon, lat) tuples/Shapely Point objects. \"\n                f\"Received type: {type(source)}.\"\n            )\n\n        return cells.to_geodataframe()\n\n    def map_wp_pop(\n        self,\n        country=None,\n        resolution=1000,\n        predicate=\"intersects\",\n        output_column=\"population\",\n        **kwargs,\n    ):\n        if hasattr(self, \"_country\") and country is None:\n            country = self._country\n\n        return super().map_wp_pop(\n            country, resolution, predicate, output_column, **kwargs\n        )\n</code></pre>"},{"location":"api/grid/","title":"Grid Module","text":""},{"location":"api/grid/#gigaspatial.grid","title":"<code>gigaspatial.grid</code>","text":""},{"location":"api/grid/#gigaspatial.grid.h3","title":"<code>h3</code>","text":""},{"location":"api/grid/#gigaspatial.grid.h3.CountryH3Hexagons","title":"<code>CountryH3Hexagons</code>","text":"<p>               Bases: <code>H3Hexagons</code></p> <p>H3Hexagons specialized for country-level operations.</p> <p>This class extends H3Hexagons to work specifically with country boundaries. It can only be instantiated through the create() classmethod.</p> Source code in <code>gigaspatial/grid/h3.py</code> <pre><code>class CountryH3Hexagons(H3Hexagons):\n    \"\"\"H3Hexagons specialized for country-level operations.\n\n    This class extends H3Hexagons to work specifically with country boundaries.\n    It can only be instantiated through the create() classmethod.\n    \"\"\"\n\n    country: str = Field(..., exclude=True)\n\n    def __init__(self, *args, **kwargs):\n        raise TypeError(\n            \"CountryH3Hexagons cannot be instantiated directly. \"\n            \"Use CountryH3Hexagons.create() instead.\"\n        )\n\n    @classmethod\n    def create(\n        cls,\n        country: str,\n        resolution: int,\n        contain: Literal[\"center\", \"full\", \"overlap\", \"bbox_overlap\"] = \"overlap\",\n        data_store: Optional[DataStore] = None,\n        country_geom_path: Optional[Union[str, Path]] = None,\n    ):\n        \"\"\"Create CountryH3Hexagons for a specific country.\"\"\"\n        from gigaspatial.handlers.boundaries import AdminBoundaries\n\n        instance = super().__new__(cls)\n        super(CountryH3Hexagons, instance).__init__(\n            resolution=resolution,\n            hexagons=[],\n            data_store=data_store or LocalDataStore(),\n            country=pycountry.countries.lookup(country).alpha_3,\n        )\n\n        cls.logger.info(\n            f\"Initializing H3 hexagons for country: {country} at resolution {resolution}\"\n        )\n\n        country_geom = (\n            AdminBoundaries.create(\n                country_code=country,\n                data_store=data_store,\n                path=country_geom_path,\n            )\n            .boundaries[0]\n            .geometry\n        )\n\n        hexagons = H3Hexagons.from_geometry(country_geom, resolution, contain=contain)\n\n        instance.hexagons = hexagons.hexagons\n        return instance\n</code></pre>"},{"location":"api/grid/#gigaspatial.grid.h3.CountryH3Hexagons.create","title":"<code>create(country, resolution, contain='overlap', data_store=None, country_geom_path=None)</code>  <code>classmethod</code>","text":"<p>Create CountryH3Hexagons for a specific country.</p> Source code in <code>gigaspatial/grid/h3.py</code> <pre><code>@classmethod\ndef create(\n    cls,\n    country: str,\n    resolution: int,\n    contain: Literal[\"center\", \"full\", \"overlap\", \"bbox_overlap\"] = \"overlap\",\n    data_store: Optional[DataStore] = None,\n    country_geom_path: Optional[Union[str, Path]] = None,\n):\n    \"\"\"Create CountryH3Hexagons for a specific country.\"\"\"\n    from gigaspatial.handlers.boundaries import AdminBoundaries\n\n    instance = super().__new__(cls)\n    super(CountryH3Hexagons, instance).__init__(\n        resolution=resolution,\n        hexagons=[],\n        data_store=data_store or LocalDataStore(),\n        country=pycountry.countries.lookup(country).alpha_3,\n    )\n\n    cls.logger.info(\n        f\"Initializing H3 hexagons for country: {country} at resolution {resolution}\"\n    )\n\n    country_geom = (\n        AdminBoundaries.create(\n            country_code=country,\n            data_store=data_store,\n            path=country_geom_path,\n        )\n        .boundaries[0]\n        .geometry\n    )\n\n    hexagons = H3Hexagons.from_geometry(country_geom, resolution, contain=contain)\n\n    instance.hexagons = hexagons.hexagons\n    return instance\n</code></pre>"},{"location":"api/grid/#gigaspatial.grid.h3.H3Hexagons","title":"<code>H3Hexagons</code>","text":"<p>               Bases: <code>BaseModel</code></p> Source code in <code>gigaspatial/grid/h3.py</code> <pre><code>class H3Hexagons(BaseModel):\n    resolution: int = Field(..., ge=0, le=15)\n    hexagons: List[str] = Field(default_factory=list)\n    data_store: DataStore = Field(default_factory=LocalDataStore, exclude=True)\n    logger: ClassVar = config.get_logger(\"H3Hexagons\")\n\n    class Config:\n        arbitrary_types_allowed = True\n\n    @classmethod\n    def from_hexagons(cls, hexagons: List[str]):\n        \"\"\"Create H3Hexagons from list of H3 cell IDs.\"\"\"\n        if not hexagons:\n            cls.logger.warning(\"No hexagons provided to from_hexagons.\")\n            return cls(resolution=0, hexagons=[])\n\n        cls.logger.info(\n            f\"Initializing H3Hexagons from {len(hexagons)} provided hexagons.\"\n        )\n        # Get resolution from first hexagon\n        resolution = h3.get_resolution(hexagons[0])\n        return cls(resolution=resolution, hexagons=list(set(hexagons)))\n\n    @classmethod\n    def from_bounds(\n        cls, xmin: float, ymin: float, xmax: float, ymax: float, resolution: int\n    ):\n        \"\"\"Create H3Hexagons from boundary coordinates.\"\"\"\n        cls.logger.info(\n            f\"Creating H3Hexagons from bounds: ({xmin}, {ymin}, {xmax}, {ymax}) at resolution: {resolution}\"\n        )\n\n        # Create a LatLong bounding box polygon\n        latlong_bbox_coords = [\n            [ymin, xmin],\n            [ymax, xmin],\n            [ymax, xmax],\n            [ymin, xmax],\n            [ymin, xmin],\n        ]\n\n        # Get H3 cells that intersect with the bounding box\n        poly = h3.LatLngPoly(latlong_bbox_coords)\n        hexagons = h3.h3shape_to_cells(poly, res=resolution)\n\n        return cls(resolution=resolution, hexagons=list(hexagons))\n\n    @classmethod\n    def from_spatial(\n        cls,\n        source: Union[\n            BaseGeometry,\n            gpd.GeoDataFrame,\n            List[Union[Point, Tuple[float, float]]],  # points\n        ],\n        resolution: int,\n        contain: Literal[\"center\", \"full\", \"overlap\", \"bbox_overlap\"] = \"overlap\",\n        **kwargs,\n    ):\n        cls.logger.info(\n            f\"Creating H3Hexagons from spatial source (type: {type(source)}) at resolution: {resolution} with predicate: {contain}\"\n        )\n        if isinstance(source, gpd.GeoDataFrame):\n            if source.crs != \"EPSG:4326\":\n                source = source.to_crs(\"EPSG:4326\")\n\n            is_point_series = source.geometry.geom_type == \"Point\"\n            all_are_points = is_point_series.all()\n\n            if all_are_points:\n                source = source.geometry.to_list()\n            else:\n                source = source.geometry.unary_union\n\n        if isinstance(source, BaseGeometry):\n            return cls.from_geometry(\n                geometry=source, resolution=resolution, contain=contain, **kwargs\n            )\n        elif isinstance(source, Iterable) and all(\n            isinstance(pt, Point) or len(pt) == 2 for pt in source\n        ):\n            return cls.from_points(points=source, resolution=resolution, **kwargs)\n        else:\n            raise ValueError(\"Unsupported source type for H3Hexagons.from_spatial\")\n\n    @classmethod\n    def from_geometry(\n        cls,\n        geometry: BaseGeometry,\n        resolution: int,\n        contain: Literal[\"center\", \"full\", \"overlap\", \"bbox_overlap\"] = \"overlap\",\n        **kwargs,\n    ):\n        \"\"\"Create H3Hexagons from a geometry.\"\"\"\n        cls.logger.info(\n            f\"Creating H3Hexagons from geometry (bounds: {geometry.bounds}) at resolution: {resolution} with predicate: {contain}\"\n        )\n\n        if isinstance(geometry, Point):\n            return cls.from_points([geometry])\n\n        # Convert shapely geometry to GeoJSON-like format\n        if hasattr(geometry, \"__geo_interface__\"):\n            geojson_geom = geometry.__geo_interface__\n        else:\n            # Fallback for complex geometries\n            import json\n            from shapely.geometry import mapping\n\n            geojson_geom = mapping(geometry)\n\n        h3_geom = h3.geo_to_h3shape(geojson_geom)\n\n        hexagons = h3.h3shape_to_cells_experimental(\n            h3_geom, resolution, contain=contain\n        )\n\n        cls.logger.info(\n            f\"Generated {len(hexagons)} hexagons using `{contain}` spatial predicate.\"\n        )\n        return cls(resolution=resolution, hexagons=list(hexagons), **kwargs)\n\n    @classmethod\n    def from_points(\n        cls, points: List[Union[Point, Tuple[float, float]]], resolution: int, **kwargs\n    ) -&gt; \"H3Hexagons\":\n        \"\"\"Create H3Hexagons from a list of points or lat-lon pairs.\"\"\"\n        cls.logger.info(\n            f\"Creating H3Hexagons from {len(points)} points at resolution: {resolution}\"\n        )\n        hexagons = set(cls.get_hexagons_from_points(points, resolution))\n        cls.logger.info(f\"Generated {len(hexagons)} unique hexagons from points.\")\n        return cls(resolution=resolution, hexagons=list(hexagons), **kwargs)\n\n    @classmethod\n    def from_json(\n        cls, data_store: DataStore, file: Union[str, Path], **kwargs\n    ) -&gt; \"H3Hexagons\":\n        \"\"\"Load H3Hexagons from a JSON file.\"\"\"\n        cls.logger.info(\n            f\"Loading H3Hexagons from JSON file: {file} using data store: {type(data_store).__name__}\"\n        )\n        with data_store.open(str(file), \"r\") as f:\n            data = json.load(f)\n            if isinstance(data, list):  # If file contains only hexagon IDs\n                # Get resolution from first hexagon if available\n                resolution = h3.get_resolution(data[0]) if data else 0\n                data = {\n                    \"resolution\": resolution,\n                    \"hexagons\": data,\n                    **kwargs,\n                }\n            else:\n                data.update(kwargs)\n            instance = cls(**data)\n            instance.data_store = data_store\n            cls.logger.info(\n                f\"Successfully loaded {len(instance.hexagons)} hexagons from JSON file.\"\n            )\n            return instance\n\n    @property\n    def average_hexagon_area(self):\n        return h3.average_hexagon_area(self.resolution)\n\n    @property\n    def average_hexagon_edge_length(self):\n        return h3.average_hexagon_edge_length(self.resolution)\n\n    def filter_hexagons(self, hexagons: Iterable[str]) -&gt; \"H3Hexagons\":\n        \"\"\"Filter hexagons by a given set of hexagon IDs.\"\"\"\n        original_count = len(self.hexagons)\n        incoming_count = len(\n            list(hexagons)\n        )  # Convert to list to get length if it's an iterator\n\n        self.logger.info(\n            f\"Filtering {original_count} hexagons with an incoming set of {incoming_count} hexagons.\"\n        )\n        filtered_hexagons = list(set(self.hexagons) &amp; set(hexagons))\n        self.logger.info(f\"Resulting in {len(filtered_hexagons)} filtered hexagons.\")\n        return H3Hexagons(\n            resolution=self.resolution,\n            hexagons=filtered_hexagons,\n        )\n\n    def to_dataframe(self) -&gt; pd.DataFrame:\n        \"\"\"Convert to pandas DataFrame with hexagon ID and centroid coordinates.\"\"\"\n        self.logger.info(\n            f\"Converting {len(self.hexagons)} hexagons to pandas DataFrame.\"\n        )\n        if not self.hexagons:\n            self.logger.warning(\n                \"No hexagons to convert to DataFrame. Returning empty DataFrame.\"\n            )\n            return pd.DataFrame(columns=[\"hexagon\", \"latitude\", \"longitude\"])\n\n        centroids = [h3.cell_to_latlng(hex_id) for hex_id in self.hexagons]\n\n        self.logger.info(f\"Successfully converted to DataFrame.\")\n\n        return pd.DataFrame(\n            {\n                \"hexagon\": self.hexagons,\n                \"latitude\": [c[0] for c in centroids],\n                \"longitude\": [c[1] for c in centroids],\n            }\n        )\n\n    def to_geoms(self) -&gt; List[Polygon]:\n        \"\"\"Convert hexagons to shapely Polygon geometries.\"\"\"\n        self.logger.info(\n            f\"Converting {len(self.hexagons)} hexagons to shapely Polygon geometries.\"\n        )\n        return [shape(h3.cells_to_geo([hex_id])) for hex_id in self.hexagons]\n\n    def to_geodataframe(self) -&gt; gpd.GeoDataFrame:\n        \"\"\"Convert to GeoPandas GeoDataFrame.\"\"\"\n        return gpd.GeoDataFrame(\n            {\"h3\": self.hexagons, \"geometry\": self.to_geoms()}, crs=\"EPSG:4326\"\n        )\n\n    @staticmethod\n    def get_hexagons_from_points(\n        points: List[Union[Point, Tuple[float, float]]], resolution: int\n    ) -&gt; List[str]:\n        \"\"\"Get list of H3 hexagon IDs for the provided points at specified resolution.\n\n        Args:\n            points: List of points as either shapely Points or (lon, lat) tuples\n            resolution: H3 resolution level\n\n        Returns:\n            List of H3 hexagon ID strings\n        \"\"\"\n        hexagons = []\n        for p in points:\n            if isinstance(p, Point):\n                # Shapely Point has x=lon, y=lat\n                hex_id = h3.latlng_to_cell(p.y, p.x, resolution)\n            else:\n                # Assume tuple is (lon, lat) - convert to (lat, lon) for h3\n                hex_id = h3.latlng_to_cell(p[1], p[0], resolution)\n            hexagons.append(hex_id)\n        return hexagons\n\n    def get_neighbors(self, k: int = 1) -&gt; \"H3Hexagons\":\n        \"\"\"Get k-ring neighbors of all hexagons.\n\n        Args:\n            k: Distance of neighbors (1 for immediate neighbors, 2 for neighbors of neighbors, etc.)\n\n        Returns:\n            New H3Hexagons instance with neighbors included\n        \"\"\"\n        self.logger.info(\n            f\"Getting k-ring neighbors (k={k}) for {len(self.hexagons)} hexagons.\"\n        )\n\n        all_neighbors = set()\n        for hex_id in self.hexagons:\n            neighbors = h3.grid_ring(hex_id, k)\n            all_neighbors.update(neighbors)\n\n        self.logger.info(\n            f\"Found {len(all_neighbors)} total hexagons including neighbors.\"\n        )\n        return H3Hexagons(resolution=self.resolution, hexagons=list(all_neighbors))\n\n    def get_compact_representation(self) -&gt; \"H3Hexagons\":\n        \"\"\"Get compact representation by merging adjacent hexagons into parent cells where possible.\"\"\"\n        self.logger.info(f\"Compacting {len(self.hexagons)} hexagons.\")\n\n        # Convert to set for h3.compact\n        hex_set = set(self.hexagons)\n        compacted = h3.compact_cells(hex_set)\n\n        self.logger.info(f\"Compacted to {len(compacted)} hexagons.\")\n\n        # Note: compacted representation may have mixed resolutions\n        # We'll keep the original resolution as the \"target\" resolution\n        return H3Hexagons(resolution=self.resolution, hexagons=list(compacted))\n\n    def get_children(self, target_resolution: int) -&gt; \"H3Hexagons\":\n        \"\"\"Get children hexagons at higher resolution.\n\n        Args:\n            target_resolution: Target resolution (must be higher than current)\n\n        Returns:\n            New H3Hexagons instance with children at target resolution\n        \"\"\"\n        if target_resolution &lt;= self.resolution:\n            raise ValueError(\"Target resolution must be higher than current resolution\")\n\n        self.logger.info(\n            f\"Getting children at resolution {target_resolution} for {len(self.hexagons)} hexagons.\"\n        )\n\n        all_children = []\n        for hex_id in self.hexagons:\n            children = h3.cell_to_children(hex_id, target_resolution)\n            all_children.extend(children)\n\n        self.logger.info(f\"Generated {len(all_children)} children hexagons.\")\n        return H3Hexagons(resolution=target_resolution, hexagons=all_children)\n\n    def get_parents(self, target_resolution: int) -&gt; \"H3Hexagons\":\n        \"\"\"Get parent hexagons at lower resolution.\n\n        Args:\n            target_resolution: Target resolution (must be lower than current)\n\n        Returns:\n            New H3Hexagons instance with parents at target resolution\n        \"\"\"\n        if target_resolution &gt;= self.resolution:\n            raise ValueError(\"Target resolution must be lower than current resolution\")\n\n        self.logger.info(\n            f\"Getting parents at resolution {target_resolution} for {len(self.hexagons)} hexagons.\"\n        )\n\n        parents = set()\n        for hex_id in self.hexagons:\n            parent = h3.cell_to_parent(hex_id, target_resolution)\n            parents.add(parent)\n\n        self.logger.info(f\"Generated {len(parents)} parent hexagons.\")\n        return H3Hexagons(resolution=target_resolution, hexagons=list(parents))\n\n    def save(self, file: Union[str, Path], format: str = \"json\") -&gt; None:\n        \"\"\"Save H3Hexagons to file in specified format.\"\"\"\n        with self.data_store.open(str(file), \"wb\" if format == \"parquet\" else \"w\") as f:\n            if format == \"parquet\":\n                self.to_geodataframe().to_parquet(f, index=False)\n            elif format == \"geojson\":\n                f.write(self.to_geodataframe().to_json(drop_id=True))\n            elif format == \"json\":\n                json.dump(self.hexagons, f)\n            else:\n                raise ValueError(f\"Unsupported format: {format}\")\n\n    def __len__(self) -&gt; int:\n        return len(self.hexagons)\n</code></pre>"},{"location":"api/grid/#gigaspatial.grid.h3.H3Hexagons.filter_hexagons","title":"<code>filter_hexagons(hexagons)</code>","text":"<p>Filter hexagons by a given set of hexagon IDs.</p> Source code in <code>gigaspatial/grid/h3.py</code> <pre><code>def filter_hexagons(self, hexagons: Iterable[str]) -&gt; \"H3Hexagons\":\n    \"\"\"Filter hexagons by a given set of hexagon IDs.\"\"\"\n    original_count = len(self.hexagons)\n    incoming_count = len(\n        list(hexagons)\n    )  # Convert to list to get length if it's an iterator\n\n    self.logger.info(\n        f\"Filtering {original_count} hexagons with an incoming set of {incoming_count} hexagons.\"\n    )\n    filtered_hexagons = list(set(self.hexagons) &amp; set(hexagons))\n    self.logger.info(f\"Resulting in {len(filtered_hexagons)} filtered hexagons.\")\n    return H3Hexagons(\n        resolution=self.resolution,\n        hexagons=filtered_hexagons,\n    )\n</code></pre>"},{"location":"api/grid/#gigaspatial.grid.h3.H3Hexagons.from_bounds","title":"<code>from_bounds(xmin, ymin, xmax, ymax, resolution)</code>  <code>classmethod</code>","text":"<p>Create H3Hexagons from boundary coordinates.</p> Source code in <code>gigaspatial/grid/h3.py</code> <pre><code>@classmethod\ndef from_bounds(\n    cls, xmin: float, ymin: float, xmax: float, ymax: float, resolution: int\n):\n    \"\"\"Create H3Hexagons from boundary coordinates.\"\"\"\n    cls.logger.info(\n        f\"Creating H3Hexagons from bounds: ({xmin}, {ymin}, {xmax}, {ymax}) at resolution: {resolution}\"\n    )\n\n    # Create a LatLong bounding box polygon\n    latlong_bbox_coords = [\n        [ymin, xmin],\n        [ymax, xmin],\n        [ymax, xmax],\n        [ymin, xmax],\n        [ymin, xmin],\n    ]\n\n    # Get H3 cells that intersect with the bounding box\n    poly = h3.LatLngPoly(latlong_bbox_coords)\n    hexagons = h3.h3shape_to_cells(poly, res=resolution)\n\n    return cls(resolution=resolution, hexagons=list(hexagons))\n</code></pre>"},{"location":"api/grid/#gigaspatial.grid.h3.H3Hexagons.from_geometry","title":"<code>from_geometry(geometry, resolution, contain='overlap', **kwargs)</code>  <code>classmethod</code>","text":"<p>Create H3Hexagons from a geometry.</p> Source code in <code>gigaspatial/grid/h3.py</code> <pre><code>@classmethod\ndef from_geometry(\n    cls,\n    geometry: BaseGeometry,\n    resolution: int,\n    contain: Literal[\"center\", \"full\", \"overlap\", \"bbox_overlap\"] = \"overlap\",\n    **kwargs,\n):\n    \"\"\"Create H3Hexagons from a geometry.\"\"\"\n    cls.logger.info(\n        f\"Creating H3Hexagons from geometry (bounds: {geometry.bounds}) at resolution: {resolution} with predicate: {contain}\"\n    )\n\n    if isinstance(geometry, Point):\n        return cls.from_points([geometry])\n\n    # Convert shapely geometry to GeoJSON-like format\n    if hasattr(geometry, \"__geo_interface__\"):\n        geojson_geom = geometry.__geo_interface__\n    else:\n        # Fallback for complex geometries\n        import json\n        from shapely.geometry import mapping\n\n        geojson_geom = mapping(geometry)\n\n    h3_geom = h3.geo_to_h3shape(geojson_geom)\n\n    hexagons = h3.h3shape_to_cells_experimental(\n        h3_geom, resolution, contain=contain\n    )\n\n    cls.logger.info(\n        f\"Generated {len(hexagons)} hexagons using `{contain}` spatial predicate.\"\n    )\n    return cls(resolution=resolution, hexagons=list(hexagons), **kwargs)\n</code></pre>"},{"location":"api/grid/#gigaspatial.grid.h3.H3Hexagons.from_hexagons","title":"<code>from_hexagons(hexagons)</code>  <code>classmethod</code>","text":"<p>Create H3Hexagons from list of H3 cell IDs.</p> Source code in <code>gigaspatial/grid/h3.py</code> <pre><code>@classmethod\ndef from_hexagons(cls, hexagons: List[str]):\n    \"\"\"Create H3Hexagons from list of H3 cell IDs.\"\"\"\n    if not hexagons:\n        cls.logger.warning(\"No hexagons provided to from_hexagons.\")\n        return cls(resolution=0, hexagons=[])\n\n    cls.logger.info(\n        f\"Initializing H3Hexagons from {len(hexagons)} provided hexagons.\"\n    )\n    # Get resolution from first hexagon\n    resolution = h3.get_resolution(hexagons[0])\n    return cls(resolution=resolution, hexagons=list(set(hexagons)))\n</code></pre>"},{"location":"api/grid/#gigaspatial.grid.h3.H3Hexagons.from_json","title":"<code>from_json(data_store, file, **kwargs)</code>  <code>classmethod</code>","text":"<p>Load H3Hexagons from a JSON file.</p> Source code in <code>gigaspatial/grid/h3.py</code> <pre><code>@classmethod\ndef from_json(\n    cls, data_store: DataStore, file: Union[str, Path], **kwargs\n) -&gt; \"H3Hexagons\":\n    \"\"\"Load H3Hexagons from a JSON file.\"\"\"\n    cls.logger.info(\n        f\"Loading H3Hexagons from JSON file: {file} using data store: {type(data_store).__name__}\"\n    )\n    with data_store.open(str(file), \"r\") as f:\n        data = json.load(f)\n        if isinstance(data, list):  # If file contains only hexagon IDs\n            # Get resolution from first hexagon if available\n            resolution = h3.get_resolution(data[0]) if data else 0\n            data = {\n                \"resolution\": resolution,\n                \"hexagons\": data,\n                **kwargs,\n            }\n        else:\n            data.update(kwargs)\n        instance = cls(**data)\n        instance.data_store = data_store\n        cls.logger.info(\n            f\"Successfully loaded {len(instance.hexagons)} hexagons from JSON file.\"\n        )\n        return instance\n</code></pre>"},{"location":"api/grid/#gigaspatial.grid.h3.H3Hexagons.from_points","title":"<code>from_points(points, resolution, **kwargs)</code>  <code>classmethod</code>","text":"<p>Create H3Hexagons from a list of points or lat-lon pairs.</p> Source code in <code>gigaspatial/grid/h3.py</code> <pre><code>@classmethod\ndef from_points(\n    cls, points: List[Union[Point, Tuple[float, float]]], resolution: int, **kwargs\n) -&gt; \"H3Hexagons\":\n    \"\"\"Create H3Hexagons from a list of points or lat-lon pairs.\"\"\"\n    cls.logger.info(\n        f\"Creating H3Hexagons from {len(points)} points at resolution: {resolution}\"\n    )\n    hexagons = set(cls.get_hexagons_from_points(points, resolution))\n    cls.logger.info(f\"Generated {len(hexagons)} unique hexagons from points.\")\n    return cls(resolution=resolution, hexagons=list(hexagons), **kwargs)\n</code></pre>"},{"location":"api/grid/#gigaspatial.grid.h3.H3Hexagons.get_children","title":"<code>get_children(target_resolution)</code>","text":"<p>Get children hexagons at higher resolution.</p> <p>Parameters:</p> Name Type Description Default <code>target_resolution</code> <code>int</code> <p>Target resolution (must be higher than current)</p> required <p>Returns:</p> Type Description <code>H3Hexagons</code> <p>New H3Hexagons instance with children at target resolution</p> Source code in <code>gigaspatial/grid/h3.py</code> <pre><code>def get_children(self, target_resolution: int) -&gt; \"H3Hexagons\":\n    \"\"\"Get children hexagons at higher resolution.\n\n    Args:\n        target_resolution: Target resolution (must be higher than current)\n\n    Returns:\n        New H3Hexagons instance with children at target resolution\n    \"\"\"\n    if target_resolution &lt;= self.resolution:\n        raise ValueError(\"Target resolution must be higher than current resolution\")\n\n    self.logger.info(\n        f\"Getting children at resolution {target_resolution} for {len(self.hexagons)} hexagons.\"\n    )\n\n    all_children = []\n    for hex_id in self.hexagons:\n        children = h3.cell_to_children(hex_id, target_resolution)\n        all_children.extend(children)\n\n    self.logger.info(f\"Generated {len(all_children)} children hexagons.\")\n    return H3Hexagons(resolution=target_resolution, hexagons=all_children)\n</code></pre>"},{"location":"api/grid/#gigaspatial.grid.h3.H3Hexagons.get_compact_representation","title":"<code>get_compact_representation()</code>","text":"<p>Get compact representation by merging adjacent hexagons into parent cells where possible.</p> Source code in <code>gigaspatial/grid/h3.py</code> <pre><code>def get_compact_representation(self) -&gt; \"H3Hexagons\":\n    \"\"\"Get compact representation by merging adjacent hexagons into parent cells where possible.\"\"\"\n    self.logger.info(f\"Compacting {len(self.hexagons)} hexagons.\")\n\n    # Convert to set for h3.compact\n    hex_set = set(self.hexagons)\n    compacted = h3.compact_cells(hex_set)\n\n    self.logger.info(f\"Compacted to {len(compacted)} hexagons.\")\n\n    # Note: compacted representation may have mixed resolutions\n    # We'll keep the original resolution as the \"target\" resolution\n    return H3Hexagons(resolution=self.resolution, hexagons=list(compacted))\n</code></pre>"},{"location":"api/grid/#gigaspatial.grid.h3.H3Hexagons.get_hexagons_from_points","title":"<code>get_hexagons_from_points(points, resolution)</code>  <code>staticmethod</code>","text":"<p>Get list of H3 hexagon IDs for the provided points at specified resolution.</p> <p>Parameters:</p> Name Type Description Default <code>points</code> <code>List[Union[Point, Tuple[float, float]]]</code> <p>List of points as either shapely Points or (lon, lat) tuples</p> required <code>resolution</code> <code>int</code> <p>H3 resolution level</p> required <p>Returns:</p> Type Description <code>List[str]</code> <p>List of H3 hexagon ID strings</p> Source code in <code>gigaspatial/grid/h3.py</code> <pre><code>@staticmethod\ndef get_hexagons_from_points(\n    points: List[Union[Point, Tuple[float, float]]], resolution: int\n) -&gt; List[str]:\n    \"\"\"Get list of H3 hexagon IDs for the provided points at specified resolution.\n\n    Args:\n        points: List of points as either shapely Points or (lon, lat) tuples\n        resolution: H3 resolution level\n\n    Returns:\n        List of H3 hexagon ID strings\n    \"\"\"\n    hexagons = []\n    for p in points:\n        if isinstance(p, Point):\n            # Shapely Point has x=lon, y=lat\n            hex_id = h3.latlng_to_cell(p.y, p.x, resolution)\n        else:\n            # Assume tuple is (lon, lat) - convert to (lat, lon) for h3\n            hex_id = h3.latlng_to_cell(p[1], p[0], resolution)\n        hexagons.append(hex_id)\n    return hexagons\n</code></pre>"},{"location":"api/grid/#gigaspatial.grid.h3.H3Hexagons.get_neighbors","title":"<code>get_neighbors(k=1)</code>","text":"<p>Get k-ring neighbors of all hexagons.</p> <p>Parameters:</p> Name Type Description Default <code>k</code> <code>int</code> <p>Distance of neighbors (1 for immediate neighbors, 2 for neighbors of neighbors, etc.)</p> <code>1</code> <p>Returns:</p> Type Description <code>H3Hexagons</code> <p>New H3Hexagons instance with neighbors included</p> Source code in <code>gigaspatial/grid/h3.py</code> <pre><code>def get_neighbors(self, k: int = 1) -&gt; \"H3Hexagons\":\n    \"\"\"Get k-ring neighbors of all hexagons.\n\n    Args:\n        k: Distance of neighbors (1 for immediate neighbors, 2 for neighbors of neighbors, etc.)\n\n    Returns:\n        New H3Hexagons instance with neighbors included\n    \"\"\"\n    self.logger.info(\n        f\"Getting k-ring neighbors (k={k}) for {len(self.hexagons)} hexagons.\"\n    )\n\n    all_neighbors = set()\n    for hex_id in self.hexagons:\n        neighbors = h3.grid_ring(hex_id, k)\n        all_neighbors.update(neighbors)\n\n    self.logger.info(\n        f\"Found {len(all_neighbors)} total hexagons including neighbors.\"\n    )\n    return H3Hexagons(resolution=self.resolution, hexagons=list(all_neighbors))\n</code></pre>"},{"location":"api/grid/#gigaspatial.grid.h3.H3Hexagons.get_parents","title":"<code>get_parents(target_resolution)</code>","text":"<p>Get parent hexagons at lower resolution.</p> <p>Parameters:</p> Name Type Description Default <code>target_resolution</code> <code>int</code> <p>Target resolution (must be lower than current)</p> required <p>Returns:</p> Type Description <code>H3Hexagons</code> <p>New H3Hexagons instance with parents at target resolution</p> Source code in <code>gigaspatial/grid/h3.py</code> <pre><code>def get_parents(self, target_resolution: int) -&gt; \"H3Hexagons\":\n    \"\"\"Get parent hexagons at lower resolution.\n\n    Args:\n        target_resolution: Target resolution (must be lower than current)\n\n    Returns:\n        New H3Hexagons instance with parents at target resolution\n    \"\"\"\n    if target_resolution &gt;= self.resolution:\n        raise ValueError(\"Target resolution must be lower than current resolution\")\n\n    self.logger.info(\n        f\"Getting parents at resolution {target_resolution} for {len(self.hexagons)} hexagons.\"\n    )\n\n    parents = set()\n    for hex_id in self.hexagons:\n        parent = h3.cell_to_parent(hex_id, target_resolution)\n        parents.add(parent)\n\n    self.logger.info(f\"Generated {len(parents)} parent hexagons.\")\n    return H3Hexagons(resolution=target_resolution, hexagons=list(parents))\n</code></pre>"},{"location":"api/grid/#gigaspatial.grid.h3.H3Hexagons.save","title":"<code>save(file, format='json')</code>","text":"<p>Save H3Hexagons to file in specified format.</p> Source code in <code>gigaspatial/grid/h3.py</code> <pre><code>def save(self, file: Union[str, Path], format: str = \"json\") -&gt; None:\n    \"\"\"Save H3Hexagons to file in specified format.\"\"\"\n    with self.data_store.open(str(file), \"wb\" if format == \"parquet\" else \"w\") as f:\n        if format == \"parquet\":\n            self.to_geodataframe().to_parquet(f, index=False)\n        elif format == \"geojson\":\n            f.write(self.to_geodataframe().to_json(drop_id=True))\n        elif format == \"json\":\n            json.dump(self.hexagons, f)\n        else:\n            raise ValueError(f\"Unsupported format: {format}\")\n</code></pre>"},{"location":"api/grid/#gigaspatial.grid.h3.H3Hexagons.to_dataframe","title":"<code>to_dataframe()</code>","text":"<p>Convert to pandas DataFrame with hexagon ID and centroid coordinates.</p> Source code in <code>gigaspatial/grid/h3.py</code> <pre><code>def to_dataframe(self) -&gt; pd.DataFrame:\n    \"\"\"Convert to pandas DataFrame with hexagon ID and centroid coordinates.\"\"\"\n    self.logger.info(\n        f\"Converting {len(self.hexagons)} hexagons to pandas DataFrame.\"\n    )\n    if not self.hexagons:\n        self.logger.warning(\n            \"No hexagons to convert to DataFrame. Returning empty DataFrame.\"\n        )\n        return pd.DataFrame(columns=[\"hexagon\", \"latitude\", \"longitude\"])\n\n    centroids = [h3.cell_to_latlng(hex_id) for hex_id in self.hexagons]\n\n    self.logger.info(f\"Successfully converted to DataFrame.\")\n\n    return pd.DataFrame(\n        {\n            \"hexagon\": self.hexagons,\n            \"latitude\": [c[0] for c in centroids],\n            \"longitude\": [c[1] for c in centroids],\n        }\n    )\n</code></pre>"},{"location":"api/grid/#gigaspatial.grid.h3.H3Hexagons.to_geodataframe","title":"<code>to_geodataframe()</code>","text":"<p>Convert to GeoPandas GeoDataFrame.</p> Source code in <code>gigaspatial/grid/h3.py</code> <pre><code>def to_geodataframe(self) -&gt; gpd.GeoDataFrame:\n    \"\"\"Convert to GeoPandas GeoDataFrame.\"\"\"\n    return gpd.GeoDataFrame(\n        {\"h3\": self.hexagons, \"geometry\": self.to_geoms()}, crs=\"EPSG:4326\"\n    )\n</code></pre>"},{"location":"api/grid/#gigaspatial.grid.h3.H3Hexagons.to_geoms","title":"<code>to_geoms()</code>","text":"<p>Convert hexagons to shapely Polygon geometries.</p> Source code in <code>gigaspatial/grid/h3.py</code> <pre><code>def to_geoms(self) -&gt; List[Polygon]:\n    \"\"\"Convert hexagons to shapely Polygon geometries.\"\"\"\n    self.logger.info(\n        f\"Converting {len(self.hexagons)} hexagons to shapely Polygon geometries.\"\n    )\n    return [shape(h3.cells_to_geo([hex_id])) for hex_id in self.hexagons]\n</code></pre>"},{"location":"api/grid/#gigaspatial.grid.mercator_tiles","title":"<code>mercator_tiles</code>","text":""},{"location":"api/grid/#gigaspatial.grid.mercator_tiles.CountryMercatorTiles","title":"<code>CountryMercatorTiles</code>","text":"<p>               Bases: <code>MercatorTiles</code></p> <p>MercatorTiles specialized for country-level operations.</p> <p>This class extends MercatorTiles to work specifically with country boundaries. It can only be instantiated through the create() classmethod.</p> Source code in <code>gigaspatial/grid/mercator_tiles.py</code> <pre><code>class CountryMercatorTiles(MercatorTiles):\n    \"\"\"MercatorTiles specialized for country-level operations.\n\n    This class extends MercatorTiles to work specifically with country boundaries.\n    It can only be instantiated through the create() classmethod.\n    \"\"\"\n\n    country: str = Field(..., exclude=True)\n\n    def __init__(self, *args, **kwargs):\n        raise TypeError(\n            \"CountryMercatorTiles cannot be instantiated directly. \"\n            \"Use CountryMercatorTiles.create() instead.\"\n        )\n\n    @classmethod\n    def create(\n        cls,\n        country: str,\n        zoom_level: int,\n        predicate: str = \"intersects\",\n        data_store: Optional[DataStore] = None,\n        country_geom_path: Optional[Union[str, Path]] = None,\n    ):\n        \"\"\"Create CountryMercatorTiles for a specific country.\"\"\"\n        from gigaspatial.handlers.boundaries import AdminBoundaries\n\n        instance = super().__new__(cls)\n        super(CountryMercatorTiles, instance).__init__(\n            zoom_level=zoom_level,\n            quadkeys=[],\n            data_store=data_store or LocalDataStore(),\n            country=pycountry.countries.lookup(country).alpha_3,\n        )\n\n        cls.logger.info(\n            f\"Initializing Mercator zones for country: {country} at zoom level {zoom_level}\"\n        )\n\n        country_geom = (\n            AdminBoundaries.create(\n                country_code=country,\n                data_store=data_store,\n                path=country_geom_path,\n            )\n            .boundaries[0]\n            .geometry\n        )\n\n        tiles = MercatorTiles.from_geometry(country_geom, zoom_level, predicate)\n\n        instance.quadkeys = tiles.quadkeys\n        return instance\n</code></pre>"},{"location":"api/grid/#gigaspatial.grid.mercator_tiles.CountryMercatorTiles.create","title":"<code>create(country, zoom_level, predicate='intersects', data_store=None, country_geom_path=None)</code>  <code>classmethod</code>","text":"<p>Create CountryMercatorTiles for a specific country.</p> Source code in <code>gigaspatial/grid/mercator_tiles.py</code> <pre><code>@classmethod\ndef create(\n    cls,\n    country: str,\n    zoom_level: int,\n    predicate: str = \"intersects\",\n    data_store: Optional[DataStore] = None,\n    country_geom_path: Optional[Union[str, Path]] = None,\n):\n    \"\"\"Create CountryMercatorTiles for a specific country.\"\"\"\n    from gigaspatial.handlers.boundaries import AdminBoundaries\n\n    instance = super().__new__(cls)\n    super(CountryMercatorTiles, instance).__init__(\n        zoom_level=zoom_level,\n        quadkeys=[],\n        data_store=data_store or LocalDataStore(),\n        country=pycountry.countries.lookup(country).alpha_3,\n    )\n\n    cls.logger.info(\n        f\"Initializing Mercator zones for country: {country} at zoom level {zoom_level}\"\n    )\n\n    country_geom = (\n        AdminBoundaries.create(\n            country_code=country,\n            data_store=data_store,\n            path=country_geom_path,\n        )\n        .boundaries[0]\n        .geometry\n    )\n\n    tiles = MercatorTiles.from_geometry(country_geom, zoom_level, predicate)\n\n    instance.quadkeys = tiles.quadkeys\n    return instance\n</code></pre>"},{"location":"api/grid/#gigaspatial.grid.mercator_tiles.MercatorTiles","title":"<code>MercatorTiles</code>","text":"<p>               Bases: <code>BaseModel</code></p> Source code in <code>gigaspatial/grid/mercator_tiles.py</code> <pre><code>class MercatorTiles(BaseModel):\n    zoom_level: int = Field(..., ge=0, le=20)\n    quadkeys: List[str] = Field(default_factory=list)\n    data_store: DataStore = Field(default_factory=LocalDataStore, exclude=True)\n    logger: ClassVar = config.get_logger(\"MercatorTiles\")\n\n    class Config:\n        arbitrary_types_allowed = True\n\n    @classmethod\n    def from_quadkeys(cls, quadkeys: List[str]):\n        \"\"\"Create MercatorTiles from list of quadkeys.\"\"\"\n        if not quadkeys:\n            cls.logger.warning(\"No quadkeys provided to from_quadkeys.\")\n            return cls(zoom_level=0, quadkeys=[])\n        cls.logger.info(\n            f\"Initializing MercatorTiles from {len(quadkeys)} provided quadkeys.\"\n        )\n        return cls(zoom_level=len(quadkeys[0]), quadkeys=set(quadkeys))\n\n    @classmethod\n    def from_bounds(\n        cls, xmin: float, ymin: float, xmax: float, ymax: float, zoom_level: int\n    ):\n        \"\"\"Create MercatorTiles from boundary coordinates.\"\"\"\n        cls.logger.info(\n            f\"Creating MercatorTiles from bounds: ({xmin}, {ymin}, {xmax}, {ymax}) at zoom level: {zoom_level}\"\n        )\n        return cls(\n            zoom_level=zoom_level,\n            quadkeys=[\n                mercantile.quadkey(tile)\n                for tile in mercantile.tiles(xmin, ymin, xmax, ymax, zoom_level)\n            ],\n        )\n\n    @classmethod\n    def from_spatial(\n        cls,\n        source: Union[\n            BaseGeometry,\n            gpd.GeoDataFrame,\n            List[Union[Point, Tuple[float, float]]],  # points\n        ],\n        zoom_level: int,\n        predicate: str = \"intersects\",\n        **kwargs,\n    ):\n        cls.logger.info(\n            f\"Creating MercatorTiles from spatial source (type: {type(source)}) at zoom level: {zoom_level} with predicate: {predicate}\"\n        )\n        if isinstance(source, gpd.GeoDataFrame):\n            if source.crs != \"EPSG:4326\":\n                source = source.to_crs(\"EPSG:4326\")\n            source = source.geometry.unary_union\n\n        if isinstance(source, BaseGeometry):\n            return cls.from_geometry(\n                geometry=source, zoom_level=zoom_level, predicate=predicate, **kwargs\n            )\n        elif isinstance(source, Iterable) and all(\n            isinstance(pt, Point) or len(pt) == 2 for pt in source\n        ):\n            return cls.from_points(geometry=source, zoom_level=zoom_level, **kwargs)\n        else:\n            raise\n\n    @classmethod\n    def from_geometry(\n        cls,\n        geometry: BaseGeometry,\n        zoom_level: int,\n        predicate: str = \"intersects\",\n        **kwargs,\n    ):\n        \"\"\"Create MercatorTiles from a polygon.\"\"\"\n        cls.logger.info(\n            f\"Creating MercatorTiles from geometry (bounds: {geometry.bounds}) at zoom level: {zoom_level} with predicate: {predicate}\"\n        )\n        tiles = list(mercantile.tiles(*geometry.bounds, zoom_level))\n        quadkeys_boxes = [\n            (mercantile.quadkey(t), box(*mercantile.bounds(t))) for t in tiles\n        ]\n        quadkeys, boxes = zip(*quadkeys_boxes) if quadkeys_boxes else ([], [])\n\n        if not boxes:\n            cls.logger.warning(\n                \"No boxes generated from geometry bounds. Returning empty MercatorTiles.\"\n            )\n            return MercatorTiles(zoom_level=zoom_level, quadkeys=[])\n\n        s = STRtree(boxes)\n        result_indices = s.query(geometry, predicate=predicate)\n        filtered_quadkeys = [quadkeys[i] for i in result_indices]\n        cls.logger.info(\n            f\"Filtered down to {len(filtered_quadkeys)} quadkeys using spatial predicate.\"\n        )\n        return cls(zoom_level=zoom_level, quadkeys=filtered_quadkeys, **kwargs)\n\n    @classmethod\n    def from_points(\n        cls, points: List[Union[Point, Tuple[float, float]]], zoom_level: int, **kwargs\n    ) -&gt; \"MercatorTiles\":\n        \"\"\"Create MercatorTiles from a list of points or lat-lon pairs.\"\"\"\n        cls.logger.info(\n            f\"Creating MercatorTiles from {len(points)} points at zoom level: {zoom_level}\"\n        )\n        quadkeys = set(cls.get_quadkeys_from_points(points, zoom_level))\n        cls.logger.info(f\"Generated {len(quadkeys)} unique quadkeys from points.\")\n        return cls(zoom_level=zoom_level, quadkeys=list(quadkeys), **kwargs)\n\n    @classmethod\n    def from_json(\n        cls, data_store: DataStore, file: Union[str, Path], **kwargs\n    ) -&gt; \"MercatorTiles\":\n        \"\"\"Load MercatorTiles from a JSON file.\"\"\"\n        cls.logger.info(\n            f\"Loading MercatorTiles from JSON file: {file} using data store: {type(data_store).__name__}\"\n        )\n        with data_store.open(str(file), \"r\") as f:\n            data = json.load(f)\n            if isinstance(data, list):  # If file contains only quadkeys\n                data = {\n                    \"zoom_level\": len(data[0]) if data else 0,\n                    \"quadkeys\": data,\n                    **kwargs,\n                }\n            else:\n                data.update(kwargs)\n            instance = cls(**data)\n            instance.data_store = data_store\n            cls.logger.info(\n                f\"Successfully loaded {len(instance.quadkeys)} quadkeys from JSON file.\"\n            )\n            return instance\n\n    def filter_quadkeys(self, quadkeys: Iterable[str]) -&gt; \"MercatorTiles\":\n        \"\"\"Filter quadkeys by a given set of quadkeys.\"\"\"\n        original_count = len(self.quadkeys)\n        incoming_count = len(\n            list(quadkeys)\n        )  # Convert to list to get length if it's an iterator\n\n        self.logger.info(\n            f\"Filtering {original_count} quadkeys with an incoming set of {incoming_count} quadkeys.\"\n        )\n        filtered_quadkeys = list(set(self.quadkeys) &amp; set(quadkeys))\n        self.logger.info(f\"Resulting in {len(filtered_quadkeys)} filtered quadkeys.\")\n        return MercatorTiles(\n            zoom_level=self.zoom_level,\n            quadkeys=filtered_quadkeys,\n        )\n\n    def to_dataframe(self) -&gt; pd.DataFrame:\n        \"\"\"Convert to pandas DataFrame with quadkey and centroid coordinates.\"\"\"\n        self.logger.info(\n            f\"Converting {len(self.quadkeys)} quadkeys to pandas DataFrame.\"\n        )\n        if not self.quadkeys:\n            self.logger.warning(\n                \"No quadkeys to convert to DataFrame. Returning empty DataFrame.\"\n            )\n            return pd.DataFrame(columns=[\"quadkey\", \"latitude\", \"longitude\"])\n        tiles_data = [mercantile.quadkey_to_tile(q) for q in self.quadkeys]\n        bounds_data = [mercantile.bounds(tile) for tile in tiles_data]\n\n        centroids = [\n            (\n                (bounds.south + bounds.north) / 2,  # latitude\n                (bounds.west + bounds.east) / 2,  # longitude\n            )\n            for bounds in bounds_data\n        ]\n\n        self.logger.info(f\"Successfully converted to DataFrame.\")\n\n        return pd.DataFrame(\n            {\n                \"quadkey\": self.quadkeys,\n                \"latitude\": [c[0] for c in centroids],\n                \"longitude\": [c[1] for c in centroids],\n            }\n        )\n\n    def to_geoms(self) -&gt; List[box]:\n        self.logger.info(\n            f\"Converting {len(self.quadkeys)} quadkeys to shapely box geometries.\"\n        )\n        return [\n            box(*mercantile.bounds(mercantile.quadkey_to_tile(q)))\n            for q in self.quadkeys\n        ]\n\n    def to_geodataframe(self) -&gt; gpd.GeoDataFrame:\n        \"\"\"Convert to GeoPandas GeoDataFrame.\"\"\"\n        return gpd.GeoDataFrame(\n            {\"quadkey\": self.quadkeys, \"geometry\": self.to_geoms()}, crs=\"EPSG:4326\"\n        )\n\n    @staticmethod\n    def get_quadkeys_from_points(\n        points: List[Union[Point, Tuple[float, float]]], zoom_level: int\n    ) -&gt; List[str]:\n        \"\"\"Get list of quadkeys for the provided points at specified zoom level.\n\n        Args:\n            points: List of points as either shapely Points or (lon, lat) tuples\n            zoom_level: Zoom level for the quadkeys\n\n        Returns:\n            List of quadkey strings\n        \"\"\"\n        quadkeys = [\n            (\n                mercantile.quadkey(mercantile.tile(p.x, p.y, zoom_level))\n                if isinstance(p, Point)\n                else mercantile.quadkey(mercantile.tile(p[1], p[0], zoom_level))\n            )\n            for p in points\n        ]\n        return quadkeys\n\n    def save(self, file: Union[str, Path], format: str = \"json\") -&gt; None:\n        \"\"\"Save MercatorTiles to file in specified format.\"\"\"\n        with self.data_store.open(str(file), \"wb\" if format == \"parquet\" else \"w\") as f:\n            if format == \"parquet\":\n                self.to_geodataframe().to_parquet(f, index=False)\n            elif format == \"geojson\":\n                f.write(self.to_geodataframe().to_json(drop_id=True))\n            elif format == \"json\":\n                json.dump(self.quadkeys, f)\n            else:\n                raise ValueError(f\"Unsupported format: {format}\")\n\n    def __len__(self) -&gt; int:\n        return len(self.quadkeys)\n</code></pre>"},{"location":"api/grid/#gigaspatial.grid.mercator_tiles.MercatorTiles.filter_quadkeys","title":"<code>filter_quadkeys(quadkeys)</code>","text":"<p>Filter quadkeys by a given set of quadkeys.</p> Source code in <code>gigaspatial/grid/mercator_tiles.py</code> <pre><code>def filter_quadkeys(self, quadkeys: Iterable[str]) -&gt; \"MercatorTiles\":\n    \"\"\"Filter quadkeys by a given set of quadkeys.\"\"\"\n    original_count = len(self.quadkeys)\n    incoming_count = len(\n        list(quadkeys)\n    )  # Convert to list to get length if it's an iterator\n\n    self.logger.info(\n        f\"Filtering {original_count} quadkeys with an incoming set of {incoming_count} quadkeys.\"\n    )\n    filtered_quadkeys = list(set(self.quadkeys) &amp; set(quadkeys))\n    self.logger.info(f\"Resulting in {len(filtered_quadkeys)} filtered quadkeys.\")\n    return MercatorTiles(\n        zoom_level=self.zoom_level,\n        quadkeys=filtered_quadkeys,\n    )\n</code></pre>"},{"location":"api/grid/#gigaspatial.grid.mercator_tiles.MercatorTiles.from_bounds","title":"<code>from_bounds(xmin, ymin, xmax, ymax, zoom_level)</code>  <code>classmethod</code>","text":"<p>Create MercatorTiles from boundary coordinates.</p> Source code in <code>gigaspatial/grid/mercator_tiles.py</code> <pre><code>@classmethod\ndef from_bounds(\n    cls, xmin: float, ymin: float, xmax: float, ymax: float, zoom_level: int\n):\n    \"\"\"Create MercatorTiles from boundary coordinates.\"\"\"\n    cls.logger.info(\n        f\"Creating MercatorTiles from bounds: ({xmin}, {ymin}, {xmax}, {ymax}) at zoom level: {zoom_level}\"\n    )\n    return cls(\n        zoom_level=zoom_level,\n        quadkeys=[\n            mercantile.quadkey(tile)\n            for tile in mercantile.tiles(xmin, ymin, xmax, ymax, zoom_level)\n        ],\n    )\n</code></pre>"},{"location":"api/grid/#gigaspatial.grid.mercator_tiles.MercatorTiles.from_geometry","title":"<code>from_geometry(geometry, zoom_level, predicate='intersects', **kwargs)</code>  <code>classmethod</code>","text":"<p>Create MercatorTiles from a polygon.</p> Source code in <code>gigaspatial/grid/mercator_tiles.py</code> <pre><code>@classmethod\ndef from_geometry(\n    cls,\n    geometry: BaseGeometry,\n    zoom_level: int,\n    predicate: str = \"intersects\",\n    **kwargs,\n):\n    \"\"\"Create MercatorTiles from a polygon.\"\"\"\n    cls.logger.info(\n        f\"Creating MercatorTiles from geometry (bounds: {geometry.bounds}) at zoom level: {zoom_level} with predicate: {predicate}\"\n    )\n    tiles = list(mercantile.tiles(*geometry.bounds, zoom_level))\n    quadkeys_boxes = [\n        (mercantile.quadkey(t), box(*mercantile.bounds(t))) for t in tiles\n    ]\n    quadkeys, boxes = zip(*quadkeys_boxes) if quadkeys_boxes else ([], [])\n\n    if not boxes:\n        cls.logger.warning(\n            \"No boxes generated from geometry bounds. Returning empty MercatorTiles.\"\n        )\n        return MercatorTiles(zoom_level=zoom_level, quadkeys=[])\n\n    s = STRtree(boxes)\n    result_indices = s.query(geometry, predicate=predicate)\n    filtered_quadkeys = [quadkeys[i] for i in result_indices]\n    cls.logger.info(\n        f\"Filtered down to {len(filtered_quadkeys)} quadkeys using spatial predicate.\"\n    )\n    return cls(zoom_level=zoom_level, quadkeys=filtered_quadkeys, **kwargs)\n</code></pre>"},{"location":"api/grid/#gigaspatial.grid.mercator_tiles.MercatorTiles.from_json","title":"<code>from_json(data_store, file, **kwargs)</code>  <code>classmethod</code>","text":"<p>Load MercatorTiles from a JSON file.</p> Source code in <code>gigaspatial/grid/mercator_tiles.py</code> <pre><code>@classmethod\ndef from_json(\n    cls, data_store: DataStore, file: Union[str, Path], **kwargs\n) -&gt; \"MercatorTiles\":\n    \"\"\"Load MercatorTiles from a JSON file.\"\"\"\n    cls.logger.info(\n        f\"Loading MercatorTiles from JSON file: {file} using data store: {type(data_store).__name__}\"\n    )\n    with data_store.open(str(file), \"r\") as f:\n        data = json.load(f)\n        if isinstance(data, list):  # If file contains only quadkeys\n            data = {\n                \"zoom_level\": len(data[0]) if data else 0,\n                \"quadkeys\": data,\n                **kwargs,\n            }\n        else:\n            data.update(kwargs)\n        instance = cls(**data)\n        instance.data_store = data_store\n        cls.logger.info(\n            f\"Successfully loaded {len(instance.quadkeys)} quadkeys from JSON file.\"\n        )\n        return instance\n</code></pre>"},{"location":"api/grid/#gigaspatial.grid.mercator_tiles.MercatorTiles.from_points","title":"<code>from_points(points, zoom_level, **kwargs)</code>  <code>classmethod</code>","text":"<p>Create MercatorTiles from a list of points or lat-lon pairs.</p> Source code in <code>gigaspatial/grid/mercator_tiles.py</code> <pre><code>@classmethod\ndef from_points(\n    cls, points: List[Union[Point, Tuple[float, float]]], zoom_level: int, **kwargs\n) -&gt; \"MercatorTiles\":\n    \"\"\"Create MercatorTiles from a list of points or lat-lon pairs.\"\"\"\n    cls.logger.info(\n        f\"Creating MercatorTiles from {len(points)} points at zoom level: {zoom_level}\"\n    )\n    quadkeys = set(cls.get_quadkeys_from_points(points, zoom_level))\n    cls.logger.info(f\"Generated {len(quadkeys)} unique quadkeys from points.\")\n    return cls(zoom_level=zoom_level, quadkeys=list(quadkeys), **kwargs)\n</code></pre>"},{"location":"api/grid/#gigaspatial.grid.mercator_tiles.MercatorTiles.from_quadkeys","title":"<code>from_quadkeys(quadkeys)</code>  <code>classmethod</code>","text":"<p>Create MercatorTiles from list of quadkeys.</p> Source code in <code>gigaspatial/grid/mercator_tiles.py</code> <pre><code>@classmethod\ndef from_quadkeys(cls, quadkeys: List[str]):\n    \"\"\"Create MercatorTiles from list of quadkeys.\"\"\"\n    if not quadkeys:\n        cls.logger.warning(\"No quadkeys provided to from_quadkeys.\")\n        return cls(zoom_level=0, quadkeys=[])\n    cls.logger.info(\n        f\"Initializing MercatorTiles from {len(quadkeys)} provided quadkeys.\"\n    )\n    return cls(zoom_level=len(quadkeys[0]), quadkeys=set(quadkeys))\n</code></pre>"},{"location":"api/grid/#gigaspatial.grid.mercator_tiles.MercatorTiles.get_quadkeys_from_points","title":"<code>get_quadkeys_from_points(points, zoom_level)</code>  <code>staticmethod</code>","text":"<p>Get list of quadkeys for the provided points at specified zoom level.</p> <p>Parameters:</p> Name Type Description Default <code>points</code> <code>List[Union[Point, Tuple[float, float]]]</code> <p>List of points as either shapely Points or (lon, lat) tuples</p> required <code>zoom_level</code> <code>int</code> <p>Zoom level for the quadkeys</p> required <p>Returns:</p> Type Description <code>List[str]</code> <p>List of quadkey strings</p> Source code in <code>gigaspatial/grid/mercator_tiles.py</code> <pre><code>@staticmethod\ndef get_quadkeys_from_points(\n    points: List[Union[Point, Tuple[float, float]]], zoom_level: int\n) -&gt; List[str]:\n    \"\"\"Get list of quadkeys for the provided points at specified zoom level.\n\n    Args:\n        points: List of points as either shapely Points or (lon, lat) tuples\n        zoom_level: Zoom level for the quadkeys\n\n    Returns:\n        List of quadkey strings\n    \"\"\"\n    quadkeys = [\n        (\n            mercantile.quadkey(mercantile.tile(p.x, p.y, zoom_level))\n            if isinstance(p, Point)\n            else mercantile.quadkey(mercantile.tile(p[1], p[0], zoom_level))\n        )\n        for p in points\n    ]\n    return quadkeys\n</code></pre>"},{"location":"api/grid/#gigaspatial.grid.mercator_tiles.MercatorTiles.save","title":"<code>save(file, format='json')</code>","text":"<p>Save MercatorTiles to file in specified format.</p> Source code in <code>gigaspatial/grid/mercator_tiles.py</code> <pre><code>def save(self, file: Union[str, Path], format: str = \"json\") -&gt; None:\n    \"\"\"Save MercatorTiles to file in specified format.\"\"\"\n    with self.data_store.open(str(file), \"wb\" if format == \"parquet\" else \"w\") as f:\n        if format == \"parquet\":\n            self.to_geodataframe().to_parquet(f, index=False)\n        elif format == \"geojson\":\n            f.write(self.to_geodataframe().to_json(drop_id=True))\n        elif format == \"json\":\n            json.dump(self.quadkeys, f)\n        else:\n            raise ValueError(f\"Unsupported format: {format}\")\n</code></pre>"},{"location":"api/grid/#gigaspatial.grid.mercator_tiles.MercatorTiles.to_dataframe","title":"<code>to_dataframe()</code>","text":"<p>Convert to pandas DataFrame with quadkey and centroid coordinates.</p> Source code in <code>gigaspatial/grid/mercator_tiles.py</code> <pre><code>def to_dataframe(self) -&gt; pd.DataFrame:\n    \"\"\"Convert to pandas DataFrame with quadkey and centroid coordinates.\"\"\"\n    self.logger.info(\n        f\"Converting {len(self.quadkeys)} quadkeys to pandas DataFrame.\"\n    )\n    if not self.quadkeys:\n        self.logger.warning(\n            \"No quadkeys to convert to DataFrame. Returning empty DataFrame.\"\n        )\n        return pd.DataFrame(columns=[\"quadkey\", \"latitude\", \"longitude\"])\n    tiles_data = [mercantile.quadkey_to_tile(q) for q in self.quadkeys]\n    bounds_data = [mercantile.bounds(tile) for tile in tiles_data]\n\n    centroids = [\n        (\n            (bounds.south + bounds.north) / 2,  # latitude\n            (bounds.west + bounds.east) / 2,  # longitude\n        )\n        for bounds in bounds_data\n    ]\n\n    self.logger.info(f\"Successfully converted to DataFrame.\")\n\n    return pd.DataFrame(\n        {\n            \"quadkey\": self.quadkeys,\n            \"latitude\": [c[0] for c in centroids],\n            \"longitude\": [c[1] for c in centroids],\n        }\n    )\n</code></pre>"},{"location":"api/grid/#gigaspatial.grid.mercator_tiles.MercatorTiles.to_geodataframe","title":"<code>to_geodataframe()</code>","text":"<p>Convert to GeoPandas GeoDataFrame.</p> Source code in <code>gigaspatial/grid/mercator_tiles.py</code> <pre><code>def to_geodataframe(self) -&gt; gpd.GeoDataFrame:\n    \"\"\"Convert to GeoPandas GeoDataFrame.\"\"\"\n    return gpd.GeoDataFrame(\n        {\"quadkey\": self.quadkeys, \"geometry\": self.to_geoms()}, crs=\"EPSG:4326\"\n    )\n</code></pre>"},{"location":"api/grid/#gigaspatial.grid.s2","title":"<code>s2</code>","text":""},{"location":"api/grid/#gigaspatial.grid.s2.CountryS2Cells","title":"<code>CountryS2Cells</code>","text":"<p>               Bases: <code>S2Cells</code></p> <p>S2Cells specialized for country-level operations.</p> <p>This class extends S2Cells to work specifically with country boundaries. It can only be instantiated through the create() classmethod.</p> Source code in <code>gigaspatial/grid/s2.py</code> <pre><code>class CountryS2Cells(S2Cells):\n    \"\"\"S2Cells specialized for country-level operations.\n\n    This class extends S2Cells to work specifically with country boundaries.\n    It can only be instantiated through the create() classmethod.\n    \"\"\"\n\n    country: str = Field(..., exclude=True)\n\n    def __init__(self, *args, **kwargs):\n        raise TypeError(\n            \"CountryS2Cells cannot be instantiated directly. \"\n            \"Use CountryS2Cells.create() instead.\"\n        )\n\n    @classmethod\n    def create(\n        cls,\n        country: str,\n        level: int,\n        max_cells: int = 1000,\n        data_store: Optional[DataStore] = None,\n        country_geom_path: Optional[Union[str, Path]] = None,\n    ):\n        \"\"\"Create CountryS2Cells for a specific country.\"\"\"\n        from gigaspatial.handlers.boundaries import AdminBoundaries\n\n        instance = super().__new__(cls)\n        super(CountryS2Cells, instance).__init__(\n            level=level,\n            cells=[],\n            data_store=data_store or LocalDataStore(),\n            country=pycountry.countries.lookup(country).alpha_3,\n        )\n\n        cls.logger.info(\n            f\"Initializing S2 cells for country: {country} at level {level}\"\n        )\n\n        country_geom = (\n            AdminBoundaries.create(\n                country_code=country,\n                data_store=data_store,\n                path=country_geom_path,\n            )\n            .boundaries[0]\n            .geometry\n        )\n\n        cells = S2Cells.from_geometry(country_geom, level, max_cells=max_cells)\n        instance.cells = cells.cells\n\n        return instance\n</code></pre>"},{"location":"api/grid/#gigaspatial.grid.s2.CountryS2Cells.create","title":"<code>create(country, level, max_cells=1000, data_store=None, country_geom_path=None)</code>  <code>classmethod</code>","text":"<p>Create CountryS2Cells for a specific country.</p> Source code in <code>gigaspatial/grid/s2.py</code> <pre><code>@classmethod\ndef create(\n    cls,\n    country: str,\n    level: int,\n    max_cells: int = 1000,\n    data_store: Optional[DataStore] = None,\n    country_geom_path: Optional[Union[str, Path]] = None,\n):\n    \"\"\"Create CountryS2Cells for a specific country.\"\"\"\n    from gigaspatial.handlers.boundaries import AdminBoundaries\n\n    instance = super().__new__(cls)\n    super(CountryS2Cells, instance).__init__(\n        level=level,\n        cells=[],\n        data_store=data_store or LocalDataStore(),\n        country=pycountry.countries.lookup(country).alpha_3,\n    )\n\n    cls.logger.info(\n        f\"Initializing S2 cells for country: {country} at level {level}\"\n    )\n\n    country_geom = (\n        AdminBoundaries.create(\n            country_code=country,\n            data_store=data_store,\n            path=country_geom_path,\n        )\n        .boundaries[0]\n        .geometry\n    )\n\n    cells = S2Cells.from_geometry(country_geom, level, max_cells=max_cells)\n    instance.cells = cells.cells\n\n    return instance\n</code></pre>"},{"location":"api/grid/#gigaspatial.grid.s2.S2Cells","title":"<code>S2Cells</code>","text":"<p>               Bases: <code>BaseModel</code></p> <p>S2Cells class for generating and managing Google S2 cell grids.</p> <p>S2 uses levels 0-30, where higher levels represent finer resolution. Level 0 covers the largest area and level 30 the smallest.</p> Source code in <code>gigaspatial/grid/s2.py</code> <pre><code>class S2Cells(BaseModel):\n    \"\"\"S2Cells class for generating and managing Google S2 cell grids.\n\n    S2 uses levels 0-30, where higher levels represent finer resolution.\n    Level 0 covers the largest area and level 30 the smallest.\n    \"\"\"\n\n    level: int = Field(..., ge=0, le=30)\n    cells: List[int] = Field(default_factory=list)  # S2 cell IDs as integers\n    data_store: DataStore = Field(default_factory=LocalDataStore, exclude=True)\n    logger: ClassVar = config.get_logger(\"S2Cells\")\n\n    class Config:\n        arbitrary_types_allowed = True\n\n    @classmethod\n    def from_cells(cls, cells: List[Union[int, str]]):\n        \"\"\"Create S2Cells from list of S2 cell IDs (integers or tokens).\"\"\"\n        if not cells:\n            cls.logger.warning(\"No cells provided to from_cells.\")\n            return cls(level=0, cells=[])\n\n        cls.logger.info(f\"Initializing S2Cells from {len(cells)} provided cells.\")\n\n        # Convert tokens to integers if needed\n        cell_ids = []\n        for cell in cells:\n            if isinstance(cell, str):\n                cell_ids.append(CellId.from_token(cell).id())\n            else:\n                cell_ids.append(cell)\n\n        # Get level from first cell\n        level = CellId(cell_ids[0]).level()\n        return cls(level=level, cells=list(set(cell_ids)))\n\n    @classmethod\n    def from_bounds(\n        cls,\n        xmin: float,\n        ymin: float,\n        xmax: float,\n        ymax: float,\n        level: int,\n        max_cells: int = 100,\n    ):\n        \"\"\"Create S2Cells from boundary coordinates.\n\n        Args:\n            xmin, ymin, xmax, ymax: Bounding box coordinates in degrees\n            level: S2 level (0-30)\n            max_cells: Maximum number of cells to generate\n        \"\"\"\n        cls.logger.info(\n            f\"Creating S2Cells from bounds: ({xmin}, {ymin}, {xmax}, {ymax}) at level: {level}\"\n        )\n\n        # Create a LatLngRect for the bounding box\n        rect = LatLngRect(\n            LatLng.from_degrees(ymin, xmin), LatLng.from_degrees(ymax, xmax)\n        )\n\n        # Use RegionCoverer to get cells\n        coverer = RegionCoverer()\n        coverer.min_level = level\n        coverer.max_level = level\n        coverer.max_cells = max_cells\n\n        covering = coverer.get_covering(rect)\n        cells = [cell.id() for cell in covering]\n\n        cls.logger.info(f\"Generated {len(cells)} cells from bounds.\")\n        return cls(level=level, cells=cells)\n\n    @classmethod\n    def from_spatial(\n        cls,\n        source: Union[\n            BaseGeometry,\n            gpd.GeoDataFrame,\n            List[Union[Point, Tuple[float, float]]],\n        ],\n        level: int,\n        max_cells: int = 1000,\n        **kwargs,\n    ):\n        \"\"\"Create S2Cells from various spatial sources.\"\"\"\n        cls.logger.info(\n            f\"Creating S2Cells from spatial source (type: {type(source)}) at level: {level}\"\n        )\n\n        if isinstance(source, gpd.GeoDataFrame):\n            if source.crs != \"EPSG:4326\":\n                source = source.to_crs(\"EPSG:4326\")\n            is_point_series = source.geometry.geom_type == \"Point\"\n            all_are_points = is_point_series.all()\n            if all_are_points:\n                source = source.geometry.to_list()\n            else:\n                source = source.geometry.unary_union\n\n        if isinstance(source, BaseGeometry):\n            return cls.from_geometry(\n                geometry=source, level=level, max_cells=max_cells, **kwargs\n            )\n        elif isinstance(source, Iterable) and all(\n            isinstance(pt, Point) or len(pt) == 2 for pt in source\n        ):\n            return cls.from_points(points=source, level=level, **kwargs)\n        else:\n            raise ValueError(\"Unsupported source type for S2Cells.from_spatial\")\n\n    @classmethod\n    def from_geometry(\n        cls,\n        geometry: BaseGeometry,\n        level: int,\n        max_cells: int = 1000,\n        **kwargs,\n    ):\n        \"\"\"Create S2Cells from a geometry.\n\n        Args:\n            geometry: Shapely geometry\n            level: S2 level (0-30)\n            max_cells: Maximum number of cells to generate\n        \"\"\"\n        cls.logger.info(\n            f\"Creating S2Cells from geometry (bounds: {geometry.bounds}) at level: {level}\"\n        )\n\n        if isinstance(geometry, Point):\n            return cls.from_points([geometry], level)\n\n        # For polygons and other shapes, use bounding box with RegionCoverer\n        # Then filter to actual intersection\n        minx, miny, maxx, maxy = geometry.bounds\n\n        rect = LatLngRect(\n            LatLng.from_degrees(miny, minx), LatLng.from_degrees(maxy, maxx)\n        )\n\n        coverer = RegionCoverer()\n        coverer.min_level = level\n        coverer.max_level = level\n        coverer.max_cells = max_cells\n\n        covering = coverer.get_covering(rect)\n\n        # Filter cells that actually intersect the geometry\n        cells = []\n        for cell_id in covering:\n            cell = Cell(cell_id)\n            # Create polygon from cell vertices\n            vertices = []\n            for i in range(4):\n                vertex = cell.get_vertex(i)\n                lat_lng = LatLng.from_point(vertex)\n                vertices.append((lat_lng.lng().degrees, lat_lng.lat().degrees))\n            vertices.append(vertices[0])  # Close the polygon\n\n            cell_polygon = Polygon(vertices)\n            if cell_polygon.intersects(geometry):\n                cells.append(cell_id.id())\n\n        cls.logger.info(f\"Generated {len(cells)} cells from geometry.\")\n        return cls(level=level, cells=cells, **kwargs)\n\n    @classmethod\n    def from_points(\n        cls, points: List[Union[Point, Tuple[float, float]]], level: int, **kwargs\n    ) -&gt; \"S2Cells\":\n        \"\"\"Create S2Cells from a list of points or lat-lon pairs.\"\"\"\n        cls.logger.info(f\"Creating S2Cells from {len(points)} points at level: {level}\")\n\n        cells = set(cls.get_cells_from_points(points, level))\n        cls.logger.info(f\"Generated {len(cells)} unique cells from points.\")\n        return cls(level=level, cells=list(cells), **kwargs)\n\n    @classmethod\n    def from_json(\n        cls, data_store: DataStore, file: Union[str, Path], **kwargs\n    ) -&gt; \"S2Cells\":\n        \"\"\"Load S2Cells from a JSON file.\"\"\"\n        cls.logger.info(\n            f\"Loading S2Cells from JSON file: {file} using data store: {type(data_store).__name__}\"\n        )\n\n        with data_store.open(str(file), \"r\") as f:\n            data = json.load(f)\n\n        if isinstance(data, list):  # If file contains only cell IDs\n            # Get level from first cell if available\n            level = CellId(data[0]).level() if data else 0\n            data = {\n                \"level\": level,\n                \"cells\": data,\n                **kwargs,\n            }\n        else:\n            data.update(kwargs)\n\n        instance = cls(**data)\n        instance.data_store = data_store\n        cls.logger.info(\n            f\"Successfully loaded {len(instance.cells)} cells from JSON file.\"\n        )\n        return instance\n\n    @property\n    def average_cell_area(self):\n        \"\"\"Average area of cells at this level in square meters.\"\"\"\n        # Approximate area calculation based on S2 geometry\n        # Earth surface area is ~510 trillion square meters\n        # Each level quadruples the number of cells\n        earth_area = 510_000_000_000_000  # m^2\n        num_cells_at_level = 6 * (4**self.level)  # 6 faces, each subdivided\n        return earth_area / num_cells_at_level\n\n    def filter_cells(self, cells: Iterable[int]) -&gt; \"S2Cells\":\n        \"\"\"Filter cells by a given set of cell IDs.\"\"\"\n        original_count = len(self.cells)\n        incoming_count = len(list(cells))\n\n        self.logger.info(\n            f\"Filtering {original_count} cells with an incoming set of {incoming_count} cells.\"\n        )\n\n        filtered_cells = list(set(self.cells) &amp; set(cells))\n        self.logger.info(f\"Resulting in {len(filtered_cells)} filtered cells.\")\n\n        return S2Cells(\n            level=self.level,\n            cells=filtered_cells,\n        )\n\n    def to_dataframe(self) -&gt; pd.DataFrame:\n        \"\"\"Convert to pandas DataFrame with cell ID and centroid coordinates.\"\"\"\n        self.logger.info(f\"Converting {len(self.cells)} cells to pandas DataFrame.\")\n\n        if not self.cells:\n            self.logger.warning(\n                \"No cells to convert to DataFrame. Returning empty DataFrame.\"\n            )\n            return pd.DataFrame(\n                columns=[\"cell_id\", \"cell_token\", \"latitude\", \"longitude\"]\n            )\n\n        data = []\n        for cell_id in self.cells:\n            cell = Cell(CellId(cell_id))\n            center = LatLng.from_point(cell.get_center())\n            data.append(\n                {\n                    \"cell_id\": cell_id,\n                    \"cell_token\": CellId(cell_id).to_token(),\n                    \"latitude\": center.lat().degrees,\n                    \"longitude\": center.lng().degrees,\n                }\n            )\n\n        self.logger.info(f\"Successfully converted to DataFrame.\")\n        return pd.DataFrame(data)\n\n    def to_geoms(self) -&gt; List[Polygon]:\n        \"\"\"Convert cells to shapely Polygon geometries.\"\"\"\n        self.logger.info(\n            f\"Converting {len(self.cells)} cells to shapely Polygon geometries.\"\n        )\n\n        polygons = []\n        for cell_id in self.cells:\n            cell = Cell(CellId(cell_id))\n            vertices = []\n            for i in range(4):\n                vertex = cell.get_vertex(i)\n                lat_lng = LatLng.from_point(vertex)\n                vertices.append((lat_lng.lng().degrees, lat_lng.lat().degrees))\n            vertices.append(vertices[0])  # Close the polygon\n            polygons.append(Polygon(vertices))\n\n        return polygons\n\n    def to_geodataframe(self) -&gt; gpd.GeoDataFrame:\n        \"\"\"Convert to GeoPandas GeoDataFrame.\"\"\"\n        return gpd.GeoDataFrame(\n            {\n                \"cell_id\": self.cells,\n                \"cell_token\": [CellId(c).to_token() for c in self.cells],\n                \"geometry\": self.to_geoms(),\n            },\n            crs=\"EPSG:4326\",\n        )\n\n    @staticmethod\n    def get_cells_from_points(\n        points: List[Union[Point, Tuple[float, float]]], level: int\n    ) -&gt; List[int]:\n        \"\"\"Get list of S2 cell IDs for the provided points at specified level.\n\n        Args:\n            points: List of points as either shapely Points or (lon, lat) tuples\n            level: S2 level\n\n        Returns:\n            List of S2 cell IDs as integers\n        \"\"\"\n        cells = []\n        for p in points:\n            if isinstance(p, Point):\n                # Shapely Point has x=lon, y=lat\n                lat_lng = LatLng.from_degrees(p.y, p.x)\n            else:\n                # Assume tuple is (lon, lat)\n                lat_lng = LatLng.from_degrees(p[1], p[0])\n\n            cell_id = CellId.from_lat_lng(lat_lng).parent(level)\n            cells.append(cell_id.id())\n\n        return cells\n\n    def get_neighbors(self, direct_only: bool = True) -&gt; \"S2Cells\":\n        \"\"\"Get neighbors of all cells.\n\n        Args:\n            direct_only: If True, get only direct edge neighbors (4 per cell).\n                        If False, get all 8 neighbors including corners.\n\n        Returns:\n            New S2Cells instance with neighbors included\n        \"\"\"\n        self.logger.info(\n            f\"Getting neighbors for {len(self.cells)} cells (direct_only={direct_only}).\"\n        )\n\n        all_neighbors = set()\n        for cell_id in self.cells:\n            cell = CellId(cell_id)\n            # Get edge neighbors\n            for i in range(4):\n                neighbors = cell.get_edge_neighbors()\n                all_neighbors.update([n.id() for n in neighbors])\n\n            if not direct_only:\n                # Get corner neighbors\n                for i in range(4):\n                    vertex_neighbors = cell.get_vertex_neighbors(i)\n                    all_neighbors.update([n.id() for n in vertex_neighbors])\n\n        self.logger.info(f\"Found {len(all_neighbors)} total cells including neighbors.\")\n\n        return S2Cells(level=self.level, cells=list(all_neighbors))\n\n    def get_children(self, target_level: int) -&gt; \"S2Cells\":\n        \"\"\"Get children cells at higher level.\n\n        Args:\n            target_level: Target level (must be higher than current)\n\n        Returns:\n            New S2Cells instance with children at target level\n        \"\"\"\n        if target_level &lt;= self.level:\n            raise ValueError(\"Target level must be higher than current level\")\n\n        self.logger.info(\n            f\"Getting children at level {target_level} for {len(self.cells)} cells.\"\n        )\n\n        all_children = []\n        for cell_id in self.cells:\n            cell = CellId(cell_id)\n            # Get all children at target level\n            child = cell.child_begin(target_level)\n            end = cell.child_end(target_level)\n\n            while child != end:\n                all_children.append(child.id())\n                child = child.next()\n\n        self.logger.info(f\"Generated {len(all_children)} children cells.\")\n        return S2Cells(level=target_level, cells=all_children)\n\n    def get_parents(self, target_level: int) -&gt; \"S2Cells\":\n        \"\"\"Get parent cells at lower level.\n\n        Args:\n            target_level: Target level (must be lower than current)\n\n        Returns:\n            New S2Cells instance with parents at target level\n        \"\"\"\n        if target_level &gt;= self.level:\n            raise ValueError(\"Target level must be lower than current level\")\n\n        self.logger.info(\n            f\"Getting parents at level {target_level} for {len(self.cells)} cells.\"\n        )\n\n        parents = set()\n        for cell_id in self.cells:\n            parent = CellId(cell_id).parent(target_level)\n            parents.add(parent.id())\n\n        self.logger.info(f\"Generated {len(parents)} parent cells.\")\n        return S2Cells(level=target_level, cells=list(parents))\n\n    def save(self, file: Union[str, Path], format: str = \"json\") -&gt; None:\n        \"\"\"Save S2Cells to file in specified format.\"\"\"\n        with self.data_store.open(str(file), \"wb\" if format == \"parquet\" else \"w\") as f:\n            if format == \"parquet\":\n                self.to_geodataframe().to_parquet(f, index=False)\n            elif format == \"geojson\":\n                f.write(self.to_geodataframe().to_json(drop_id=True))\n            elif format == \"json\":\n                json.dump(self.cells, f)\n            else:\n                raise ValueError(f\"Unsupported format: {format}\")\n\n    def __len__(self) -&gt; int:\n        return len(self.cells)\n</code></pre>"},{"location":"api/grid/#gigaspatial.grid.s2.S2Cells.average_cell_area","title":"<code>average_cell_area</code>  <code>property</code>","text":"<p>Average area of cells at this level in square meters.</p>"},{"location":"api/grid/#gigaspatial.grid.s2.S2Cells.filter_cells","title":"<code>filter_cells(cells)</code>","text":"<p>Filter cells by a given set of cell IDs.</p> Source code in <code>gigaspatial/grid/s2.py</code> <pre><code>def filter_cells(self, cells: Iterable[int]) -&gt; \"S2Cells\":\n    \"\"\"Filter cells by a given set of cell IDs.\"\"\"\n    original_count = len(self.cells)\n    incoming_count = len(list(cells))\n\n    self.logger.info(\n        f\"Filtering {original_count} cells with an incoming set of {incoming_count} cells.\"\n    )\n\n    filtered_cells = list(set(self.cells) &amp; set(cells))\n    self.logger.info(f\"Resulting in {len(filtered_cells)} filtered cells.\")\n\n    return S2Cells(\n        level=self.level,\n        cells=filtered_cells,\n    )\n</code></pre>"},{"location":"api/grid/#gigaspatial.grid.s2.S2Cells.from_bounds","title":"<code>from_bounds(xmin, ymin, xmax, ymax, level, max_cells=100)</code>  <code>classmethod</code>","text":"<p>Create S2Cells from boundary coordinates.</p> <p>Parameters:</p> Name Type Description Default <code>xmin,</code> <code>(ymin, xmax, ymax)</code> <p>Bounding box coordinates in degrees</p> required <code>level</code> <code>int</code> <p>S2 level (0-30)</p> required <code>max_cells</code> <code>int</code> <p>Maximum number of cells to generate</p> <code>100</code> Source code in <code>gigaspatial/grid/s2.py</code> <pre><code>@classmethod\ndef from_bounds(\n    cls,\n    xmin: float,\n    ymin: float,\n    xmax: float,\n    ymax: float,\n    level: int,\n    max_cells: int = 100,\n):\n    \"\"\"Create S2Cells from boundary coordinates.\n\n    Args:\n        xmin, ymin, xmax, ymax: Bounding box coordinates in degrees\n        level: S2 level (0-30)\n        max_cells: Maximum number of cells to generate\n    \"\"\"\n    cls.logger.info(\n        f\"Creating S2Cells from bounds: ({xmin}, {ymin}, {xmax}, {ymax}) at level: {level}\"\n    )\n\n    # Create a LatLngRect for the bounding box\n    rect = LatLngRect(\n        LatLng.from_degrees(ymin, xmin), LatLng.from_degrees(ymax, xmax)\n    )\n\n    # Use RegionCoverer to get cells\n    coverer = RegionCoverer()\n    coverer.min_level = level\n    coverer.max_level = level\n    coverer.max_cells = max_cells\n\n    covering = coverer.get_covering(rect)\n    cells = [cell.id() for cell in covering]\n\n    cls.logger.info(f\"Generated {len(cells)} cells from bounds.\")\n    return cls(level=level, cells=cells)\n</code></pre>"},{"location":"api/grid/#gigaspatial.grid.s2.S2Cells.from_cells","title":"<code>from_cells(cells)</code>  <code>classmethod</code>","text":"<p>Create S2Cells from list of S2 cell IDs (integers or tokens).</p> Source code in <code>gigaspatial/grid/s2.py</code> <pre><code>@classmethod\ndef from_cells(cls, cells: List[Union[int, str]]):\n    \"\"\"Create S2Cells from list of S2 cell IDs (integers or tokens).\"\"\"\n    if not cells:\n        cls.logger.warning(\"No cells provided to from_cells.\")\n        return cls(level=0, cells=[])\n\n    cls.logger.info(f\"Initializing S2Cells from {len(cells)} provided cells.\")\n\n    # Convert tokens to integers if needed\n    cell_ids = []\n    for cell in cells:\n        if isinstance(cell, str):\n            cell_ids.append(CellId.from_token(cell).id())\n        else:\n            cell_ids.append(cell)\n\n    # Get level from first cell\n    level = CellId(cell_ids[0]).level()\n    return cls(level=level, cells=list(set(cell_ids)))\n</code></pre>"},{"location":"api/grid/#gigaspatial.grid.s2.S2Cells.from_geometry","title":"<code>from_geometry(geometry, level, max_cells=1000, **kwargs)</code>  <code>classmethod</code>","text":"<p>Create S2Cells from a geometry.</p> <p>Parameters:</p> Name Type Description Default <code>geometry</code> <code>BaseGeometry</code> <p>Shapely geometry</p> required <code>level</code> <code>int</code> <p>S2 level (0-30)</p> required <code>max_cells</code> <code>int</code> <p>Maximum number of cells to generate</p> <code>1000</code> Source code in <code>gigaspatial/grid/s2.py</code> <pre><code>@classmethod\ndef from_geometry(\n    cls,\n    geometry: BaseGeometry,\n    level: int,\n    max_cells: int = 1000,\n    **kwargs,\n):\n    \"\"\"Create S2Cells from a geometry.\n\n    Args:\n        geometry: Shapely geometry\n        level: S2 level (0-30)\n        max_cells: Maximum number of cells to generate\n    \"\"\"\n    cls.logger.info(\n        f\"Creating S2Cells from geometry (bounds: {geometry.bounds}) at level: {level}\"\n    )\n\n    if isinstance(geometry, Point):\n        return cls.from_points([geometry], level)\n\n    # For polygons and other shapes, use bounding box with RegionCoverer\n    # Then filter to actual intersection\n    minx, miny, maxx, maxy = geometry.bounds\n\n    rect = LatLngRect(\n        LatLng.from_degrees(miny, minx), LatLng.from_degrees(maxy, maxx)\n    )\n\n    coverer = RegionCoverer()\n    coverer.min_level = level\n    coverer.max_level = level\n    coverer.max_cells = max_cells\n\n    covering = coverer.get_covering(rect)\n\n    # Filter cells that actually intersect the geometry\n    cells = []\n    for cell_id in covering:\n        cell = Cell(cell_id)\n        # Create polygon from cell vertices\n        vertices = []\n        for i in range(4):\n            vertex = cell.get_vertex(i)\n            lat_lng = LatLng.from_point(vertex)\n            vertices.append((lat_lng.lng().degrees, lat_lng.lat().degrees))\n        vertices.append(vertices[0])  # Close the polygon\n\n        cell_polygon = Polygon(vertices)\n        if cell_polygon.intersects(geometry):\n            cells.append(cell_id.id())\n\n    cls.logger.info(f\"Generated {len(cells)} cells from geometry.\")\n    return cls(level=level, cells=cells, **kwargs)\n</code></pre>"},{"location":"api/grid/#gigaspatial.grid.s2.S2Cells.from_json","title":"<code>from_json(data_store, file, **kwargs)</code>  <code>classmethod</code>","text":"<p>Load S2Cells from a JSON file.</p> Source code in <code>gigaspatial/grid/s2.py</code> <pre><code>@classmethod\ndef from_json(\n    cls, data_store: DataStore, file: Union[str, Path], **kwargs\n) -&gt; \"S2Cells\":\n    \"\"\"Load S2Cells from a JSON file.\"\"\"\n    cls.logger.info(\n        f\"Loading S2Cells from JSON file: {file} using data store: {type(data_store).__name__}\"\n    )\n\n    with data_store.open(str(file), \"r\") as f:\n        data = json.load(f)\n\n    if isinstance(data, list):  # If file contains only cell IDs\n        # Get level from first cell if available\n        level = CellId(data[0]).level() if data else 0\n        data = {\n            \"level\": level,\n            \"cells\": data,\n            **kwargs,\n        }\n    else:\n        data.update(kwargs)\n\n    instance = cls(**data)\n    instance.data_store = data_store\n    cls.logger.info(\n        f\"Successfully loaded {len(instance.cells)} cells from JSON file.\"\n    )\n    return instance\n</code></pre>"},{"location":"api/grid/#gigaspatial.grid.s2.S2Cells.from_points","title":"<code>from_points(points, level, **kwargs)</code>  <code>classmethod</code>","text":"<p>Create S2Cells from a list of points or lat-lon pairs.</p> Source code in <code>gigaspatial/grid/s2.py</code> <pre><code>@classmethod\ndef from_points(\n    cls, points: List[Union[Point, Tuple[float, float]]], level: int, **kwargs\n) -&gt; \"S2Cells\":\n    \"\"\"Create S2Cells from a list of points or lat-lon pairs.\"\"\"\n    cls.logger.info(f\"Creating S2Cells from {len(points)} points at level: {level}\")\n\n    cells = set(cls.get_cells_from_points(points, level))\n    cls.logger.info(f\"Generated {len(cells)} unique cells from points.\")\n    return cls(level=level, cells=list(cells), **kwargs)\n</code></pre>"},{"location":"api/grid/#gigaspatial.grid.s2.S2Cells.from_spatial","title":"<code>from_spatial(source, level, max_cells=1000, **kwargs)</code>  <code>classmethod</code>","text":"<p>Create S2Cells from various spatial sources.</p> Source code in <code>gigaspatial/grid/s2.py</code> <pre><code>@classmethod\ndef from_spatial(\n    cls,\n    source: Union[\n        BaseGeometry,\n        gpd.GeoDataFrame,\n        List[Union[Point, Tuple[float, float]]],\n    ],\n    level: int,\n    max_cells: int = 1000,\n    **kwargs,\n):\n    \"\"\"Create S2Cells from various spatial sources.\"\"\"\n    cls.logger.info(\n        f\"Creating S2Cells from spatial source (type: {type(source)}) at level: {level}\"\n    )\n\n    if isinstance(source, gpd.GeoDataFrame):\n        if source.crs != \"EPSG:4326\":\n            source = source.to_crs(\"EPSG:4326\")\n        is_point_series = source.geometry.geom_type == \"Point\"\n        all_are_points = is_point_series.all()\n        if all_are_points:\n            source = source.geometry.to_list()\n        else:\n            source = source.geometry.unary_union\n\n    if isinstance(source, BaseGeometry):\n        return cls.from_geometry(\n            geometry=source, level=level, max_cells=max_cells, **kwargs\n        )\n    elif isinstance(source, Iterable) and all(\n        isinstance(pt, Point) or len(pt) == 2 for pt in source\n    ):\n        return cls.from_points(points=source, level=level, **kwargs)\n    else:\n        raise ValueError(\"Unsupported source type for S2Cells.from_spatial\")\n</code></pre>"},{"location":"api/grid/#gigaspatial.grid.s2.S2Cells.get_cells_from_points","title":"<code>get_cells_from_points(points, level)</code>  <code>staticmethod</code>","text":"<p>Get list of S2 cell IDs for the provided points at specified level.</p> <p>Parameters:</p> Name Type Description Default <code>points</code> <code>List[Union[Point, Tuple[float, float]]]</code> <p>List of points as either shapely Points or (lon, lat) tuples</p> required <code>level</code> <code>int</code> <p>S2 level</p> required <p>Returns:</p> Type Description <code>List[int]</code> <p>List of S2 cell IDs as integers</p> Source code in <code>gigaspatial/grid/s2.py</code> <pre><code>@staticmethod\ndef get_cells_from_points(\n    points: List[Union[Point, Tuple[float, float]]], level: int\n) -&gt; List[int]:\n    \"\"\"Get list of S2 cell IDs for the provided points at specified level.\n\n    Args:\n        points: List of points as either shapely Points or (lon, lat) tuples\n        level: S2 level\n\n    Returns:\n        List of S2 cell IDs as integers\n    \"\"\"\n    cells = []\n    for p in points:\n        if isinstance(p, Point):\n            # Shapely Point has x=lon, y=lat\n            lat_lng = LatLng.from_degrees(p.y, p.x)\n        else:\n            # Assume tuple is (lon, lat)\n            lat_lng = LatLng.from_degrees(p[1], p[0])\n\n        cell_id = CellId.from_lat_lng(lat_lng).parent(level)\n        cells.append(cell_id.id())\n\n    return cells\n</code></pre>"},{"location":"api/grid/#gigaspatial.grid.s2.S2Cells.get_children","title":"<code>get_children(target_level)</code>","text":"<p>Get children cells at higher level.</p> <p>Parameters:</p> Name Type Description Default <code>target_level</code> <code>int</code> <p>Target level (must be higher than current)</p> required <p>Returns:</p> Type Description <code>S2Cells</code> <p>New S2Cells instance with children at target level</p> Source code in <code>gigaspatial/grid/s2.py</code> <pre><code>def get_children(self, target_level: int) -&gt; \"S2Cells\":\n    \"\"\"Get children cells at higher level.\n\n    Args:\n        target_level: Target level (must be higher than current)\n\n    Returns:\n        New S2Cells instance with children at target level\n    \"\"\"\n    if target_level &lt;= self.level:\n        raise ValueError(\"Target level must be higher than current level\")\n\n    self.logger.info(\n        f\"Getting children at level {target_level} for {len(self.cells)} cells.\"\n    )\n\n    all_children = []\n    for cell_id in self.cells:\n        cell = CellId(cell_id)\n        # Get all children at target level\n        child = cell.child_begin(target_level)\n        end = cell.child_end(target_level)\n\n        while child != end:\n            all_children.append(child.id())\n            child = child.next()\n\n    self.logger.info(f\"Generated {len(all_children)} children cells.\")\n    return S2Cells(level=target_level, cells=all_children)\n</code></pre>"},{"location":"api/grid/#gigaspatial.grid.s2.S2Cells.get_neighbors","title":"<code>get_neighbors(direct_only=True)</code>","text":"<p>Get neighbors of all cells.</p> <p>Parameters:</p> Name Type Description Default <code>direct_only</code> <code>bool</code> <p>If True, get only direct edge neighbors (4 per cell).         If False, get all 8 neighbors including corners.</p> <code>True</code> <p>Returns:</p> Type Description <code>S2Cells</code> <p>New S2Cells instance with neighbors included</p> Source code in <code>gigaspatial/grid/s2.py</code> <pre><code>def get_neighbors(self, direct_only: bool = True) -&gt; \"S2Cells\":\n    \"\"\"Get neighbors of all cells.\n\n    Args:\n        direct_only: If True, get only direct edge neighbors (4 per cell).\n                    If False, get all 8 neighbors including corners.\n\n    Returns:\n        New S2Cells instance with neighbors included\n    \"\"\"\n    self.logger.info(\n        f\"Getting neighbors for {len(self.cells)} cells (direct_only={direct_only}).\"\n    )\n\n    all_neighbors = set()\n    for cell_id in self.cells:\n        cell = CellId(cell_id)\n        # Get edge neighbors\n        for i in range(4):\n            neighbors = cell.get_edge_neighbors()\n            all_neighbors.update([n.id() for n in neighbors])\n\n        if not direct_only:\n            # Get corner neighbors\n            for i in range(4):\n                vertex_neighbors = cell.get_vertex_neighbors(i)\n                all_neighbors.update([n.id() for n in vertex_neighbors])\n\n    self.logger.info(f\"Found {len(all_neighbors)} total cells including neighbors.\")\n\n    return S2Cells(level=self.level, cells=list(all_neighbors))\n</code></pre>"},{"location":"api/grid/#gigaspatial.grid.s2.S2Cells.get_parents","title":"<code>get_parents(target_level)</code>","text":"<p>Get parent cells at lower level.</p> <p>Parameters:</p> Name Type Description Default <code>target_level</code> <code>int</code> <p>Target level (must be lower than current)</p> required <p>Returns:</p> Type Description <code>S2Cells</code> <p>New S2Cells instance with parents at target level</p> Source code in <code>gigaspatial/grid/s2.py</code> <pre><code>def get_parents(self, target_level: int) -&gt; \"S2Cells\":\n    \"\"\"Get parent cells at lower level.\n\n    Args:\n        target_level: Target level (must be lower than current)\n\n    Returns:\n        New S2Cells instance with parents at target level\n    \"\"\"\n    if target_level &gt;= self.level:\n        raise ValueError(\"Target level must be lower than current level\")\n\n    self.logger.info(\n        f\"Getting parents at level {target_level} for {len(self.cells)} cells.\"\n    )\n\n    parents = set()\n    for cell_id in self.cells:\n        parent = CellId(cell_id).parent(target_level)\n        parents.add(parent.id())\n\n    self.logger.info(f\"Generated {len(parents)} parent cells.\")\n    return S2Cells(level=target_level, cells=list(parents))\n</code></pre>"},{"location":"api/grid/#gigaspatial.grid.s2.S2Cells.save","title":"<code>save(file, format='json')</code>","text":"<p>Save S2Cells to file in specified format.</p> Source code in <code>gigaspatial/grid/s2.py</code> <pre><code>def save(self, file: Union[str, Path], format: str = \"json\") -&gt; None:\n    \"\"\"Save S2Cells to file in specified format.\"\"\"\n    with self.data_store.open(str(file), \"wb\" if format == \"parquet\" else \"w\") as f:\n        if format == \"parquet\":\n            self.to_geodataframe().to_parquet(f, index=False)\n        elif format == \"geojson\":\n            f.write(self.to_geodataframe().to_json(drop_id=True))\n        elif format == \"json\":\n            json.dump(self.cells, f)\n        else:\n            raise ValueError(f\"Unsupported format: {format}\")\n</code></pre>"},{"location":"api/grid/#gigaspatial.grid.s2.S2Cells.to_dataframe","title":"<code>to_dataframe()</code>","text":"<p>Convert to pandas DataFrame with cell ID and centroid coordinates.</p> Source code in <code>gigaspatial/grid/s2.py</code> <pre><code>def to_dataframe(self) -&gt; pd.DataFrame:\n    \"\"\"Convert to pandas DataFrame with cell ID and centroid coordinates.\"\"\"\n    self.logger.info(f\"Converting {len(self.cells)} cells to pandas DataFrame.\")\n\n    if not self.cells:\n        self.logger.warning(\n            \"No cells to convert to DataFrame. Returning empty DataFrame.\"\n        )\n        return pd.DataFrame(\n            columns=[\"cell_id\", \"cell_token\", \"latitude\", \"longitude\"]\n        )\n\n    data = []\n    for cell_id in self.cells:\n        cell = Cell(CellId(cell_id))\n        center = LatLng.from_point(cell.get_center())\n        data.append(\n            {\n                \"cell_id\": cell_id,\n                \"cell_token\": CellId(cell_id).to_token(),\n                \"latitude\": center.lat().degrees,\n                \"longitude\": center.lng().degrees,\n            }\n        )\n\n    self.logger.info(f\"Successfully converted to DataFrame.\")\n    return pd.DataFrame(data)\n</code></pre>"},{"location":"api/grid/#gigaspatial.grid.s2.S2Cells.to_geodataframe","title":"<code>to_geodataframe()</code>","text":"<p>Convert to GeoPandas GeoDataFrame.</p> Source code in <code>gigaspatial/grid/s2.py</code> <pre><code>def to_geodataframe(self) -&gt; gpd.GeoDataFrame:\n    \"\"\"Convert to GeoPandas GeoDataFrame.\"\"\"\n    return gpd.GeoDataFrame(\n        {\n            \"cell_id\": self.cells,\n            \"cell_token\": [CellId(c).to_token() for c in self.cells],\n            \"geometry\": self.to_geoms(),\n        },\n        crs=\"EPSG:4326\",\n    )\n</code></pre>"},{"location":"api/grid/#gigaspatial.grid.s2.S2Cells.to_geoms","title":"<code>to_geoms()</code>","text":"<p>Convert cells to shapely Polygon geometries.</p> Source code in <code>gigaspatial/grid/s2.py</code> <pre><code>def to_geoms(self) -&gt; List[Polygon]:\n    \"\"\"Convert cells to shapely Polygon geometries.\"\"\"\n    self.logger.info(\n        f\"Converting {len(self.cells)} cells to shapely Polygon geometries.\"\n    )\n\n    polygons = []\n    for cell_id in self.cells:\n        cell = Cell(CellId(cell_id))\n        vertices = []\n        for i in range(4):\n            vertex = cell.get_vertex(i)\n            lat_lng = LatLng.from_point(vertex)\n            vertices.append((lat_lng.lng().degrees, lat_lng.lat().degrees))\n        vertices.append(vertices[0])  # Close the polygon\n        polygons.append(Polygon(vertices))\n\n    return polygons\n</code></pre>"},{"location":"api/handlers/","title":"Handlers Module","text":""},{"location":"api/handlers/#gigaspatial.handlers","title":"<code>gigaspatial.handlers</code>","text":""},{"location":"api/handlers/#gigaspatial.handlers.base","title":"<code>base</code>","text":""},{"location":"api/handlers/#gigaspatial.handlers.base.BaseHandler","title":"<code>BaseHandler</code>","text":"<p>               Bases: <code>ABC</code></p> <p>Abstract base class that orchestrates configuration, downloading, and reading functionality.</p> <p>This class serves as the main entry point for dataset handlers, providing a unified interface for data acquisition and loading. It manages the lifecycle of config, downloader, and reader components.</p> <p>Subclasses should implement the abstract methods to provide specific handler types and define how components are created and interact.</p> Source code in <code>gigaspatial/handlers/base.py</code> <pre><code>class BaseHandler(ABC):\n    \"\"\"\n    Abstract base class that orchestrates configuration, downloading, and reading functionality.\n\n    This class serves as the main entry point for dataset handlers, providing a unified\n    interface for data acquisition and loading. It manages the lifecycle of config,\n    downloader, and reader components.\n\n    Subclasses should implement the abstract methods to provide specific handler types\n    and define how components are created and interact.\n    \"\"\"\n\n    def __init__(\n        self,\n        config: Optional[BaseHandlerConfig] = None,\n        downloader: Optional[BaseHandlerDownloader] = None,\n        reader: Optional[BaseHandlerReader] = None,\n        data_store: Optional[DataStore] = None,\n        logger: Optional[logging.Logger] = None,\n    ):\n        \"\"\"\n        Initialize the BaseHandler with optional components.\n\n        Args:\n            config: Configuration object. If None, will be created via create_config()\n            downloader: Downloader instance. If None, will be created via create_downloader()\n            reader: Reader instance. If None, will be created via create_reader()\n            data_store: Data store instance. Defaults to LocalDataStore if not provided\n            logger: Logger instance. If not provided, creates one based on class name\n        \"\"\"\n        # Initialize data store first as it's used by other components\n        self.data_store = data_store or LocalDataStore()\n\n        # Initialize logger\n        self.logger = logger or global_config.get_logger(self.__class__.__name__)\n\n        # Initialize or create config\n        self._config = config\n        if self._config is None:\n            self._config = self.create_config(\n                data_store=self.data_store, logger=self.logger\n            )\n\n        # Initialize or create downloader\n        self._downloader = downloader\n        if self._downloader is None:\n            self._downloader = self.create_downloader(\n                config=self._config, data_store=self.data_store, logger=self.logger\n            )\n\n        # Initialize or create reader\n        self._reader = reader\n        if self._reader is None:\n            self._reader = self.create_reader(\n                config=self._config, data_store=self.data_store, logger=self.logger\n            )\n\n    @property\n    def config(self) -&gt; BaseHandlerConfig:\n        \"\"\"Get the configuration object.\"\"\"\n        return self._config\n\n    @property\n    def downloader(self) -&gt; BaseHandlerDownloader:\n        \"\"\"Get the downloader object.\"\"\"\n        return self._downloader\n\n    @property\n    def reader(self) -&gt; BaseHandlerReader:\n        \"\"\"Get the reader object.\"\"\"\n        return self._reader\n\n    # Abstract factory methods for creating components\n    @abstractmethod\n    def create_config(\n        self, data_store: DataStore, logger: logging.Logger, **kwargs\n    ) -&gt; BaseHandlerConfig:\n        \"\"\"\n        Create and return a configuration object for this handler.\n\n        Args:\n            data_store: The data store instance to use\n            logger: The logger instance to use\n            **kwargs: Additional configuration parameters\n\n        Returns:\n            Configured BaseHandlerConfig instance\n        \"\"\"\n        pass\n\n    @abstractmethod\n    def create_downloader(\n        self,\n        config: BaseHandlerConfig,\n        data_store: DataStore,\n        logger: logging.Logger,\n        **kwargs,\n    ) -&gt; BaseHandlerDownloader:\n        \"\"\"\n        Create and return a downloader object for this handler.\n\n        Args:\n            config: The configuration object\n            data_store: The data store instance to use\n            logger: The logger instance to use\n            **kwargs: Additional downloader parameters\n\n        Returns:\n            Configured BaseHandlerDownloader instance\n        \"\"\"\n        pass\n\n    @abstractmethod\n    def create_reader(\n        self,\n        config: BaseHandlerConfig,\n        data_store: DataStore,\n        logger: logging.Logger,\n        **kwargs,\n    ) -&gt; BaseHandlerReader:\n        \"\"\"\n        Create and return a reader object for this handler.\n\n        Args:\n            config: The configuration object\n            data_store: The data store instance to use\n            logger: The logger instance to use\n            **kwargs: Additional reader parameters\n\n        Returns:\n            Configured BaseHandlerReader instance\n        \"\"\"\n        pass\n\n    # High-level interface methods\n    def ensure_data_available(\n        self,\n        source: Union[\n            str,  # country\n            List[Union[tuple, Point]],  # points\n            BaseGeometry,  # geometry\n            gpd.GeoDataFrame,  # geodataframe\n            Path,  # path\n            List[Union[str, Path]],  # list of paths\n        ],\n        force_download: bool = False,\n        **kwargs,\n    ) -&gt; bool:\n        \"\"\"\n        Ensure that data is available for the given source.\n\n        This method checks if the required data exists locally, and if not (or if\n        force_download is True), downloads it using the downloader.\n\n        Args:\n            source: The data source specification\n            force_download: If True, download even if data exists locally\n            **kwargs: Additional parameters passed to download methods\n\n        Returns:\n            bool: True if data is available after this operation\n        \"\"\"\n        try:\n            # Get relevant units (cached if already computed for this source)\n            data_units = self.config.get_relevant_data_units(\n                source, force_recompute=force_download, **kwargs\n            )\n            data_paths = self.config.get_data_unit_paths(data_units, **kwargs)\n\n            # Check if data exists (unless force download)\n            if not force_download:\n                missing_paths = [\n                    path\n                    for path in data_paths\n                    if not self.data_store.file_exists(str(path))\n                ]\n            else:\n                # If force_download, treat all as missing\n                missing_paths = data_paths\n\n            if not missing_paths:\n                self.logger.info(\"All required data is already available\")\n                return True\n\n            # Map units to paths (assumes correspondence order; adapt if needed)\n            path_to_unit = dict(zip(data_paths, data_units))\n            if force_download:\n                units_to_download = data_units\n            else:\n                units_to_download = [\n                    path_to_unit[p] for p in missing_paths if p in path_to_unit\n                ]\n\n            if units_to_download:\n                self.downloader.download_data_units(units_to_download, **kwargs)\n            else:\n                # Fallback - download by source if unit mapping isn't available\n                self.downloader.download(source, **kwargs)\n\n            # After attempted download, check again\n            remaining_missing = [\n                path\n                for path in data_paths\n                if not self.data_store.file_exists(str(path))\n            ]\n            if remaining_missing:\n                self.logger.error(\n                    f\"Some data still missing after download: {remaining_missing}\"\n                )\n                return False\n\n            return True\n        except Exception as e:\n            self.logger.error(f\"Failed to ensure data availability: {e}\")\n            return False\n\n    def load_data(\n        self,\n        source: Union[\n            str,  # country\n            List[Union[tuple, Point]],  # points\n            BaseGeometry,  # geometry\n            gpd.GeoDataFrame,  # geodataframe\n            Path,  # path\n            List[Union[str, Path]],  # list of paths\n        ],\n        crop_to_source: bool = False,\n        ensure_available: bool = True,\n        **kwargs,\n    ) -&gt; Any:\n        \"\"\"\n        Load data from the given source.\n\n        Args:\n            source: The data source specification\n            ensure_available: If True, ensure data is downloaded before loading\n            **kwargs: Additional parameters passed to load methods\n\n        Returns:\n            Loaded data (type depends on specific handler implementation)\n        \"\"\"\n        if ensure_available:\n            if not self.ensure_data_available(source, **kwargs):\n                raise RuntimeError(\"Could not ensure data availability for loading\")\n\n        return self.reader.load(source, crop_to_source=crop_to_source, **kwargs)\n\n    def download_and_load(\n        self,\n        source: Union[\n            str,  # country\n            List[Union[tuple, Point]],  # points\n            BaseGeometry,  # geometry\n            gpd.GeoDataFrame,  # geodataframe\n            Path,  # path\n            List[Union[str, Path]],  # list of paths\n        ],\n        crop_to_source: bool = False,\n        force_download: bool = False,\n        **kwargs,\n    ) -&gt; Any:\n        \"\"\"\n        Convenience method to download (if needed) and load data in one call.\n\n        Args:\n            source: The data source specification\n            force_download: If True, download even if data exists locally\n            **kwargs: Additional parameters\n\n        Returns:\n            Loaded data\n        \"\"\"\n        self.ensure_data_available(source, force_download=force_download, **kwargs)\n        return self.reader.load(source, crop_to_source=crop_to_source, **kwargs)\n\n    def get_available_data_info(\n        self,\n        source: Union[\n            str,  # country\n            List[Union[tuple, Point]],  # points\n            BaseGeometry,  # geometry\n            gpd.GeoDataFrame,  # geodataframe\n        ],\n        **kwargs,\n    ) -&gt; dict:\n        \"\"\"\n        Get information about available data for the given source.\n\n        Args:\n            source: The data source specification\n            **kwargs: Additional parameters\n\n        Returns:\n            dict: Information about data availability, paths, etc.\n        \"\"\"\n        try:\n            if hasattr(self.config, \"get_relevant_data_units\"):\n                data_units = self.config.get_relevant_data_units(source, **kwargs)\n                data_paths = self.config.get_data_unit_paths(data_units, **kwargs)\n            else:\n                data_paths = self.reader.resolve_source_paths(source, **kwargs)\n\n            existing_paths = [\n                path for path in data_paths if self.data_store.file_exists(str(path))\n            ]\n            missing_paths = [\n                path\n                for path in data_paths\n                if not self.data_store.file_exists(str(path))\n            ]\n\n            return {\n                \"total_data_units\": len(data_paths),\n                \"available_data_units\": len(existing_paths),\n                \"missing_data_units\": len(missing_paths),\n                \"available_paths\": existing_paths,\n                \"missing_paths\": missing_paths,\n                \"all_available\": len(missing_paths) == 0,\n            }\n\n        except Exception as e:\n            self.logger.error(f\"Failed to get data info: {e}\")\n            return {\n                \"error\": str(e),\n                \"total_data_units\": 0,\n                \"available_data_units\": 0,\n                \"missing_data_units\": 0,\n                \"available_paths\": [],\n                \"missing_paths\": [],\n                \"all_available\": False,\n            }\n\n    def cleanup(self):\n        \"\"\"\n        Cleanup resources used by the handler.\n\n        Override in subclasses if specific cleanup is needed.\n        \"\"\"\n        self.logger.info(f\"Cleaning up {self.__class__.__name__}\")\n        # Subclasses can override to add specific cleanup logic\n\n    def __enter__(self):\n        \"\"\"Context manager entry.\"\"\"\n        return self\n\n    def __exit__(self, exc_type, exc_val, exc_tb):\n        \"\"\"Context manager exit.\"\"\"\n        self.cleanup()\n\n    def __repr__(self) -&gt; str:\n        \"\"\"String representation of the handler.\"\"\"\n        return (\n            f\"{self.__class__.__name__}(\"\n            f\"config={self.config.__class__.__name__}, \"\n            f\"downloader={self.downloader.__class__.__name__}, \"\n            f\"reader={self.reader.__class__.__name__})\"\n        )\n</code></pre>"},{"location":"api/handlers/#gigaspatial.handlers.base.BaseHandler.config","title":"<code>config: BaseHandlerConfig</code>  <code>property</code>","text":"<p>Get the configuration object.</p>"},{"location":"api/handlers/#gigaspatial.handlers.base.BaseHandler.downloader","title":"<code>downloader: BaseHandlerDownloader</code>  <code>property</code>","text":"<p>Get the downloader object.</p>"},{"location":"api/handlers/#gigaspatial.handlers.base.BaseHandler.reader","title":"<code>reader: BaseHandlerReader</code>  <code>property</code>","text":"<p>Get the reader object.</p>"},{"location":"api/handlers/#gigaspatial.handlers.base.BaseHandler.__enter__","title":"<code>__enter__()</code>","text":"<p>Context manager entry.</p> Source code in <code>gigaspatial/handlers/base.py</code> <pre><code>def __enter__(self):\n    \"\"\"Context manager entry.\"\"\"\n    return self\n</code></pre>"},{"location":"api/handlers/#gigaspatial.handlers.base.BaseHandler.__exit__","title":"<code>__exit__(exc_type, exc_val, exc_tb)</code>","text":"<p>Context manager exit.</p> Source code in <code>gigaspatial/handlers/base.py</code> <pre><code>def __exit__(self, exc_type, exc_val, exc_tb):\n    \"\"\"Context manager exit.\"\"\"\n    self.cleanup()\n</code></pre>"},{"location":"api/handlers/#gigaspatial.handlers.base.BaseHandler.__init__","title":"<code>__init__(config=None, downloader=None, reader=None, data_store=None, logger=None)</code>","text":"<p>Initialize the BaseHandler with optional components.</p> <p>Parameters:</p> Name Type Description Default <code>config</code> <code>Optional[BaseHandlerConfig]</code> <p>Configuration object. If None, will be created via create_config()</p> <code>None</code> <code>downloader</code> <code>Optional[BaseHandlerDownloader]</code> <p>Downloader instance. If None, will be created via create_downloader()</p> <code>None</code> <code>reader</code> <code>Optional[BaseHandlerReader]</code> <p>Reader instance. If None, will be created via create_reader()</p> <code>None</code> <code>data_store</code> <code>Optional[DataStore]</code> <p>Data store instance. Defaults to LocalDataStore if not provided</p> <code>None</code> <code>logger</code> <code>Optional[Logger]</code> <p>Logger instance. If not provided, creates one based on class name</p> <code>None</code> Source code in <code>gigaspatial/handlers/base.py</code> <pre><code>def __init__(\n    self,\n    config: Optional[BaseHandlerConfig] = None,\n    downloader: Optional[BaseHandlerDownloader] = None,\n    reader: Optional[BaseHandlerReader] = None,\n    data_store: Optional[DataStore] = None,\n    logger: Optional[logging.Logger] = None,\n):\n    \"\"\"\n    Initialize the BaseHandler with optional components.\n\n    Args:\n        config: Configuration object. If None, will be created via create_config()\n        downloader: Downloader instance. If None, will be created via create_downloader()\n        reader: Reader instance. If None, will be created via create_reader()\n        data_store: Data store instance. Defaults to LocalDataStore if not provided\n        logger: Logger instance. If not provided, creates one based on class name\n    \"\"\"\n    # Initialize data store first as it's used by other components\n    self.data_store = data_store or LocalDataStore()\n\n    # Initialize logger\n    self.logger = logger or global_config.get_logger(self.__class__.__name__)\n\n    # Initialize or create config\n    self._config = config\n    if self._config is None:\n        self._config = self.create_config(\n            data_store=self.data_store, logger=self.logger\n        )\n\n    # Initialize or create downloader\n    self._downloader = downloader\n    if self._downloader is None:\n        self._downloader = self.create_downloader(\n            config=self._config, data_store=self.data_store, logger=self.logger\n        )\n\n    # Initialize or create reader\n    self._reader = reader\n    if self._reader is None:\n        self._reader = self.create_reader(\n            config=self._config, data_store=self.data_store, logger=self.logger\n        )\n</code></pre>"},{"location":"api/handlers/#gigaspatial.handlers.base.BaseHandler.__repr__","title":"<code>__repr__()</code>","text":"<p>String representation of the handler.</p> Source code in <code>gigaspatial/handlers/base.py</code> <pre><code>def __repr__(self) -&gt; str:\n    \"\"\"String representation of the handler.\"\"\"\n    return (\n        f\"{self.__class__.__name__}(\"\n        f\"config={self.config.__class__.__name__}, \"\n        f\"downloader={self.downloader.__class__.__name__}, \"\n        f\"reader={self.reader.__class__.__name__})\"\n    )\n</code></pre>"},{"location":"api/handlers/#gigaspatial.handlers.base.BaseHandler.cleanup","title":"<code>cleanup()</code>","text":"<p>Cleanup resources used by the handler.</p> <p>Override in subclasses if specific cleanup is needed.</p> Source code in <code>gigaspatial/handlers/base.py</code> <pre><code>def cleanup(self):\n    \"\"\"\n    Cleanup resources used by the handler.\n\n    Override in subclasses if specific cleanup is needed.\n    \"\"\"\n    self.logger.info(f\"Cleaning up {self.__class__.__name__}\")\n</code></pre>"},{"location":"api/handlers/#gigaspatial.handlers.base.BaseHandler.create_config","title":"<code>create_config(data_store, logger, **kwargs)</code>  <code>abstractmethod</code>","text":"<p>Create and return a configuration object for this handler.</p> <p>Parameters:</p> Name Type Description Default <code>data_store</code> <code>DataStore</code> <p>The data store instance to use</p> required <code>logger</code> <code>Logger</code> <p>The logger instance to use</p> required <code>**kwargs</code> <p>Additional configuration parameters</p> <code>{}</code> <p>Returns:</p> Type Description <code>BaseHandlerConfig</code> <p>Configured BaseHandlerConfig instance</p> Source code in <code>gigaspatial/handlers/base.py</code> <pre><code>@abstractmethod\ndef create_config(\n    self, data_store: DataStore, logger: logging.Logger, **kwargs\n) -&gt; BaseHandlerConfig:\n    \"\"\"\n    Create and return a configuration object for this handler.\n\n    Args:\n        data_store: The data store instance to use\n        logger: The logger instance to use\n        **kwargs: Additional configuration parameters\n\n    Returns:\n        Configured BaseHandlerConfig instance\n    \"\"\"\n    pass\n</code></pre>"},{"location":"api/handlers/#gigaspatial.handlers.base.BaseHandler.create_downloader","title":"<code>create_downloader(config, data_store, logger, **kwargs)</code>  <code>abstractmethod</code>","text":"<p>Create and return a downloader object for this handler.</p> <p>Parameters:</p> Name Type Description Default <code>config</code> <code>BaseHandlerConfig</code> <p>The configuration object</p> required <code>data_store</code> <code>DataStore</code> <p>The data store instance to use</p> required <code>logger</code> <code>Logger</code> <p>The logger instance to use</p> required <code>**kwargs</code> <p>Additional downloader parameters</p> <code>{}</code> <p>Returns:</p> Type Description <code>BaseHandlerDownloader</code> <p>Configured BaseHandlerDownloader instance</p> Source code in <code>gigaspatial/handlers/base.py</code> <pre><code>@abstractmethod\ndef create_downloader(\n    self,\n    config: BaseHandlerConfig,\n    data_store: DataStore,\n    logger: logging.Logger,\n    **kwargs,\n) -&gt; BaseHandlerDownloader:\n    \"\"\"\n    Create and return a downloader object for this handler.\n\n    Args:\n        config: The configuration object\n        data_store: The data store instance to use\n        logger: The logger instance to use\n        **kwargs: Additional downloader parameters\n\n    Returns:\n        Configured BaseHandlerDownloader instance\n    \"\"\"\n    pass\n</code></pre>"},{"location":"api/handlers/#gigaspatial.handlers.base.BaseHandler.create_reader","title":"<code>create_reader(config, data_store, logger, **kwargs)</code>  <code>abstractmethod</code>","text":"<p>Create and return a reader object for this handler.</p> <p>Parameters:</p> Name Type Description Default <code>config</code> <code>BaseHandlerConfig</code> <p>The configuration object</p> required <code>data_store</code> <code>DataStore</code> <p>The data store instance to use</p> required <code>logger</code> <code>Logger</code> <p>The logger instance to use</p> required <code>**kwargs</code> <p>Additional reader parameters</p> <code>{}</code> <p>Returns:</p> Type Description <code>BaseHandlerReader</code> <p>Configured BaseHandlerReader instance</p> Source code in <code>gigaspatial/handlers/base.py</code> <pre><code>@abstractmethod\ndef create_reader(\n    self,\n    config: BaseHandlerConfig,\n    data_store: DataStore,\n    logger: logging.Logger,\n    **kwargs,\n) -&gt; BaseHandlerReader:\n    \"\"\"\n    Create and return a reader object for this handler.\n\n    Args:\n        config: The configuration object\n        data_store: The data store instance to use\n        logger: The logger instance to use\n        **kwargs: Additional reader parameters\n\n    Returns:\n        Configured BaseHandlerReader instance\n    \"\"\"\n    pass\n</code></pre>"},{"location":"api/handlers/#gigaspatial.handlers.base.BaseHandler.download_and_load","title":"<code>download_and_load(source, crop_to_source=False, force_download=False, **kwargs)</code>","text":"<p>Convenience method to download (if needed) and load data in one call.</p> <p>Parameters:</p> Name Type Description Default <code>source</code> <code>Union[str, List[Union[tuple, Point]], BaseGeometry, GeoDataFrame, Path, List[Union[str, Path]]]</code> <p>The data source specification</p> required <code>force_download</code> <code>bool</code> <p>If True, download even if data exists locally</p> <code>False</code> <code>**kwargs</code> <p>Additional parameters</p> <code>{}</code> <p>Returns:</p> Type Description <code>Any</code> <p>Loaded data</p> Source code in <code>gigaspatial/handlers/base.py</code> <pre><code>def download_and_load(\n    self,\n    source: Union[\n        str,  # country\n        List[Union[tuple, Point]],  # points\n        BaseGeometry,  # geometry\n        gpd.GeoDataFrame,  # geodataframe\n        Path,  # path\n        List[Union[str, Path]],  # list of paths\n    ],\n    crop_to_source: bool = False,\n    force_download: bool = False,\n    **kwargs,\n) -&gt; Any:\n    \"\"\"\n    Convenience method to download (if needed) and load data in one call.\n\n    Args:\n        source: The data source specification\n        force_download: If True, download even if data exists locally\n        **kwargs: Additional parameters\n\n    Returns:\n        Loaded data\n    \"\"\"\n    self.ensure_data_available(source, force_download=force_download, **kwargs)\n    return self.reader.load(source, crop_to_source=crop_to_source, **kwargs)\n</code></pre>"},{"location":"api/handlers/#gigaspatial.handlers.base.BaseHandler.ensure_data_available","title":"<code>ensure_data_available(source, force_download=False, **kwargs)</code>","text":"<p>Ensure that data is available for the given source.</p> <p>This method checks if the required data exists locally, and if not (or if force_download is True), downloads it using the downloader.</p> <p>Parameters:</p> Name Type Description Default <code>source</code> <code>Union[str, List[Union[tuple, Point]], BaseGeometry, GeoDataFrame, Path, List[Union[str, Path]]]</code> <p>The data source specification</p> required <code>force_download</code> <code>bool</code> <p>If True, download even if data exists locally</p> <code>False</code> <code>**kwargs</code> <p>Additional parameters passed to download methods</p> <code>{}</code> <p>Returns:</p> Name Type Description <code>bool</code> <code>bool</code> <p>True if data is available after this operation</p> Source code in <code>gigaspatial/handlers/base.py</code> <pre><code>def ensure_data_available(\n    self,\n    source: Union[\n        str,  # country\n        List[Union[tuple, Point]],  # points\n        BaseGeometry,  # geometry\n        gpd.GeoDataFrame,  # geodataframe\n        Path,  # path\n        List[Union[str, Path]],  # list of paths\n    ],\n    force_download: bool = False,\n    **kwargs,\n) -&gt; bool:\n    \"\"\"\n    Ensure that data is available for the given source.\n\n    This method checks if the required data exists locally, and if not (or if\n    force_download is True), downloads it using the downloader.\n\n    Args:\n        source: The data source specification\n        force_download: If True, download even if data exists locally\n        **kwargs: Additional parameters passed to download methods\n\n    Returns:\n        bool: True if data is available after this operation\n    \"\"\"\n    try:\n        # Get relevant units (cached if already computed for this source)\n        data_units = self.config.get_relevant_data_units(\n            source, force_recompute=force_download, **kwargs\n        )\n        data_paths = self.config.get_data_unit_paths(data_units, **kwargs)\n\n        # Check if data exists (unless force download)\n        if not force_download:\n            missing_paths = [\n                path\n                for path in data_paths\n                if not self.data_store.file_exists(str(path))\n            ]\n        else:\n            # If force_download, treat all as missing\n            missing_paths = data_paths\n\n        if not missing_paths:\n            self.logger.info(\"All required data is already available\")\n            return True\n\n        # Map units to paths (assumes correspondence order; adapt if needed)\n        path_to_unit = dict(zip(data_paths, data_units))\n        if force_download:\n            units_to_download = data_units\n        else:\n            units_to_download = [\n                path_to_unit[p] for p in missing_paths if p in path_to_unit\n            ]\n\n        if units_to_download:\n            self.downloader.download_data_units(units_to_download, **kwargs)\n        else:\n            # Fallback - download by source if unit mapping isn't available\n            self.downloader.download(source, **kwargs)\n\n        # After attempted download, check again\n        remaining_missing = [\n            path\n            for path in data_paths\n            if not self.data_store.file_exists(str(path))\n        ]\n        if remaining_missing:\n            self.logger.error(\n                f\"Some data still missing after download: {remaining_missing}\"\n            )\n            return False\n\n        return True\n    except Exception as e:\n        self.logger.error(f\"Failed to ensure data availability: {e}\")\n        return False\n</code></pre>"},{"location":"api/handlers/#gigaspatial.handlers.base.BaseHandler.get_available_data_info","title":"<code>get_available_data_info(source, **kwargs)</code>","text":"<p>Get information about available data for the given source.</p> <p>Parameters:</p> Name Type Description Default <code>source</code> <code>Union[str, List[Union[tuple, Point]], BaseGeometry, GeoDataFrame]</code> <p>The data source specification</p> required <code>**kwargs</code> <p>Additional parameters</p> <code>{}</code> <p>Returns:</p> Name Type Description <code>dict</code> <code>dict</code> <p>Information about data availability, paths, etc.</p> Source code in <code>gigaspatial/handlers/base.py</code> <pre><code>def get_available_data_info(\n    self,\n    source: Union[\n        str,  # country\n        List[Union[tuple, Point]],  # points\n        BaseGeometry,  # geometry\n        gpd.GeoDataFrame,  # geodataframe\n    ],\n    **kwargs,\n) -&gt; dict:\n    \"\"\"\n    Get information about available data for the given source.\n\n    Args:\n        source: The data source specification\n        **kwargs: Additional parameters\n\n    Returns:\n        dict: Information about data availability, paths, etc.\n    \"\"\"\n    try:\n        if hasattr(self.config, \"get_relevant_data_units\"):\n            data_units = self.config.get_relevant_data_units(source, **kwargs)\n            data_paths = self.config.get_data_unit_paths(data_units, **kwargs)\n        else:\n            data_paths = self.reader.resolve_source_paths(source, **kwargs)\n\n        existing_paths = [\n            path for path in data_paths if self.data_store.file_exists(str(path))\n        ]\n        missing_paths = [\n            path\n            for path in data_paths\n            if not self.data_store.file_exists(str(path))\n        ]\n\n        return {\n            \"total_data_units\": len(data_paths),\n            \"available_data_units\": len(existing_paths),\n            \"missing_data_units\": len(missing_paths),\n            \"available_paths\": existing_paths,\n            \"missing_paths\": missing_paths,\n            \"all_available\": len(missing_paths) == 0,\n        }\n\n    except Exception as e:\n        self.logger.error(f\"Failed to get data info: {e}\")\n        return {\n            \"error\": str(e),\n            \"total_data_units\": 0,\n            \"available_data_units\": 0,\n            \"missing_data_units\": 0,\n            \"available_paths\": [],\n            \"missing_paths\": [],\n            \"all_available\": False,\n        }\n</code></pre>"},{"location":"api/handlers/#gigaspatial.handlers.base.BaseHandler.load_data","title":"<code>load_data(source, crop_to_source=False, ensure_available=True, **kwargs)</code>","text":"<p>Load data from the given source.</p> <p>Parameters:</p> Name Type Description Default <code>source</code> <code>Union[str, List[Union[tuple, Point]], BaseGeometry, GeoDataFrame, Path, List[Union[str, Path]]]</code> <p>The data source specification</p> required <code>ensure_available</code> <code>bool</code> <p>If True, ensure data is downloaded before loading</p> <code>True</code> <code>**kwargs</code> <p>Additional parameters passed to load methods</p> <code>{}</code> <p>Returns:</p> Type Description <code>Any</code> <p>Loaded data (type depends on specific handler implementation)</p> Source code in <code>gigaspatial/handlers/base.py</code> <pre><code>def load_data(\n    self,\n    source: Union[\n        str,  # country\n        List[Union[tuple, Point]],  # points\n        BaseGeometry,  # geometry\n        gpd.GeoDataFrame,  # geodataframe\n        Path,  # path\n        List[Union[str, Path]],  # list of paths\n    ],\n    crop_to_source: bool = False,\n    ensure_available: bool = True,\n    **kwargs,\n) -&gt; Any:\n    \"\"\"\n    Load data from the given source.\n\n    Args:\n        source: The data source specification\n        ensure_available: If True, ensure data is downloaded before loading\n        **kwargs: Additional parameters passed to load methods\n\n    Returns:\n        Loaded data (type depends on specific handler implementation)\n    \"\"\"\n    if ensure_available:\n        if not self.ensure_data_available(source, **kwargs):\n            raise RuntimeError(\"Could not ensure data availability for loading\")\n\n    return self.reader.load(source, crop_to_source=crop_to_source, **kwargs)\n</code></pre>"},{"location":"api/handlers/#gigaspatial.handlers.base.BaseHandlerConfig","title":"<code>BaseHandlerConfig</code>  <code>dataclass</code>","text":"<p>               Bases: <code>ABC</code></p> <p>Abstract base class for handler configuration objects. Provides standard fields for path, parallelism, data store, and logger. Extend this class for dataset-specific configuration.</p> Source code in <code>gigaspatial/handlers/base.py</code> <pre><code>@dataclass\nclass BaseHandlerConfig(ABC):\n    \"\"\"\n    Abstract base class for handler configuration objects.\n    Provides standard fields for path, parallelism, data store, and logger.\n    Extend this class for dataset-specific configuration.\n    \"\"\"\n\n    base_path: Path = None\n    n_workers: int = multiprocessing.cpu_count()\n    data_store: DataStore = field(default_factory=LocalDataStore)\n    logger: logging.Logger = field(default=None, repr=False)\n\n    def __post_init__(self):\n        if self.logger is None:\n            self.logger = global_config.get_logger(self.__class__.__name__)\n\n        self._unit_cache = {}\n\n    def _cache_key(self, source, **kwargs):\n        \"\"\"Create a canonical cache key from source.\"\"\"\n        if isinstance(source, str):\n            return (\"country\", source)\n        if isinstance(source, BaseGeometry):\n            return (\"geometry\", source.wkt)\n        if isinstance(source, gpd.GeoDataFrame):\n            return (\"geometry\", str(source.geometry.unary_union.wkt))\n        if isinstance(source, Iterable) and all(\n            isinstance(p, (Point, tuple)) for p in source\n        ):\n            pt_str = tuple(\n                (p.x, p.y) if isinstance(p, Point) else tuple(p) for p in source\n            )\n            return (\"points\", pt_str)\n        return (\"other\", str(source))\n\n    def get_relevant_data_units(\n        self,\n        source: Union[\n            str,  # country\n            List[Union[Tuple[float, float], Point]],  # points\n            BaseGeometry,  # geometry\n            gpd.GeoDataFrame,  # geodataframe\n        ],\n        force_recompute: bool = False,\n        **kwargs,\n    ):\n        key = self._cache_key(source, **kwargs)\n\n        # Check cache unless forced recompute\n        if not force_recompute and key in self._unit_cache:\n            self.logger.debug(f\"Using cached units for {key[0]}: {key[1][:50]}...\")\n            units, _ = self._unit_cache[key]  # Unpack tuple, only return units\n            return units\n\n        # Convert source to geometry and compute units\n        geometry = self.extract_search_geometry(source, **kwargs)\n        units = self.get_relevant_data_units_by_geometry(geometry, **kwargs)\n\n        # Cache both units and geometry as tuple\n        self._unit_cache[key] = (units, geometry)\n        return units\n\n    @abstractmethod\n    def get_relevant_data_units_by_geometry(\n        self, geometry: Union[BaseGeometry, gpd.GeoDataFrame], **kwargs\n    ) -&gt; Any:\n        \"\"\"\n        Given a geometry, return a list of relevant data unit identifiers (e.g., tiles, files, resources).\n        \"\"\"\n        pass\n\n    @abstractmethod\n    def get_data_unit_path(self, unit: Any, **kwargs) -&gt; list:\n        \"\"\"\n        Given a data unit identifier, return the corresponding file path.\n        \"\"\"\n        pass\n\n    def get_data_unit_paths(self, units: Union[Iterable[Any]], **kwargs) -&gt; list:\n        \"\"\"\n        Given data unit identifiers, return the corresponding file paths.\n        \"\"\"\n        if not isinstance(units, Iterable):\n            units = [units]\n\n        if not units:\n            return []\n\n        return [self.get_data_unit_path(unit=unit, **kwargs) for unit in units]\n\n    def extract_search_geometry(self, source, **kwargs):\n        \"\"\"General method to extract a canonical geometry from supported source types.\"\"\"\n        if isinstance(source, str):\n            # Use the admin boundary as geometry\n            from gigaspatial.handlers.boundaries import AdminBoundaries\n\n            return (\n                AdminBoundaries.create(country_code=source, **kwargs)\n                .boundaries[0]\n                .geometry\n            )\n        elif isinstance(source, gpd.GeoDataFrame):\n            if crs := kwargs.get(\"crs\", None):\n\n                if not source.crs:\n                    raise ValueError(\n                        \"Cannot extract search geometry. Please set a crs on the source object first.\"\n                    )\n\n                if source.crs != crs:\n                    source = source.to_crs(crs)\n\n            return source.geometry.union_all()\n        elif isinstance(\n            source,\n            BaseGeometry,\n        ):\n            return source\n        elif isinstance(source, Iterable) and all(\n            isinstance(p, (Point, Iterable)) for p in source\n        ):\n            points = [p if isinstance(p, Point) else Point(p[1], p[0]) for p in source]\n            return MultiPoint(points)\n        else:\n            raise ValueError(f\"Unsupported source type: {type(source)}\")\n\n    def get_cached_search_geometry(self, source):\n        key = self._cache_key(source)\n        result = self._unit_cache.get(key)\n        if result:\n            _, geometry = result\n            return geometry\n        return None\n\n    def clear_unit_cache(self):\n        \"\"\"Clear cached units.\"\"\"\n        self._unit_cache.clear()\n        self.logger.debug(\"Unit cache cleared\")\n</code></pre>"},{"location":"api/handlers/#gigaspatial.handlers.base.BaseHandlerConfig.clear_unit_cache","title":"<code>clear_unit_cache()</code>","text":"<p>Clear cached units.</p> Source code in <code>gigaspatial/handlers/base.py</code> <pre><code>def clear_unit_cache(self):\n    \"\"\"Clear cached units.\"\"\"\n    self._unit_cache.clear()\n    self.logger.debug(\"Unit cache cleared\")\n</code></pre>"},{"location":"api/handlers/#gigaspatial.handlers.base.BaseHandlerConfig.extract_search_geometry","title":"<code>extract_search_geometry(source, **kwargs)</code>","text":"<p>General method to extract a canonical geometry from supported source types.</p> Source code in <code>gigaspatial/handlers/base.py</code> <pre><code>def extract_search_geometry(self, source, **kwargs):\n    \"\"\"General method to extract a canonical geometry from supported source types.\"\"\"\n    if isinstance(source, str):\n        # Use the admin boundary as geometry\n        from gigaspatial.handlers.boundaries import AdminBoundaries\n\n        return (\n            AdminBoundaries.create(country_code=source, **kwargs)\n            .boundaries[0]\n            .geometry\n        )\n    elif isinstance(source, gpd.GeoDataFrame):\n        if crs := kwargs.get(\"crs\", None):\n\n            if not source.crs:\n                raise ValueError(\n                    \"Cannot extract search geometry. Please set a crs on the source object first.\"\n                )\n\n            if source.crs != crs:\n                source = source.to_crs(crs)\n\n        return source.geometry.union_all()\n    elif isinstance(\n        source,\n        BaseGeometry,\n    ):\n        return source\n    elif isinstance(source, Iterable) and all(\n        isinstance(p, (Point, Iterable)) for p in source\n    ):\n        points = [p if isinstance(p, Point) else Point(p[1], p[0]) for p in source]\n        return MultiPoint(points)\n    else:\n        raise ValueError(f\"Unsupported source type: {type(source)}\")\n</code></pre>"},{"location":"api/handlers/#gigaspatial.handlers.base.BaseHandlerConfig.get_data_unit_path","title":"<code>get_data_unit_path(unit, **kwargs)</code>  <code>abstractmethod</code>","text":"<p>Given a data unit identifier, return the corresponding file path.</p> Source code in <code>gigaspatial/handlers/base.py</code> <pre><code>@abstractmethod\ndef get_data_unit_path(self, unit: Any, **kwargs) -&gt; list:\n    \"\"\"\n    Given a data unit identifier, return the corresponding file path.\n    \"\"\"\n    pass\n</code></pre>"},{"location":"api/handlers/#gigaspatial.handlers.base.BaseHandlerConfig.get_data_unit_paths","title":"<code>get_data_unit_paths(units, **kwargs)</code>","text":"<p>Given data unit identifiers, return the corresponding file paths.</p> Source code in <code>gigaspatial/handlers/base.py</code> <pre><code>def get_data_unit_paths(self, units: Union[Iterable[Any]], **kwargs) -&gt; list:\n    \"\"\"\n    Given data unit identifiers, return the corresponding file paths.\n    \"\"\"\n    if not isinstance(units, Iterable):\n        units = [units]\n\n    if not units:\n        return []\n\n    return [self.get_data_unit_path(unit=unit, **kwargs) for unit in units]\n</code></pre>"},{"location":"api/handlers/#gigaspatial.handlers.base.BaseHandlerConfig.get_relevant_data_units_by_geometry","title":"<code>get_relevant_data_units_by_geometry(geometry, **kwargs)</code>  <code>abstractmethod</code>","text":"<p>Given a geometry, return a list of relevant data unit identifiers (e.g., tiles, files, resources).</p> Source code in <code>gigaspatial/handlers/base.py</code> <pre><code>@abstractmethod\ndef get_relevant_data_units_by_geometry(\n    self, geometry: Union[BaseGeometry, gpd.GeoDataFrame], **kwargs\n) -&gt; Any:\n    \"\"\"\n    Given a geometry, return a list of relevant data unit identifiers (e.g., tiles, files, resources).\n    \"\"\"\n    pass\n</code></pre>"},{"location":"api/handlers/#gigaspatial.handlers.base.BaseHandlerDownloader","title":"<code>BaseHandlerDownloader</code>","text":"<p>               Bases: <code>ABC</code></p> <p>Abstract base class for handler downloader classes. Standardizes config, data_store, and logger initialization. Extend this class for dataset-specific downloaders.</p> Source code in <code>gigaspatial/handlers/base.py</code> <pre><code>class BaseHandlerDownloader(ABC):\n    \"\"\"\n    Abstract base class for handler downloader classes.\n    Standardizes config, data_store, and logger initialization.\n    Extend this class for dataset-specific downloaders.\n    \"\"\"\n\n    def __init__(\n        self,\n        config: Optional[BaseHandlerConfig] = None,\n        data_store: Optional[DataStore] = None,\n        logger: Optional[logging.Logger] = None,\n    ):\n        self.config = config\n        if data_store:\n            self.data_store = data_store\n        elif config and hasattr(config, \"data_store\"):\n            self.data_store = config.data_store\n        else:\n            self.data_store = LocalDataStore()\n\n        self.logger = (\n            logger\n            or (getattr(config, \"logger\", None) if config else None)\n            or global_config.get_logger(self.__class__.__name__)\n        )\n\n    @abstractmethod\n    def download_data_unit(self, *args, **kwargs):\n        \"\"\"\n        Abstract method to download data. Implement in subclasses.\n        \"\"\"\n        pass\n\n    @abstractmethod\n    def download_data_units(self, *args, **kwargs):\n        \"\"\"\n        Abstract method to download data. Implement in subclasses.\n        \"\"\"\n        pass\n\n    def download(self, source, **kwargs):\n        \"\"\"\n        Given source download the data.\n        \"\"\"\n        units = self.config.get_relevant_data_units(source, **kwargs)\n        return self.download_data_units(units, **kwargs)\n</code></pre>"},{"location":"api/handlers/#gigaspatial.handlers.base.BaseHandlerDownloader.download","title":"<code>download(source, **kwargs)</code>","text":"<p>Given source download the data.</p> Source code in <code>gigaspatial/handlers/base.py</code> <pre><code>def download(self, source, **kwargs):\n    \"\"\"\n    Given source download the data.\n    \"\"\"\n    units = self.config.get_relevant_data_units(source, **kwargs)\n    return self.download_data_units(units, **kwargs)\n</code></pre>"},{"location":"api/handlers/#gigaspatial.handlers.base.BaseHandlerDownloader.download_data_unit","title":"<code>download_data_unit(*args, **kwargs)</code>  <code>abstractmethod</code>","text":"<p>Abstract method to download data. Implement in subclasses.</p> Source code in <code>gigaspatial/handlers/base.py</code> <pre><code>@abstractmethod\ndef download_data_unit(self, *args, **kwargs):\n    \"\"\"\n    Abstract method to download data. Implement in subclasses.\n    \"\"\"\n    pass\n</code></pre>"},{"location":"api/handlers/#gigaspatial.handlers.base.BaseHandlerDownloader.download_data_units","title":"<code>download_data_units(*args, **kwargs)</code>  <code>abstractmethod</code>","text":"<p>Abstract method to download data. Implement in subclasses.</p> Source code in <code>gigaspatial/handlers/base.py</code> <pre><code>@abstractmethod\ndef download_data_units(self, *args, **kwargs):\n    \"\"\"\n    Abstract method to download data. Implement in subclasses.\n    \"\"\"\n    pass\n</code></pre>"},{"location":"api/handlers/#gigaspatial.handlers.base.BaseHandlerReader","title":"<code>BaseHandlerReader</code>","text":"<p>               Bases: <code>ABC</code></p> <p>Abstract base class for handler reader classes. Provides common methods for resolving source paths and loading data. Supports resolving by country, points, geometry, GeoDataFrame, or explicit paths. Includes generic loader functions for raster and tabular data.</p> Source code in <code>gigaspatial/handlers/base.py</code> <pre><code>class BaseHandlerReader(ABC):\n    \"\"\"\n    Abstract base class for handler reader classes.\n    Provides common methods for resolving source paths and loading data.\n    Supports resolving by country, points, geometry, GeoDataFrame, or explicit paths.\n    Includes generic loader functions for raster and tabular data.\n    \"\"\"\n\n    def __init__(\n        self,\n        config: Optional[BaseHandlerConfig] = None,\n        data_store: Optional[DataStore] = None,\n        logger: Optional[logging.Logger] = None,\n    ):\n        self.config = config\n        if data_store:\n            self.data_store = data_store\n        elif config and hasattr(config, \"data_store\"):\n            self.data_store = config.data_store\n        else:\n            self.data_store = LocalDataStore()\n\n        self.logger = (\n            logger\n            or (getattr(config, \"logger\", None) if config else None)\n            or global_config.get_logger(self.__class__.__name__)\n        )\n\n    def resolve_source_paths(\n        self,\n        source: Union[\n            str,  # country code\n            List[Union[Tuple[float, float], Point]],  # points\n            BaseGeometry,\n            gpd.GeoDataFrame,\n            Path,  # path\n            str,  # path\n            List[Union[str, Path]],\n        ],\n        **kwargs,\n    ) -&gt; List[Union[str, Path]]:\n        \"\"\"\n        Resolve source data paths based on the type of source input.\n\n        Args:\n            source: Can be a country code or name (str), list of points, geometry, GeoDataFrame, or explicit path(s)\n            **kwargs: Additional parameters for path resolution\n\n        Returns:\n            List of resolved source paths\n        \"\"\"\n        if (\n            isinstance(source, Path)\n            or (\n                isinstance(source, (list, tuple, set))\n                and all(isinstance(p, (str, Path)) for p in source)\n            )\n            or (isinstance(source, str) and \".\" in source)\n        ):\n            return self.resolve_by_paths(source)\n\n        data_units = self.config.get_relevant_data_units(source, **kwargs)\n        data_paths = self.config.get_data_unit_paths(data_units, **kwargs)\n\n        self.logger.info(f\"Resolved {len(data_paths)} paths!\")\n        return data_paths\n\n    def resolve_by_paths(\n        self, paths: Union[Path, str, List[Union[str, Path]]], **kwargs\n    ) -&gt; List[Union[str, Path]]:\n        \"\"\"\n        Return explicit paths as a list.\n        \"\"\"\n        if isinstance(paths, (str, Path)):\n            return [paths]\n        return list(paths)\n\n    def _pre_load_hook(self, source_data_path, **kwargs) -&gt; Any:\n        \"\"\"Hook called before loading data.\"\"\"\n        if isinstance(source_data_path, (Path, str)):\n            source_data_path = [source_data_path]\n\n        if not source_data_path:\n            self.logger.warning(\"No paths found!\")\n            return []\n\n        source_data_paths = [str(file_path) for file_path in source_data_path]\n\n        self.logger.info(\n            f\"Pre-loading validation complete for {len(source_data_path)} files\"\n        )\n        return source_data_paths\n\n    def _post_load_hook(self, data, **kwargs) -&gt; Any:\n        \"\"\"Hook called after loading data.\"\"\"\n        if isinstance(data, Iterable):\n            if len(data) == 0:\n                self.logger.warning(\"No data was loaded from the source files\")\n                return data\n\n            self.logger.info(f\"{len(data)} valid data records.\")\n\n        self.logger.info(f\"Post-load processing complete.\")\n\n        return data\n\n    def _check_file_exists(self, file_paths: List[Union[str, Path]]):\n        \"\"\"\n        Check that all specified files exist in the data store.\n\n        Args:\n            file_paths (List[Union[str, Path]]): List of file paths to check.\n\n        Raises:\n            RuntimeError: If any file does not exist in the data store.\n        \"\"\"\n        for file_path in file_paths:\n            if not self.data_store.file_exists(str(file_path)):\n                raise RuntimeError(\n                    f\"Source file does not exist in the data store: {file_path}\"\n                )\n\n    def _load_raster_data(\n        self,\n        raster_paths: List[Union[str, Path]],\n        merge_rasters: bool = False,\n        **kwargs,\n    ) -&gt; Union[List[TifProcessor], TifProcessor]:\n        \"\"\"\n        Load raster data from file paths.\n\n        Args:\n            raster_paths (List[Union[str, Path]]): List of file paths to raster files.\n            merge_rasters (bool): If True, all rasters will be merged into a single TifProcessor.\n                                  Defaults to False.\n\n        Returns:\n            Union[List[TifProcessor], TifProcessor]: List of TifProcessor objects or a single\n                                                    TifProcessor if merge_rasters is True.\n        \"\"\"\n        if merge_rasters and len(raster_paths) &gt; 1:\n            self.logger.info(\n                f\"Merging {len(raster_paths)} rasters into a single TifProcessor.\"\n            )\n            return TifProcessor(raster_paths, self.data_store, **kwargs)\n        else:\n            return [\n                TifProcessor(data_path, self.data_store, **kwargs)\n                for data_path in raster_paths\n            ]\n\n    def _load_tabular_data(\n        self, file_paths: List[Union[str, Path]], read_function: Callable = read_dataset\n    ) -&gt; Union[pd.DataFrame, gpd.GeoDataFrame]:\n        \"\"\"\n        Load and concatenate tabular data from multiple files.\n\n        Args:\n            file_paths (List[Union[str, Path]]): List of file paths to load data from.\n            read_function (Callable): Function to use for reading individual files.\n                Defaults to read_dataset. Should accept (data_store, file_path) arguments.\n\n        Returns:\n            Union[pd.DataFrame, gpd.GeoDataFrame]: Concatenated data from all files.\n                Returns empty DataFrame if no data is loaded.\n        \"\"\"\n        all_data = []\n        for file_path in file_paths:\n            all_data.append(read_function(self.data_store, file_path))\n        if not all_data:\n            return pd.DataFrame()\n        result = pd.concat(all_data, ignore_index=True)\n        return result\n\n    def crop_to_geometry(self, data, geometry, predicate=\"intersects\", **kwargs):\n\n        # Project geometry to the projection of the data if data has projection\n        geom_crs = kwargs.get(\"crs\", \"EPSG:4326\")\n        if hasattr(data, \"crs\") and data.crs != geom_crs:\n            geometry = (\n                gpd.GeoDataFrame(geometry=[geometry], crs=\"EPSG:4326\")\n                .to_crs(data.crs)\n                .geometry[0]\n            )\n\n        # Tabular (GeoDataFrame) case\n        if isinstance(data, (pd.DataFrame, gpd.GeoDataFrame)):\n            if isinstance(data, pd.DataFrame):\n                from gigaspatial.processing.geo import convert_to_geodataframe\n\n                try:\n                    data = convert_to_geodataframe(data, **kwargs)\n                except:\n                    return data\n\n            # Clip to geometry\n            return data[getattr(data.geometry, predicate)(geometry)]\n\n        # Raster case\n        if isinstance(data, TifProcessor):\n            return data.clip_to_geometry(geometry=geometry, **kwargs)\n\n        return data\n\n    @abstractmethod\n    def load_from_paths(\n        self, source_data_path: List[Union[str, Path]], **kwargs\n    ) -&gt; Any:\n        \"\"\"\n        Abstract method to load source data from paths.\n\n        Args:\n            source_data_path: List of source paths\n            **kwargs: Additional parameters for data loading\n\n        Returns:\n            Loaded data (DataFrame, GeoDataFrame, etc.)\n        \"\"\"\n        pass\n\n    def load(\n        self,\n        source: Union[\n            str,  # country\n            List[Union[Tuple[float, float], Point]],  # points\n            BaseGeometry,\n            gpd.GeoDataFrame,\n            Path,\n            str,\n            List[Union[str, Path]],\n        ],\n        crop_to_source: bool = False,\n        **kwargs,\n    ) -&gt; Any:\n        \"\"\"\n        Load data from the given source.\n\n        Args:\n            source: The data source (country code/name, points, geometry, paths, etc.).\n            crop_to_source : bool, default False\n                If True, crop loaded data to the exact source geometry\n            **kwargs: Additional parameters to pass to the loading process.\n\n        Returns:\n            The loaded data. The type depends on the subclass implementation.\n        \"\"\"\n        source_data_paths = self.resolve_source_paths(source, **kwargs)\n        if not source_data_paths:\n            self.logger.warning(\n                \"No source data paths resolved. There's no matching data to load!\"\n            )\n            return None\n        processed_paths = self._pre_load_hook(source_data_paths, **kwargs)\n        if not processed_paths:\n            self.logger.warning(\"No valid paths to load data from.\")\n            return None\n\n        loaded_data = self.load_from_paths(processed_paths, **kwargs)\n        loaded_data = self._post_load_hook(loaded_data, **kwargs)\n\n        # Apply cropping if requested\n        if crop_to_source and loaded_data is not None:\n            search_geometry = self.config.get_cached_search_geometry(source)\n            if search_geometry is not None and isinstance(\n                search_geometry, BaseGeometry\n            ):\n                loaded_data = self.crop_to_geometry(loaded_data, search_geometry)\n            else:\n                # If no cached geometry, compute it\n                search_geometry = self.config.extract_search_geometry(source, **kwargs)\n                if isinstance(search_geometry, BaseGeometry):\n                    loaded_data = self.crop_to_geometry(loaded_data, search_geometry)\n\n        return loaded_data\n</code></pre>"},{"location":"api/handlers/#gigaspatial.handlers.base.BaseHandlerReader.load","title":"<code>load(source, crop_to_source=False, **kwargs)</code>","text":"<p>Load data from the given source.</p> <p>Parameters:</p> Name Type Description Default <code>source</code> <code>Union[str, List[Union[Tuple[float, float], Point]], BaseGeometry, GeoDataFrame, Path, str, List[Union[str, Path]]]</code> <p>The data source (country code/name, points, geometry, paths, etc.).</p> required <code>crop_to_source</code> <p>bool, default False If True, crop loaded data to the exact source geometry</p> <code>False</code> <code>**kwargs</code> <p>Additional parameters to pass to the loading process.</p> <code>{}</code> <p>Returns:</p> Type Description <code>Any</code> <p>The loaded data. The type depends on the subclass implementation.</p> Source code in <code>gigaspatial/handlers/base.py</code> <pre><code>def load(\n    self,\n    source: Union[\n        str,  # country\n        List[Union[Tuple[float, float], Point]],  # points\n        BaseGeometry,\n        gpd.GeoDataFrame,\n        Path,\n        str,\n        List[Union[str, Path]],\n    ],\n    crop_to_source: bool = False,\n    **kwargs,\n) -&gt; Any:\n    \"\"\"\n    Load data from the given source.\n\n    Args:\n        source: The data source (country code/name, points, geometry, paths, etc.).\n        crop_to_source : bool, default False\n            If True, crop loaded data to the exact source geometry\n        **kwargs: Additional parameters to pass to the loading process.\n\n    Returns:\n        The loaded data. The type depends on the subclass implementation.\n    \"\"\"\n    source_data_paths = self.resolve_source_paths(source, **kwargs)\n    if not source_data_paths:\n        self.logger.warning(\n            \"No source data paths resolved. There's no matching data to load!\"\n        )\n        return None\n    processed_paths = self._pre_load_hook(source_data_paths, **kwargs)\n    if not processed_paths:\n        self.logger.warning(\"No valid paths to load data from.\")\n        return None\n\n    loaded_data = self.load_from_paths(processed_paths, **kwargs)\n    loaded_data = self._post_load_hook(loaded_data, **kwargs)\n\n    # Apply cropping if requested\n    if crop_to_source and loaded_data is not None:\n        search_geometry = self.config.get_cached_search_geometry(source)\n        if search_geometry is not None and isinstance(\n            search_geometry, BaseGeometry\n        ):\n            loaded_data = self.crop_to_geometry(loaded_data, search_geometry)\n        else:\n            # If no cached geometry, compute it\n            search_geometry = self.config.extract_search_geometry(source, **kwargs)\n            if isinstance(search_geometry, BaseGeometry):\n                loaded_data = self.crop_to_geometry(loaded_data, search_geometry)\n\n    return loaded_data\n</code></pre>"},{"location":"api/handlers/#gigaspatial.handlers.base.BaseHandlerReader.load_from_paths","title":"<code>load_from_paths(source_data_path, **kwargs)</code>  <code>abstractmethod</code>","text":"<p>Abstract method to load source data from paths.</p> <p>Parameters:</p> Name Type Description Default <code>source_data_path</code> <code>List[Union[str, Path]]</code> <p>List of source paths</p> required <code>**kwargs</code> <p>Additional parameters for data loading</p> <code>{}</code> <p>Returns:</p> Type Description <code>Any</code> <p>Loaded data (DataFrame, GeoDataFrame, etc.)</p> Source code in <code>gigaspatial/handlers/base.py</code> <pre><code>@abstractmethod\ndef load_from_paths(\n    self, source_data_path: List[Union[str, Path]], **kwargs\n) -&gt; Any:\n    \"\"\"\n    Abstract method to load source data from paths.\n\n    Args:\n        source_data_path: List of source paths\n        **kwargs: Additional parameters for data loading\n\n    Returns:\n        Loaded data (DataFrame, GeoDataFrame, etc.)\n    \"\"\"\n    pass\n</code></pre>"},{"location":"api/handlers/#gigaspatial.handlers.base.BaseHandlerReader.resolve_by_paths","title":"<code>resolve_by_paths(paths, **kwargs)</code>","text":"<p>Return explicit paths as a list.</p> Source code in <code>gigaspatial/handlers/base.py</code> <pre><code>def resolve_by_paths(\n    self, paths: Union[Path, str, List[Union[str, Path]]], **kwargs\n) -&gt; List[Union[str, Path]]:\n    \"\"\"\n    Return explicit paths as a list.\n    \"\"\"\n    if isinstance(paths, (str, Path)):\n        return [paths]\n    return list(paths)\n</code></pre>"},{"location":"api/handlers/#gigaspatial.handlers.base.BaseHandlerReader.resolve_source_paths","title":"<code>resolve_source_paths(source, **kwargs)</code>","text":"<p>Resolve source data paths based on the type of source input.</p> <p>Parameters:</p> Name Type Description Default <code>source</code> <code>Union[str, List[Union[Tuple[float, float], Point]], BaseGeometry, GeoDataFrame, Path, str, List[Union[str, Path]]]</code> <p>Can be a country code or name (str), list of points, geometry, GeoDataFrame, or explicit path(s)</p> required <code>**kwargs</code> <p>Additional parameters for path resolution</p> <code>{}</code> <p>Returns:</p> Type Description <code>List[Union[str, Path]]</code> <p>List of resolved source paths</p> Source code in <code>gigaspatial/handlers/base.py</code> <pre><code>def resolve_source_paths(\n    self,\n    source: Union[\n        str,  # country code\n        List[Union[Tuple[float, float], Point]],  # points\n        BaseGeometry,\n        gpd.GeoDataFrame,\n        Path,  # path\n        str,  # path\n        List[Union[str, Path]],\n    ],\n    **kwargs,\n) -&gt; List[Union[str, Path]]:\n    \"\"\"\n    Resolve source data paths based on the type of source input.\n\n    Args:\n        source: Can be a country code or name (str), list of points, geometry, GeoDataFrame, or explicit path(s)\n        **kwargs: Additional parameters for path resolution\n\n    Returns:\n        List of resolved source paths\n    \"\"\"\n    if (\n        isinstance(source, Path)\n        or (\n            isinstance(source, (list, tuple, set))\n            and all(isinstance(p, (str, Path)) for p in source)\n        )\n        or (isinstance(source, str) and \".\" in source)\n    ):\n        return self.resolve_by_paths(source)\n\n    data_units = self.config.get_relevant_data_units(source, **kwargs)\n    data_paths = self.config.get_data_unit_paths(data_units, **kwargs)\n\n    self.logger.info(f\"Resolved {len(data_paths)} paths!\")\n    return data_paths\n</code></pre>"},{"location":"api/handlers/#gigaspatial.handlers.boundaries","title":"<code>boundaries</code>","text":""},{"location":"api/handlers/#gigaspatial.handlers.boundaries.AdminBoundaries","title":"<code>AdminBoundaries</code>","text":"<p>               Bases: <code>BaseModel</code></p> <p>Base class for administrative boundary data with flexible fields.</p> Source code in <code>gigaspatial/handlers/boundaries.py</code> <pre><code>class AdminBoundaries(BaseModel):\n    \"\"\"Base class for administrative boundary data with flexible fields.\"\"\"\n\n    boundaries: List[AdminBoundary] = Field(default_factory=list)\n    level: int = Field(\n        ...,\n        ge=0,\n        le=4,\n        description=\"Administrative level (e.g., 0=country, 1=state, etc.)\",\n    )\n\n    logger: ClassVar = global_config.get_logger(\"AdminBoundaries\")\n\n    _schema_config: ClassVar[Dict[str, Dict[str, str]]] = {\n        \"gadm\": {\n            \"country_code\": \"GID_0\",\n            \"id\": \"GID_{level}\",\n            \"name\": \"NAME_{level}\",\n            \"parent_id\": \"GID_{parent_level}\",\n        },\n        \"internal\": {\n            \"id\": \"admin{level}_id_giga\",\n            \"name\": \"name\",\n            \"name_en\": \"name_en\",\n            \"country_code\": \"iso_3166_1_alpha_3\",\n        },\n        \"geoBoundaries\": {\n            \"id\": \"shapeID\",\n            \"name\": \"shapeName\",\n            \"country_code\": \"shapeGroup\",\n        },\n    }\n\n    def to_geodataframe(self) -&gt; gpd.GeoDataFrame:\n        \"\"\"Convert the AdminBoundaries to a GeoDataFrame.\"\"\"\n        if not self.boundaries:\n            if hasattr(self, \"_empty_schema\"):\n                columns = self._empty_schema\n            else:\n                columns = [\"id\", \"name\", \"country_code\", \"geometry\"]\n                if self.level &gt; 0:\n                    columns.append(\"parent_id\")\n\n            return gpd.GeoDataFrame(columns=columns, geometry=\"geometry\", crs=4326)\n\n        return gpd.GeoDataFrame(\n            [boundary.model_dump() for boundary in self.boundaries],\n            geometry=\"geometry\",\n            crs=4326,\n        )\n\n    @classmethod\n    def get_schema_config(cls) -&gt; Dict[str, Dict[str, str]]:\n        \"\"\"Return field mappings for different data sources\"\"\"\n        return cls._schema_config\n\n    @classmethod\n    def from_gadm(\n        cls, country_code: str, admin_level: int = 0, **kwargs\n    ) -&gt; \"AdminBoundaries\":\n        \"\"\"Load and create instance from GADM data.\"\"\"\n        url = f\"https://geodata.ucdavis.edu/gadm/gadm4.1/json/gadm41_{country_code}_{admin_level}.json\"\n        cls.logger.info(\n            f\"Loading GADM data for country: {country_code}, admin level: {admin_level} from URL: {url}\"\n        )\n        try:\n            gdf = gpd.read_file(url)\n\n            gdf = cls._map_fields(gdf, \"gadm\", admin_level)\n\n            if admin_level == 0:\n                gdf[\"country_code\"] = gdf[\"id\"]\n                gdf[\"name\"] = gdf[\"COUNTRY\"]\n            elif admin_level == 1:\n                gdf[\"country_code\"] = gdf[\"parent_id\"]\n\n            boundaries = [\n                AdminBoundary(**row_dict) for row_dict in gdf.to_dict(\"records\")\n            ]\n            cls.logger.info(f\"Created {len(boundaries)} AdminBoundary objects.\")\n            return cls(\n                boundaries=boundaries, level=admin_level, country_code=country_code\n            )\n\n        except (ValueError, HTTPError, FileNotFoundError) as e:\n            cls.logger.warning(\n                f\"Error loading GADM data for {country_code} at admin level {admin_level}: {str(e)}\"\n            )\n            cls.logger.info(\"Falling back to empty instance\")\n            return cls._create_empty_instance(country_code, admin_level, \"gadm\")\n\n    @classmethod\n    def from_data_store(\n        cls,\n        data_store: DataStore,\n        path: Union[str, \"Path\"],\n        admin_level: int = 0,\n        **kwargs,\n    ) -&gt; \"AdminBoundaries\":\n        \"\"\"Load and create instance from internal data store.\"\"\"\n        cls.logger.info(\n            f\"Loading data from data store at path: {path}, admin level: {admin_level}\"\n        )\n        try:\n            gdf = read_dataset(data_store, str(path), **kwargs)\n\n            if gdf.empty:\n                cls.logger.warning(f\"No data found at {path}.\")\n                return cls._create_empty_instance(None, admin_level, \"internal\")\n\n            gdf = cls._map_fields(gdf, \"internal\", admin_level)\n\n            if admin_level == 0:\n                gdf[\"id\"] = gdf[\"country_code\"]\n            else:\n                gdf[\"parent_id\"] = gdf[\"id\"].apply(lambda x: x[:-3])\n\n            boundaries = [\n                AdminBoundary(**row_dict) for row_dict in gdf.to_dict(\"records\")\n            ]\n            cls.logger.info(f\"Created {len(boundaries)} AdminBoundary objects.\")\n            return cls(boundaries=boundaries, level=admin_level)\n\n        except (FileNotFoundError, KeyError) as e:\n            cls.logger.warning(\n                f\"No data found at {path} for admin level {admin_level}: {str(e)}\"\n            )\n            cls.logger.info(\"Falling back to empty instance\")\n            return cls._create_empty_instance(None, admin_level, \"internal\")\n\n    @classmethod\n    def from_georepo(\n        cls,\n        country_code: str = None,\n        admin_level: int = 0,\n        **kwargs,\n    ) -&gt; \"AdminBoundaries\":\n        \"\"\"\n        Load and create instance from GeoRepo (UNICEF) API.\n\n        Args:\n            country: Country name (if using name-based lookup)\n            iso3: ISO3 code (if using code-based lookup)\n            admin_level: Administrative level (0=country, 1=state, etc.)\n            api_key: GeoRepo API key (optional)\n            email: GeoRepo user email (optional)\n            kwargs: Extra arguments (ignored)\n\n        Returns:\n            AdminBoundaries instance\n        \"\"\"\n        cls.logger.info(\n            f\"Loading data from UNICEF GeoRepo for country: {country_code}, admin level: {admin_level}\"\n        )\n        from gigaspatial.handlers.unicef_georepo import get_country_boundaries_by_iso3\n\n        # Fetch boundaries from GeoRepo\n        geojson = get_country_boundaries_by_iso3(country_code, admin_level=admin_level)\n\n        features = geojson.get(\"features\", [])\n        boundaries = []\n        parent_level = admin_level - 1\n\n        for feat in features:\n            props = feat.get(\"properties\", {})\n            geometry = feat.get(\"geometry\")\n            shapely_geom = shape(geometry) if geometry else None\n            # For admin_level 0, no parent_id\n            parent_id = None\n            if admin_level &gt; 0:\n                parent_id = props.get(f\"adm{parent_level}_ucode\")\n\n            boundary = AdminBoundary(\n                id=props.get(\"ucode\"),\n                name=props.get(\"name\"),\n                name_en=props.get(\"name_en\"),\n                geometry=shapely_geom,\n                parent_id=parent_id,\n                country_code=country_code,\n            )\n            boundaries.append(boundary)\n\n        cls.logger.info(\n            f\"Created {len(boundaries)} AdminBoundary objects from GeoRepo data.\"\n        )\n\n        # Try to infer country_code from first boundary if not set\n        if boundaries and not boundaries[0].country_code:\n            boundaries[0].country_code = boundaries[0].id[:3]\n\n        return cls(boundaries=boundaries, level=admin_level)\n\n    @classmethod\n    def from_geoboundaries(cls, country_code, admin_level: int = 0):\n        cls.logger.info(\n            f\"Searching for geoBoundaries data for country: {country_code}, admin level: {admin_level}\"\n        )\n\n        country_datasets = HDXConfig.search_datasets(\n            query=f'dataseries_name:\"geoBoundaries - Subnational Administrative Boundaries\" AND groups:\"{country_code.lower()}\"',\n            rows=1,\n        )\n        if not country_datasets:\n            cls.logger.error(f\"No datasets found for country: {country_code}\")\n            raise ValueError(\n                \"No resources found for the specified country. Please check your search parameters and try again.\"\n            )\n\n        cls.logger.info(f\"Found dataset: {country_datasets[0].get('title', 'Unknown')}\")\n\n        resources = [\n            resource\n            for resource in country_datasets[0].get_resources()\n            if (\n                resource.data[\"name\"]\n                == f\"geoBoundaries-{country_code.upper()}-ADM{admin_level}.geojson\"\n            )\n        ]\n\n        if not resources:\n            cls.logger.error(\n                f\"No resources found for {country_code} at admin level {admin_level}\"\n            )\n            raise ValueError(\n                \"No resources found for the specified criteria. Please check your search parameters and try again.\"\n            )\n\n        cls.logger.info(f\"Found resource: {resources[0].data.get('name', 'Unknown')}\")\n\n        try:\n            cls.logger.info(\"Downloading and processing boundary data...\")\n            with tempfile.TemporaryDirectory() as tmpdir:\n                url, local_path = resources[0].download(folder=tmpdir)\n                cls.logger.debug(f\"Downloaded file to temporary path: {local_path}\")\n                with open(local_path, \"rb\") as f:\n                    gdf = gpd.read_file(f)\n\n            gdf = cls._map_fields(gdf, \"geoBoundaries\", admin_level)\n            boundaries = [\n                AdminBoundary(**row_dict) for row_dict in gdf.to_dict(\"records\")\n            ]\n            cls.logger.info(\n                f\"Successfully created {len(boundaries)} AdminBoundary objects\"\n            )\n            return cls(boundaries=boundaries, level=admin_level)\n\n        except (ValueError, HTTPError, FileNotFoundError) as e:\n            cls.logger.warning(\n                f\"Error loading geoBoundaries data for {country_code} at admin level {admin_level}: {str(e)}\"\n            )\n            cls.logger.info(\"Falling back to empty instance\")\n            return cls._create_empty_instance(\n                country_code, admin_level, \"geoBoundaries\"\n            )\n\n    @classmethod\n    def from_global_country_boundaries(cls, scale: str = \"medium\") -&gt; \"AdminBoundaries\":\n        \"\"\"\n        Load global country boundaries from Natural Earth Data.\n\n        Args:\n            scale (str): One of 'large', 'medium', 'small'.\n                - 'large'  -&gt; 10m\n                - 'medium' -&gt; 50m\n                - 'small'  -&gt; 110m\n        Returns:\n            AdminBoundaries: All country boundaries at admin_level=0\n        \"\"\"\n        scale_map = {\n            \"large\": \"10m\",\n            \"medium\": \"50m\",\n            \"small\": \"110m\",\n        }\n        if scale not in scale_map:\n            raise ValueError(\n                f\"Invalid scale '{scale}'. Choose from 'large', 'medium', 'small'.\"\n            )\n        scale_folder = scale_map[scale]\n        url = f\"https://naciscdn.org/naturalearth/{scale_folder}/cultural/ne_{scale_folder}_admin_0_countries.zip\"\n        cls.logger.info(f\"Loading Natural Earth global country boundaries from {url}\")\n        try:\n            gdf = gpd.read_file(url)\n            # Map fields to AdminBoundary schema\n            boundaries = []\n            for _, row in gdf.iterrows():\n                iso_a3 = row.get(\"ISO_A3_EH\") or row.get(\"ISO_A3\") or row.get(\"ADM0_A3\")\n                name = row.get(\"NAME\") or row.get(\"ADMIN\") or row.get(\"SOVEREIGNT\")\n                geometry = row.get(\"geometry\")\n                if not iso_a3 or not name or geometry is None:\n                    continue\n                boundary = AdminBoundary(\n                    id=iso_a3,\n                    name=name,\n                    geometry=geometry,\n                    country_code=iso_a3,\n                )\n                boundaries.append(boundary)\n            cls.logger.info(\n                f\"Loaded {len(boundaries)} country boundaries from Natural Earth.\"\n            )\n            return cls(boundaries=boundaries, level=0)\n        except Exception as e:\n            cls.logger.error(f\"Failed to load Natural Earth global boundaries: {e}\")\n            raise\n\n    @classmethod\n    def create(\n        cls,\n        country_code: Optional[str] = None,\n        admin_level: int = 0,\n        data_store: Optional[DataStore] = None,\n        path: Optional[Union[str, \"Path\"]] = None,\n        **kwargs,\n    ) -&gt; \"AdminBoundaries\":\n        \"\"\"\n        Factory method to create an AdminBoundaries instance using various data sources,\n        depending on the provided parameters and global configuration.\n\n        Loading Logic:\n            1. If a `data_store` is provided and either a `path` is given or\n               `global_config.ADMIN_BOUNDARIES_DATA_DIR` is set:\n                - If `path` is not provided but `country_code` is, the path is constructed\n                  using `global_config.get_admin_path()`.\n                - Loads boundaries from the specified data store and path.\n\n            2. If only `country_code` is provided (no data_store):\n                - Attempts to load boundaries from GeoRepo (if available).\n                - If GeoRepo is unavailable, attempts to load from GADM.\n                - If GADM fails, falls back to geoBoundaries.\n                - Raises an error if all sources fail.\n\n            3. If neither `country_code` nor `data_store` is provided:\n                - Raises a ValueError.\n\n        Args:\n            country_code (Optional[str]): ISO country code (2 or 3 letter) or country name.\n            admin_level (int): Administrative level (0=country, 1=state/province, etc.).\n            data_store (Optional[DataStore]): Optional data store instance for loading from existing data.\n            path (Optional[Union[str, Path]]): Optional path to data file (used with data_store).\n            **kwargs: Additional arguments passed to the underlying creation methods.\n\n        Returns:\n            AdminBoundaries: Configured instance.\n\n        Raises:\n            ValueError: If neither country_code nor (data_store, path) are provided,\n                        or if country_code lookup fails.\n            RuntimeError: If all data sources fail to load boundaries.\n\n        Examples:\n            # Load from a data store (path auto-generated if not provided)\n            boundaries = AdminBoundaries.create(country_code=\"USA\", admin_level=1, data_store=store)\n\n            # Load from a specific file in a data store\n            boundaries = AdminBoundaries.create(data_store=store, path=\"data.shp\")\n\n            # Load from online sources (GeoRepo, GADM, geoBoundaries)\n            boundaries = AdminBoundaries.create(country_code=\"USA\", admin_level=1)\n        \"\"\"\n        cls.logger.info(\n            f\"Creating AdminBoundaries instance. Country: {country_code}, \"\n            f\"admin level: {admin_level}, data_store provided: {data_store is not None}, \"\n            f\"path provided: {path is not None}\"\n        )\n\n        from_data_store = data_store is not None and (\n            global_config.ADMIN_BOUNDARIES_DATA_DIR is not None or path is not None\n        )\n\n        # Validate input parameters\n        if not country_code and not data_store:\n            raise ValueError(\"Either country_code or data_store must be provided.\")\n\n        if from_data_store and not path and not country_code:\n            raise ValueError(\n                \"If data_store is provided, either path or country_code must also be specified.\"\n            )\n\n        # Handle data store path first\n        if from_data_store:\n            iso3_code = None\n            if country_code:\n                try:\n                    iso3_code = pycountry.countries.lookup(country_code).alpha_3\n                except LookupError as e:\n                    raise ValueError(f\"Invalid country code '{country_code}': {e}\")\n\n            # Generate path if not provided\n            if path is None and iso3_code:\n                path = global_config.get_admin_path(\n                    country_code=iso3_code,\n                    admin_level=admin_level,\n                )\n\n            return cls.from_data_store(data_store, path, admin_level, **kwargs)\n\n        # Handle country code path\n        if country_code is not None:\n            try:\n                iso3_code = pycountry.countries.lookup(country_code).alpha_3\n            except LookupError as e:\n                raise ValueError(f\"Invalid country code '{country_code}': {e}\")\n\n            # Try GeoRepo first\n            if cls._try_georepo(iso3_code, admin_level):\n                return cls.from_georepo(iso3_code, admin_level=admin_level)\n\n            # Fallback to GADM\n            try:\n                cls.logger.info(\"Attempting to load from GADM.\")\n                return cls.from_gadm(iso3_code, admin_level, **kwargs)\n            except Exception as e:\n                cls.logger.warning(\n                    f\"GADM loading failed: {e}. Falling back to geoBoundaries.\"\n                )\n\n            # Final fallback to geoBoundaries\n            try:\n                return cls.from_geoboundaries(iso3_code, admin_level)\n            except Exception as e:\n                cls.logger.error(f\"All data sources failed. geoBoundaries error: {e}\")\n                raise RuntimeError(\n                    f\"Failed to load administrative boundaries for {country_code} \"\n                    f\"from all available sources (GeoRepo, GADM, geoBoundaries).\"\n                ) from e\n\n        # This should never be reached due to validation above\n        raise ValueError(\"Unexpected error: no valid data source could be determined.\")\n\n    @classmethod\n    def _try_georepo(cls, iso3_code: str, admin_level: int) -&gt; bool:\n        \"\"\"Helper method to test GeoRepo availability.\n\n        Args:\n            iso3_code: ISO3 country code\n            admin_level: Administrative level\n\n        Returns:\n            bool: True if GeoRepo is available and working, False otherwise\n        \"\"\"\n        try:\n            from gigaspatial.handlers.unicef_georepo import GeoRepoClient\n\n            client = GeoRepoClient()\n            if client.check_connection():\n                cls.logger.info(\"GeoRepo connection successful.\")\n                return True\n            else:\n                cls.logger.info(\"GeoRepo connection failed.\")\n                return False\n\n        except ImportError:\n            cls.logger.info(\"GeoRepo client not available (import failed).\")\n            return False\n        except ValueError as e:\n            cls.logger.warning(f\"GeoRepo initialization failed: {e}\")\n            return False\n        except Exception as e:\n            cls.logger.warning(f\"GeoRepo error: {e}\")\n            return False\n\n    @classmethod\n    def _create_empty_instance(\n        cls, country_code: Optional[str], admin_level: int, source_type: str\n    ) -&gt; \"AdminBoundaries\":\n        \"\"\"Create an empty instance with the required schema structure.\"\"\"\n        # for to_geodataframe() to use later\n        instance = cls(boundaries=[], level=admin_level, country_code=country_code)\n\n        schema_fields = set(cls.get_schema_config()[source_type].keys())\n        schema_fields.update([\"geometry\", \"country_code\", \"id\", \"name\", \"name_en\"])\n        if admin_level &gt; 0:\n            schema_fields.add(\"parent_id\")\n\n        instance._empty_schema = list(schema_fields)\n        return instance\n\n    @classmethod\n    def _map_fields(\n        cls,\n        gdf: gpd.GeoDataFrame,\n        source: str,\n        current_level: int,\n    ) -&gt; gpd.GeoDataFrame:\n        \"\"\"Map source fields to schema fields\"\"\"\n        config = cls.get_schema_config().get(source, {})\n        parent_level = current_level - 1\n\n        field_mapping = {}\n        for k, v in config.items():\n            if \"{parent_level}\" in v:\n                field_mapping[v.format(parent_level=parent_level)] = k\n            elif \"{level}\" in v:\n                field_mapping[v.format(level=current_level)] = k\n            else:\n                field_mapping[v] = k\n\n        return gdf.rename(columns=field_mapping)\n</code></pre>"},{"location":"api/handlers/#gigaspatial.handlers.boundaries.AdminBoundaries.create","title":"<code>create(country_code=None, admin_level=0, data_store=None, path=None, **kwargs)</code>  <code>classmethod</code>","text":"<p>Factory method to create an AdminBoundaries instance using various data sources, depending on the provided parameters and global configuration.</p> Loading Logic <ol> <li> <p>If a <code>data_store</code> is provided and either a <code>path</code> is given or    <code>global_config.ADMIN_BOUNDARIES_DATA_DIR</code> is set:</p> <ul> <li>If <code>path</code> is not provided but <code>country_code</code> is, the path is constructed   using <code>global_config.get_admin_path()</code>.</li> <li>Loads boundaries from the specified data store and path.</li> </ul> </li> <li> <p>If only <code>country_code</code> is provided (no data_store):</p> <ul> <li>Attempts to load boundaries from GeoRepo (if available).</li> <li>If GeoRepo is unavailable, attempts to load from GADM.</li> <li>If GADM fails, falls back to geoBoundaries.</li> <li>Raises an error if all sources fail.</li> </ul> </li> <li> <p>If neither <code>country_code</code> nor <code>data_store</code> is provided:</p> <ul> <li>Raises a ValueError.</li> </ul> </li> </ol> <p>Parameters:</p> Name Type Description Default <code>country_code</code> <code>Optional[str]</code> <p>ISO country code (2 or 3 letter) or country name.</p> <code>None</code> <code>admin_level</code> <code>int</code> <p>Administrative level (0=country, 1=state/province, etc.).</p> <code>0</code> <code>data_store</code> <code>Optional[DataStore]</code> <p>Optional data store instance for loading from existing data.</p> <code>None</code> <code>path</code> <code>Optional[Union[str, Path]]</code> <p>Optional path to data file (used with data_store).</p> <code>None</code> <code>**kwargs</code> <p>Additional arguments passed to the underlying creation methods.</p> <code>{}</code> <p>Returns:</p> Name Type Description <code>AdminBoundaries</code> <code>AdminBoundaries</code> <p>Configured instance.</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If neither country_code nor (data_store, path) are provided,         or if country_code lookup fails.</p> <code>RuntimeError</code> <p>If all data sources fail to load boundaries.</p> <p>Examples:</p>"},{"location":"api/handlers/#gigaspatial.handlers.boundaries.AdminBoundaries.create--load-from-a-data-store-path-auto-generated-if-not-provided","title":"Load from a data store (path auto-generated if not provided)","text":"<p>boundaries = AdminBoundaries.create(country_code=\"USA\", admin_level=1, data_store=store)</p>"},{"location":"api/handlers/#gigaspatial.handlers.boundaries.AdminBoundaries.create--load-from-a-specific-file-in-a-data-store","title":"Load from a specific file in a data store","text":"<p>boundaries = AdminBoundaries.create(data_store=store, path=\"data.shp\")</p>"},{"location":"api/handlers/#gigaspatial.handlers.boundaries.AdminBoundaries.create--load-from-online-sources-georepo-gadm-geoboundaries","title":"Load from online sources (GeoRepo, GADM, geoBoundaries)","text":"<p>boundaries = AdminBoundaries.create(country_code=\"USA\", admin_level=1)</p> Source code in <code>gigaspatial/handlers/boundaries.py</code> <pre><code>@classmethod\ndef create(\n    cls,\n    country_code: Optional[str] = None,\n    admin_level: int = 0,\n    data_store: Optional[DataStore] = None,\n    path: Optional[Union[str, \"Path\"]] = None,\n    **kwargs,\n) -&gt; \"AdminBoundaries\":\n    \"\"\"\n    Factory method to create an AdminBoundaries instance using various data sources,\n    depending on the provided parameters and global configuration.\n\n    Loading Logic:\n        1. If a `data_store` is provided and either a `path` is given or\n           `global_config.ADMIN_BOUNDARIES_DATA_DIR` is set:\n            - If `path` is not provided but `country_code` is, the path is constructed\n              using `global_config.get_admin_path()`.\n            - Loads boundaries from the specified data store and path.\n\n        2. If only `country_code` is provided (no data_store):\n            - Attempts to load boundaries from GeoRepo (if available).\n            - If GeoRepo is unavailable, attempts to load from GADM.\n            - If GADM fails, falls back to geoBoundaries.\n            - Raises an error if all sources fail.\n\n        3. If neither `country_code` nor `data_store` is provided:\n            - Raises a ValueError.\n\n    Args:\n        country_code (Optional[str]): ISO country code (2 or 3 letter) or country name.\n        admin_level (int): Administrative level (0=country, 1=state/province, etc.).\n        data_store (Optional[DataStore]): Optional data store instance for loading from existing data.\n        path (Optional[Union[str, Path]]): Optional path to data file (used with data_store).\n        **kwargs: Additional arguments passed to the underlying creation methods.\n\n    Returns:\n        AdminBoundaries: Configured instance.\n\n    Raises:\n        ValueError: If neither country_code nor (data_store, path) are provided,\n                    or if country_code lookup fails.\n        RuntimeError: If all data sources fail to load boundaries.\n\n    Examples:\n        # Load from a data store (path auto-generated if not provided)\n        boundaries = AdminBoundaries.create(country_code=\"USA\", admin_level=1, data_store=store)\n\n        # Load from a specific file in a data store\n        boundaries = AdminBoundaries.create(data_store=store, path=\"data.shp\")\n\n        # Load from online sources (GeoRepo, GADM, geoBoundaries)\n        boundaries = AdminBoundaries.create(country_code=\"USA\", admin_level=1)\n    \"\"\"\n    cls.logger.info(\n        f\"Creating AdminBoundaries instance. Country: {country_code}, \"\n        f\"admin level: {admin_level}, data_store provided: {data_store is not None}, \"\n        f\"path provided: {path is not None}\"\n    )\n\n    from_data_store = data_store is not None and (\n        global_config.ADMIN_BOUNDARIES_DATA_DIR is not None or path is not None\n    )\n\n    # Validate input parameters\n    if not country_code and not data_store:\n        raise ValueError(\"Either country_code or data_store must be provided.\")\n\n    if from_data_store and not path and not country_code:\n        raise ValueError(\n            \"If data_store is provided, either path or country_code must also be specified.\"\n        )\n\n    # Handle data store path first\n    if from_data_store:\n        iso3_code = None\n        if country_code:\n            try:\n                iso3_code = pycountry.countries.lookup(country_code).alpha_3\n            except LookupError as e:\n                raise ValueError(f\"Invalid country code '{country_code}': {e}\")\n\n        # Generate path if not provided\n        if path is None and iso3_code:\n            path = global_config.get_admin_path(\n                country_code=iso3_code,\n                admin_level=admin_level,\n            )\n\n        return cls.from_data_store(data_store, path, admin_level, **kwargs)\n\n    # Handle country code path\n    if country_code is not None:\n        try:\n            iso3_code = pycountry.countries.lookup(country_code).alpha_3\n        except LookupError as e:\n            raise ValueError(f\"Invalid country code '{country_code}': {e}\")\n\n        # Try GeoRepo first\n        if cls._try_georepo(iso3_code, admin_level):\n            return cls.from_georepo(iso3_code, admin_level=admin_level)\n\n        # Fallback to GADM\n        try:\n            cls.logger.info(\"Attempting to load from GADM.\")\n            return cls.from_gadm(iso3_code, admin_level, **kwargs)\n        except Exception as e:\n            cls.logger.warning(\n                f\"GADM loading failed: {e}. Falling back to geoBoundaries.\"\n            )\n\n        # Final fallback to geoBoundaries\n        try:\n            return cls.from_geoboundaries(iso3_code, admin_level)\n        except Exception as e:\n            cls.logger.error(f\"All data sources failed. geoBoundaries error: {e}\")\n            raise RuntimeError(\n                f\"Failed to load administrative boundaries for {country_code} \"\n                f\"from all available sources (GeoRepo, GADM, geoBoundaries).\"\n            ) from e\n\n    # This should never be reached due to validation above\n    raise ValueError(\"Unexpected error: no valid data source could be determined.\")\n</code></pre>"},{"location":"api/handlers/#gigaspatial.handlers.boundaries.AdminBoundaries.from_data_store","title":"<code>from_data_store(data_store, path, admin_level=0, **kwargs)</code>  <code>classmethod</code>","text":"<p>Load and create instance from internal data store.</p> Source code in <code>gigaspatial/handlers/boundaries.py</code> <pre><code>@classmethod\ndef from_data_store(\n    cls,\n    data_store: DataStore,\n    path: Union[str, \"Path\"],\n    admin_level: int = 0,\n    **kwargs,\n) -&gt; \"AdminBoundaries\":\n    \"\"\"Load and create instance from internal data store.\"\"\"\n    cls.logger.info(\n        f\"Loading data from data store at path: {path}, admin level: {admin_level}\"\n    )\n    try:\n        gdf = read_dataset(data_store, str(path), **kwargs)\n\n        if gdf.empty:\n            cls.logger.warning(f\"No data found at {path}.\")\n            return cls._create_empty_instance(None, admin_level, \"internal\")\n\n        gdf = cls._map_fields(gdf, \"internal\", admin_level)\n\n        if admin_level == 0:\n            gdf[\"id\"] = gdf[\"country_code\"]\n        else:\n            gdf[\"parent_id\"] = gdf[\"id\"].apply(lambda x: x[:-3])\n\n        boundaries = [\n            AdminBoundary(**row_dict) for row_dict in gdf.to_dict(\"records\")\n        ]\n        cls.logger.info(f\"Created {len(boundaries)} AdminBoundary objects.\")\n        return cls(boundaries=boundaries, level=admin_level)\n\n    except (FileNotFoundError, KeyError) as e:\n        cls.logger.warning(\n            f\"No data found at {path} for admin level {admin_level}: {str(e)}\"\n        )\n        cls.logger.info(\"Falling back to empty instance\")\n        return cls._create_empty_instance(None, admin_level, \"internal\")\n</code></pre>"},{"location":"api/handlers/#gigaspatial.handlers.boundaries.AdminBoundaries.from_gadm","title":"<code>from_gadm(country_code, admin_level=0, **kwargs)</code>  <code>classmethod</code>","text":"<p>Load and create instance from GADM data.</p> Source code in <code>gigaspatial/handlers/boundaries.py</code> <pre><code>@classmethod\ndef from_gadm(\n    cls, country_code: str, admin_level: int = 0, **kwargs\n) -&gt; \"AdminBoundaries\":\n    \"\"\"Load and create instance from GADM data.\"\"\"\n    url = f\"https://geodata.ucdavis.edu/gadm/gadm4.1/json/gadm41_{country_code}_{admin_level}.json\"\n    cls.logger.info(\n        f\"Loading GADM data for country: {country_code}, admin level: {admin_level} from URL: {url}\"\n    )\n    try:\n        gdf = gpd.read_file(url)\n\n        gdf = cls._map_fields(gdf, \"gadm\", admin_level)\n\n        if admin_level == 0:\n            gdf[\"country_code\"] = gdf[\"id\"]\n            gdf[\"name\"] = gdf[\"COUNTRY\"]\n        elif admin_level == 1:\n            gdf[\"country_code\"] = gdf[\"parent_id\"]\n\n        boundaries = [\n            AdminBoundary(**row_dict) for row_dict in gdf.to_dict(\"records\")\n        ]\n        cls.logger.info(f\"Created {len(boundaries)} AdminBoundary objects.\")\n        return cls(\n            boundaries=boundaries, level=admin_level, country_code=country_code\n        )\n\n    except (ValueError, HTTPError, FileNotFoundError) as e:\n        cls.logger.warning(\n            f\"Error loading GADM data for {country_code} at admin level {admin_level}: {str(e)}\"\n        )\n        cls.logger.info(\"Falling back to empty instance\")\n        return cls._create_empty_instance(country_code, admin_level, \"gadm\")\n</code></pre>"},{"location":"api/handlers/#gigaspatial.handlers.boundaries.AdminBoundaries.from_georepo","title":"<code>from_georepo(country_code=None, admin_level=0, **kwargs)</code>  <code>classmethod</code>","text":"<p>Load and create instance from GeoRepo (UNICEF) API.</p> <p>Parameters:</p> Name Type Description Default <code>country</code> <p>Country name (if using name-based lookup)</p> required <code>iso3</code> <p>ISO3 code (if using code-based lookup)</p> required <code>admin_level</code> <code>int</code> <p>Administrative level (0=country, 1=state, etc.)</p> <code>0</code> <code>api_key</code> <p>GeoRepo API key (optional)</p> required <code>email</code> <p>GeoRepo user email (optional)</p> required <code>kwargs</code> <p>Extra arguments (ignored)</p> <code>{}</code> <p>Returns:</p> Type Description <code>AdminBoundaries</code> <p>AdminBoundaries instance</p> Source code in <code>gigaspatial/handlers/boundaries.py</code> <pre><code>@classmethod\ndef from_georepo(\n    cls,\n    country_code: str = None,\n    admin_level: int = 0,\n    **kwargs,\n) -&gt; \"AdminBoundaries\":\n    \"\"\"\n    Load and create instance from GeoRepo (UNICEF) API.\n\n    Args:\n        country: Country name (if using name-based lookup)\n        iso3: ISO3 code (if using code-based lookup)\n        admin_level: Administrative level (0=country, 1=state, etc.)\n        api_key: GeoRepo API key (optional)\n        email: GeoRepo user email (optional)\n        kwargs: Extra arguments (ignored)\n\n    Returns:\n        AdminBoundaries instance\n    \"\"\"\n    cls.logger.info(\n        f\"Loading data from UNICEF GeoRepo for country: {country_code}, admin level: {admin_level}\"\n    )\n    from gigaspatial.handlers.unicef_georepo import get_country_boundaries_by_iso3\n\n    # Fetch boundaries from GeoRepo\n    geojson = get_country_boundaries_by_iso3(country_code, admin_level=admin_level)\n\n    features = geojson.get(\"features\", [])\n    boundaries = []\n    parent_level = admin_level - 1\n\n    for feat in features:\n        props = feat.get(\"properties\", {})\n        geometry = feat.get(\"geometry\")\n        shapely_geom = shape(geometry) if geometry else None\n        # For admin_level 0, no parent_id\n        parent_id = None\n        if admin_level &gt; 0:\n            parent_id = props.get(f\"adm{parent_level}_ucode\")\n\n        boundary = AdminBoundary(\n            id=props.get(\"ucode\"),\n            name=props.get(\"name\"),\n            name_en=props.get(\"name_en\"),\n            geometry=shapely_geom,\n            parent_id=parent_id,\n            country_code=country_code,\n        )\n        boundaries.append(boundary)\n\n    cls.logger.info(\n        f\"Created {len(boundaries)} AdminBoundary objects from GeoRepo data.\"\n    )\n\n    # Try to infer country_code from first boundary if not set\n    if boundaries and not boundaries[0].country_code:\n        boundaries[0].country_code = boundaries[0].id[:3]\n\n    return cls(boundaries=boundaries, level=admin_level)\n</code></pre>"},{"location":"api/handlers/#gigaspatial.handlers.boundaries.AdminBoundaries.from_global_country_boundaries","title":"<code>from_global_country_boundaries(scale='medium')</code>  <code>classmethod</code>","text":"<p>Load global country boundaries from Natural Earth Data.</p> <p>Parameters:</p> Name Type Description Default <code>scale</code> <code>str</code> <p>One of 'large', 'medium', 'small'. - 'large'  -&gt; 10m - 'medium' -&gt; 50m - 'small'  -&gt; 110m</p> <code>'medium'</code> Source code in <code>gigaspatial/handlers/boundaries.py</code> <pre><code>@classmethod\ndef from_global_country_boundaries(cls, scale: str = \"medium\") -&gt; \"AdminBoundaries\":\n    \"\"\"\n    Load global country boundaries from Natural Earth Data.\n\n    Args:\n        scale (str): One of 'large', 'medium', 'small'.\n            - 'large'  -&gt; 10m\n            - 'medium' -&gt; 50m\n            - 'small'  -&gt; 110m\n    Returns:\n        AdminBoundaries: All country boundaries at admin_level=0\n    \"\"\"\n    scale_map = {\n        \"large\": \"10m\",\n        \"medium\": \"50m\",\n        \"small\": \"110m\",\n    }\n    if scale not in scale_map:\n        raise ValueError(\n            f\"Invalid scale '{scale}'. Choose from 'large', 'medium', 'small'.\"\n        )\n    scale_folder = scale_map[scale]\n    url = f\"https://naciscdn.org/naturalearth/{scale_folder}/cultural/ne_{scale_folder}_admin_0_countries.zip\"\n    cls.logger.info(f\"Loading Natural Earth global country boundaries from {url}\")\n    try:\n        gdf = gpd.read_file(url)\n        # Map fields to AdminBoundary schema\n        boundaries = []\n        for _, row in gdf.iterrows():\n            iso_a3 = row.get(\"ISO_A3_EH\") or row.get(\"ISO_A3\") or row.get(\"ADM0_A3\")\n            name = row.get(\"NAME\") or row.get(\"ADMIN\") or row.get(\"SOVEREIGNT\")\n            geometry = row.get(\"geometry\")\n            if not iso_a3 or not name or geometry is None:\n                continue\n            boundary = AdminBoundary(\n                id=iso_a3,\n                name=name,\n                geometry=geometry,\n                country_code=iso_a3,\n            )\n            boundaries.append(boundary)\n        cls.logger.info(\n            f\"Loaded {len(boundaries)} country boundaries from Natural Earth.\"\n        )\n        return cls(boundaries=boundaries, level=0)\n    except Exception as e:\n        cls.logger.error(f\"Failed to load Natural Earth global boundaries: {e}\")\n        raise\n</code></pre>"},{"location":"api/handlers/#gigaspatial.handlers.boundaries.AdminBoundaries.get_schema_config","title":"<code>get_schema_config()</code>  <code>classmethod</code>","text":"<p>Return field mappings for different data sources</p> Source code in <code>gigaspatial/handlers/boundaries.py</code> <pre><code>@classmethod\ndef get_schema_config(cls) -&gt; Dict[str, Dict[str, str]]:\n    \"\"\"Return field mappings for different data sources\"\"\"\n    return cls._schema_config\n</code></pre>"},{"location":"api/handlers/#gigaspatial.handlers.boundaries.AdminBoundaries.to_geodataframe","title":"<code>to_geodataframe()</code>","text":"<p>Convert the AdminBoundaries to a GeoDataFrame.</p> Source code in <code>gigaspatial/handlers/boundaries.py</code> <pre><code>def to_geodataframe(self) -&gt; gpd.GeoDataFrame:\n    \"\"\"Convert the AdminBoundaries to a GeoDataFrame.\"\"\"\n    if not self.boundaries:\n        if hasattr(self, \"_empty_schema\"):\n            columns = self._empty_schema\n        else:\n            columns = [\"id\", \"name\", \"country_code\", \"geometry\"]\n            if self.level &gt; 0:\n                columns.append(\"parent_id\")\n\n        return gpd.GeoDataFrame(columns=columns, geometry=\"geometry\", crs=4326)\n\n    return gpd.GeoDataFrame(\n        [boundary.model_dump() for boundary in self.boundaries],\n        geometry=\"geometry\",\n        crs=4326,\n    )\n</code></pre>"},{"location":"api/handlers/#gigaspatial.handlers.boundaries.AdminBoundary","title":"<code>AdminBoundary</code>","text":"<p>               Bases: <code>BaseModel</code></p> <p>Base class for administrative boundary data with flexible fields.</p> Source code in <code>gigaspatial/handlers/boundaries.py</code> <pre><code>class AdminBoundary(BaseModel):\n    \"\"\"Base class for administrative boundary data with flexible fields.\"\"\"\n\n    id: str = Field(..., description=\"Unique identifier for the administrative unit\")\n    name: str = Field(..., description=\"Primary local name\")\n    geometry: Union[Polygon, MultiPolygon] = Field(\n        ..., description=\"Geometry of the administrative boundary\"\n    )\n\n    name_en: Optional[str] = Field(\n        None, description=\"English name if different from local name\"\n    )\n    parent_id: Optional[str] = Field(\n        None, description=\"ID of parent administrative unit\"\n    )\n    country_code: Optional[str] = Field(\n        None, min_length=3, max_length=3, description=\"ISO 3166-1 alpha-3 country code\"\n    )\n\n    class Config:\n        arbitrary_types_allowed = True\n</code></pre>"},{"location":"api/handlers/#gigaspatial.handlers.ghsl","title":"<code>ghsl</code>","text":""},{"location":"api/handlers/#gigaspatial.handlers.ghsl.CoordSystem","title":"<code>CoordSystem</code>","text":"<p>               Bases: <code>int</code>, <code>Enum</code></p> <p>Enum for coordinate systems used by GHSL datasets.</p> Source code in <code>gigaspatial/handlers/ghsl.py</code> <pre><code>class CoordSystem(int, Enum):\n    \"\"\"Enum for coordinate systems used by GHSL datasets.\"\"\"\n\n    WGS84 = 4326\n    Mollweide = 54009\n</code></pre>"},{"location":"api/handlers/#gigaspatial.handlers.ghsl.GHSLDataConfig","title":"<code>GHSLDataConfig</code>  <code>dataclass</code>","text":"<p>               Bases: <code>BaseHandlerConfig</code></p> Source code in <code>gigaspatial/handlers/ghsl.py</code> <pre><code>@dataclass(config=ConfigDict(arbitrary_types_allowed=True))\nclass GHSLDataConfig(BaseHandlerConfig):\n    # constants\n    AVAILABLE_YEARS: List = Field(default=np.append(np.arange(1975, 2031, 5), 2018))\n    AVAILABLE_RESOLUTIONS: List = Field(default=[10, 100, 1000])\n\n    # base config\n    GHSL_DB_BASE_URL: HttpUrl = Field(\n        default=\"https://jeodpp.jrc.ec.europa.eu/ftp/jrc-opendata/GHSL/\"\n    )\n    TILES_URL: str = \"https://ghsl.jrc.ec.europa.eu/download/GHSL_data_{}_shapefile.zip\"\n\n    # user config\n    base_path: Path = Field(default=global_config.get_path(\"ghsl\", \"bronze\"))\n    coord_system: CoordSystem = CoordSystem.WGS84\n    release: str = \"R2023A\"\n\n    product: Literal[\n        \"GHS_BUILT_S\",\n        \"GHS_BUILT_H_AGBH\",\n        \"GHS_BUILT_H_ANBH\",\n        \"GHS_BUILT_V\",\n        \"GHS_POP\",\n        \"GHS_SMOD\",\n    ] = Field(...)\n    year: int = 2020\n    resolution: int = 100\n\n    def __post_init__(self):\n        super().__post_init__()\n\n    def _load_tiles(self):\n        \"\"\"Load GHSL tiles from tiles shapefile.\"\"\"\n        # Create a default, unverified context\n        ssl._create_default_https_context = ssl._create_unverified_context\n\n        try:\n            self.tiles_gdf = gpd.read_file(self.TILES_URL)\n        except Exception as e:\n            self.logger.error(f\"Failed to download tiles shapefile: {e}\")\n            raise ValueError(\n                f\"Could not download GHSL tiles from {self.TILES_URL}\"\n            ) from e\n\n    @field_validator(\"year\")\n    def validate_year(cls, value: str) -&gt; int:\n        if value in cls.AVAILABLE_YEARS:\n            return value\n        raise ValueError(\n            f\"No datasets found for the provided year: {value}\\nAvailable years are: {cls.AVAILABLE_YEARS}\"\n        )\n\n    @field_validator(\"resolution\")\n    def validate_resolution(cls, value: str) -&gt; int:\n        if value in cls.AVAILABLE_RESOLUTIONS:\n            return value\n        raise ValueError(\n            f\"No datasets found for the provided resolution: {value}\\nAvailable resolutions are: {cls.AVAILABLE_RESOLUTIONS}\"\n        )\n\n    @model_validator(mode=\"after\")\n    def validate_configuration(self):\n        \"\"\"\n        Validate that the configuration is valid based on dataset availability constraints.\n\n        Specific rules:\n        -\n        \"\"\"\n        if self.year == 2018 and self.product in [\"GHS_BUILT_V\", \"GHS_POP\", \"GHS_SMOD\"]:\n            raise ValueError(f\"{self.product} product is not available for 2018\")\n\n        if self.resolution == 10 and self.product != \"GHS_BUILT_H\":\n            raise ValueError(\n                f\"{self.product} product is not available at 10 (10m) resolution\"\n            )\n\n        if \"GHS_BUILT_H\" in self.product:\n            if self.year != 2018:\n                self.logger.warning(\n                    \"Building height product is only available for 2018, year is set as 2018\"\n                )\n                self.year = 2018\n\n        if self.product == \"GHS_BUILT_S\":\n            if self.year == 2018 and self.resolution != 10:\n                self.logger.warning(\n                    \"Built-up surface product for 2018 is only available at 10m resolution, resolution is set as 10m\"\n                )\n                self.resolution = 10\n\n            if self.resolution == 10 and self.year != 2018:\n                self.logger.warning(\n                    \"Built-up surface product at resolution 10 is only available for 2018, year is set as 2018\"\n                )\n                self.year = 2018\n\n            if self.resolution == 10 and self.coord_system != CoordSystem.Mollweide:\n                self.logger.warning(\n                    f\"Built-up surface product at resolution 10 is only available with Mollweide ({CoordSystem.Mollweide}) projection, coordinate system is set as Mollweide\"\n                )\n                self.coord_system = CoordSystem.Mollweide\n\n        if self.product == \"GHS_SMOD\":\n            if self.resolution != 1000:\n                self.logger.warning(\n                    f\"Settlement model (SMOD) product is only available at 1000 (1km) resolution, resolution is set as 1000\"\n                )\n                self.resolution = 1000\n\n            if self.coord_system != CoordSystem.Mollweide:\n                self.logger.warning(\n                    f\"Settlement model (SMOD) product is only available with Mollweide ({CoordSystem.Mollweide}) projection, coordinate system is set as Mollweide\"\n                )\n                self.coord_system = CoordSystem.Mollweide\n\n        self.TILES_URL = self.TILES_URL.format(self.coord_system.value)\n        self._load_tiles()\n\n        return self\n\n    @property\n    def crs(self) -&gt; str:\n        return \"EPSG:4326\" if self.coord_system == CoordSystem.WGS84 else \"ESRI:54009\"\n\n    def get_relevant_data_units_by_geometry(\n        self, geometry: Union[BaseGeometry, gpd.GeoDataFrame], **kwargs\n    ) -&gt; List[dict]:\n        \"\"\"\n        Return intersecting tiles for a given geometry or GeoDataFrame.\n        \"\"\"\n        crs = kwargs.get(\"crs\", \"EPSG:4326\")  # assume WGS84 4326 if no crs passed\n        if self.tiles_gdf.crs != crs:\n            geometry = (\n                gpd.GeoDataFrame(geometry=[geometry], crs=crs)\n                .to_crs(self.tiles_gdf.crs)\n                .geometry[0]\n            )\n\n        # Find intersecting tiles\n        mask = (tile_geom.intersects(geometry) for tile_geom in self.tiles_gdf.geometry)\n\n        intersecting_tiles = self.tiles_gdf.loc[mask, \"tile_id\"].to_list()\n\n        return intersecting_tiles\n\n    def get_data_unit_path(self, unit: str = None, file_ext=\".zip\", **kwargs) -&gt; Path:\n        \"\"\"Construct and return the path for the configured dataset or dataset tile.\"\"\"\n        info = self._get_product_info()\n\n        tile_path = (\n            self.base_path\n            / info[\"product_folder\"]\n            / (\n                f\"{info['product_name']}_V{info['product_version']}_0\"\n                + (f\"_{unit}\" if unit else \"\")\n                + file_ext\n            )\n        )\n\n        return tile_path\n\n    def compute_dataset_url(self, tile_id=None) -&gt; str:\n        \"\"\"Compute the download URL for a GHSL dataset.\"\"\"\n        info = self._get_product_info()\n\n        path_segments = [\n            str(self.GHSL_DB_BASE_URL),\n            info[\"product_folder\"],\n            info[\"product_name\"],\n            f\"V{info['product_version']}-0\",\n            \"tiles\" if tile_id else \"\",\n            f\"{info['product_name']}_V{info['product_version']}_0\"\n            + (f\"_{tile_id}\" if tile_id else \"\")\n            + \".zip\",\n        ]\n\n        return \"/\".join(path_segments)\n\n    def _get_product_info(self) -&gt; dict:\n        \"\"\"Generate and return common product information used in multiple methods.\"\"\"\n        resolution_str = (\n            str(self.resolution)\n            if self.coord_system == CoordSystem.Mollweide\n            else (\"3ss\" if self.resolution == 100 else \"30ss\")\n        )\n        product_folder = f\"{self.product}_GLOBE_{self.release}\"\n        product_name = f\"{self.product}_E{self.year}_GLOBE_{self.release}_{self.coord_system.value}_{resolution_str}\"\n        product_version = 2 if self.product == \"GHS_SMOD\" else 1\n\n        return {\n            \"resolution_str\": resolution_str,\n            \"product_folder\": product_folder,\n            \"product_name\": product_name,\n            \"product_version\": product_version,\n        }\n\n    def __repr__(self) -&gt; str:\n        \"\"\"Return a string representation of the GHSL dataset configuration.\"\"\"\n        return (\n            f\"GHSLDataConfig(\"\n            f\"product='{self.product}', \"\n            f\"year={self.year}, \"\n            f\"resolution={self.resolution}, \"\n            f\"coord_system={self.coord_system.name}, \"\n            f\"release='{self.release}'\"\n            f\")\"\n        )\n</code></pre>"},{"location":"api/handlers/#gigaspatial.handlers.ghsl.GHSLDataConfig.__repr__","title":"<code>__repr__()</code>","text":"<p>Return a string representation of the GHSL dataset configuration.</p> Source code in <code>gigaspatial/handlers/ghsl.py</code> <pre><code>def __repr__(self) -&gt; str:\n    \"\"\"Return a string representation of the GHSL dataset configuration.\"\"\"\n    return (\n        f\"GHSLDataConfig(\"\n        f\"product='{self.product}', \"\n        f\"year={self.year}, \"\n        f\"resolution={self.resolution}, \"\n        f\"coord_system={self.coord_system.name}, \"\n        f\"release='{self.release}'\"\n        f\")\"\n    )\n</code></pre>"},{"location":"api/handlers/#gigaspatial.handlers.ghsl.GHSLDataConfig.compute_dataset_url","title":"<code>compute_dataset_url(tile_id=None)</code>","text":"<p>Compute the download URL for a GHSL dataset.</p> Source code in <code>gigaspatial/handlers/ghsl.py</code> <pre><code>def compute_dataset_url(self, tile_id=None) -&gt; str:\n    \"\"\"Compute the download URL for a GHSL dataset.\"\"\"\n    info = self._get_product_info()\n\n    path_segments = [\n        str(self.GHSL_DB_BASE_URL),\n        info[\"product_folder\"],\n        info[\"product_name\"],\n        f\"V{info['product_version']}-0\",\n        \"tiles\" if tile_id else \"\",\n        f\"{info['product_name']}_V{info['product_version']}_0\"\n        + (f\"_{tile_id}\" if tile_id else \"\")\n        + \".zip\",\n    ]\n\n    return \"/\".join(path_segments)\n</code></pre>"},{"location":"api/handlers/#gigaspatial.handlers.ghsl.GHSLDataConfig.get_data_unit_path","title":"<code>get_data_unit_path(unit=None, file_ext='.zip', **kwargs)</code>","text":"<p>Construct and return the path for the configured dataset or dataset tile.</p> Source code in <code>gigaspatial/handlers/ghsl.py</code> <pre><code>def get_data_unit_path(self, unit: str = None, file_ext=\".zip\", **kwargs) -&gt; Path:\n    \"\"\"Construct and return the path for the configured dataset or dataset tile.\"\"\"\n    info = self._get_product_info()\n\n    tile_path = (\n        self.base_path\n        / info[\"product_folder\"]\n        / (\n            f\"{info['product_name']}_V{info['product_version']}_0\"\n            + (f\"_{unit}\" if unit else \"\")\n            + file_ext\n        )\n    )\n\n    return tile_path\n</code></pre>"},{"location":"api/handlers/#gigaspatial.handlers.ghsl.GHSLDataConfig.get_relevant_data_units_by_geometry","title":"<code>get_relevant_data_units_by_geometry(geometry, **kwargs)</code>","text":"<p>Return intersecting tiles for a given geometry or GeoDataFrame.</p> Source code in <code>gigaspatial/handlers/ghsl.py</code> <pre><code>def get_relevant_data_units_by_geometry(\n    self, geometry: Union[BaseGeometry, gpd.GeoDataFrame], **kwargs\n) -&gt; List[dict]:\n    \"\"\"\n    Return intersecting tiles for a given geometry or GeoDataFrame.\n    \"\"\"\n    crs = kwargs.get(\"crs\", \"EPSG:4326\")  # assume WGS84 4326 if no crs passed\n    if self.tiles_gdf.crs != crs:\n        geometry = (\n            gpd.GeoDataFrame(geometry=[geometry], crs=crs)\n            .to_crs(self.tiles_gdf.crs)\n            .geometry[0]\n        )\n\n    # Find intersecting tiles\n    mask = (tile_geom.intersects(geometry) for tile_geom in self.tiles_gdf.geometry)\n\n    intersecting_tiles = self.tiles_gdf.loc[mask, \"tile_id\"].to_list()\n\n    return intersecting_tiles\n</code></pre>"},{"location":"api/handlers/#gigaspatial.handlers.ghsl.GHSLDataConfig.validate_configuration","title":"<code>validate_configuration()</code>","text":"<p>Validate that the configuration is valid based on dataset availability constraints.</p>"},{"location":"api/handlers/#gigaspatial.handlers.ghsl.GHSLDataConfig.validate_configuration--specific-rules","title":"Specific rules:","text":"Source code in <code>gigaspatial/handlers/ghsl.py</code> <pre><code>@model_validator(mode=\"after\")\ndef validate_configuration(self):\n    \"\"\"\n    Validate that the configuration is valid based on dataset availability constraints.\n\n    Specific rules:\n    -\n    \"\"\"\n    if self.year == 2018 and self.product in [\"GHS_BUILT_V\", \"GHS_POP\", \"GHS_SMOD\"]:\n        raise ValueError(f\"{self.product} product is not available for 2018\")\n\n    if self.resolution == 10 and self.product != \"GHS_BUILT_H\":\n        raise ValueError(\n            f\"{self.product} product is not available at 10 (10m) resolution\"\n        )\n\n    if \"GHS_BUILT_H\" in self.product:\n        if self.year != 2018:\n            self.logger.warning(\n                \"Building height product is only available for 2018, year is set as 2018\"\n            )\n            self.year = 2018\n\n    if self.product == \"GHS_BUILT_S\":\n        if self.year == 2018 and self.resolution != 10:\n            self.logger.warning(\n                \"Built-up surface product for 2018 is only available at 10m resolution, resolution is set as 10m\"\n            )\n            self.resolution = 10\n\n        if self.resolution == 10 and self.year != 2018:\n            self.logger.warning(\n                \"Built-up surface product at resolution 10 is only available for 2018, year is set as 2018\"\n            )\n            self.year = 2018\n\n        if self.resolution == 10 and self.coord_system != CoordSystem.Mollweide:\n            self.logger.warning(\n                f\"Built-up surface product at resolution 10 is only available with Mollweide ({CoordSystem.Mollweide}) projection, coordinate system is set as Mollweide\"\n            )\n            self.coord_system = CoordSystem.Mollweide\n\n    if self.product == \"GHS_SMOD\":\n        if self.resolution != 1000:\n            self.logger.warning(\n                f\"Settlement model (SMOD) product is only available at 1000 (1km) resolution, resolution is set as 1000\"\n            )\n            self.resolution = 1000\n\n        if self.coord_system != CoordSystem.Mollweide:\n            self.logger.warning(\n                f\"Settlement model (SMOD) product is only available with Mollweide ({CoordSystem.Mollweide}) projection, coordinate system is set as Mollweide\"\n            )\n            self.coord_system = CoordSystem.Mollweide\n\n    self.TILES_URL = self.TILES_URL.format(self.coord_system.value)\n    self._load_tiles()\n\n    return self\n</code></pre>"},{"location":"api/handlers/#gigaspatial.handlers.ghsl.GHSLDataDownloader","title":"<code>GHSLDataDownloader</code>","text":"<p>               Bases: <code>BaseHandlerDownloader</code></p> <p>A class to handle downloads of GHSL datasets.</p> Source code in <code>gigaspatial/handlers/ghsl.py</code> <pre><code>class GHSLDataDownloader(BaseHandlerDownloader):\n    \"\"\"A class to handle downloads of GHSL datasets.\"\"\"\n\n    def __init__(\n        self,\n        config: Union[GHSLDataConfig, dict[str, Union[str, int]]],\n        data_store: Optional[DataStore] = None,\n        logger: Optional[logging.Logger] = None,\n    ):\n        \"\"\"\n        Initialize the downloader.\n\n        Args:\n            config: Configuration for the GHSL dataset, either as a GHSLDataConfig object or a dictionary of parameters\n            data_store: Optional data storage interface. If not provided, uses LocalDataStore.\n            logger: Optional custom logger. If not provided, uses default logger.\n        \"\"\"\n        config = (\n            config if isinstance(config, GHSLDataConfig) else GHSLDataConfig(**config)\n        )\n        super().__init__(config=config, data_store=data_store, logger=logger)\n\n    def download_data_unit(\n        self,\n        tile_id: str,\n        extract: bool = True,\n        file_pattern: Optional[str] = r\".*\\.tif$\",\n        **kwargs,\n    ) -&gt; Optional[Union[Path, List[Path]]]:\n        \"\"\"\n        Downloads and optionally extracts files for a given tile.\n\n        Args:\n            tile_id: tile ID to process.\n            extract: If True and the downloaded file is a zip, extract its contents. Defaults to True.\n            file_pattern: Optional regex pattern to filter extracted files (if extract=True).\n            **kwargs: Additional parameters passed to download methods\n\n        Returns:\n            Path to the downloaded file if extract=False,\n            List of paths to the extracted files if extract=True,\n            None on failure.\n        \"\"\"\n        url = self.config.compute_dataset_url(tile_id=tile_id)\n        output_path = self.config.get_data_unit_path(tile_id)\n\n        if not extract:\n            return self._download_file(url, output_path)\n\n        extracted_files: List[Path] = []\n        temp_downloaded_path: Optional[Path] = None\n\n        try:\n            with tempfile.NamedTemporaryFile(delete=False, suffix=\".zip\") as temp_file:\n                temp_downloaded_path = Path(temp_file.name)\n                self.logger.debug(\n                    f\"Downloading {url} to temporary file: {temp_downloaded_path}\"\n                )\n\n                response = requests.get(url, stream=True)\n                response.raise_for_status()\n\n                total_size = int(response.headers.get(\"content-length\", 0))\n\n                with tqdm(\n                    total=total_size,\n                    unit=\"B\",\n                    unit_scale=True,\n                    desc=f\"Downloading {tile_id}\",\n                ) as pbar:\n                    for chunk in response.iter_content(chunk_size=8192):\n                        if chunk:\n                            temp_file.write(chunk)\n                            pbar.update(len(chunk))\n\n            self.logger.info(f\"Successfully downloaded temporary file!\")\n\n            with zipfile.ZipFile(str(temp_downloaded_path), \"r\") as zip_ref:\n                if file_pattern:\n                    import re\n\n                    pattern = re.compile(file_pattern)\n                    files_to_extract = [\n                        f for f in zip_ref.namelist() if pattern.match(f)\n                    ]\n                else:\n                    files_to_extract = zip_ref.namelist()\n\n                for file in files_to_extract:\n                    extracted_path = output_path.parent / Path(file).name\n                    with zip_ref.open(file) as source:\n                        file_content = source.read()\n                        self.data_store.write_file(str(extracted_path), file_content)\n                    extracted_files.append(extracted_path)\n                    self.logger.info(f\"Extracted {file} to {extracted_path}\")\n\n            Path(temp_file.name).unlink()\n            return extracted_files\n\n        except requests.exceptions.RequestException as e:\n            self.logger.error(f\"Failed to download {url} to temporary file: {e}\")\n            return None\n        except zipfile.BadZipFile:\n            self.logger.error(f\"Downloaded file for {tile_id} is not a valid zip file.\")\n            return None\n        except Exception as e:\n            self.logger.error(f\"Error downloading/extracting tile {tile_id}: {e}\")\n            return None\n        finally:\n            if temp_downloaded_path and temp_downloaded_path.exists():\n                try:\n                    temp_downloaded_path.unlink()\n                    self.logger.debug(f\"Deleted temporary file: {temp_downloaded_path}\")\n                except OSError as e:\n                    self.logger.warning(\n                        f\"Could not delete temporary file {temp_downloaded_path}: {e}\"\n                    )\n\n    def download_data_units(\n        self,\n        tile_ids: List[str],\n        extract: bool = True,\n        file_pattern: Optional[str] = r\".*\\.tif$\",\n        **kwargs,\n    ) -&gt; List[Optional[Union[Path, List[Path]]]]:\n        \"\"\"\n        Downloads multiple tiles in parallel, with an option to extract them.\n\n        Args:\n            tile_ids: A list of tile IDs to download.\n            extract: If True and the downloaded files are zips, extract their contents. Defaults to True.\n            file_pattern: Optional regex pattern to filter extracted files (if extract=True).\n            **kwargs: Additional parameters passed to download methods\n\n        Returns:\n            A list where each element corresponds to a tile ID and contains:\n            - Path to the downloaded file if extract=False.\n            - List of paths to extracted files if extract=True.\n            - None if the download or extraction failed for a tile.\n        \"\"\"\n        if not tile_ids:\n            self.logger.warning(\"No tiles to download\")\n            return []\n\n        with multiprocessing.Pool(processes=self.config.n_workers) as pool:\n            download_func = functools.partial(\n                self.download_data_unit, extract=extract, file_pattern=file_pattern\n            )\n            file_paths = list(\n                tqdm(\n                    pool.imap(download_func, tile_ids),\n                    total=len(tile_ids),\n                    desc=f\"Downloading data\",\n                )\n            )\n\n        return file_paths\n\n    def download(\n        self,\n        source: Union[\n            str,  # country\n            List[Union[Tuple[float, float], Point]],  # points\n            BaseGeometry,  # shapely geoms\n            gpd.GeoDataFrame,\n        ],\n        extract: bool = True,\n        file_pattern: Optional[str] = r\".*\\.tif$\",\n        **kwargs,\n    ) -&gt; List[Optional[Union[Path, List[Path]]]]:\n        \"\"\"\n        Download GHSL data for a specified geographic region.\n\n        The region can be defined by a country code/name, a list of points,\n        a Shapely geometry, or a GeoDataFrame. This method identifies the\n        relevant GHSL tiles intersecting the region and downloads the\n        specified type of data (polygons or points) for those tiles in parallel.\n\n        Args:\n            source: Defines the geographic area for which to download data.\n                    Can be:\n                      - A string representing a country code or name.\n                      - A list of (latitude, longitude) tuples or Shapely Point objects.\n                      - A Shapely BaseGeometry object (e.g., Polygon, MultiPolygon).\n                      - A GeoDataFrame with geometry column in EPSG:4326.\n            extract: If True and the downloaded files are zips, extract their contents. Defaults to True.\n            file_pattern: Optional regex pattern to filter extracted files (if extract=True).\n            **kwargs: Additional keyword arguments. These will be passed down to\n                      `AdminBoundaries.create()` (if `source` is a country)\n                      and to `self.download_data_units()`.\n\n        Returns:\n            A list of local file paths for the successfully downloaded tiles.\n            Returns an empty list if no data is found for the region or if\n            all downloads fail.\n        \"\"\"\n\n        tiles = self.config.get_relevant_data_units(source, **kwargs)\n        return self.download_data_units(\n            tiles, extract=extract, file_pattern=file_pattern, **kwargs\n        )\n\n    def download_by_country(\n        self,\n        country_code: str,\n        data_store: Optional[DataStore] = None,\n        country_geom_path: Optional[Union[str, Path]] = None,\n        extract: bool = True,\n        file_pattern: Optional[str] = r\".*\\.tif$\",\n        **kwargs,\n    ) -&gt; List[Optional[Union[Path, List[Path]]]]:\n        \"\"\"\n        Download GHSL data for a specific country.\n\n        This is a convenience method to download data for an entire country\n        using its code or name.\n\n        Args:\n            country_code: The country code (e.g., 'USA', 'GBR') or name.\n            data_store: Optional instance of a `DataStore` to be used by\n                        `AdminBoundaries` for loading country boundaries. If None,\n                        `AdminBoundaries` will use its default data loading.\n            country_geom_path: Optional path to a GeoJSON file containing the\n                               country boundary. If provided, this boundary is used\n                               instead of the default from `AdminBoundaries`.\n            extract: If True and the downloaded files are zips, extract their contents. Defaults to True.\n            file_pattern: Optional regex pattern to filter extracted files (if extract=True).\n            **kwargs: Additional keyword arguments that are passed to\n                      `download_data_units`. For example, `extract` to download and extract.\n\n        Returns:\n            A list of local file paths for the successfully downloaded tiles\n            for the specified country.\n        \"\"\"\n        return self.download(\n            source=country_code,\n            data_store=data_store,\n            path=country_geom_path,\n            extract=extract,\n            file_pattern=file_pattern,\n            **kwargs,\n        )\n\n    def _download_file(self, url: str, output_path: Path) -&gt; Optional[Path]:\n        \"\"\"\n        Downloads a file from a URL to a specified output path with a progress bar.\n\n        Args:\n            url: The URL to download from.\n            output_path: The local path to save the downloaded file.\n\n        Returns:\n            The path to the downloaded file on success, None on failure.\n        \"\"\"\n        try:\n            response = requests.get(url, stream=True)\n            response.raise_for_status()\n\n            total_size = int(response.headers.get(\"content-length\", 0))\n\n            with self.data_store.open(str(output_path), \"wb\") as file:\n                with tqdm(\n                    total=total_size,\n                    unit=\"B\",\n                    unit_scale=True,\n                    desc=f\"Downloading {output_path.name}\",\n                ) as pbar:\n                    for chunk in response.iter_content(chunk_size=8192):\n                        if chunk:\n                            file.write(chunk)\n                            pbar.update(len(chunk))\n\n            self.logger.debug(f\"Successfully downloaded: {url} to {output_path}\")\n            return output_path\n\n        except requests.exceptions.RequestException as e:\n            self.logger.error(f\"Failed to download {url}: {str(e)}\")\n            return None\n        except Exception as e:\n            self.logger.error(f\"Unexpected error downloading {url}: {str(e)}\")\n            return None\n</code></pre>"},{"location":"api/handlers/#gigaspatial.handlers.ghsl.GHSLDataDownloader.__init__","title":"<code>__init__(config, data_store=None, logger=None)</code>","text":"<p>Initialize the downloader.</p> <p>Parameters:</p> Name Type Description Default <code>config</code> <code>Union[GHSLDataConfig, dict[str, Union[str, int]]]</code> <p>Configuration for the GHSL dataset, either as a GHSLDataConfig object or a dictionary of parameters</p> required <code>data_store</code> <code>Optional[DataStore]</code> <p>Optional data storage interface. If not provided, uses LocalDataStore.</p> <code>None</code> <code>logger</code> <code>Optional[Logger]</code> <p>Optional custom logger. If not provided, uses default logger.</p> <code>None</code> Source code in <code>gigaspatial/handlers/ghsl.py</code> <pre><code>def __init__(\n    self,\n    config: Union[GHSLDataConfig, dict[str, Union[str, int]]],\n    data_store: Optional[DataStore] = None,\n    logger: Optional[logging.Logger] = None,\n):\n    \"\"\"\n    Initialize the downloader.\n\n    Args:\n        config: Configuration for the GHSL dataset, either as a GHSLDataConfig object or a dictionary of parameters\n        data_store: Optional data storage interface. If not provided, uses LocalDataStore.\n        logger: Optional custom logger. If not provided, uses default logger.\n    \"\"\"\n    config = (\n        config if isinstance(config, GHSLDataConfig) else GHSLDataConfig(**config)\n    )\n    super().__init__(config=config, data_store=data_store, logger=logger)\n</code></pre>"},{"location":"api/handlers/#gigaspatial.handlers.ghsl.GHSLDataDownloader.download","title":"<code>download(source, extract=True, file_pattern='.*\\\\.tif$', **kwargs)</code>","text":"<p>Download GHSL data for a specified geographic region.</p> <p>The region can be defined by a country code/name, a list of points, a Shapely geometry, or a GeoDataFrame. This method identifies the relevant GHSL tiles intersecting the region and downloads the specified type of data (polygons or points) for those tiles in parallel.</p> <p>Parameters:</p> Name Type Description Default <code>source</code> <code>Union[str, List[Union[Tuple[float, float], Point]], BaseGeometry, GeoDataFrame]</code> <p>Defines the geographic area for which to download data.     Can be:       - A string representing a country code or name.       - A list of (latitude, longitude) tuples or Shapely Point objects.       - A Shapely BaseGeometry object (e.g., Polygon, MultiPolygon).       - A GeoDataFrame with geometry column in EPSG:4326.</p> required <code>extract</code> <code>bool</code> <p>If True and the downloaded files are zips, extract their contents. Defaults to True.</p> <code>True</code> <code>file_pattern</code> <code>Optional[str]</code> <p>Optional regex pattern to filter extracted files (if extract=True).</p> <code>'.*\\\\.tif$'</code> <code>**kwargs</code> <p>Additional keyword arguments. These will be passed down to       <code>AdminBoundaries.create()</code> (if <code>source</code> is a country)       and to <code>self.download_data_units()</code>.</p> <code>{}</code> <p>Returns:</p> Type Description <code>List[Optional[Union[Path, List[Path]]]]</code> <p>A list of local file paths for the successfully downloaded tiles.</p> <code>List[Optional[Union[Path, List[Path]]]]</code> <p>Returns an empty list if no data is found for the region or if</p> <code>List[Optional[Union[Path, List[Path]]]]</code> <p>all downloads fail.</p> Source code in <code>gigaspatial/handlers/ghsl.py</code> <pre><code>def download(\n    self,\n    source: Union[\n        str,  # country\n        List[Union[Tuple[float, float], Point]],  # points\n        BaseGeometry,  # shapely geoms\n        gpd.GeoDataFrame,\n    ],\n    extract: bool = True,\n    file_pattern: Optional[str] = r\".*\\.tif$\",\n    **kwargs,\n) -&gt; List[Optional[Union[Path, List[Path]]]]:\n    \"\"\"\n    Download GHSL data for a specified geographic region.\n\n    The region can be defined by a country code/name, a list of points,\n    a Shapely geometry, or a GeoDataFrame. This method identifies the\n    relevant GHSL tiles intersecting the region and downloads the\n    specified type of data (polygons or points) for those tiles in parallel.\n\n    Args:\n        source: Defines the geographic area for which to download data.\n                Can be:\n                  - A string representing a country code or name.\n                  - A list of (latitude, longitude) tuples or Shapely Point objects.\n                  - A Shapely BaseGeometry object (e.g., Polygon, MultiPolygon).\n                  - A GeoDataFrame with geometry column in EPSG:4326.\n        extract: If True and the downloaded files are zips, extract their contents. Defaults to True.\n        file_pattern: Optional regex pattern to filter extracted files (if extract=True).\n        **kwargs: Additional keyword arguments. These will be passed down to\n                  `AdminBoundaries.create()` (if `source` is a country)\n                  and to `self.download_data_units()`.\n\n    Returns:\n        A list of local file paths for the successfully downloaded tiles.\n        Returns an empty list if no data is found for the region or if\n        all downloads fail.\n    \"\"\"\n\n    tiles = self.config.get_relevant_data_units(source, **kwargs)\n    return self.download_data_units(\n        tiles, extract=extract, file_pattern=file_pattern, **kwargs\n    )\n</code></pre>"},{"location":"api/handlers/#gigaspatial.handlers.ghsl.GHSLDataDownloader.download_by_country","title":"<code>download_by_country(country_code, data_store=None, country_geom_path=None, extract=True, file_pattern='.*\\\\.tif$', **kwargs)</code>","text":"<p>Download GHSL data for a specific country.</p> <p>This is a convenience method to download data for an entire country using its code or name.</p> <p>Parameters:</p> Name Type Description Default <code>country_code</code> <code>str</code> <p>The country code (e.g., 'USA', 'GBR') or name.</p> required <code>data_store</code> <code>Optional[DataStore]</code> <p>Optional instance of a <code>DataStore</code> to be used by         <code>AdminBoundaries</code> for loading country boundaries. If None,         <code>AdminBoundaries</code> will use its default data loading.</p> <code>None</code> <code>country_geom_path</code> <code>Optional[Union[str, Path]]</code> <p>Optional path to a GeoJSON file containing the                country boundary. If provided, this boundary is used                instead of the default from <code>AdminBoundaries</code>.</p> <code>None</code> <code>extract</code> <code>bool</code> <p>If True and the downloaded files are zips, extract their contents. Defaults to True.</p> <code>True</code> <code>file_pattern</code> <code>Optional[str]</code> <p>Optional regex pattern to filter extracted files (if extract=True).</p> <code>'.*\\\\.tif$'</code> <code>**kwargs</code> <p>Additional keyword arguments that are passed to       <code>download_data_units</code>. For example, <code>extract</code> to download and extract.</p> <code>{}</code> <p>Returns:</p> Type Description <code>List[Optional[Union[Path, List[Path]]]]</code> <p>A list of local file paths for the successfully downloaded tiles</p> <code>List[Optional[Union[Path, List[Path]]]]</code> <p>for the specified country.</p> Source code in <code>gigaspatial/handlers/ghsl.py</code> <pre><code>def download_by_country(\n    self,\n    country_code: str,\n    data_store: Optional[DataStore] = None,\n    country_geom_path: Optional[Union[str, Path]] = None,\n    extract: bool = True,\n    file_pattern: Optional[str] = r\".*\\.tif$\",\n    **kwargs,\n) -&gt; List[Optional[Union[Path, List[Path]]]]:\n    \"\"\"\n    Download GHSL data for a specific country.\n\n    This is a convenience method to download data for an entire country\n    using its code or name.\n\n    Args:\n        country_code: The country code (e.g., 'USA', 'GBR') or name.\n        data_store: Optional instance of a `DataStore` to be used by\n                    `AdminBoundaries` for loading country boundaries. If None,\n                    `AdminBoundaries` will use its default data loading.\n        country_geom_path: Optional path to a GeoJSON file containing the\n                           country boundary. If provided, this boundary is used\n                           instead of the default from `AdminBoundaries`.\n        extract: If True and the downloaded files are zips, extract their contents. Defaults to True.\n        file_pattern: Optional regex pattern to filter extracted files (if extract=True).\n        **kwargs: Additional keyword arguments that are passed to\n                  `download_data_units`. For example, `extract` to download and extract.\n\n    Returns:\n        A list of local file paths for the successfully downloaded tiles\n        for the specified country.\n    \"\"\"\n    return self.download(\n        source=country_code,\n        data_store=data_store,\n        path=country_geom_path,\n        extract=extract,\n        file_pattern=file_pattern,\n        **kwargs,\n    )\n</code></pre>"},{"location":"api/handlers/#gigaspatial.handlers.ghsl.GHSLDataDownloader.download_data_unit","title":"<code>download_data_unit(tile_id, extract=True, file_pattern='.*\\\\.tif$', **kwargs)</code>","text":"<p>Downloads and optionally extracts files for a given tile.</p> <p>Parameters:</p> Name Type Description Default <code>tile_id</code> <code>str</code> <p>tile ID to process.</p> required <code>extract</code> <code>bool</code> <p>If True and the downloaded file is a zip, extract its contents. Defaults to True.</p> <code>True</code> <code>file_pattern</code> <code>Optional[str]</code> <p>Optional regex pattern to filter extracted files (if extract=True).</p> <code>'.*\\\\.tif$'</code> <code>**kwargs</code> <p>Additional parameters passed to download methods</p> <code>{}</code> <p>Returns:</p> Type Description <code>Optional[Union[Path, List[Path]]]</code> <p>Path to the downloaded file if extract=False,</p> <code>Optional[Union[Path, List[Path]]]</code> <p>List of paths to the extracted files if extract=True,</p> <code>Optional[Union[Path, List[Path]]]</code> <p>None on failure.</p> Source code in <code>gigaspatial/handlers/ghsl.py</code> <pre><code>def download_data_unit(\n    self,\n    tile_id: str,\n    extract: bool = True,\n    file_pattern: Optional[str] = r\".*\\.tif$\",\n    **kwargs,\n) -&gt; Optional[Union[Path, List[Path]]]:\n    \"\"\"\n    Downloads and optionally extracts files for a given tile.\n\n    Args:\n        tile_id: tile ID to process.\n        extract: If True and the downloaded file is a zip, extract its contents. Defaults to True.\n        file_pattern: Optional regex pattern to filter extracted files (if extract=True).\n        **kwargs: Additional parameters passed to download methods\n\n    Returns:\n        Path to the downloaded file if extract=False,\n        List of paths to the extracted files if extract=True,\n        None on failure.\n    \"\"\"\n    url = self.config.compute_dataset_url(tile_id=tile_id)\n    output_path = self.config.get_data_unit_path(tile_id)\n\n    if not extract:\n        return self._download_file(url, output_path)\n\n    extracted_files: List[Path] = []\n    temp_downloaded_path: Optional[Path] = None\n\n    try:\n        with tempfile.NamedTemporaryFile(delete=False, suffix=\".zip\") as temp_file:\n            temp_downloaded_path = Path(temp_file.name)\n            self.logger.debug(\n                f\"Downloading {url} to temporary file: {temp_downloaded_path}\"\n            )\n\n            response = requests.get(url, stream=True)\n            response.raise_for_status()\n\n            total_size = int(response.headers.get(\"content-length\", 0))\n\n            with tqdm(\n                total=total_size,\n                unit=\"B\",\n                unit_scale=True,\n                desc=f\"Downloading {tile_id}\",\n            ) as pbar:\n                for chunk in response.iter_content(chunk_size=8192):\n                    if chunk:\n                        temp_file.write(chunk)\n                        pbar.update(len(chunk))\n\n        self.logger.info(f\"Successfully downloaded temporary file!\")\n\n        with zipfile.ZipFile(str(temp_downloaded_path), \"r\") as zip_ref:\n            if file_pattern:\n                import re\n\n                pattern = re.compile(file_pattern)\n                files_to_extract = [\n                    f for f in zip_ref.namelist() if pattern.match(f)\n                ]\n            else:\n                files_to_extract = zip_ref.namelist()\n\n            for file in files_to_extract:\n                extracted_path = output_path.parent / Path(file).name\n                with zip_ref.open(file) as source:\n                    file_content = source.read()\n                    self.data_store.write_file(str(extracted_path), file_content)\n                extracted_files.append(extracted_path)\n                self.logger.info(f\"Extracted {file} to {extracted_path}\")\n\n        Path(temp_file.name).unlink()\n        return extracted_files\n\n    except requests.exceptions.RequestException as e:\n        self.logger.error(f\"Failed to download {url} to temporary file: {e}\")\n        return None\n    except zipfile.BadZipFile:\n        self.logger.error(f\"Downloaded file for {tile_id} is not a valid zip file.\")\n        return None\n    except Exception as e:\n        self.logger.error(f\"Error downloading/extracting tile {tile_id}: {e}\")\n        return None\n    finally:\n        if temp_downloaded_path and temp_downloaded_path.exists():\n            try:\n                temp_downloaded_path.unlink()\n                self.logger.debug(f\"Deleted temporary file: {temp_downloaded_path}\")\n            except OSError as e:\n                self.logger.warning(\n                    f\"Could not delete temporary file {temp_downloaded_path}: {e}\"\n                )\n</code></pre>"},{"location":"api/handlers/#gigaspatial.handlers.ghsl.GHSLDataDownloader.download_data_units","title":"<code>download_data_units(tile_ids, extract=True, file_pattern='.*\\\\.tif$', **kwargs)</code>","text":"<p>Downloads multiple tiles in parallel, with an option to extract them.</p> <p>Parameters:</p> Name Type Description Default <code>tile_ids</code> <code>List[str]</code> <p>A list of tile IDs to download.</p> required <code>extract</code> <code>bool</code> <p>If True and the downloaded files are zips, extract their contents. Defaults to True.</p> <code>True</code> <code>file_pattern</code> <code>Optional[str]</code> <p>Optional regex pattern to filter extracted files (if extract=True).</p> <code>'.*\\\\.tif$'</code> <code>**kwargs</code> <p>Additional parameters passed to download methods</p> <code>{}</code> <p>Returns:</p> Type Description <code>List[Optional[Union[Path, List[Path]]]]</code> <p>A list where each element corresponds to a tile ID and contains:</p> <code>List[Optional[Union[Path, List[Path]]]]</code> <ul> <li>Path to the downloaded file if extract=False.</li> </ul> <code>List[Optional[Union[Path, List[Path]]]]</code> <ul> <li>List of paths to extracted files if extract=True.</li> </ul> <code>List[Optional[Union[Path, List[Path]]]]</code> <ul> <li>None if the download or extraction failed for a tile.</li> </ul> Source code in <code>gigaspatial/handlers/ghsl.py</code> <pre><code>def download_data_units(\n    self,\n    tile_ids: List[str],\n    extract: bool = True,\n    file_pattern: Optional[str] = r\".*\\.tif$\",\n    **kwargs,\n) -&gt; List[Optional[Union[Path, List[Path]]]]:\n    \"\"\"\n    Downloads multiple tiles in parallel, with an option to extract them.\n\n    Args:\n        tile_ids: A list of tile IDs to download.\n        extract: If True and the downloaded files are zips, extract their contents. Defaults to True.\n        file_pattern: Optional regex pattern to filter extracted files (if extract=True).\n        **kwargs: Additional parameters passed to download methods\n\n    Returns:\n        A list where each element corresponds to a tile ID and contains:\n        - Path to the downloaded file if extract=False.\n        - List of paths to extracted files if extract=True.\n        - None if the download or extraction failed for a tile.\n    \"\"\"\n    if not tile_ids:\n        self.logger.warning(\"No tiles to download\")\n        return []\n\n    with multiprocessing.Pool(processes=self.config.n_workers) as pool:\n        download_func = functools.partial(\n            self.download_data_unit, extract=extract, file_pattern=file_pattern\n        )\n        file_paths = list(\n            tqdm(\n                pool.imap(download_func, tile_ids),\n                total=len(tile_ids),\n                desc=f\"Downloading data\",\n            )\n        )\n\n    return file_paths\n</code></pre>"},{"location":"api/handlers/#gigaspatial.handlers.ghsl.GHSLDataHandler","title":"<code>GHSLDataHandler</code>","text":"<p>               Bases: <code>BaseHandler</code></p> <p>Handler for GHSL (Global Human Settlement Layer) dataset.</p> <p>This class provides a unified interface for downloading and loading GHSL data. It manages the lifecycle of configuration, downloading, and reading components.</p> Source code in <code>gigaspatial/handlers/ghsl.py</code> <pre><code>class GHSLDataHandler(BaseHandler):\n    \"\"\"\n    Handler for GHSL (Global Human Settlement Layer) dataset.\n\n    This class provides a unified interface for downloading and loading GHSL data.\n    It manages the lifecycle of configuration, downloading, and reading components.\n    \"\"\"\n\n    def __init__(\n        self,\n        product: Literal[\n            \"GHS_BUILT_S\",\n            \"GHS_BUILT_H_AGBH\",\n            \"GHS_BUILT_H_ANBH\",\n            \"GHS_BUILT_V\",\n            \"GHS_POP\",\n            \"GHS_SMOD\",\n        ],\n        year: int = 2020,\n        resolution: int = 100,\n        config: Optional[GHSLDataConfig] = None,\n        downloader: Optional[GHSLDataDownloader] = None,\n        reader: Optional[GHSLDataReader] = None,\n        data_store: Optional[DataStore] = None,\n        logger: Optional[logging.Logger] = None,\n        **kwargs,\n    ):\n        \"\"\"\n        Initialize the GHSLDataHandler.\n\n        Args:\n            product: The GHSL product to use. Must be one of:\n                    - GHS_BUILT_S: Built-up surface\n                    - GHS_BUILT_H_AGBH: Average building height\n                    - GHS_BUILT_H_ANBH: Average number of building heights\n                    - GHS_BUILT_V: Building volume\n                    - GHS_POP: Population\n                    - GHS_SMOD: Settlement model\n            year: The year of the data (default: 2020)\n            resolution: The resolution in meters (default: 100)\n            config: Optional configuration object\n            downloader: Optional downloader instance\n            reader: Optional reader instance\n            data_store: Optional data store instance\n            logger: Optional logger instance\n            **kwargs: Additional configuration parameters\n        \"\"\"\n        self._product = product\n        self._year = year\n        self._resolution = resolution\n        super().__init__(\n            config=config,\n            downloader=downloader,\n            reader=reader,\n            data_store=data_store,\n            logger=logger,\n        )\n\n    def create_config(\n        self, data_store: DataStore, logger: logging.Logger, **kwargs\n    ) -&gt; GHSLDataConfig:\n        \"\"\"\n        Create and return a GHSLDataConfig instance.\n\n        Args:\n            data_store: The data store instance to use\n            logger: The logger instance to use\n            **kwargs: Additional configuration parameters\n\n        Returns:\n            Configured GHSLDataConfig instance\n        \"\"\"\n        return GHSLDataConfig(\n            product=self._product,\n            year=self._year,\n            resolution=self._resolution,\n            data_store=data_store,\n            logger=logger,\n            **kwargs,\n        )\n\n    def create_downloader(\n        self,\n        config: GHSLDataConfig,\n        data_store: DataStore,\n        logger: logging.Logger,\n        **kwargs,\n    ) -&gt; GHSLDataDownloader:\n        \"\"\"\n        Create and return a GHSLDataDownloader instance.\n\n        Args:\n            config: The configuration object\n            data_store: The data store instance to use\n            logger: The logger instance to use\n            **kwargs: Additional downloader parameters\n\n        Returns:\n            Configured GHSLDataDownloader instance\n        \"\"\"\n        return GHSLDataDownloader(\n            config=config, data_store=data_store, logger=logger, **kwargs\n        )\n\n    def create_reader(\n        self,\n        config: GHSLDataConfig,\n        data_store: DataStore,\n        logger: logging.Logger,\n        **kwargs,\n    ) -&gt; GHSLDataReader:\n        \"\"\"\n        Create and return a GHSLDataReader instance.\n\n        Args:\n            config: The configuration object\n            data_store: The data store instance to use\n            logger: The logger instance to use\n            **kwargs: Additional reader parameters\n\n        Returns:\n            Configured GHSLDataReader instance\n        \"\"\"\n        return GHSLDataReader(\n            config=config, data_store=data_store, logger=logger, **kwargs\n        )\n\n    def load_data(\n        self,\n        source: Union[\n            str,  # country\n            List[Union[tuple, Point]],  # points\n            BaseGeometry,  # geometry\n            gpd.GeoDataFrame,  # geodataframe\n            Path,  # path\n            List[Union[str, Path]],  # list of paths\n        ],\n        crop_to_source: bool = False,\n        ensure_available: bool = True,\n        merge_rasters: bool = False,\n        **kwargs,\n    ):\n        return super().load_data(\n            source=source,\n            crop_to_source=crop_to_source,\n            ensure_available=ensure_available,\n            file_ext=\".tif\",\n            extract=True,\n            file_pattern=r\".*\\.tif$\",\n            merge_rasters=merge_rasters,\n            **kwargs,\n        )\n\n    def load_into_dataframe(\n        self,\n        source: Union[\n            str,  # country\n            List[Union[tuple, Point]],  # points\n            BaseGeometry,  # geometry\n            gpd.GeoDataFrame,  # geodataframe\n            Path,  # path\n            List[Union[str, Path]],  # list of paths\n        ],\n        crop_to_source: bool = False,\n        ensure_available: bool = True,\n        **kwargs,\n    ) -&gt; pd.DataFrame:\n        \"\"\"\n        Load GHSL data into a pandas DataFrame.\n\n        Args:\n            source: The data source specification\n            ensure_available: If True, ensure data is downloaded before loading\n            **kwargs: Additional parameters passed to load methods\n\n        Returns:\n            DataFrame containing the GHSL data\n        \"\"\"\n        tif_processors = self.load_data(\n            source=source,\n            crop_to_source=crop_to_source,\n            ensure_available=ensure_available,\n            **kwargs,\n        )\n        if isinstance(tif_processors, TifProcessor):\n            return tif_processors.to_dataframe(**kwargs)\n        return pd.concat(\n            [tp.to_dataframe(**kwargs) for tp in tif_processors], ignore_index=True\n        )\n\n    def load_into_geodataframe(\n        self,\n        source: Union[\n            str,  # country\n            List[Union[tuple, Point]],  # points\n            BaseGeometry,  # geometry\n            gpd.GeoDataFrame,  # geodataframe\n            Path,  # path\n            List[Union[str, Path]],  # list of paths\n        ],\n        crop_to_source: bool = False,\n        ensure_available: bool = True,\n        **kwargs,\n    ) -&gt; gpd.GeoDataFrame:\n        \"\"\"\n        Load GHSL data into a geopandas GeoDataFrame.\n\n        Args:\n            source: The data source specification\n            ensure_available: If True, ensure data is downloaded before loading\n            **kwargs: Additional parameters passed to load methods\n\n        Returns:\n            GeoDataFrame containing the GHSL data\n        \"\"\"\n        tif_processors = self.load_data(\n            source=source,\n            crop_to_source=crop_to_source,\n            ensure_available=ensure_available,\n            **kwargs,\n        )\n        if isinstance(tif_processors, TifProcessor):\n            return tif_processors.to_geodataframe(**kwargs)\n        return pd.concat(\n            [tp.to_geodataframe(**kwargs) for tp in tif_processors], ignore_index=True\n        )\n\n    def get_available_data_info(\n        self,\n        source: Union[\n            str,  # country\n            List[Union[tuple, Point]],  # points\n            BaseGeometry,  # geometry\n            gpd.GeoDataFrame,  # geodataframe\n        ],\n        **kwargs,\n    ) -&gt; dict:\n        return super().get_available_data_info(source, file_ext=\".tif\", **kwargs)\n</code></pre>"},{"location":"api/handlers/#gigaspatial.handlers.ghsl.GHSLDataHandler.__init__","title":"<code>__init__(product, year=2020, resolution=100, config=None, downloader=None, reader=None, data_store=None, logger=None, **kwargs)</code>","text":"<p>Initialize the GHSLDataHandler.</p> <p>Parameters:</p> Name Type Description Default <code>product</code> <code>Literal['GHS_BUILT_S', 'GHS_BUILT_H_AGBH', 'GHS_BUILT_H_ANBH', 'GHS_BUILT_V', 'GHS_POP', 'GHS_SMOD']</code> <p>The GHSL product to use. Must be one of:     - GHS_BUILT_S: Built-up surface     - GHS_BUILT_H_AGBH: Average building height     - GHS_BUILT_H_ANBH: Average number of building heights     - GHS_BUILT_V: Building volume     - GHS_POP: Population     - GHS_SMOD: Settlement model</p> required <code>year</code> <code>int</code> <p>The year of the data (default: 2020)</p> <code>2020</code> <code>resolution</code> <code>int</code> <p>The resolution in meters (default: 100)</p> <code>100</code> <code>config</code> <code>Optional[GHSLDataConfig]</code> <p>Optional configuration object</p> <code>None</code> <code>downloader</code> <code>Optional[GHSLDataDownloader]</code> <p>Optional downloader instance</p> <code>None</code> <code>reader</code> <code>Optional[GHSLDataReader]</code> <p>Optional reader instance</p> <code>None</code> <code>data_store</code> <code>Optional[DataStore]</code> <p>Optional data store instance</p> <code>None</code> <code>logger</code> <code>Optional[Logger]</code> <p>Optional logger instance</p> <code>None</code> <code>**kwargs</code> <p>Additional configuration parameters</p> <code>{}</code> Source code in <code>gigaspatial/handlers/ghsl.py</code> <pre><code>def __init__(\n    self,\n    product: Literal[\n        \"GHS_BUILT_S\",\n        \"GHS_BUILT_H_AGBH\",\n        \"GHS_BUILT_H_ANBH\",\n        \"GHS_BUILT_V\",\n        \"GHS_POP\",\n        \"GHS_SMOD\",\n    ],\n    year: int = 2020,\n    resolution: int = 100,\n    config: Optional[GHSLDataConfig] = None,\n    downloader: Optional[GHSLDataDownloader] = None,\n    reader: Optional[GHSLDataReader] = None,\n    data_store: Optional[DataStore] = None,\n    logger: Optional[logging.Logger] = None,\n    **kwargs,\n):\n    \"\"\"\n    Initialize the GHSLDataHandler.\n\n    Args:\n        product: The GHSL product to use. Must be one of:\n                - GHS_BUILT_S: Built-up surface\n                - GHS_BUILT_H_AGBH: Average building height\n                - GHS_BUILT_H_ANBH: Average number of building heights\n                - GHS_BUILT_V: Building volume\n                - GHS_POP: Population\n                - GHS_SMOD: Settlement model\n        year: The year of the data (default: 2020)\n        resolution: The resolution in meters (default: 100)\n        config: Optional configuration object\n        downloader: Optional downloader instance\n        reader: Optional reader instance\n        data_store: Optional data store instance\n        logger: Optional logger instance\n        **kwargs: Additional configuration parameters\n    \"\"\"\n    self._product = product\n    self._year = year\n    self._resolution = resolution\n    super().__init__(\n        config=config,\n        downloader=downloader,\n        reader=reader,\n        data_store=data_store,\n        logger=logger,\n    )\n</code></pre>"},{"location":"api/handlers/#gigaspatial.handlers.ghsl.GHSLDataHandler.create_config","title":"<code>create_config(data_store, logger, **kwargs)</code>","text":"<p>Create and return a GHSLDataConfig instance.</p> <p>Parameters:</p> Name Type Description Default <code>data_store</code> <code>DataStore</code> <p>The data store instance to use</p> required <code>logger</code> <code>Logger</code> <p>The logger instance to use</p> required <code>**kwargs</code> <p>Additional configuration parameters</p> <code>{}</code> <p>Returns:</p> Type Description <code>GHSLDataConfig</code> <p>Configured GHSLDataConfig instance</p> Source code in <code>gigaspatial/handlers/ghsl.py</code> <pre><code>def create_config(\n    self, data_store: DataStore, logger: logging.Logger, **kwargs\n) -&gt; GHSLDataConfig:\n    \"\"\"\n    Create and return a GHSLDataConfig instance.\n\n    Args:\n        data_store: The data store instance to use\n        logger: The logger instance to use\n        **kwargs: Additional configuration parameters\n\n    Returns:\n        Configured GHSLDataConfig instance\n    \"\"\"\n    return GHSLDataConfig(\n        product=self._product,\n        year=self._year,\n        resolution=self._resolution,\n        data_store=data_store,\n        logger=logger,\n        **kwargs,\n    )\n</code></pre>"},{"location":"api/handlers/#gigaspatial.handlers.ghsl.GHSLDataHandler.create_downloader","title":"<code>create_downloader(config, data_store, logger, **kwargs)</code>","text":"<p>Create and return a GHSLDataDownloader instance.</p> <p>Parameters:</p> Name Type Description Default <code>config</code> <code>GHSLDataConfig</code> <p>The configuration object</p> required <code>data_store</code> <code>DataStore</code> <p>The data store instance to use</p> required <code>logger</code> <code>Logger</code> <p>The logger instance to use</p> required <code>**kwargs</code> <p>Additional downloader parameters</p> <code>{}</code> <p>Returns:</p> Type Description <code>GHSLDataDownloader</code> <p>Configured GHSLDataDownloader instance</p> Source code in <code>gigaspatial/handlers/ghsl.py</code> <pre><code>def create_downloader(\n    self,\n    config: GHSLDataConfig,\n    data_store: DataStore,\n    logger: logging.Logger,\n    **kwargs,\n) -&gt; GHSLDataDownloader:\n    \"\"\"\n    Create and return a GHSLDataDownloader instance.\n\n    Args:\n        config: The configuration object\n        data_store: The data store instance to use\n        logger: The logger instance to use\n        **kwargs: Additional downloader parameters\n\n    Returns:\n        Configured GHSLDataDownloader instance\n    \"\"\"\n    return GHSLDataDownloader(\n        config=config, data_store=data_store, logger=logger, **kwargs\n    )\n</code></pre>"},{"location":"api/handlers/#gigaspatial.handlers.ghsl.GHSLDataHandler.create_reader","title":"<code>create_reader(config, data_store, logger, **kwargs)</code>","text":"<p>Create and return a GHSLDataReader instance.</p> <p>Parameters:</p> Name Type Description Default <code>config</code> <code>GHSLDataConfig</code> <p>The configuration object</p> required <code>data_store</code> <code>DataStore</code> <p>The data store instance to use</p> required <code>logger</code> <code>Logger</code> <p>The logger instance to use</p> required <code>**kwargs</code> <p>Additional reader parameters</p> <code>{}</code> <p>Returns:</p> Type Description <code>GHSLDataReader</code> <p>Configured GHSLDataReader instance</p> Source code in <code>gigaspatial/handlers/ghsl.py</code> <pre><code>def create_reader(\n    self,\n    config: GHSLDataConfig,\n    data_store: DataStore,\n    logger: logging.Logger,\n    **kwargs,\n) -&gt; GHSLDataReader:\n    \"\"\"\n    Create and return a GHSLDataReader instance.\n\n    Args:\n        config: The configuration object\n        data_store: The data store instance to use\n        logger: The logger instance to use\n        **kwargs: Additional reader parameters\n\n    Returns:\n        Configured GHSLDataReader instance\n    \"\"\"\n    return GHSLDataReader(\n        config=config, data_store=data_store, logger=logger, **kwargs\n    )\n</code></pre>"},{"location":"api/handlers/#gigaspatial.handlers.ghsl.GHSLDataHandler.load_into_dataframe","title":"<code>load_into_dataframe(source, crop_to_source=False, ensure_available=True, **kwargs)</code>","text":"<p>Load GHSL data into a pandas DataFrame.</p> <p>Parameters:</p> Name Type Description Default <code>source</code> <code>Union[str, List[Union[tuple, Point]], BaseGeometry, GeoDataFrame, Path, List[Union[str, Path]]]</code> <p>The data source specification</p> required <code>ensure_available</code> <code>bool</code> <p>If True, ensure data is downloaded before loading</p> <code>True</code> <code>**kwargs</code> <p>Additional parameters passed to load methods</p> <code>{}</code> <p>Returns:</p> Type Description <code>DataFrame</code> <p>DataFrame containing the GHSL data</p> Source code in <code>gigaspatial/handlers/ghsl.py</code> <pre><code>def load_into_dataframe(\n    self,\n    source: Union[\n        str,  # country\n        List[Union[tuple, Point]],  # points\n        BaseGeometry,  # geometry\n        gpd.GeoDataFrame,  # geodataframe\n        Path,  # path\n        List[Union[str, Path]],  # list of paths\n    ],\n    crop_to_source: bool = False,\n    ensure_available: bool = True,\n    **kwargs,\n) -&gt; pd.DataFrame:\n    \"\"\"\n    Load GHSL data into a pandas DataFrame.\n\n    Args:\n        source: The data source specification\n        ensure_available: If True, ensure data is downloaded before loading\n        **kwargs: Additional parameters passed to load methods\n\n    Returns:\n        DataFrame containing the GHSL data\n    \"\"\"\n    tif_processors = self.load_data(\n        source=source,\n        crop_to_source=crop_to_source,\n        ensure_available=ensure_available,\n        **kwargs,\n    )\n    if isinstance(tif_processors, TifProcessor):\n        return tif_processors.to_dataframe(**kwargs)\n    return pd.concat(\n        [tp.to_dataframe(**kwargs) for tp in tif_processors], ignore_index=True\n    )\n</code></pre>"},{"location":"api/handlers/#gigaspatial.handlers.ghsl.GHSLDataHandler.load_into_geodataframe","title":"<code>load_into_geodataframe(source, crop_to_source=False, ensure_available=True, **kwargs)</code>","text":"<p>Load GHSL data into a geopandas GeoDataFrame.</p> <p>Parameters:</p> Name Type Description Default <code>source</code> <code>Union[str, List[Union[tuple, Point]], BaseGeometry, GeoDataFrame, Path, List[Union[str, Path]]]</code> <p>The data source specification</p> required <code>ensure_available</code> <code>bool</code> <p>If True, ensure data is downloaded before loading</p> <code>True</code> <code>**kwargs</code> <p>Additional parameters passed to load methods</p> <code>{}</code> <p>Returns:</p> Type Description <code>GeoDataFrame</code> <p>GeoDataFrame containing the GHSL data</p> Source code in <code>gigaspatial/handlers/ghsl.py</code> <pre><code>def load_into_geodataframe(\n    self,\n    source: Union[\n        str,  # country\n        List[Union[tuple, Point]],  # points\n        BaseGeometry,  # geometry\n        gpd.GeoDataFrame,  # geodataframe\n        Path,  # path\n        List[Union[str, Path]],  # list of paths\n    ],\n    crop_to_source: bool = False,\n    ensure_available: bool = True,\n    **kwargs,\n) -&gt; gpd.GeoDataFrame:\n    \"\"\"\n    Load GHSL data into a geopandas GeoDataFrame.\n\n    Args:\n        source: The data source specification\n        ensure_available: If True, ensure data is downloaded before loading\n        **kwargs: Additional parameters passed to load methods\n\n    Returns:\n        GeoDataFrame containing the GHSL data\n    \"\"\"\n    tif_processors = self.load_data(\n        source=source,\n        crop_to_source=crop_to_source,\n        ensure_available=ensure_available,\n        **kwargs,\n    )\n    if isinstance(tif_processors, TifProcessor):\n        return tif_processors.to_geodataframe(**kwargs)\n    return pd.concat(\n        [tp.to_geodataframe(**kwargs) for tp in tif_processors], ignore_index=True\n    )\n</code></pre>"},{"location":"api/handlers/#gigaspatial.handlers.ghsl.GHSLDataReader","title":"<code>GHSLDataReader</code>","text":"<p>               Bases: <code>BaseHandlerReader</code></p> Source code in <code>gigaspatial/handlers/ghsl.py</code> <pre><code>class GHSLDataReader(BaseHandlerReader):\n\n    def __init__(\n        self,\n        config: Union[GHSLDataConfig, dict[str, Union[str, int]]],\n        data_store: Optional[DataStore] = None,\n        logger: Optional[logging.Logger] = None,\n    ):\n        \"\"\"\n        Initialize the reader.\n\n        Args:\n            config: Configuration for the GHSL dataset, either as a GHSLDataConfig object or a dictionary of parameters\n            data_store: Optional data storage interface. If not provided, uses LocalDataStore.\n            logger: Optional custom logger. If not provided, uses default logger.\n        \"\"\"\n        config = (\n            config if isinstance(config, GHSLDataConfig) else GHSLDataConfig(**config)\n        )\n        super().__init__(config=config, data_store=data_store, logger=logger)\n\n    def load_from_paths(\n        self,\n        source_data_path: List[Union[str, Path]],\n        merge_rasters: bool = False,\n        **kwargs,\n    ) -&gt; Union[List[TifProcessor], TifProcessor]:\n        \"\"\"\n        Load TifProcessors from GHSL dataset.\n        Args:\n            source_data_path: List of file paths to load\n            merge_rasters: If True, all rasters will be merged into a single TifProcessor.\n                           Defaults to False.\n        Returns:\n            Union[List[TifProcessor], TifProcessor]: List of TifProcessor objects for accessing the raster data or a single\n                                                    TifProcessor if merge_rasters is True.\n        \"\"\"\n        return self._load_raster_data(\n            raster_paths=source_data_path, merge_rasters=merge_rasters\n        )\n\n    def load(\n        self,\n        source,\n        crop_to_source: bool = False,\n        merge_rasters: bool = False,\n        **kwargs,\n    ):\n        return super().load(\n            source=source,\n            crop_to_source=crop_to_source,\n            file_ext=kwargs.pop(\"file_ext\", \".tif\"),\n            merge_rasters=merge_rasters,\n            **kwargs,\n        )\n</code></pre>"},{"location":"api/handlers/#gigaspatial.handlers.ghsl.GHSLDataReader.__init__","title":"<code>__init__(config, data_store=None, logger=None)</code>","text":"<p>Initialize the reader.</p> <p>Parameters:</p> Name Type Description Default <code>config</code> <code>Union[GHSLDataConfig, dict[str, Union[str, int]]]</code> <p>Configuration for the GHSL dataset, either as a GHSLDataConfig object or a dictionary of parameters</p> required <code>data_store</code> <code>Optional[DataStore]</code> <p>Optional data storage interface. If not provided, uses LocalDataStore.</p> <code>None</code> <code>logger</code> <code>Optional[Logger]</code> <p>Optional custom logger. If not provided, uses default logger.</p> <code>None</code> Source code in <code>gigaspatial/handlers/ghsl.py</code> <pre><code>def __init__(\n    self,\n    config: Union[GHSLDataConfig, dict[str, Union[str, int]]],\n    data_store: Optional[DataStore] = None,\n    logger: Optional[logging.Logger] = None,\n):\n    \"\"\"\n    Initialize the reader.\n\n    Args:\n        config: Configuration for the GHSL dataset, either as a GHSLDataConfig object or a dictionary of parameters\n        data_store: Optional data storage interface. If not provided, uses LocalDataStore.\n        logger: Optional custom logger. If not provided, uses default logger.\n    \"\"\"\n    config = (\n        config if isinstance(config, GHSLDataConfig) else GHSLDataConfig(**config)\n    )\n    super().__init__(config=config, data_store=data_store, logger=logger)\n</code></pre>"},{"location":"api/handlers/#gigaspatial.handlers.ghsl.GHSLDataReader.load_from_paths","title":"<code>load_from_paths(source_data_path, merge_rasters=False, **kwargs)</code>","text":"<p>Load TifProcessors from GHSL dataset. Args:     source_data_path: List of file paths to load     merge_rasters: If True, all rasters will be merged into a single TifProcessor.                    Defaults to False. Returns:     Union[List[TifProcessor], TifProcessor]: List of TifProcessor objects for accessing the raster data or a single                                             TifProcessor if merge_rasters is True.</p> Source code in <code>gigaspatial/handlers/ghsl.py</code> <pre><code>def load_from_paths(\n    self,\n    source_data_path: List[Union[str, Path]],\n    merge_rasters: bool = False,\n    **kwargs,\n) -&gt; Union[List[TifProcessor], TifProcessor]:\n    \"\"\"\n    Load TifProcessors from GHSL dataset.\n    Args:\n        source_data_path: List of file paths to load\n        merge_rasters: If True, all rasters will be merged into a single TifProcessor.\n                       Defaults to False.\n    Returns:\n        Union[List[TifProcessor], TifProcessor]: List of TifProcessor objects for accessing the raster data or a single\n                                                TifProcessor if merge_rasters is True.\n    \"\"\"\n    return self._load_raster_data(\n        raster_paths=source_data_path, merge_rasters=merge_rasters\n    )\n</code></pre>"},{"location":"api/handlers/#gigaspatial.handlers.giga","title":"<code>giga</code>","text":""},{"location":"api/handlers/#gigaspatial.handlers.giga.GigaSchoolLocationFetcher","title":"<code>GigaSchoolLocationFetcher</code>","text":"<p>Fetch and process school location data from the Giga School Geolocation Data API.</p> Source code in <code>gigaspatial/handlers/giga.py</code> <pre><code>@dataclass(config=ConfigDict(arbitrary_types_allowed=True))\nclass GigaSchoolLocationFetcher:\n    \"\"\"\n    Fetch and process school location data from the Giga School Geolocation Data API.\n    \"\"\"\n\n    country: str = Field(...)\n    api_url: str = Field(\n        default=\"https://uni-ooi-giga-maps-service.azurewebsites.net/api/v1/schools_location/country/{isocode3}\",\n        description=\"Base URL for the Giga School API\",\n    )\n    api_key: str = global_config.GIGA_SCHOOL_LOCATION_API_KEY\n    page_size: int = Field(default=1000, description=\"Number of records per API page\")\n    sleep_time: float = Field(\n        default=0.2, description=\"Sleep time between API requests\"\n    )\n\n    logger: logging.Logger = Field(default=None, repr=False)\n\n    def __post_init__(self):\n        try:\n            self.country = pycountry.countries.lookup(self.country).alpha_3\n        except LookupError:\n            raise ValueError(f\"Invalid country code provided: {self.country}\")\n        self.api_url = self.api_url.format(isocode3=self.country)\n        if self.logger is None:\n            self.logger = global_config.get_logger(self.__class__.__name__)\n\n    def fetch_locations(\n        self, process_geospatial: bool = False, **kwargs\n    ) -&gt; Union[pd.DataFrame, gpd.GeoDataFrame]:\n        \"\"\"\n        Fetch and process school locations.\n\n        Args:\n            process_geospatial (bool): Whether to process geospatial data and return a GeoDataFrame. Defaults to False.\n            **kwargs: Additional parameters for customization\n                - page_size: Override default page size\n                - sleep_time: Override default sleep time between requests\n                - max_pages: Limit the number of pages to fetch\n\n        Returns:\n            pd.DataFrame: School locations with geospatial info.\n        \"\"\"\n        # Override defaults with kwargs if provided\n        page_size = kwargs.get(\"page_size\", self.page_size)\n        sleep_time = kwargs.get(\"sleep_time\", self.sleep_time)\n        max_pages = kwargs.get(\"max_pages\", None)\n\n        # Prepare headers\n        headers = {\n            \"Authorization\": f\"Bearer {self.api_key}\",\n            \"Accept\": \"application/json\",\n        }\n\n        all_data = []\n        page = 1\n\n        self.logger.info(\n            f\"Starting to fetch school locations for country: {self.country}\"\n        )\n\n        while True:\n            # Check if we've reached max_pages limit\n            if max_pages and page &gt; max_pages:\n                self.logger.info(f\"Reached maximum pages limit: {max_pages}\")\n                break\n\n            params = {\"page\": page, \"size\": page_size}\n\n            try:\n                self.logger.debug(f\"Fetching page {page} with params: {params}\")\n                response = requests.get(self.api_url, headers=headers, params=params)\n                response.raise_for_status()\n\n                parsed = response.json()\n                data = parsed.get(\"data\", [])\n\n            except requests.exceptions.RequestException as e:\n                self.logger.error(f\"Request failed on page {page}: {e}\")\n                break\n            except ValueError as e:\n                self.logger.error(f\"Failed to parse JSON response on page {page}: {e}\")\n                break\n\n            # Check if we got any data\n            if not data:\n                self.logger.info(f\"No data on page {page}. Stopping.\")\n                break\n\n            all_data.extend(data)\n            self.logger.info(f\"Fetched page {page} with {len(data)} records\")\n\n            # If we got fewer records than page_size, we've reached the end\n            if len(data) &lt; page_size:\n                self.logger.info(\"Reached end of data (partial page received)\")\n                break\n\n            page += 1\n\n            # Sleep to be respectful to the API\n            if sleep_time &gt; 0:\n                time.sleep(sleep_time)\n\n        self.logger.info(f\"Finished fetching. Total records: {len(all_data)}\")\n\n        # Convert to DataFrame and process\n        if not all_data:\n            self.logger.warning(\"No data fetched, returning empty DataFrame\")\n            return pd.DataFrame()\n\n        df = pd.DataFrame(all_data)\n\n        if process_geospatial:\n            df = self._process_geospatial_data(df)\n\n        return df\n\n    def _process_geospatial_data(self, df: pd.DataFrame) -&gt; gpd.GeoDataFrame:\n        \"\"\"\n        Process and enhance the DataFrame with geospatial information.\n\n        Args:\n            df: Raw DataFrame from API\n\n        Returns:\n            pd.DataFrame: Enhanced DataFrame with geospatial data\n        \"\"\"\n        if df.empty:\n            return df\n\n        df[\"geometry\"] = df.apply(\n            lambda row: Point(row[\"longitude\"], row[\"latitude\"]), axis=1\n        )\n        self.logger.info(f\"Created geometry for all {len(df)} records\")\n\n        return gpd.GeoDataFrame(df, geometry=\"geometry\", crs=\"EPSG:4326\")\n</code></pre>"},{"location":"api/handlers/#gigaspatial.handlers.giga.GigaSchoolLocationFetcher.fetch_locations","title":"<code>fetch_locations(process_geospatial=False, **kwargs)</code>","text":"<p>Fetch and process school locations.</p> <p>Parameters:</p> Name Type Description Default <code>process_geospatial</code> <code>bool</code> <p>Whether to process geospatial data and return a GeoDataFrame. Defaults to False.</p> <code>False</code> <code>**kwargs</code> <p>Additional parameters for customization - page_size: Override default page size - sleep_time: Override default sleep time between requests - max_pages: Limit the number of pages to fetch</p> <code>{}</code> <p>Returns:</p> Type Description <code>Union[DataFrame, GeoDataFrame]</code> <p>pd.DataFrame: School locations with geospatial info.</p> Source code in <code>gigaspatial/handlers/giga.py</code> <pre><code>def fetch_locations(\n    self, process_geospatial: bool = False, **kwargs\n) -&gt; Union[pd.DataFrame, gpd.GeoDataFrame]:\n    \"\"\"\n    Fetch and process school locations.\n\n    Args:\n        process_geospatial (bool): Whether to process geospatial data and return a GeoDataFrame. Defaults to False.\n        **kwargs: Additional parameters for customization\n            - page_size: Override default page size\n            - sleep_time: Override default sleep time between requests\n            - max_pages: Limit the number of pages to fetch\n\n    Returns:\n        pd.DataFrame: School locations with geospatial info.\n    \"\"\"\n    # Override defaults with kwargs if provided\n    page_size = kwargs.get(\"page_size\", self.page_size)\n    sleep_time = kwargs.get(\"sleep_time\", self.sleep_time)\n    max_pages = kwargs.get(\"max_pages\", None)\n\n    # Prepare headers\n    headers = {\n        \"Authorization\": f\"Bearer {self.api_key}\",\n        \"Accept\": \"application/json\",\n    }\n\n    all_data = []\n    page = 1\n\n    self.logger.info(\n        f\"Starting to fetch school locations for country: {self.country}\"\n    )\n\n    while True:\n        # Check if we've reached max_pages limit\n        if max_pages and page &gt; max_pages:\n            self.logger.info(f\"Reached maximum pages limit: {max_pages}\")\n            break\n\n        params = {\"page\": page, \"size\": page_size}\n\n        try:\n            self.logger.debug(f\"Fetching page {page} with params: {params}\")\n            response = requests.get(self.api_url, headers=headers, params=params)\n            response.raise_for_status()\n\n            parsed = response.json()\n            data = parsed.get(\"data\", [])\n\n        except requests.exceptions.RequestException as e:\n            self.logger.error(f\"Request failed on page {page}: {e}\")\n            break\n        except ValueError as e:\n            self.logger.error(f\"Failed to parse JSON response on page {page}: {e}\")\n            break\n\n        # Check if we got any data\n        if not data:\n            self.logger.info(f\"No data on page {page}. Stopping.\")\n            break\n\n        all_data.extend(data)\n        self.logger.info(f\"Fetched page {page} with {len(data)} records\")\n\n        # If we got fewer records than page_size, we've reached the end\n        if len(data) &lt; page_size:\n            self.logger.info(\"Reached end of data (partial page received)\")\n            break\n\n        page += 1\n\n        # Sleep to be respectful to the API\n        if sleep_time &gt; 0:\n            time.sleep(sleep_time)\n\n    self.logger.info(f\"Finished fetching. Total records: {len(all_data)}\")\n\n    # Convert to DataFrame and process\n    if not all_data:\n        self.logger.warning(\"No data fetched, returning empty DataFrame\")\n        return pd.DataFrame()\n\n    df = pd.DataFrame(all_data)\n\n    if process_geospatial:\n        df = self._process_geospatial_data(df)\n\n    return df\n</code></pre>"},{"location":"api/handlers/#gigaspatial.handlers.giga.GigaSchoolMeasurementsFetcher","title":"<code>GigaSchoolMeasurementsFetcher</code>","text":"<p>Fetch and process school daily realtime connectivity measurements from the Giga API. This includes download/upload speeds, latency, and connectivity performance data.</p> Source code in <code>gigaspatial/handlers/giga.py</code> <pre><code>@dataclass(config=ConfigDict(arbitrary_types_allowed=True))\nclass GigaSchoolMeasurementsFetcher:\n    \"\"\"\n    Fetch and process school daily realtime connectivity measurements from the Giga API.\n    This includes download/upload speeds, latency, and connectivity performance data.\n    \"\"\"\n\n    country: str = Field(...)\n    start_date: Union[str, date, datetime] = Field(...)\n    end_date: Union[str, date, datetime] = Field(...)\n    api_url: str = Field(\n        default=\"https://uni-ooi-giga-maps-service.azurewebsites.net/api/v1/all_measurements\",\n        description=\"Base URL for the Giga School Measurements API\",\n    )\n    api_key: str = global_config.GIGA_SCHOOL_MEASUREMENTS_API_KEY\n    page_size: int = Field(default=1000, description=\"Number of records per API page\")\n    sleep_time: float = Field(\n        default=0.2, description=\"Sleep time between API requests\"\n    )\n    giga_id_school: Optional[str] = Field(\n        default=None, description=\"Optional specific giga school ID to fetch\"\n    )\n\n    logger: logging.Logger = Field(default=None, repr=False)\n\n    def __post_init__(self):\n        try:\n            self.country = pycountry.countries.lookup(self.country).alpha_3\n        except LookupError:\n            raise ValueError(f\"Invalid country code provided: {self.country}\")\n\n        # Convert dates to string format if needed\n        self.start_date = self._format_date(self.start_date)\n        self.end_date = self._format_date(self.end_date)\n\n        # Validate date range\n        if self.start_date &gt; self.end_date:\n            raise ValueError(\"start_date must be before or equal to end_date\")\n\n        if self.logger is None:\n            self.logger = global_config.get_logger(self.__class__.__name__)\n\n    def _format_date(self, date_input: Union[str, date, datetime]) -&gt; str:\n        \"\"\"\n        Convert date input to string format expected by API (YYYY-MM-DD).\n\n        Args:\n            date_input: Date in various formats\n\n        Returns:\n            str: Date in YYYY-MM-DD format\n        \"\"\"\n        if isinstance(date_input, str):\n            # Assume it's already in correct format or parse it\n            try:\n                parsed_date = datetime.strptime(date_input, \"%Y-%m-%d\")\n                return date_input\n            except ValueError:\n                try:\n                    parsed_date = pd.to_datetime(date_input)\n                    return parsed_date.strftime(\"%Y-%m-%d\")\n                except:\n                    raise ValueError(\n                        f\"Invalid date format: {date_input}. Expected YYYY-MM-DD\"\n                    )\n        elif isinstance(date_input, (date, datetime)):\n            return date_input.strftime(\"%Y-%m-%d\")\n        else:\n            raise ValueError(f\"Invalid date type: {type(date_input)}\")\n\n    def fetch_measurements(self, **kwargs) -&gt; pd.DataFrame:\n        \"\"\"\n        Fetch and process school connectivity measurements.\n\n        Args:\n            **kwargs: Additional parameters for customization\n                - page_size: Override default page size\n                - sleep_time: Override default sleep time between requests\n                - max_pages: Limit the number of pages to fetch\n                - giga_id_school: Override default giga_id_school filter\n                - start_date: Override default start_date\n                - end_date: Override default end_date\n\n        Returns:\n            pd.DataFrame: School measurements with connectivity performance data.\n        \"\"\"\n        # Override defaults with kwargs if provided\n        page_size = kwargs.get(\"page_size\", self.page_size)\n        sleep_time = kwargs.get(\"sleep_time\", self.sleep_time)\n        max_pages = kwargs.get(\"max_pages\", None)\n        giga_id_school = kwargs.get(\"giga_id_school\", self.giga_id_school)\n        start_date = kwargs.get(\"start_date\", self.start_date)\n        end_date = kwargs.get(\"end_date\", self.end_date)\n\n        # Format dates if overridden\n        if start_date != self.start_date:\n            start_date = self._format_date(start_date)\n        if end_date != self.end_date:\n            end_date = self._format_date(end_date)\n\n        # Prepare headers\n        headers = {\n            \"Authorization\": f\"Bearer {self.api_key}\",\n            \"Accept\": \"application/json\",\n        }\n\n        all_data = []\n        page = 1\n\n        self.logger.info(\n            f\"Starting to fetch measurements for country: {self.country} \"\n            f\"from {start_date} to {end_date}\"\n        )\n\n        if giga_id_school:\n            self.logger.info(f\"Filtering for specific school ID: {giga_id_school}\")\n\n        while True:\n            # Check if we've reached max_pages limit\n            if max_pages and page &gt; max_pages:\n                self.logger.info(f\"Reached maximum pages limit: {max_pages}\")\n                break\n\n            # Build parameters\n            params = {\n                \"country_iso3_code\": self.country,\n                \"start_date\": start_date,\n                \"end_date\": end_date,\n                \"page\": page,\n                \"size\": page_size,\n            }\n\n            # Add giga_id_school filter if specified\n            if giga_id_school:\n                params[\"giga_id_school\"] = giga_id_school\n\n            try:\n                self.logger.debug(f\"Fetching page {page} with params: {params}\")\n                response = requests.get(self.api_url, headers=headers, params=params)\n                response.raise_for_status()\n\n                parsed = response.json()\n                data = parsed.get(\"data\", [])\n\n            except requests.exceptions.RequestException as e:\n                self.logger.error(f\"Request failed on page {page}: {e}\")\n                break\n            except ValueError as e:\n                self.logger.error(f\"Failed to parse JSON response on page {page}: {e}\")\n                break\n\n            # Check if we got any data\n            if not data:\n                self.logger.info(f\"No data on page {page}. Stopping.\")\n                break\n\n            all_data.extend(data)\n            self.logger.info(f\"Fetched page {page} with {len(data)} records\")\n\n            # If we got fewer records than page_size, we've reached the end\n            if len(data) &lt; page_size:\n                self.logger.info(\"Reached end of data (partial page received)\")\n                break\n\n            # If filtering by specific school ID, we might only need one page\n            if giga_id_school and len(all_data) &gt; 0:\n                self.logger.info(\n                    \"Specific school ID requested, checking if more data needed\"\n                )\n\n            page += 1\n\n            # Sleep to be respectful to the API\n            if sleep_time &gt; 0:\n                time.sleep(sleep_time)\n\n        self.logger.info(f\"Finished fetching. Total records: {len(all_data)}\")\n\n        # Convert to DataFrame and process\n        if not all_data:\n            self.logger.warning(\"No data fetched, returning empty DataFrame\")\n            return pd.DataFrame()\n\n        df = pd.DataFrame(all_data)\n        df = self._process_measurements_data(df)\n\n        return df\n\n    def _process_measurements_data(self, df: pd.DataFrame) -&gt; pd.DataFrame:\n        \"\"\"\n        Process and enhance the DataFrame with measurement performance metrics.\n\n        Args:\n            df: Raw DataFrame from API\n\n        Returns:\n            pd.DataFrame: Enhanced DataFrame with processed measurement data\n        \"\"\"\n        if df.empty:\n            return df\n\n        # Convert date column to datetime\n        if \"date\" in df.columns:\n            df[\"date\"] = pd.to_datetime(df[\"date\"], errors=\"coerce\")\n            df[\"date_only\"] = df[\"date\"].dt.date\n            df[\"year\"] = df[\"date\"].dt.year\n            df[\"month\"] = df[\"date\"].dt.month\n            df[\"day_of_week\"] = df[\"date\"].dt.day_name()\n            self.logger.info(\"Processed date fields\")\n\n        # Process speed measurements\n        numeric_columns = [\"download_speed\", \"upload_speed\", \"latency\"]\n        for col in numeric_columns:\n            if col in df.columns:\n                df[col] = pd.to_numeric(df[col], errors=\"coerce\")\n\n        # Create performance categories\n        if \"download_speed\" in df.columns:\n            df[\"download_speed_category\"] = pd.cut(\n                df[\"download_speed\"],\n                bins=[0, 5, 25, 100, float(\"inf\")],\n                labels=[\n                    \"Very Slow (&lt;5 Mbps)\",\n                    \"Slow (5-25 Mbps)\",\n                    \"Moderate (25-100 Mbps)\",\n                    \"Fast (&gt;100 Mbps)\",\n                ],\n                include_lowest=True,\n            )\n\n        if \"upload_speed\" in df.columns:\n            df[\"upload_speed_category\"] = pd.cut(\n                df[\"upload_speed\"],\n                bins=[0, 1, 10, 50, float(\"inf\")],\n                labels=[\n                    \"Very Slow (&lt;1 Mbps)\",\n                    \"Slow (1-10 Mbps)\",\n                    \"Moderate (10-50 Mbps)\",\n                    \"Fast (&gt;50 Mbps)\",\n                ],\n                include_lowest=True,\n            )\n\n        if \"latency\" in df.columns:\n            df[\"latency_category\"] = pd.cut(\n                df[\"latency\"],\n                bins=[0, 50, 150, 300, float(\"inf\")],\n                labels=[\n                    \"Excellent (&lt;50ms)\",\n                    \"Good (50-150ms)\",\n                    \"Fair (150-300ms)\",\n                    \"Poor (&gt;300ms)\",\n                ],\n                include_lowest=True,\n            )\n\n        # Create quality flags\n        if \"download_speed\" in df.columns and \"upload_speed\" in df.columns:\n            df[\"has_broadband\"] = (df[\"download_speed\"] &gt;= 25) &amp; (\n                df[\"upload_speed\"] &gt;= 3\n            )\n            df[\"has_basic_connectivity\"] = (df[\"download_speed\"] &gt;= 1) &amp; (\n                df[\"upload_speed\"] &gt;= 0.5\n            )\n\n        # Flag measurements with missing data\n        df[\"has_complete_measurement\"] = (\n            df[\"download_speed\"].notna()\n            &amp; df[\"upload_speed\"].notna()\n            &amp; df[\"latency\"].notna()\n        )\n\n        self.logger.info(f\"Processed measurement data for {len(df)} records\")\n\n        return df\n\n    def get_performance_summary(self, df: pd.DataFrame) -&gt; dict:\n        \"\"\"\n        Generate a comprehensive summary of connectivity performance metrics.\n\n        Args:\n            df: DataFrame with measurement data\n\n        Returns:\n            dict: Summary statistics about connectivity performance\n        \"\"\"\n        if df.empty:\n            return {\"error\": \"No data available\"}\n\n        summary = {\n            \"total_measurements\": len(df),\n            \"country\": (\n                df[\"country_iso3_code\"].iloc[0]\n                if \"country_iso3_code\" in df.columns\n                else \"Unknown\"\n            ),\n            \"date_range\": {\n                \"start\": (\n                    df[\"date\"].min().strftime(\"%Y-%m-%d\")\n                    if \"date\" in df.columns\n                    else None\n                ),\n                \"end\": (\n                    df[\"date\"].max().strftime(\"%Y-%m-%d\")\n                    if \"date\" in df.columns\n                    else None\n                ),\n            },\n        }\n\n        # School coverage\n        if \"giga_id_school\" in df.columns:\n            unique_schools = df[\"giga_id_school\"].nunique()\n            summary[\"unique_schools_measured\"] = unique_schools\n            summary[\"avg_measurements_per_school\"] = (\n                len(df) / unique_schools if unique_schools &gt; 0 else 0\n            )\n\n        # Speed statistics\n        for speed_col in [\"download_speed\", \"upload_speed\"]:\n            if speed_col in df.columns:\n                speed_data = df[speed_col].dropna()\n                if len(speed_data) &gt; 0:\n                    summary[f\"{speed_col}_stats\"] = {\n                        \"mean\": float(speed_data.mean()),\n                        \"median\": float(speed_data.median()),\n                        \"min\": float(speed_data.min()),\n                        \"max\": float(speed_data.max()),\n                        \"std\": float(speed_data.std()),\n                    }\n\n        # Latency statistics\n        if \"latency\" in df.columns:\n            latency_data = df[\"latency\"].dropna()\n            if len(latency_data) &gt; 0:\n                summary[\"latency_stats\"] = {\n                    \"mean\": float(latency_data.mean()),\n                    \"median\": float(latency_data.median()),\n                    \"min\": float(latency_data.min()),\n                    \"max\": float(latency_data.max()),\n                    \"std\": float(latency_data.std()),\n                }\n\n        # Performance categories\n        for cat_col in [\n            \"download_speed_category\",\n            \"upload_speed_category\",\n            \"latency_category\",\n        ]:\n            if cat_col in df.columns:\n                cat_counts = df[cat_col].value_counts().to_dict()\n                summary[cat_col.replace(\"_category\", \"_breakdown\")] = cat_counts\n\n        # Quality metrics\n        if \"has_broadband\" in df.columns:\n            summary[\"broadband_capable_measurements\"] = int(df[\"has_broadband\"].sum())\n            summary[\"broadband_percentage\"] = float(df[\"has_broadband\"].mean() * 100)\n\n        if \"has_basic_connectivity\" in df.columns:\n            summary[\"basic_connectivity_measurements\"] = int(\n                df[\"has_basic_connectivity\"].sum()\n            )\n            summary[\"basic_connectivity_percentage\"] = float(\n                df[\"has_basic_connectivity\"].mean() * 100\n            )\n\n        # Data completeness\n        if \"has_complete_measurement\" in df.columns:\n            summary[\"complete_measurements\"] = int(df[\"has_complete_measurement\"].sum())\n            summary[\"data_completeness_percentage\"] = float(\n                df[\"has_complete_measurement\"].mean() * 100\n            )\n\n        # Data sources\n        if \"data_source\" in df.columns:\n            source_counts = df[\"data_source\"].value_counts().to_dict()\n            summary[\"data_sources\"] = source_counts\n\n        # Temporal patterns\n        if \"day_of_week\" in df.columns:\n            day_counts = df[\"day_of_week\"].value_counts().to_dict()\n            summary[\"measurements_by_day_of_week\"] = day_counts\n\n        self.logger.info(\"Generated performance summary\")\n        return summary\n\n    def get_school_performance_comparison(\n        self, df: pd.DataFrame, top_n: int = 10\n    ) -&gt; dict:\n        \"\"\"\n        Compare performance across schools.\n\n        Args:\n            df: DataFrame with measurement data\n            top_n: Number of top/bottom schools to include\n\n        Returns:\n            dict: School performance comparison\n        \"\"\"\n        if df.empty or \"giga_id_school\" not in df.columns:\n            return {\"error\": \"No school data available\"}\n\n        school_stats = (\n            df.groupby(\"giga_id_school\")\n            .agg(\n                {\n                    \"download_speed\": [\"mean\", \"median\", \"count\"],\n                    \"upload_speed\": [\"mean\", \"median\"],\n                    \"latency\": [\"mean\", \"median\"],\n                    \"has_broadband\": (\n                        \"mean\" if \"has_broadband\" in df.columns else lambda x: None\n                    ),\n                }\n            )\n            .round(2)\n        )\n\n        # Flatten column names\n        school_stats.columns = [\"_\".join(col).strip() for col in school_stats.columns]\n\n        # Sort by download speed\n        if \"download_speed_mean\" in school_stats.columns:\n            top_schools = school_stats.nlargest(top_n, \"download_speed_mean\")\n            bottom_schools = school_stats.nsmallest(top_n, \"download_speed_mean\")\n\n            return {\n                \"top_performing_schools\": top_schools.to_dict(\"index\"),\n                \"bottom_performing_schools\": bottom_schools.to_dict(\"index\"),\n                \"total_schools_analyzed\": len(school_stats),\n            }\n\n        return {\"error\": \"Insufficient data for school comparison\"}\n</code></pre>"},{"location":"api/handlers/#gigaspatial.handlers.giga.GigaSchoolMeasurementsFetcher.fetch_measurements","title":"<code>fetch_measurements(**kwargs)</code>","text":"<p>Fetch and process school connectivity measurements.</p> <p>Parameters:</p> Name Type Description Default <code>**kwargs</code> <p>Additional parameters for customization - page_size: Override default page size - sleep_time: Override default sleep time between requests - max_pages: Limit the number of pages to fetch - giga_id_school: Override default giga_id_school filter - start_date: Override default start_date - end_date: Override default end_date</p> <code>{}</code> <p>Returns:</p> Type Description <code>DataFrame</code> <p>pd.DataFrame: School measurements with connectivity performance data.</p> Source code in <code>gigaspatial/handlers/giga.py</code> <pre><code>def fetch_measurements(self, **kwargs) -&gt; pd.DataFrame:\n    \"\"\"\n    Fetch and process school connectivity measurements.\n\n    Args:\n        **kwargs: Additional parameters for customization\n            - page_size: Override default page size\n            - sleep_time: Override default sleep time between requests\n            - max_pages: Limit the number of pages to fetch\n            - giga_id_school: Override default giga_id_school filter\n            - start_date: Override default start_date\n            - end_date: Override default end_date\n\n    Returns:\n        pd.DataFrame: School measurements with connectivity performance data.\n    \"\"\"\n    # Override defaults with kwargs if provided\n    page_size = kwargs.get(\"page_size\", self.page_size)\n    sleep_time = kwargs.get(\"sleep_time\", self.sleep_time)\n    max_pages = kwargs.get(\"max_pages\", None)\n    giga_id_school = kwargs.get(\"giga_id_school\", self.giga_id_school)\n    start_date = kwargs.get(\"start_date\", self.start_date)\n    end_date = kwargs.get(\"end_date\", self.end_date)\n\n    # Format dates if overridden\n    if start_date != self.start_date:\n        start_date = self._format_date(start_date)\n    if end_date != self.end_date:\n        end_date = self._format_date(end_date)\n\n    # Prepare headers\n    headers = {\n        \"Authorization\": f\"Bearer {self.api_key}\",\n        \"Accept\": \"application/json\",\n    }\n\n    all_data = []\n    page = 1\n\n    self.logger.info(\n        f\"Starting to fetch measurements for country: {self.country} \"\n        f\"from {start_date} to {end_date}\"\n    )\n\n    if giga_id_school:\n        self.logger.info(f\"Filtering for specific school ID: {giga_id_school}\")\n\n    while True:\n        # Check if we've reached max_pages limit\n        if max_pages and page &gt; max_pages:\n            self.logger.info(f\"Reached maximum pages limit: {max_pages}\")\n            break\n\n        # Build parameters\n        params = {\n            \"country_iso3_code\": self.country,\n            \"start_date\": start_date,\n            \"end_date\": end_date,\n            \"page\": page,\n            \"size\": page_size,\n        }\n\n        # Add giga_id_school filter if specified\n        if giga_id_school:\n            params[\"giga_id_school\"] = giga_id_school\n\n        try:\n            self.logger.debug(f\"Fetching page {page} with params: {params}\")\n            response = requests.get(self.api_url, headers=headers, params=params)\n            response.raise_for_status()\n\n            parsed = response.json()\n            data = parsed.get(\"data\", [])\n\n        except requests.exceptions.RequestException as e:\n            self.logger.error(f\"Request failed on page {page}: {e}\")\n            break\n        except ValueError as e:\n            self.logger.error(f\"Failed to parse JSON response on page {page}: {e}\")\n            break\n\n        # Check if we got any data\n        if not data:\n            self.logger.info(f\"No data on page {page}. Stopping.\")\n            break\n\n        all_data.extend(data)\n        self.logger.info(f\"Fetched page {page} with {len(data)} records\")\n\n        # If we got fewer records than page_size, we've reached the end\n        if len(data) &lt; page_size:\n            self.logger.info(\"Reached end of data (partial page received)\")\n            break\n\n        # If filtering by specific school ID, we might only need one page\n        if giga_id_school and len(all_data) &gt; 0:\n            self.logger.info(\n                \"Specific school ID requested, checking if more data needed\"\n            )\n\n        page += 1\n\n        # Sleep to be respectful to the API\n        if sleep_time &gt; 0:\n            time.sleep(sleep_time)\n\n    self.logger.info(f\"Finished fetching. Total records: {len(all_data)}\")\n\n    # Convert to DataFrame and process\n    if not all_data:\n        self.logger.warning(\"No data fetched, returning empty DataFrame\")\n        return pd.DataFrame()\n\n    df = pd.DataFrame(all_data)\n    df = self._process_measurements_data(df)\n\n    return df\n</code></pre>"},{"location":"api/handlers/#gigaspatial.handlers.giga.GigaSchoolMeasurementsFetcher.get_performance_summary","title":"<code>get_performance_summary(df)</code>","text":"<p>Generate a comprehensive summary of connectivity performance metrics.</p> <p>Parameters:</p> Name Type Description Default <code>df</code> <code>DataFrame</code> <p>DataFrame with measurement data</p> required <p>Returns:</p> Name Type Description <code>dict</code> <code>dict</code> <p>Summary statistics about connectivity performance</p> Source code in <code>gigaspatial/handlers/giga.py</code> <pre><code>def get_performance_summary(self, df: pd.DataFrame) -&gt; dict:\n    \"\"\"\n    Generate a comprehensive summary of connectivity performance metrics.\n\n    Args:\n        df: DataFrame with measurement data\n\n    Returns:\n        dict: Summary statistics about connectivity performance\n    \"\"\"\n    if df.empty:\n        return {\"error\": \"No data available\"}\n\n    summary = {\n        \"total_measurements\": len(df),\n        \"country\": (\n            df[\"country_iso3_code\"].iloc[0]\n            if \"country_iso3_code\" in df.columns\n            else \"Unknown\"\n        ),\n        \"date_range\": {\n            \"start\": (\n                df[\"date\"].min().strftime(\"%Y-%m-%d\")\n                if \"date\" in df.columns\n                else None\n            ),\n            \"end\": (\n                df[\"date\"].max().strftime(\"%Y-%m-%d\")\n                if \"date\" in df.columns\n                else None\n            ),\n        },\n    }\n\n    # School coverage\n    if \"giga_id_school\" in df.columns:\n        unique_schools = df[\"giga_id_school\"].nunique()\n        summary[\"unique_schools_measured\"] = unique_schools\n        summary[\"avg_measurements_per_school\"] = (\n            len(df) / unique_schools if unique_schools &gt; 0 else 0\n        )\n\n    # Speed statistics\n    for speed_col in [\"download_speed\", \"upload_speed\"]:\n        if speed_col in df.columns:\n            speed_data = df[speed_col].dropna()\n            if len(speed_data) &gt; 0:\n                summary[f\"{speed_col}_stats\"] = {\n                    \"mean\": float(speed_data.mean()),\n                    \"median\": float(speed_data.median()),\n                    \"min\": float(speed_data.min()),\n                    \"max\": float(speed_data.max()),\n                    \"std\": float(speed_data.std()),\n                }\n\n    # Latency statistics\n    if \"latency\" in df.columns:\n        latency_data = df[\"latency\"].dropna()\n        if len(latency_data) &gt; 0:\n            summary[\"latency_stats\"] = {\n                \"mean\": float(latency_data.mean()),\n                \"median\": float(latency_data.median()),\n                \"min\": float(latency_data.min()),\n                \"max\": float(latency_data.max()),\n                \"std\": float(latency_data.std()),\n            }\n\n    # Performance categories\n    for cat_col in [\n        \"download_speed_category\",\n        \"upload_speed_category\",\n        \"latency_category\",\n    ]:\n        if cat_col in df.columns:\n            cat_counts = df[cat_col].value_counts().to_dict()\n            summary[cat_col.replace(\"_category\", \"_breakdown\")] = cat_counts\n\n    # Quality metrics\n    if \"has_broadband\" in df.columns:\n        summary[\"broadband_capable_measurements\"] = int(df[\"has_broadband\"].sum())\n        summary[\"broadband_percentage\"] = float(df[\"has_broadband\"].mean() * 100)\n\n    if \"has_basic_connectivity\" in df.columns:\n        summary[\"basic_connectivity_measurements\"] = int(\n            df[\"has_basic_connectivity\"].sum()\n        )\n        summary[\"basic_connectivity_percentage\"] = float(\n            df[\"has_basic_connectivity\"].mean() * 100\n        )\n\n    # Data completeness\n    if \"has_complete_measurement\" in df.columns:\n        summary[\"complete_measurements\"] = int(df[\"has_complete_measurement\"].sum())\n        summary[\"data_completeness_percentage\"] = float(\n            df[\"has_complete_measurement\"].mean() * 100\n        )\n\n    # Data sources\n    if \"data_source\" in df.columns:\n        source_counts = df[\"data_source\"].value_counts().to_dict()\n        summary[\"data_sources\"] = source_counts\n\n    # Temporal patterns\n    if \"day_of_week\" in df.columns:\n        day_counts = df[\"day_of_week\"].value_counts().to_dict()\n        summary[\"measurements_by_day_of_week\"] = day_counts\n\n    self.logger.info(\"Generated performance summary\")\n    return summary\n</code></pre>"},{"location":"api/handlers/#gigaspatial.handlers.giga.GigaSchoolMeasurementsFetcher.get_school_performance_comparison","title":"<code>get_school_performance_comparison(df, top_n=10)</code>","text":"<p>Compare performance across schools.</p> <p>Parameters:</p> Name Type Description Default <code>df</code> <code>DataFrame</code> <p>DataFrame with measurement data</p> required <code>top_n</code> <code>int</code> <p>Number of top/bottom schools to include</p> <code>10</code> <p>Returns:</p> Name Type Description <code>dict</code> <code>dict</code> <p>School performance comparison</p> Source code in <code>gigaspatial/handlers/giga.py</code> <pre><code>def get_school_performance_comparison(\n    self, df: pd.DataFrame, top_n: int = 10\n) -&gt; dict:\n    \"\"\"\n    Compare performance across schools.\n\n    Args:\n        df: DataFrame with measurement data\n        top_n: Number of top/bottom schools to include\n\n    Returns:\n        dict: School performance comparison\n    \"\"\"\n    if df.empty or \"giga_id_school\" not in df.columns:\n        return {\"error\": \"No school data available\"}\n\n    school_stats = (\n        df.groupby(\"giga_id_school\")\n        .agg(\n            {\n                \"download_speed\": [\"mean\", \"median\", \"count\"],\n                \"upload_speed\": [\"mean\", \"median\"],\n                \"latency\": [\"mean\", \"median\"],\n                \"has_broadband\": (\n                    \"mean\" if \"has_broadband\" in df.columns else lambda x: None\n                ),\n            }\n        )\n        .round(2)\n    )\n\n    # Flatten column names\n    school_stats.columns = [\"_\".join(col).strip() for col in school_stats.columns]\n\n    # Sort by download speed\n    if \"download_speed_mean\" in school_stats.columns:\n        top_schools = school_stats.nlargest(top_n, \"download_speed_mean\")\n        bottom_schools = school_stats.nsmallest(top_n, \"download_speed_mean\")\n\n        return {\n            \"top_performing_schools\": top_schools.to_dict(\"index\"),\n            \"bottom_performing_schools\": bottom_schools.to_dict(\"index\"),\n            \"total_schools_analyzed\": len(school_stats),\n        }\n\n    return {\"error\": \"Insufficient data for school comparison\"}\n</code></pre>"},{"location":"api/handlers/#gigaspatial.handlers.giga.GigaSchoolProfileFetcher","title":"<code>GigaSchoolProfileFetcher</code>","text":"<p>Fetch and process school profile data from the Giga School Profile API. This includes connectivity information and other school details.</p> Source code in <code>gigaspatial/handlers/giga.py</code> <pre><code>@dataclass(config=ConfigDict(arbitrary_types_allowed=True))\nclass GigaSchoolProfileFetcher:\n    \"\"\"\n    Fetch and process school profile data from the Giga School Profile API.\n    This includes connectivity information and other school details.\n    \"\"\"\n\n    country: str = Field(...)\n    api_url: str = Field(\n        default=\"https://uni-ooi-giga-maps-service.azurewebsites.net/api/v1/schools_profile/\",\n        description=\"Base URL for the Giga School Profile API\",\n    )\n    api_key: str = global_config.GIGA_SCHOOL_PROFILE_API_KEY\n    page_size: int = Field(default=1000, description=\"Number of records per API page\")\n    sleep_time: float = Field(\n        default=0.2, description=\"Sleep time between API requests\"\n    )\n    giga_id_school: Optional[str] = Field(\n        default=None, description=\"Optional specific giga school ID to fetch\"\n    )\n\n    logger: logging.Logger = Field(default=None, repr=False)\n\n    def __post_init__(self):\n        try:\n            self.country = pycountry.countries.lookup(self.country).alpha_3\n        except LookupError:\n            raise ValueError(f\"Invalid country code provided: {self.country}\")\n\n        if self.logger is None:\n            self.logger = global_config.get_logger(self.__class__.__name__)\n\n    def fetch_profiles(self, **kwargs) -&gt; pd.DataFrame:\n        \"\"\"\n        Fetch and process school profiles including connectivity information.\n\n        Args:\n            **kwargs: Additional parameters for customization\n                - page_size: Override default page size\n                - sleep_time: Override default sleep time between requests\n                - max_pages: Limit the number of pages to fetch\n                - giga_id_school: Override default giga_id_school filter\n\n        Returns:\n            pd.DataFrame: School profiles with connectivity and geospatial info.\n        \"\"\"\n        # Override defaults with kwargs if provided\n        page_size = kwargs.get(\"page_size\", self.page_size)\n        sleep_time = kwargs.get(\"sleep_time\", self.sleep_time)\n        max_pages = kwargs.get(\"max_pages\", None)\n        giga_id_school = kwargs.get(\"giga_id_school\", self.giga_id_school)\n\n        # Prepare headers\n        headers = {\n            \"Authorization\": f\"Bearer {self.api_key}\",\n            \"Accept\": \"application/json\",\n        }\n\n        all_data = []\n        page = 1\n\n        self.logger.info(\n            f\"Starting to fetch school profiles for country: {self.country}\"\n        )\n\n        if giga_id_school:\n            self.logger.info(f\"Filtering for specific school ID: {giga_id_school}\")\n\n        while True:\n            # Check if we've reached max_pages limit\n            if max_pages and page &gt; max_pages:\n                self.logger.info(f\"Reached maximum pages limit: {max_pages}\")\n                break\n\n            # Build parameters\n            params = {\n                \"country_iso3_code\": self.country,\n                \"page\": page,\n                \"size\": page_size,\n            }\n\n            # Add giga_id_school filter if specified\n            if giga_id_school:\n                params[\"giga_id_school\"] = giga_id_school\n\n            try:\n                self.logger.debug(f\"Fetching page {page} with params: {params}\")\n                response = requests.get(self.api_url, headers=headers, params=params)\n                response.raise_for_status()\n\n                parsed = response.json()\n                data = parsed.get(\"data\", [])\n\n            except requests.exceptions.RequestException as e:\n                self.logger.error(f\"Request failed on page {page}: {e}\")\n                break\n            except ValueError as e:\n                self.logger.error(f\"Failed to parse JSON response on page {page}: {e}\")\n                break\n\n            # Check if we got any data\n            if not data:\n                self.logger.info(f\"No data on page {page}. Stopping.\")\n                break\n\n            all_data.extend(data)\n            self.logger.info(f\"Fetched page {page} with {len(data)} records\")\n\n            # If we got fewer records than page_size, we've reached the end\n            if len(data) &lt; page_size:\n                self.logger.info(\"Reached end of data (partial page received)\")\n                break\n\n            # If filtering by specific school ID, we likely only need one page\n            if giga_id_school:\n                self.logger.info(\n                    \"Specific school ID requested, stopping after first page\"\n                )\n                break\n\n            page += 1\n\n            # Sleep to be respectful to the API\n            if sleep_time &gt; 0:\n                time.sleep(sleep_time)\n\n        self.logger.info(f\"Finished fetching. Total records: {len(all_data)}\")\n\n        # Convert to DataFrame and process\n        if not all_data:\n            self.logger.warning(\"No data fetched, returning empty DataFrame\")\n            return pd.DataFrame()\n\n        df = pd.DataFrame(all_data)\n\n        return df\n\n    def get_connectivity_summary(self, df: pd.DataFrame) -&gt; dict:\n        \"\"\"\n        Generate a summary of connectivity statistics from the fetched data.\n\n        Args:\n            df: DataFrame with school profile data\n\n        Returns:\n            dict: Summary statistics about connectivity\n        \"\"\"\n        if df.empty:\n            return {\"error\": \"No data available\"}\n\n        summary = {\n            \"total_schools\": len(df),\n            \"country\": (\n                df[\"country_iso3_code\"].iloc[0]\n                if \"country_iso3_code\" in df.columns\n                else \"Unknown\"\n            ),\n        }\n\n        # Administrative region analysis\n        if \"admin1\" in df.columns:\n            admin1_counts = df[\"admin1\"].value_counts().head(10).to_dict()\n            summary[\"top_admin1_regions\"] = admin1_counts\n\n        if \"admin2\" in df.columns:\n            admin2_counts = df[\"admin2\"].value_counts().head(10).to_dict()\n            summary[\"top_admin2_regions\"] = admin2_counts\n\n        # Connectivity analysis\n        if \"connectivity\" in df.columns:\n            connected_count = df[\"connectivity\"].sum()\n            summary[\"schools_with_connectivity\"] = int(connected_count)\n            summary[\"connectivity_percentage\"] = connected_count / len(df) * 100\n\n        if \"connectivity_RT\" in df.columns:\n            rt_connected_count = df[\"connectivity_RT\"].sum()\n            summary[\"schools_with_realtime_connectivity\"] = int(rt_connected_count)\n            summary[\"realtime_connectivity_percentage\"] = (\n                rt_connected_count / len(df) * 100\n            )\n\n        # Connectivity type analysis\n        if \"connectivity_type\" in df.columns:\n\n            if not all(df.connectivity_type.isna()):\n                from collections import Counter\n\n                type_counts = dict(Counter(df.connectivity_type.dropna().to_list()))\n                summary[\"connectivity_types_breakdown\"] = type_counts\n\n        # Data source analysis\n        if \"connectivity_RT_datasource\" in df.columns:\n            datasource_counts = (\n                df[\"connectivity_RT_datasource\"].value_counts().to_dict()\n            )\n            summary[\"realtime_connectivity_datasources\"] = datasource_counts\n\n        if \"school_data_source\" in df.columns:\n            school_datasource_counts = df[\"school_data_source\"].value_counts().to_dict()\n            summary[\"school_data_sources\"] = school_datasource_counts\n\n        self.logger.info(\"Generated connectivity summary\")\n        return summary\n</code></pre>"},{"location":"api/handlers/#gigaspatial.handlers.giga.GigaSchoolProfileFetcher.fetch_profiles","title":"<code>fetch_profiles(**kwargs)</code>","text":"<p>Fetch and process school profiles including connectivity information.</p> <p>Parameters:</p> Name Type Description Default <code>**kwargs</code> <p>Additional parameters for customization - page_size: Override default page size - sleep_time: Override default sleep time between requests - max_pages: Limit the number of pages to fetch - giga_id_school: Override default giga_id_school filter</p> <code>{}</code> <p>Returns:</p> Type Description <code>DataFrame</code> <p>pd.DataFrame: School profiles with connectivity and geospatial info.</p> Source code in <code>gigaspatial/handlers/giga.py</code> <pre><code>def fetch_profiles(self, **kwargs) -&gt; pd.DataFrame:\n    \"\"\"\n    Fetch and process school profiles including connectivity information.\n\n    Args:\n        **kwargs: Additional parameters for customization\n            - page_size: Override default page size\n            - sleep_time: Override default sleep time between requests\n            - max_pages: Limit the number of pages to fetch\n            - giga_id_school: Override default giga_id_school filter\n\n    Returns:\n        pd.DataFrame: School profiles with connectivity and geospatial info.\n    \"\"\"\n    # Override defaults with kwargs if provided\n    page_size = kwargs.get(\"page_size\", self.page_size)\n    sleep_time = kwargs.get(\"sleep_time\", self.sleep_time)\n    max_pages = kwargs.get(\"max_pages\", None)\n    giga_id_school = kwargs.get(\"giga_id_school\", self.giga_id_school)\n\n    # Prepare headers\n    headers = {\n        \"Authorization\": f\"Bearer {self.api_key}\",\n        \"Accept\": \"application/json\",\n    }\n\n    all_data = []\n    page = 1\n\n    self.logger.info(\n        f\"Starting to fetch school profiles for country: {self.country}\"\n    )\n\n    if giga_id_school:\n        self.logger.info(f\"Filtering for specific school ID: {giga_id_school}\")\n\n    while True:\n        # Check if we've reached max_pages limit\n        if max_pages and page &gt; max_pages:\n            self.logger.info(f\"Reached maximum pages limit: {max_pages}\")\n            break\n\n        # Build parameters\n        params = {\n            \"country_iso3_code\": self.country,\n            \"page\": page,\n            \"size\": page_size,\n        }\n\n        # Add giga_id_school filter if specified\n        if giga_id_school:\n            params[\"giga_id_school\"] = giga_id_school\n\n        try:\n            self.logger.debug(f\"Fetching page {page} with params: {params}\")\n            response = requests.get(self.api_url, headers=headers, params=params)\n            response.raise_for_status()\n\n            parsed = response.json()\n            data = parsed.get(\"data\", [])\n\n        except requests.exceptions.RequestException as e:\n            self.logger.error(f\"Request failed on page {page}: {e}\")\n            break\n        except ValueError as e:\n            self.logger.error(f\"Failed to parse JSON response on page {page}: {e}\")\n            break\n\n        # Check if we got any data\n        if not data:\n            self.logger.info(f\"No data on page {page}. Stopping.\")\n            break\n\n        all_data.extend(data)\n        self.logger.info(f\"Fetched page {page} with {len(data)} records\")\n\n        # If we got fewer records than page_size, we've reached the end\n        if len(data) &lt; page_size:\n            self.logger.info(\"Reached end of data (partial page received)\")\n            break\n\n        # If filtering by specific school ID, we likely only need one page\n        if giga_id_school:\n            self.logger.info(\n                \"Specific school ID requested, stopping after first page\"\n            )\n            break\n\n        page += 1\n\n        # Sleep to be respectful to the API\n        if sleep_time &gt; 0:\n            time.sleep(sleep_time)\n\n    self.logger.info(f\"Finished fetching. Total records: {len(all_data)}\")\n\n    # Convert to DataFrame and process\n    if not all_data:\n        self.logger.warning(\"No data fetched, returning empty DataFrame\")\n        return pd.DataFrame()\n\n    df = pd.DataFrame(all_data)\n\n    return df\n</code></pre>"},{"location":"api/handlers/#gigaspatial.handlers.giga.GigaSchoolProfileFetcher.get_connectivity_summary","title":"<code>get_connectivity_summary(df)</code>","text":"<p>Generate a summary of connectivity statistics from the fetched data.</p> <p>Parameters:</p> Name Type Description Default <code>df</code> <code>DataFrame</code> <p>DataFrame with school profile data</p> required <p>Returns:</p> Name Type Description <code>dict</code> <code>dict</code> <p>Summary statistics about connectivity</p> Source code in <code>gigaspatial/handlers/giga.py</code> <pre><code>def get_connectivity_summary(self, df: pd.DataFrame) -&gt; dict:\n    \"\"\"\n    Generate a summary of connectivity statistics from the fetched data.\n\n    Args:\n        df: DataFrame with school profile data\n\n    Returns:\n        dict: Summary statistics about connectivity\n    \"\"\"\n    if df.empty:\n        return {\"error\": \"No data available\"}\n\n    summary = {\n        \"total_schools\": len(df),\n        \"country\": (\n            df[\"country_iso3_code\"].iloc[0]\n            if \"country_iso3_code\" in df.columns\n            else \"Unknown\"\n        ),\n    }\n\n    # Administrative region analysis\n    if \"admin1\" in df.columns:\n        admin1_counts = df[\"admin1\"].value_counts().head(10).to_dict()\n        summary[\"top_admin1_regions\"] = admin1_counts\n\n    if \"admin2\" in df.columns:\n        admin2_counts = df[\"admin2\"].value_counts().head(10).to_dict()\n        summary[\"top_admin2_regions\"] = admin2_counts\n\n    # Connectivity analysis\n    if \"connectivity\" in df.columns:\n        connected_count = df[\"connectivity\"].sum()\n        summary[\"schools_with_connectivity\"] = int(connected_count)\n        summary[\"connectivity_percentage\"] = connected_count / len(df) * 100\n\n    if \"connectivity_RT\" in df.columns:\n        rt_connected_count = df[\"connectivity_RT\"].sum()\n        summary[\"schools_with_realtime_connectivity\"] = int(rt_connected_count)\n        summary[\"realtime_connectivity_percentage\"] = (\n            rt_connected_count / len(df) * 100\n        )\n\n    # Connectivity type analysis\n    if \"connectivity_type\" in df.columns:\n\n        if not all(df.connectivity_type.isna()):\n            from collections import Counter\n\n            type_counts = dict(Counter(df.connectivity_type.dropna().to_list()))\n            summary[\"connectivity_types_breakdown\"] = type_counts\n\n    # Data source analysis\n    if \"connectivity_RT_datasource\" in df.columns:\n        datasource_counts = (\n            df[\"connectivity_RT_datasource\"].value_counts().to_dict()\n        )\n        summary[\"realtime_connectivity_datasources\"] = datasource_counts\n\n    if \"school_data_source\" in df.columns:\n        school_datasource_counts = df[\"school_data_source\"].value_counts().to_dict()\n        summary[\"school_data_sources\"] = school_datasource_counts\n\n    self.logger.info(\"Generated connectivity summary\")\n    return summary\n</code></pre>"},{"location":"api/handlers/#gigaspatial.handlers.google_open_buildings","title":"<code>google_open_buildings</code>","text":""},{"location":"api/handlers/#gigaspatial.handlers.google_open_buildings.GoogleOpenBuildingsConfig","title":"<code>GoogleOpenBuildingsConfig</code>  <code>dataclass</code>","text":"<p>               Bases: <code>BaseHandlerConfig</code></p> <p>Configuration for Google Open Buildings dataset files. Implements the BaseHandlerConfig interface for data unit resolution.</p> Source code in <code>gigaspatial/handlers/google_open_buildings.py</code> <pre><code>@dataclass\nclass GoogleOpenBuildingsConfig(BaseHandlerConfig):\n    \"\"\"\n    Configuration for Google Open Buildings dataset files.\n    Implements the BaseHandlerConfig interface for data unit resolution.\n    \"\"\"\n\n    TILES_URL: str = (\n        \"https://openbuildings-public-dot-gweb-research.uw.r.appspot.com/public/tiles.geojson\"\n    )\n    base_path: Path = global_config.get_path(\"google_open_buildings\", \"bronze\")\n    data_types: tuple = (\"polygons\", \"points\")\n\n    def __post_init__(self):\n        super().__post_init__()\n        self._load_s2_tiles()\n\n    def _load_s2_tiles(self):\n        \"\"\"Load S2 tiles from GeoJSON file.\"\"\"\n        response = requests.get(self.TILES_URL)\n        response.raise_for_status()\n        self.tiles_gdf = gpd.GeoDataFrame.from_features(\n            response.json()[\"features\"], crs=\"EPSG:4326\"\n        )\n\n    def get_relevant_data_units(self, source, force_recompute: bool = False, **kwargs):\n        return super().get_relevant_data_units(\n            source, force_recompute, crs=\"EPSG:4326\", **kwargs\n        )\n\n    def get_relevant_data_units_by_geometry(\n        self, geometry: Union[BaseGeometry, gpd.GeoDataFrame], **kwargs\n    ) -&gt; List[dict]:\n        \"\"\"\n        Return intersecting tiles for a given geometry or GeoDataFrame.\n        \"\"\"\n        mask = (tile_geom.intersects(geometry) for tile_geom in self.tiles_gdf.geometry)\n        return self.tiles_gdf.loc[mask, [\"tile_id\", \"tile_url\", \"size_mb\"]].to_dict(\n            \"records\"\n        )\n\n    def get_data_unit_path(\n        self,\n        unit: Union[pd.Series, dict, str],\n        data_type: str = \"polygons\",\n        **kwargs,\n    ) -&gt; Path:\n        \"\"\"\n        Given a tile row or tile_id, return the corresponding file path.\n        \"\"\"\n        tile_id = (\n            unit[\"tile_id\"]\n            if isinstance(unit, pd.Series) or isinstance(unit, dict)\n            else unit\n        )\n        return self.base_path / f\"{data_type}_s2_level_4_{tile_id}_buildings.csv.gz\"\n\n    def get_data_unit_paths(\n        self,\n        units: Union[pd.DataFrame, Iterable[Union[dict, str]]],\n        data_type: str = \"polygons\",\n        **kwargs,\n    ) -&gt; list:\n        \"\"\"\n        Given data unit identifiers, return the corresponding file paths.\n        \"\"\"\n        if isinstance(units, pd.DataFrame):\n            return [\n                self.get_data_unit_path(row, data_type=data_type, **kwargs)\n                for _, row in units.iterrows()\n            ]\n        return super().get_data_unit_paths(units, data_type=data_type)\n</code></pre>"},{"location":"api/handlers/#gigaspatial.handlers.google_open_buildings.GoogleOpenBuildingsConfig.get_data_unit_path","title":"<code>get_data_unit_path(unit, data_type='polygons', **kwargs)</code>","text":"<p>Given a tile row or tile_id, return the corresponding file path.</p> Source code in <code>gigaspatial/handlers/google_open_buildings.py</code> <pre><code>def get_data_unit_path(\n    self,\n    unit: Union[pd.Series, dict, str],\n    data_type: str = \"polygons\",\n    **kwargs,\n) -&gt; Path:\n    \"\"\"\n    Given a tile row or tile_id, return the corresponding file path.\n    \"\"\"\n    tile_id = (\n        unit[\"tile_id\"]\n        if isinstance(unit, pd.Series) or isinstance(unit, dict)\n        else unit\n    )\n    return self.base_path / f\"{data_type}_s2_level_4_{tile_id}_buildings.csv.gz\"\n</code></pre>"},{"location":"api/handlers/#gigaspatial.handlers.google_open_buildings.GoogleOpenBuildingsConfig.get_data_unit_paths","title":"<code>get_data_unit_paths(units, data_type='polygons', **kwargs)</code>","text":"<p>Given data unit identifiers, return the corresponding file paths.</p> Source code in <code>gigaspatial/handlers/google_open_buildings.py</code> <pre><code>def get_data_unit_paths(\n    self,\n    units: Union[pd.DataFrame, Iterable[Union[dict, str]]],\n    data_type: str = \"polygons\",\n    **kwargs,\n) -&gt; list:\n    \"\"\"\n    Given data unit identifiers, return the corresponding file paths.\n    \"\"\"\n    if isinstance(units, pd.DataFrame):\n        return [\n            self.get_data_unit_path(row, data_type=data_type, **kwargs)\n            for _, row in units.iterrows()\n        ]\n    return super().get_data_unit_paths(units, data_type=data_type)\n</code></pre>"},{"location":"api/handlers/#gigaspatial.handlers.google_open_buildings.GoogleOpenBuildingsConfig.get_relevant_data_units_by_geometry","title":"<code>get_relevant_data_units_by_geometry(geometry, **kwargs)</code>","text":"<p>Return intersecting tiles for a given geometry or GeoDataFrame.</p> Source code in <code>gigaspatial/handlers/google_open_buildings.py</code> <pre><code>def get_relevant_data_units_by_geometry(\n    self, geometry: Union[BaseGeometry, gpd.GeoDataFrame], **kwargs\n) -&gt; List[dict]:\n    \"\"\"\n    Return intersecting tiles for a given geometry or GeoDataFrame.\n    \"\"\"\n    mask = (tile_geom.intersects(geometry) for tile_geom in self.tiles_gdf.geometry)\n    return self.tiles_gdf.loc[mask, [\"tile_id\", \"tile_url\", \"size_mb\"]].to_dict(\n        \"records\"\n    )\n</code></pre>"},{"location":"api/handlers/#gigaspatial.handlers.google_open_buildings.GoogleOpenBuildingsDownloader","title":"<code>GoogleOpenBuildingsDownloader</code>","text":"<p>               Bases: <code>BaseHandlerDownloader</code></p> <p>A class to handle downloads of Google's Open Buildings dataset.</p> Source code in <code>gigaspatial/handlers/google_open_buildings.py</code> <pre><code>class GoogleOpenBuildingsDownloader(BaseHandlerDownloader):\n    \"\"\"A class to handle downloads of Google's Open Buildings dataset.\"\"\"\n\n    def __init__(\n        self,\n        config: Optional[GoogleOpenBuildingsConfig] = None,\n        data_store: Optional[DataStore] = None,\n        logger: Optional[logging.Logger] = None,\n    ):\n        \"\"\"\n        Initialize the downloader.\n\n        Args:\n            config: Optional configuration for file paths and download settings.\n                    If None, a default `GoogleOpenBuildingsConfig` is used.\n            data_store: Optional instance of a `DataStore` for managing data\n                        storage. If None, a `LocalDataStore` is used.\n            logger: Optional custom logger instance. If None, a default logger\n                    named after the module is created and used.\n        \"\"\"\n        config = config or GoogleOpenBuildingsConfig()\n        super().__init__(config=config, data_store=data_store, logger=logger)\n\n    def download_data_unit(\n        self,\n        tile_info: Union[pd.Series, dict],\n        data_type: Literal[\"polygons\", \"points\"] = \"polygons\",\n    ) -&gt; Optional[str]:\n        \"\"\"\n        Download data file for a single tile.\n\n        data_type: The type of building data to download ('polygons' or 'points').\n            Defaults to 'polygons'.\n        \"\"\"\n\n        tile_url = tile_info[\"tile_url\"]\n        if data_type == \"points\":\n            tile_url = tile_url.replace(\"polygons\", \"points\")\n\n        try:\n            response = requests.get(tile_url, stream=True)\n            response.raise_for_status()\n\n            file_path = str(\n                self.config.get_data_unit_path(\n                    tile_info[\"tile_id\"], data_type=data_type\n                )\n            )\n\n            with self.data_store.open(file_path, \"wb\") as file:\n                for chunk in response.iter_content(chunk_size=8192):\n                    file.write(chunk)\n\n                self.logger.debug(\n                    f\"Successfully downloaded tile: {tile_info['tile_id']}\"\n                )\n                return file_path\n\n        except requests.exceptions.RequestException as e:\n            self.logger.error(\n                f\"Failed to download tile {tile_info['tile_id']}: {str(e)}\"\n            )\n            return None\n        except Exception as e:\n            self.logger.error(f\"Unexpected error downloading dataset: {str(e)}\")\n            return None\n\n    def download_data_units(\n        self,\n        tiles: Union[pd.DataFrame, List[dict]],\n        data_type: Literal[\"polygons\", \"points\"] = \"polygons\",\n    ) -&gt; List[str]:\n        \"\"\"\n        Download data files for multiple tiles.\n\n        data_type: The type of building data to download ('polygons' or 'points').\n            Defaults to 'polygons'.\n        \"\"\"\n\n        if len(tiles) == 0:\n            self.logger.warning(f\"There is no matching data\")\n            return []\n\n        with multiprocessing.Pool(self.config.n_workers) as pool:\n            download_func = functools.partial(\n                self.download_data_unit, data_type=data_type\n            )\n            file_paths = list(\n                tqdm(\n                    pool.imap(\n                        download_func,\n                        (\n                            [row for _, row in tiles.iterrows()]\n                            if isinstance(tiles, pd.DataFrame)\n                            else tiles\n                        ),\n                    ),\n                    total=len(tiles),\n                    desc=f\"Downloading {data_type} data\",\n                )\n            )\n\n        return [path for path in file_paths if path is not None]\n\n    def download_by_country(\n        self,\n        country: str,\n        data_type: Literal[\"polygons\", \"points\"] = \"polygons\",\n        data_store: Optional[DataStore] = None,\n        country_geom_path: Optional[Union[str, Path]] = None,\n    ) -&gt; List[str]:\n        \"\"\"\n        Download Google Open Buildings data for a specific country.\n\n        This is a convenience method to download data for an entire country\n        using its code or name.\n\n        Args:\n            country: The country code (e.g., 'USA', 'GBR') or name.\n            data_type: The type of building data to download ('polygons' or 'points').\n                       Defaults to 'polygons'.\n            data_store: Optional instance of a `DataStore` to be used by\n                        `AdminBoundaries` for loading country boundaries. If None,\n                        `AdminBoundaries` will use its default data loading.\n            country_geom_path: Optional path to a GeoJSON file containing the\n                               country boundary. If provided, this boundary is used\n                               instead of the default from `AdminBoundaries`.\n\n        Returns:\n            A list of local file paths for the successfully downloaded tiles\n            for the specified country.\n        \"\"\"\n        return self.download(\n            source=country,\n            data_type=data_type,\n            data_store=data_store,\n            path=country_geom_path,\n        )\n</code></pre>"},{"location":"api/handlers/#gigaspatial.handlers.google_open_buildings.GoogleOpenBuildingsDownloader.__init__","title":"<code>__init__(config=None, data_store=None, logger=None)</code>","text":"<p>Initialize the downloader.</p> <p>Parameters:</p> Name Type Description Default <code>config</code> <code>Optional[GoogleOpenBuildingsConfig]</code> <p>Optional configuration for file paths and download settings.     If None, a default <code>GoogleOpenBuildingsConfig</code> is used.</p> <code>None</code> <code>data_store</code> <code>Optional[DataStore]</code> <p>Optional instance of a <code>DataStore</code> for managing data         storage. If None, a <code>LocalDataStore</code> is used.</p> <code>None</code> <code>logger</code> <code>Optional[Logger]</code> <p>Optional custom logger instance. If None, a default logger     named after the module is created and used.</p> <code>None</code> Source code in <code>gigaspatial/handlers/google_open_buildings.py</code> <pre><code>def __init__(\n    self,\n    config: Optional[GoogleOpenBuildingsConfig] = None,\n    data_store: Optional[DataStore] = None,\n    logger: Optional[logging.Logger] = None,\n):\n    \"\"\"\n    Initialize the downloader.\n\n    Args:\n        config: Optional configuration for file paths and download settings.\n                If None, a default `GoogleOpenBuildingsConfig` is used.\n        data_store: Optional instance of a `DataStore` for managing data\n                    storage. If None, a `LocalDataStore` is used.\n        logger: Optional custom logger instance. If None, a default logger\n                named after the module is created and used.\n    \"\"\"\n    config = config or GoogleOpenBuildingsConfig()\n    super().__init__(config=config, data_store=data_store, logger=logger)\n</code></pre>"},{"location":"api/handlers/#gigaspatial.handlers.google_open_buildings.GoogleOpenBuildingsDownloader.download_by_country","title":"<code>download_by_country(country, data_type='polygons', data_store=None, country_geom_path=None)</code>","text":"<p>Download Google Open Buildings data for a specific country.</p> <p>This is a convenience method to download data for an entire country using its code or name.</p> <p>Parameters:</p> Name Type Description Default <code>country</code> <code>str</code> <p>The country code (e.g., 'USA', 'GBR') or name.</p> required <code>data_type</code> <code>Literal['polygons', 'points']</code> <p>The type of building data to download ('polygons' or 'points').        Defaults to 'polygons'.</p> <code>'polygons'</code> <code>data_store</code> <code>Optional[DataStore]</code> <p>Optional instance of a <code>DataStore</code> to be used by         <code>AdminBoundaries</code> for loading country boundaries. If None,         <code>AdminBoundaries</code> will use its default data loading.</p> <code>None</code> <code>country_geom_path</code> <code>Optional[Union[str, Path]]</code> <p>Optional path to a GeoJSON file containing the                country boundary. If provided, this boundary is used                instead of the default from <code>AdminBoundaries</code>.</p> <code>None</code> <p>Returns:</p> Type Description <code>List[str]</code> <p>A list of local file paths for the successfully downloaded tiles</p> <code>List[str]</code> <p>for the specified country.</p> Source code in <code>gigaspatial/handlers/google_open_buildings.py</code> <pre><code>def download_by_country(\n    self,\n    country: str,\n    data_type: Literal[\"polygons\", \"points\"] = \"polygons\",\n    data_store: Optional[DataStore] = None,\n    country_geom_path: Optional[Union[str, Path]] = None,\n) -&gt; List[str]:\n    \"\"\"\n    Download Google Open Buildings data for a specific country.\n\n    This is a convenience method to download data for an entire country\n    using its code or name.\n\n    Args:\n        country: The country code (e.g., 'USA', 'GBR') or name.\n        data_type: The type of building data to download ('polygons' or 'points').\n                   Defaults to 'polygons'.\n        data_store: Optional instance of a `DataStore` to be used by\n                    `AdminBoundaries` for loading country boundaries. If None,\n                    `AdminBoundaries` will use its default data loading.\n        country_geom_path: Optional path to a GeoJSON file containing the\n                           country boundary. If provided, this boundary is used\n                           instead of the default from `AdminBoundaries`.\n\n    Returns:\n        A list of local file paths for the successfully downloaded tiles\n        for the specified country.\n    \"\"\"\n    return self.download(\n        source=country,\n        data_type=data_type,\n        data_store=data_store,\n        path=country_geom_path,\n    )\n</code></pre>"},{"location":"api/handlers/#gigaspatial.handlers.google_open_buildings.GoogleOpenBuildingsDownloader.download_data_unit","title":"<code>download_data_unit(tile_info, data_type='polygons')</code>","text":"<p>Download data file for a single tile.</p> The type of building data to download ('polygons' or 'points'). <p>Defaults to 'polygons'.</p> Source code in <code>gigaspatial/handlers/google_open_buildings.py</code> <pre><code>def download_data_unit(\n    self,\n    tile_info: Union[pd.Series, dict],\n    data_type: Literal[\"polygons\", \"points\"] = \"polygons\",\n) -&gt; Optional[str]:\n    \"\"\"\n    Download data file for a single tile.\n\n    data_type: The type of building data to download ('polygons' or 'points').\n        Defaults to 'polygons'.\n    \"\"\"\n\n    tile_url = tile_info[\"tile_url\"]\n    if data_type == \"points\":\n        tile_url = tile_url.replace(\"polygons\", \"points\")\n\n    try:\n        response = requests.get(tile_url, stream=True)\n        response.raise_for_status()\n\n        file_path = str(\n            self.config.get_data_unit_path(\n                tile_info[\"tile_id\"], data_type=data_type\n            )\n        )\n\n        with self.data_store.open(file_path, \"wb\") as file:\n            for chunk in response.iter_content(chunk_size=8192):\n                file.write(chunk)\n\n            self.logger.debug(\n                f\"Successfully downloaded tile: {tile_info['tile_id']}\"\n            )\n            return file_path\n\n    except requests.exceptions.RequestException as e:\n        self.logger.error(\n            f\"Failed to download tile {tile_info['tile_id']}: {str(e)}\"\n        )\n        return None\n    except Exception as e:\n        self.logger.error(f\"Unexpected error downloading dataset: {str(e)}\")\n        return None\n</code></pre>"},{"location":"api/handlers/#gigaspatial.handlers.google_open_buildings.GoogleOpenBuildingsDownloader.download_data_units","title":"<code>download_data_units(tiles, data_type='polygons')</code>","text":"<p>Download data files for multiple tiles.</p> The type of building data to download ('polygons' or 'points'). <p>Defaults to 'polygons'.</p> Source code in <code>gigaspatial/handlers/google_open_buildings.py</code> <pre><code>def download_data_units(\n    self,\n    tiles: Union[pd.DataFrame, List[dict]],\n    data_type: Literal[\"polygons\", \"points\"] = \"polygons\",\n) -&gt; List[str]:\n    \"\"\"\n    Download data files for multiple tiles.\n\n    data_type: The type of building data to download ('polygons' or 'points').\n        Defaults to 'polygons'.\n    \"\"\"\n\n    if len(tiles) == 0:\n        self.logger.warning(f\"There is no matching data\")\n        return []\n\n    with multiprocessing.Pool(self.config.n_workers) as pool:\n        download_func = functools.partial(\n            self.download_data_unit, data_type=data_type\n        )\n        file_paths = list(\n            tqdm(\n                pool.imap(\n                    download_func,\n                    (\n                        [row for _, row in tiles.iterrows()]\n                        if isinstance(tiles, pd.DataFrame)\n                        else tiles\n                    ),\n                ),\n                total=len(tiles),\n                desc=f\"Downloading {data_type} data\",\n            )\n        )\n\n    return [path for path in file_paths if path is not None]\n</code></pre>"},{"location":"api/handlers/#gigaspatial.handlers.google_open_buildings.GoogleOpenBuildingsHandler","title":"<code>GoogleOpenBuildingsHandler</code>","text":"<p>               Bases: <code>BaseHandler</code></p> <p>Handler for Google Open Buildings dataset.</p> <p>This class provides a unified interface for downloading and loading Google Open Buildings data. It manages the lifecycle of configuration, downloading, and reading components.</p> Source code in <code>gigaspatial/handlers/google_open_buildings.py</code> <pre><code>class GoogleOpenBuildingsHandler(BaseHandler):\n    \"\"\"\n    Handler for Google Open Buildings dataset.\n\n    This class provides a unified interface for downloading and loading Google Open Buildings data.\n    It manages the lifecycle of configuration, downloading, and reading components.\n    \"\"\"\n\n    def create_config(\n        self, data_store: DataStore, logger: logging.Logger, **kwargs\n    ) -&gt; GoogleOpenBuildingsConfig:\n        \"\"\"\n        Create and return a GoogleOpenBuildingsConfig instance.\n\n        Args:\n            data_store: The data store instance to use\n            logger: The logger instance to use\n            **kwargs: Additional configuration parameters\n\n        Returns:\n            Configured GoogleOpenBuildingsConfig instance\n        \"\"\"\n        return GoogleOpenBuildingsConfig(data_store=data_store, logger=logger, **kwargs)\n\n    def create_downloader(\n        self,\n        config: GoogleOpenBuildingsConfig,\n        data_store: DataStore,\n        logger: logging.Logger,\n        **kwargs,\n    ) -&gt; GoogleOpenBuildingsDownloader:\n        \"\"\"\n        Create and return a GoogleOpenBuildingsDownloader instance.\n\n        Args:\n            config: The configuration object\n            data_store: The data store instance to use\n            logger: The logger instance to use\n            **kwargs: Additional downloader parameters\n\n        Returns:\n            Configured GoogleOpenBuildingsDownloader instance\n        \"\"\"\n        return GoogleOpenBuildingsDownloader(\n            config=config, data_store=data_store, logger=logger, **kwargs\n        )\n\n    def create_reader(\n        self,\n        config: GoogleOpenBuildingsConfig,\n        data_store: DataStore,\n        logger: logging.Logger,\n        **kwargs,\n    ) -&gt; GoogleOpenBuildingsReader:\n        \"\"\"\n        Create and return a GoogleOpenBuildingsReader instance.\n\n        Args:\n            config: The configuration object\n            data_store: The data store instance to use\n            logger: The logger instance to use\n            **kwargs: Additional reader parameters\n\n        Returns:\n            Configured GoogleOpenBuildingsReader instance\n        \"\"\"\n        return GoogleOpenBuildingsReader(\n            config=config, data_store=data_store, logger=logger, **kwargs\n        )\n\n    def load_points(\n        self,\n        source: Union[\n            str,  # country\n            List[Union[tuple, Point]],  # points\n            BaseGeometry,  # geometry\n            gpd.GeoDataFrame,  # geodataframe\n            Path,  # path\n            List[Union[str, Path]],  # list of paths\n        ],\n        crop_to_source: bool = False,\n        ensure_available: bool = True,\n        **kwargs,\n    ) -&gt; gpd.GeoDataFrame:\n        \"\"\"\n        Load point data from Google Open Buildings dataset.\n\n        Args:\n            source: The data source specification\n            ensure_available: If True, ensure data is downloaded before loading\n            **kwargs: Additional parameters passed to load methods\n\n        Returns:\n            GeoDataFrame containing building point data\n        \"\"\"\n        return self.load_data(\n            source=source,\n            crop_to_source=crop_to_source,\n            ensure_available=ensure_available,\n            data_type=\"points\",\n            **kwargs,\n        )\n\n    def load_polygons(\n        self,\n        source: Union[\n            str,  # country\n            List[Union[tuple, Point]],  # points\n            BaseGeometry,  # geometry\n            gpd.GeoDataFrame,  # geodataframe\n            Path,  # path\n            List[Union[str, Path]],  # list of paths\n        ],\n        crop_to_source: bool = False,\n        ensure_available: bool = True,\n        **kwargs,\n    ) -&gt; gpd.GeoDataFrame:\n        \"\"\"\n        Load polygon data from Google Open Buildings dataset.\n\n        Args:\n            source: The data source specification\n            ensure_available: If True, ensure data is downloaded before loading\n            **kwargs: Additional parameters passed to load methods\n\n        Returns:\n            GeoDataFrame containing building polygon data\n        \"\"\"\n        return self.load_data(\n            source=source,\n            crop_to_source=crop_to_source,\n            ensure_available=ensure_available,\n            data_type=\"polygons\",\n            **kwargs,\n        )\n</code></pre>"},{"location":"api/handlers/#gigaspatial.handlers.google_open_buildings.GoogleOpenBuildingsHandler.create_config","title":"<code>create_config(data_store, logger, **kwargs)</code>","text":"<p>Create and return a GoogleOpenBuildingsConfig instance.</p> <p>Parameters:</p> Name Type Description Default <code>data_store</code> <code>DataStore</code> <p>The data store instance to use</p> required <code>logger</code> <code>Logger</code> <p>The logger instance to use</p> required <code>**kwargs</code> <p>Additional configuration parameters</p> <code>{}</code> <p>Returns:</p> Type Description <code>GoogleOpenBuildingsConfig</code> <p>Configured GoogleOpenBuildingsConfig instance</p> Source code in <code>gigaspatial/handlers/google_open_buildings.py</code> <pre><code>def create_config(\n    self, data_store: DataStore, logger: logging.Logger, **kwargs\n) -&gt; GoogleOpenBuildingsConfig:\n    \"\"\"\n    Create and return a GoogleOpenBuildingsConfig instance.\n\n    Args:\n        data_store: The data store instance to use\n        logger: The logger instance to use\n        **kwargs: Additional configuration parameters\n\n    Returns:\n        Configured GoogleOpenBuildingsConfig instance\n    \"\"\"\n    return GoogleOpenBuildingsConfig(data_store=data_store, logger=logger, **kwargs)\n</code></pre>"},{"location":"api/handlers/#gigaspatial.handlers.google_open_buildings.GoogleOpenBuildingsHandler.create_downloader","title":"<code>create_downloader(config, data_store, logger, **kwargs)</code>","text":"<p>Create and return a GoogleOpenBuildingsDownloader instance.</p> <p>Parameters:</p> Name Type Description Default <code>config</code> <code>GoogleOpenBuildingsConfig</code> <p>The configuration object</p> required <code>data_store</code> <code>DataStore</code> <p>The data store instance to use</p> required <code>logger</code> <code>Logger</code> <p>The logger instance to use</p> required <code>**kwargs</code> <p>Additional downloader parameters</p> <code>{}</code> <p>Returns:</p> Type Description <code>GoogleOpenBuildingsDownloader</code> <p>Configured GoogleOpenBuildingsDownloader instance</p> Source code in <code>gigaspatial/handlers/google_open_buildings.py</code> <pre><code>def create_downloader(\n    self,\n    config: GoogleOpenBuildingsConfig,\n    data_store: DataStore,\n    logger: logging.Logger,\n    **kwargs,\n) -&gt; GoogleOpenBuildingsDownloader:\n    \"\"\"\n    Create and return a GoogleOpenBuildingsDownloader instance.\n\n    Args:\n        config: The configuration object\n        data_store: The data store instance to use\n        logger: The logger instance to use\n        **kwargs: Additional downloader parameters\n\n    Returns:\n        Configured GoogleOpenBuildingsDownloader instance\n    \"\"\"\n    return GoogleOpenBuildingsDownloader(\n        config=config, data_store=data_store, logger=logger, **kwargs\n    )\n</code></pre>"},{"location":"api/handlers/#gigaspatial.handlers.google_open_buildings.GoogleOpenBuildingsHandler.create_reader","title":"<code>create_reader(config, data_store, logger, **kwargs)</code>","text":"<p>Create and return a GoogleOpenBuildingsReader instance.</p> <p>Parameters:</p> Name Type Description Default <code>config</code> <code>GoogleOpenBuildingsConfig</code> <p>The configuration object</p> required <code>data_store</code> <code>DataStore</code> <p>The data store instance to use</p> required <code>logger</code> <code>Logger</code> <p>The logger instance to use</p> required <code>**kwargs</code> <p>Additional reader parameters</p> <code>{}</code> <p>Returns:</p> Type Description <code>GoogleOpenBuildingsReader</code> <p>Configured GoogleOpenBuildingsReader instance</p> Source code in <code>gigaspatial/handlers/google_open_buildings.py</code> <pre><code>def create_reader(\n    self,\n    config: GoogleOpenBuildingsConfig,\n    data_store: DataStore,\n    logger: logging.Logger,\n    **kwargs,\n) -&gt; GoogleOpenBuildingsReader:\n    \"\"\"\n    Create and return a GoogleOpenBuildingsReader instance.\n\n    Args:\n        config: The configuration object\n        data_store: The data store instance to use\n        logger: The logger instance to use\n        **kwargs: Additional reader parameters\n\n    Returns:\n        Configured GoogleOpenBuildingsReader instance\n    \"\"\"\n    return GoogleOpenBuildingsReader(\n        config=config, data_store=data_store, logger=logger, **kwargs\n    )\n</code></pre>"},{"location":"api/handlers/#gigaspatial.handlers.google_open_buildings.GoogleOpenBuildingsHandler.load_points","title":"<code>load_points(source, crop_to_source=False, ensure_available=True, **kwargs)</code>","text":"<p>Load point data from Google Open Buildings dataset.</p> <p>Parameters:</p> Name Type Description Default <code>source</code> <code>Union[str, List[Union[tuple, Point]], BaseGeometry, GeoDataFrame, Path, List[Union[str, Path]]]</code> <p>The data source specification</p> required <code>ensure_available</code> <code>bool</code> <p>If True, ensure data is downloaded before loading</p> <code>True</code> <code>**kwargs</code> <p>Additional parameters passed to load methods</p> <code>{}</code> <p>Returns:</p> Type Description <code>GeoDataFrame</code> <p>GeoDataFrame containing building point data</p> Source code in <code>gigaspatial/handlers/google_open_buildings.py</code> <pre><code>def load_points(\n    self,\n    source: Union[\n        str,  # country\n        List[Union[tuple, Point]],  # points\n        BaseGeometry,  # geometry\n        gpd.GeoDataFrame,  # geodataframe\n        Path,  # path\n        List[Union[str, Path]],  # list of paths\n    ],\n    crop_to_source: bool = False,\n    ensure_available: bool = True,\n    **kwargs,\n) -&gt; gpd.GeoDataFrame:\n    \"\"\"\n    Load point data from Google Open Buildings dataset.\n\n    Args:\n        source: The data source specification\n        ensure_available: If True, ensure data is downloaded before loading\n        **kwargs: Additional parameters passed to load methods\n\n    Returns:\n        GeoDataFrame containing building point data\n    \"\"\"\n    return self.load_data(\n        source=source,\n        crop_to_source=crop_to_source,\n        ensure_available=ensure_available,\n        data_type=\"points\",\n        **kwargs,\n    )\n</code></pre>"},{"location":"api/handlers/#gigaspatial.handlers.google_open_buildings.GoogleOpenBuildingsHandler.load_polygons","title":"<code>load_polygons(source, crop_to_source=False, ensure_available=True, **kwargs)</code>","text":"<p>Load polygon data from Google Open Buildings dataset.</p> <p>Parameters:</p> Name Type Description Default <code>source</code> <code>Union[str, List[Union[tuple, Point]], BaseGeometry, GeoDataFrame, Path, List[Union[str, Path]]]</code> <p>The data source specification</p> required <code>ensure_available</code> <code>bool</code> <p>If True, ensure data is downloaded before loading</p> <code>True</code> <code>**kwargs</code> <p>Additional parameters passed to load methods</p> <code>{}</code> <p>Returns:</p> Type Description <code>GeoDataFrame</code> <p>GeoDataFrame containing building polygon data</p> Source code in <code>gigaspatial/handlers/google_open_buildings.py</code> <pre><code>def load_polygons(\n    self,\n    source: Union[\n        str,  # country\n        List[Union[tuple, Point]],  # points\n        BaseGeometry,  # geometry\n        gpd.GeoDataFrame,  # geodataframe\n        Path,  # path\n        List[Union[str, Path]],  # list of paths\n    ],\n    crop_to_source: bool = False,\n    ensure_available: bool = True,\n    **kwargs,\n) -&gt; gpd.GeoDataFrame:\n    \"\"\"\n    Load polygon data from Google Open Buildings dataset.\n\n    Args:\n        source: The data source specification\n        ensure_available: If True, ensure data is downloaded before loading\n        **kwargs: Additional parameters passed to load methods\n\n    Returns:\n        GeoDataFrame containing building polygon data\n    \"\"\"\n    return self.load_data(\n        source=source,\n        crop_to_source=crop_to_source,\n        ensure_available=ensure_available,\n        data_type=\"polygons\",\n        **kwargs,\n    )\n</code></pre>"},{"location":"api/handlers/#gigaspatial.handlers.google_open_buildings.GoogleOpenBuildingsReader","title":"<code>GoogleOpenBuildingsReader</code>","text":"<p>               Bases: <code>BaseHandlerReader</code></p> <p>Reader for Google Open Buildings data, supporting country, points, and geometry-based resolution.</p> Source code in <code>gigaspatial/handlers/google_open_buildings.py</code> <pre><code>class GoogleOpenBuildingsReader(BaseHandlerReader):\n    \"\"\"\n    Reader for Google Open Buildings data, supporting country, points, and geometry-based resolution.\n    \"\"\"\n\n    def __init__(\n        self,\n        config: Optional[GoogleOpenBuildingsConfig] = None,\n        data_store: Optional[DataStore] = None,\n        logger: Optional[logging.Logger] = None,\n    ):\n        config = config or GoogleOpenBuildingsConfig()\n        super().__init__(config=config, data_store=data_store, logger=logger)\n\n    def load_from_paths(\n        self, source_data_path: List[Union[str, Path]], **kwargs\n    ) -&gt; Union[pd.DataFrame, gpd.GeoDataFrame]:\n        \"\"\"\n        Load building data from Google Open Buildings dataset.\n        Args:\n            source_data_path: List of file paths to load\n        Returns:\n            GeoDataFrame containing building data\n        \"\"\"\n        result = self._load_tabular_data(file_paths=source_data_path)\n        return result\n\n    def load(\n        self, source, crop_to_source: bool = False, data_type=\"polygons\", **kwargs\n    ):\n        return super().load(\n            source=source, crop_to_source=crop_to_source, data_type=data_type, **kwargs\n        )\n\n    def load_points(self, source, crop_to_source: bool = False, **kwargs):\n        \"\"\"This is a convenience method to load points data\"\"\"\n        return self.load(\n            source=source, crop_to_source=crop_to_source, data_type=\"points\", **kwargs\n        )\n\n    def load_polygons(self, source, crop_to_source: bool = False, **kwargs):\n        \"\"\"This is a convenience method to load polygons data\"\"\"\n        return self.load(\n            source=source, crop_to_source=crop_to_source, data_type=\"polygons\", **kwargs\n        )\n</code></pre>"},{"location":"api/handlers/#gigaspatial.handlers.google_open_buildings.GoogleOpenBuildingsReader.load_from_paths","title":"<code>load_from_paths(source_data_path, **kwargs)</code>","text":"<p>Load building data from Google Open Buildings dataset. Args:     source_data_path: List of file paths to load Returns:     GeoDataFrame containing building data</p> Source code in <code>gigaspatial/handlers/google_open_buildings.py</code> <pre><code>def load_from_paths(\n    self, source_data_path: List[Union[str, Path]], **kwargs\n) -&gt; Union[pd.DataFrame, gpd.GeoDataFrame]:\n    \"\"\"\n    Load building data from Google Open Buildings dataset.\n    Args:\n        source_data_path: List of file paths to load\n    Returns:\n        GeoDataFrame containing building data\n    \"\"\"\n    result = self._load_tabular_data(file_paths=source_data_path)\n    return result\n</code></pre>"},{"location":"api/handlers/#gigaspatial.handlers.google_open_buildings.GoogleOpenBuildingsReader.load_points","title":"<code>load_points(source, crop_to_source=False, **kwargs)</code>","text":"<p>This is a convenience method to load points data</p> Source code in <code>gigaspatial/handlers/google_open_buildings.py</code> <pre><code>def load_points(self, source, crop_to_source: bool = False, **kwargs):\n    \"\"\"This is a convenience method to load points data\"\"\"\n    return self.load(\n        source=source, crop_to_source=crop_to_source, data_type=\"points\", **kwargs\n    )\n</code></pre>"},{"location":"api/handlers/#gigaspatial.handlers.google_open_buildings.GoogleOpenBuildingsReader.load_polygons","title":"<code>load_polygons(source, crop_to_source=False, **kwargs)</code>","text":"<p>This is a convenience method to load polygons data</p> Source code in <code>gigaspatial/handlers/google_open_buildings.py</code> <pre><code>def load_polygons(self, source, crop_to_source: bool = False, **kwargs):\n    \"\"\"This is a convenience method to load polygons data\"\"\"\n    return self.load(\n        source=source, crop_to_source=crop_to_source, data_type=\"polygons\", **kwargs\n    )\n</code></pre>"},{"location":"api/handlers/#gigaspatial.handlers.hdx","title":"<code>hdx</code>","text":""},{"location":"api/handlers/#gigaspatial.handlers.hdx.HDXConfig","title":"<code>HDXConfig</code>  <code>dataclass</code>","text":"<p>               Bases: <code>BaseHandlerConfig</code></p> <p>Configuration for HDX data access</p> Source code in <code>gigaspatial/handlers/hdx.py</code> <pre><code>@dataclass(config=ConfigDict(arbitrary_types_allowed=True))\nclass HDXConfig(BaseHandlerConfig):\n    \"\"\"Configuration for HDX data access\"\"\"\n\n    # User configuration\n    dataset_name: str = Field(\n        default=..., description=\"Name of the HDX dataset to download\"\n    )\n\n    # Optional configuration with defaults\n    base_path: Path = Field(default=global_config.get_path(\"hdx\", \"bronze\"))\n    user_agent: str = Field(\n        default=\"gigaspatial\", description=\"User agent for HDX API requests\"\n    )\n    hdx_site: str = Field(default=\"prod\", description=\"HDX site to use (prod or test)\")\n\n    # Internal state\n    _hdx_configured: bool = Field(default=False, init=False)\n    dataset: Optional[Dataset] = Field(default=None, init=False)\n\n    @staticmethod\n    def search_datasets(\n        query: str,\n        rows: int = None,\n        sort: str = \"relevance asc, metadata_modified desc\",\n        hdx_site: str = \"prod\",\n        user_agent: str = \"gigaspatial\",\n    ) -&gt; List[Dict]:\n        \"\"\"Search for datasets in HDX before initializing the class.\n\n        Args:\n            query: Search query string\n            rows: Number of results per page. Defaults to all datasets (sys.maxsize).\n            sort: Sort order - one of 'relevance', 'views_recent', 'views_total', 'last_modified' (default: 'relevance')\n            hdx_site: HDX site to use - 'prod' or 'test' (default: 'prod')\n            user_agent: User agent for HDX API requests (default: 'gigaspatial')\n\n        Returns:\n            List of dataset dictionaries containing search results\n\n        Example:\n            &gt;&gt;&gt; results = HDXConfig.search_datasets(\"population\", rows=5)\n            &gt;&gt;&gt; for dataset in results:\n            &gt;&gt;&gt;     print(f\"Name: {dataset['name']}, Title: {dataset['title']}\")\n        \"\"\"\n        try:\n            Configuration.create(\n                hdx_site=hdx_site,\n                user_agent=user_agent,\n                hdx_read_only=True,\n            )\n        except:\n            pass\n\n        try:\n            results = Dataset.search_in_hdx(query=query, rows=rows, sort=sort)\n\n            return results\n        except Exception as e:\n            logging.error(f\"Error searching HDX datasets: {str(e)}\")\n            raise\n\n    def __post_init__(self):\n        super().__post_init__()\n        try:\n            Configuration.read()\n            self._hdx_configured = True\n        except Exception:\n            self._hdx_configured = False\n        self.configure_hdx()\n        self.dataset = self.fetch_dataset()\n\n    @property\n    def output_dir_path(self) -&gt; Path:\n        \"\"\"Path to save the downloaded HDX dataset\"\"\"\n        return self.base_path / self.dataset_name\n\n    def configure_hdx(self):\n        \"\"\"Configure HDX API if not already configured\"\"\"\n        if not self._hdx_configured:\n            try:\n                Configuration.create(\n                    hdx_site=self.hdx_site,\n                    user_agent=self.user_agent,\n                    hdx_read_only=True,\n                )\n                self._hdx_configured = True\n            except Exception as e:\n                self.logger.error(f\"Error configuring HDX API: {str(e)}\")\n                raise\n\n    def fetch_dataset(self) -&gt; Dataset:\n        \"\"\"Get the HDX dataset\"\"\"\n        try:\n            self.logger.info(f\"Fetching HDX dataset: {self.dataset_name}\")\n            dataset = Dataset.read_from_hdx(self.dataset_name)\n            if not dataset:\n                raise ValueError(\n                    f\"Dataset '{self.dataset_name}' not found on HDX. \"\n                    \"Please verify the dataset name or use search_datasets() \"\n                    \"to find available datasets.\"\n                )\n            return dataset\n        except Exception as e:\n            self.logger.error(f\"Error fetching HDX dataset: {str(e)}\")\n            raise\n\n    def _match_pattern(self, value: str, pattern: str) -&gt; bool:\n        \"\"\"Check if a value matches a pattern\"\"\"\n        if isinstance(pattern, str):\n            return pattern.lower() in value.lower()\n        return value == pattern\n\n    def _get_patterns_for_value(self, value: Any) -&gt; List[str]:\n        \"\"\"Generate patterns for a given value or list of values\"\"\"\n        if isinstance(value, list):\n            patterns = []\n            for v in value:\n                patterns.extend(self._get_patterns_for_value(v))\n            return patterns\n\n        if not isinstance(value, str):\n            return [value]\n\n        patterns = []\n        value = value.lower()\n\n        # Add exact match\n        patterns.append(value)\n\n        # Add common variations\n        patterns.extend(\n            [\n                f\"/{value}_\",  # URL path with prefix\n                f\"/{value}.\",  # URL path with extension\n                f\"_{value}_\",  # Filename with value in middle\n                f\"_{value}.\",  # Filename with value at end\n            ]\n        )\n\n        # If value contains spaces, generate additional patterns\n        if \" \" in value:\n            # Generate patterns for space-less version\n            no_space = value.replace(\" \", \"\")\n            patterns.extend(self._get_patterns_for_value(no_space))\n\n            # Generate patterns for hyphenated version\n            hyphenated = value.replace(\" \", \"-\")\n            patterns.extend(self._get_patterns_for_value(hyphenated))\n\n        return patterns\n\n    def get_dataset_resources(\n        self, filter: Optional[Dict[str, Any]] = None, exact_match: bool = False\n    ) -&gt; List[Resource]:\n        \"\"\"Get resources from the HDX dataset\n\n        Args:\n            filter: Dictionary of key-value pairs to filter resources\n            exact_match: If True, perform exact matching. If False, use pattern matching\n        \"\"\"\n        try:\n            resources = self.dataset.get_resources()\n\n            # Apply resource filter if specified\n            if filter:\n                filtered_resources = []\n                for res in resources:\n                    match = True\n                    for key, value in filter.items():\n                        if key not in res.data:\n                            match = False\n                            break\n\n                        if exact_match:\n                            # For exact matching, check if value matches or is in list of values\n                            if isinstance(value, list):\n                                if res.data[key] not in value:\n                                    match = False\n                                    break\n                            elif res.data[key] != value:\n                                match = False\n                                break\n                        else:\n                            # For pattern matching, generate patterns for value(s)\n                            patterns = self._get_patterns_for_value(value)\n                            if not any(\n                                self._match_pattern(str(res.data[key]), pattern)\n                                for pattern in patterns\n                            ):\n                                match = False\n                                break\n\n                    if match:\n                        filtered_resources.append(res)\n                resources = filtered_resources\n\n            return resources\n        except Exception as e:\n            self.logger.error(f\"Error getting dataset resources: {str(e)}\")\n            raise\n\n    def get_relevant_data_units_by_geometry(\n        self, geometry: Union[BaseGeometry, gpd.GeoDataFrame], **kwargs\n    ) -&gt; List[Resource]:\n        return self.get_dataset_resources(geometry, **kwargs)\n\n    def get_data_unit_path(self, unit: str, **kwargs) -&gt; str:\n        \"\"\"Get the path for a data unit\"\"\"\n        try:\n            filename = unit.data[\"name\"]\n        except:\n            filename = unit.get(\"download_url\").split(\"/\")[-1]\n\n        return self.output_dir_path / filename\n\n    def list_resources(self) -&gt; List[str]:\n        \"\"\"List all resources in the dataset directory using the data_store.\"\"\"\n        dataset_folder = str(self.output_dir_path)\n        # Check if the dataset directory exists in the data_store\n        if not (\n            self.data_store.is_dir(dataset_folder)\n            or self.data_store.file_exists(dataset_folder)\n        ):\n            raise FileNotFoundError(\n                f\"HDX dataset not found at {dataset_folder}. \"\n                \"Download the data first using HDXDownloader.\"\n            )\n        return self.data_store.list_files(dataset_folder)\n\n    def extract_search_geometry(self, source, **kwargs):\n        \"\"\"\n        Override the base class method since geometry extraction does not apply.\n        Returns dictionary to filter.\n\n        Args:\n            source: Either a country name/code (str) or a filter dictionary\n            **kwargs: Additional keyword arguments passed to the specific method\n        \"\"\"\n        if isinstance(source, str):\n            country = pycountry.countries.lookup(source)\n            values = [country.alpha_3, country.alpha_2, country.name]\n            key = kwargs.get(\n                \"key\", \"url\"\n            )  # The key to filter on in the resource data. Defaults to `url`\n            return {key: values}\n        elif isinstance(source, dict):\n            return source\n        else:\n            raise ValueError(\n                f\"Unsupported source type: {type(source)}\"\n                \"Please use country-based (str) filtering or direct resource (dict) filtering instead.\"\n            )\n\n    def __repr__(self) -&gt; str:\n        return (\n            f\"HDXConfig(\\n\"\n            f\"  dataset_name='{self.dataset_name}'\\n\"\n            f\"  base_path='{self.base_path}'\\n\"\n            f\"  hdx_site='{self.hdx_site}'\\n\"\n            f\"  user_agent='{self.user_agent}'\\n\"\n            f\")\"\n        )\n</code></pre>"},{"location":"api/handlers/#gigaspatial.handlers.hdx.HDXConfig.output_dir_path","title":"<code>output_dir_path: Path</code>  <code>property</code>","text":"<p>Path to save the downloaded HDX dataset</p>"},{"location":"api/handlers/#gigaspatial.handlers.hdx.HDXConfig.configure_hdx","title":"<code>configure_hdx()</code>","text":"<p>Configure HDX API if not already configured</p> Source code in <code>gigaspatial/handlers/hdx.py</code> <pre><code>def configure_hdx(self):\n    \"\"\"Configure HDX API if not already configured\"\"\"\n    if not self._hdx_configured:\n        try:\n            Configuration.create(\n                hdx_site=self.hdx_site,\n                user_agent=self.user_agent,\n                hdx_read_only=True,\n            )\n            self._hdx_configured = True\n        except Exception as e:\n            self.logger.error(f\"Error configuring HDX API: {str(e)}\")\n            raise\n</code></pre>"},{"location":"api/handlers/#gigaspatial.handlers.hdx.HDXConfig.extract_search_geometry","title":"<code>extract_search_geometry(source, **kwargs)</code>","text":"<p>Override the base class method since geometry extraction does not apply. Returns dictionary to filter.</p> <p>Parameters:</p> Name Type Description Default <code>source</code> <p>Either a country name/code (str) or a filter dictionary</p> required <code>**kwargs</code> <p>Additional keyword arguments passed to the specific method</p> <code>{}</code> Source code in <code>gigaspatial/handlers/hdx.py</code> <pre><code>def extract_search_geometry(self, source, **kwargs):\n    \"\"\"\n    Override the base class method since geometry extraction does not apply.\n    Returns dictionary to filter.\n\n    Args:\n        source: Either a country name/code (str) or a filter dictionary\n        **kwargs: Additional keyword arguments passed to the specific method\n    \"\"\"\n    if isinstance(source, str):\n        country = pycountry.countries.lookup(source)\n        values = [country.alpha_3, country.alpha_2, country.name]\n        key = kwargs.get(\n            \"key\", \"url\"\n        )  # The key to filter on in the resource data. Defaults to `url`\n        return {key: values}\n    elif isinstance(source, dict):\n        return source\n    else:\n        raise ValueError(\n            f\"Unsupported source type: {type(source)}\"\n            \"Please use country-based (str) filtering or direct resource (dict) filtering instead.\"\n        )\n</code></pre>"},{"location":"api/handlers/#gigaspatial.handlers.hdx.HDXConfig.fetch_dataset","title":"<code>fetch_dataset()</code>","text":"<p>Get the HDX dataset</p> Source code in <code>gigaspatial/handlers/hdx.py</code> <pre><code>def fetch_dataset(self) -&gt; Dataset:\n    \"\"\"Get the HDX dataset\"\"\"\n    try:\n        self.logger.info(f\"Fetching HDX dataset: {self.dataset_name}\")\n        dataset = Dataset.read_from_hdx(self.dataset_name)\n        if not dataset:\n            raise ValueError(\n                f\"Dataset '{self.dataset_name}' not found on HDX. \"\n                \"Please verify the dataset name or use search_datasets() \"\n                \"to find available datasets.\"\n            )\n        return dataset\n    except Exception as e:\n        self.logger.error(f\"Error fetching HDX dataset: {str(e)}\")\n        raise\n</code></pre>"},{"location":"api/handlers/#gigaspatial.handlers.hdx.HDXConfig.get_data_unit_path","title":"<code>get_data_unit_path(unit, **kwargs)</code>","text":"<p>Get the path for a data unit</p> Source code in <code>gigaspatial/handlers/hdx.py</code> <pre><code>def get_data_unit_path(self, unit: str, **kwargs) -&gt; str:\n    \"\"\"Get the path for a data unit\"\"\"\n    try:\n        filename = unit.data[\"name\"]\n    except:\n        filename = unit.get(\"download_url\").split(\"/\")[-1]\n\n    return self.output_dir_path / filename\n</code></pre>"},{"location":"api/handlers/#gigaspatial.handlers.hdx.HDXConfig.get_dataset_resources","title":"<code>get_dataset_resources(filter=None, exact_match=False)</code>","text":"<p>Get resources from the HDX dataset</p> <p>Parameters:</p> Name Type Description Default <code>filter</code> <code>Optional[Dict[str, Any]]</code> <p>Dictionary of key-value pairs to filter resources</p> <code>None</code> <code>exact_match</code> <code>bool</code> <p>If True, perform exact matching. If False, use pattern matching</p> <code>False</code> Source code in <code>gigaspatial/handlers/hdx.py</code> <pre><code>def get_dataset_resources(\n    self, filter: Optional[Dict[str, Any]] = None, exact_match: bool = False\n) -&gt; List[Resource]:\n    \"\"\"Get resources from the HDX dataset\n\n    Args:\n        filter: Dictionary of key-value pairs to filter resources\n        exact_match: If True, perform exact matching. If False, use pattern matching\n    \"\"\"\n    try:\n        resources = self.dataset.get_resources()\n\n        # Apply resource filter if specified\n        if filter:\n            filtered_resources = []\n            for res in resources:\n                match = True\n                for key, value in filter.items():\n                    if key not in res.data:\n                        match = False\n                        break\n\n                    if exact_match:\n                        # For exact matching, check if value matches or is in list of values\n                        if isinstance(value, list):\n                            if res.data[key] not in value:\n                                match = False\n                                break\n                        elif res.data[key] != value:\n                            match = False\n                            break\n                    else:\n                        # For pattern matching, generate patterns for value(s)\n                        patterns = self._get_patterns_for_value(value)\n                        if not any(\n                            self._match_pattern(str(res.data[key]), pattern)\n                            for pattern in patterns\n                        ):\n                            match = False\n                            break\n\n                if match:\n                    filtered_resources.append(res)\n            resources = filtered_resources\n\n        return resources\n    except Exception as e:\n        self.logger.error(f\"Error getting dataset resources: {str(e)}\")\n        raise\n</code></pre>"},{"location":"api/handlers/#gigaspatial.handlers.hdx.HDXConfig.list_resources","title":"<code>list_resources()</code>","text":"<p>List all resources in the dataset directory using the data_store.</p> Source code in <code>gigaspatial/handlers/hdx.py</code> <pre><code>def list_resources(self) -&gt; List[str]:\n    \"\"\"List all resources in the dataset directory using the data_store.\"\"\"\n    dataset_folder = str(self.output_dir_path)\n    # Check if the dataset directory exists in the data_store\n    if not (\n        self.data_store.is_dir(dataset_folder)\n        or self.data_store.file_exists(dataset_folder)\n    ):\n        raise FileNotFoundError(\n            f\"HDX dataset not found at {dataset_folder}. \"\n            \"Download the data first using HDXDownloader.\"\n        )\n    return self.data_store.list_files(dataset_folder)\n</code></pre>"},{"location":"api/handlers/#gigaspatial.handlers.hdx.HDXConfig.search_datasets","title":"<code>search_datasets(query, rows=None, sort='relevance asc, metadata_modified desc', hdx_site='prod', user_agent='gigaspatial')</code>  <code>staticmethod</code>","text":"<p>Search for datasets in HDX before initializing the class.</p> <p>Parameters:</p> Name Type Description Default <code>query</code> <code>str</code> <p>Search query string</p> required <code>rows</code> <code>int</code> <p>Number of results per page. Defaults to all datasets (sys.maxsize).</p> <code>None</code> <code>sort</code> <code>str</code> <p>Sort order - one of 'relevance', 'views_recent', 'views_total', 'last_modified' (default: 'relevance')</p> <code>'relevance asc, metadata_modified desc'</code> <code>hdx_site</code> <code>str</code> <p>HDX site to use - 'prod' or 'test' (default: 'prod')</p> <code>'prod'</code> <code>user_agent</code> <code>str</code> <p>User agent for HDX API requests (default: 'gigaspatial')</p> <code>'gigaspatial'</code> <p>Returns:</p> Type Description <code>List[Dict]</code> <p>List of dataset dictionaries containing search results</p> Example <p>results = HDXConfig.search_datasets(\"population\", rows=5) for dataset in results:     print(f\"Name: {dataset['name']}, Title: {dataset['title']}\")</p> Source code in <code>gigaspatial/handlers/hdx.py</code> <pre><code>@staticmethod\ndef search_datasets(\n    query: str,\n    rows: int = None,\n    sort: str = \"relevance asc, metadata_modified desc\",\n    hdx_site: str = \"prod\",\n    user_agent: str = \"gigaspatial\",\n) -&gt; List[Dict]:\n    \"\"\"Search for datasets in HDX before initializing the class.\n\n    Args:\n        query: Search query string\n        rows: Number of results per page. Defaults to all datasets (sys.maxsize).\n        sort: Sort order - one of 'relevance', 'views_recent', 'views_total', 'last_modified' (default: 'relevance')\n        hdx_site: HDX site to use - 'prod' or 'test' (default: 'prod')\n        user_agent: User agent for HDX API requests (default: 'gigaspatial')\n\n    Returns:\n        List of dataset dictionaries containing search results\n\n    Example:\n        &gt;&gt;&gt; results = HDXConfig.search_datasets(\"population\", rows=5)\n        &gt;&gt;&gt; for dataset in results:\n        &gt;&gt;&gt;     print(f\"Name: {dataset['name']}, Title: {dataset['title']}\")\n    \"\"\"\n    try:\n        Configuration.create(\n            hdx_site=hdx_site,\n            user_agent=user_agent,\n            hdx_read_only=True,\n        )\n    except:\n        pass\n\n    try:\n        results = Dataset.search_in_hdx(query=query, rows=rows, sort=sort)\n\n        return results\n    except Exception as e:\n        logging.error(f\"Error searching HDX datasets: {str(e)}\")\n        raise\n</code></pre>"},{"location":"api/handlers/#gigaspatial.handlers.hdx.HDXDownloader","title":"<code>HDXDownloader</code>","text":"<p>               Bases: <code>BaseHandlerDownloader</code></p> <p>Downloader for HDX datasets</p> Source code in <code>gigaspatial/handlers/hdx.py</code> <pre><code>class HDXDownloader(BaseHandlerDownloader):\n    \"\"\"Downloader for HDX datasets\"\"\"\n\n    def __init__(\n        self,\n        config: Union[HDXConfig, dict],\n        data_store: Optional[DataStore] = None,\n        logger: Optional[logging.Logger] = None,\n    ):\n        config = config if isinstance(config, HDXConfig) else HDXConfig(**config)\n        super().__init__(config=config, data_store=data_store, logger=logger)\n\n    def download_data_unit(self, resource: str, **kwargs) -&gt; str:\n        \"\"\"Download a single resource\"\"\"\n        try:\n            resource_name = resource.get(\"name\", \"Unknown\")\n            self.logger.info(f\"Downloading resource: {resource_name}\")\n\n            with tempfile.TemporaryDirectory() as tmpdir:\n                url, local_path = resource.download(folder=tmpdir)\n                with open(local_path, \"rb\") as f:\n                    data = f.read()\n                # Compose the target path in the DataStore\n                target_path = str(self.config.get_data_unit_path(resource))\n                self.data_store.write_file(target_path, data)\n                self.logger.info(\n                    f\"Downloaded resource: {resource_name} to {target_path}\"\n                )\n                return target_path\n        except Exception as e:\n            self.logger.error(f\"Error downloading resource {resource_name}: {str(e)}\")\n            return None\n\n    def download_data_units(self, resources: List[Resource], **kwargs) -&gt; List[str]:\n        \"\"\"Download multiple resources sequentially\n\n        Args:\n            resources: List of HDX Resource objects\n            **kwargs: Additional keyword arguments\n\n        Returns:\n            List of paths to downloaded files\n        \"\"\"\n        if len(resources) == 0:\n            self.logger.warning(\"There is no resource to download\")\n            return []\n\n        downloaded_paths = []\n        for resource in tqdm(resources, desc=\"Downloading resources\"):\n            path = self.download_data_unit(resource)\n            if path:\n                downloaded_paths.append(path)\n\n        return downloaded_paths\n</code></pre>"},{"location":"api/handlers/#gigaspatial.handlers.hdx.HDXDownloader.download_data_unit","title":"<code>download_data_unit(resource, **kwargs)</code>","text":"<p>Download a single resource</p> Source code in <code>gigaspatial/handlers/hdx.py</code> <pre><code>def download_data_unit(self, resource: str, **kwargs) -&gt; str:\n    \"\"\"Download a single resource\"\"\"\n    try:\n        resource_name = resource.get(\"name\", \"Unknown\")\n        self.logger.info(f\"Downloading resource: {resource_name}\")\n\n        with tempfile.TemporaryDirectory() as tmpdir:\n            url, local_path = resource.download(folder=tmpdir)\n            with open(local_path, \"rb\") as f:\n                data = f.read()\n            # Compose the target path in the DataStore\n            target_path = str(self.config.get_data_unit_path(resource))\n            self.data_store.write_file(target_path, data)\n            self.logger.info(\n                f\"Downloaded resource: {resource_name} to {target_path}\"\n            )\n            return target_path\n    except Exception as e:\n        self.logger.error(f\"Error downloading resource {resource_name}: {str(e)}\")\n        return None\n</code></pre>"},{"location":"api/handlers/#gigaspatial.handlers.hdx.HDXDownloader.download_data_units","title":"<code>download_data_units(resources, **kwargs)</code>","text":"<p>Download multiple resources sequentially</p> <p>Parameters:</p> Name Type Description Default <code>resources</code> <code>List[Resource]</code> <p>List of HDX Resource objects</p> required <code>**kwargs</code> <p>Additional keyword arguments</p> <code>{}</code> <p>Returns:</p> Type Description <code>List[str]</code> <p>List of paths to downloaded files</p> Source code in <code>gigaspatial/handlers/hdx.py</code> <pre><code>def download_data_units(self, resources: List[Resource], **kwargs) -&gt; List[str]:\n    \"\"\"Download multiple resources sequentially\n\n    Args:\n        resources: List of HDX Resource objects\n        **kwargs: Additional keyword arguments\n\n    Returns:\n        List of paths to downloaded files\n    \"\"\"\n    if len(resources) == 0:\n        self.logger.warning(\"There is no resource to download\")\n        return []\n\n    downloaded_paths = []\n    for resource in tqdm(resources, desc=\"Downloading resources\"):\n        path = self.download_data_unit(resource)\n        if path:\n            downloaded_paths.append(path)\n\n    return downloaded_paths\n</code></pre>"},{"location":"api/handlers/#gigaspatial.handlers.hdx.HDXHandler","title":"<code>HDXHandler</code>","text":"<p>               Bases: <code>BaseHandler</code></p> <p>Handler for HDX datasets</p> Source code in <code>gigaspatial/handlers/hdx.py</code> <pre><code>class HDXHandler(BaseHandler):\n    \"\"\"Handler for HDX datasets\"\"\"\n\n    def __init__(\n        self,\n        dataset_name: str,\n        config: Optional[HDXConfig] = None,\n        downloader: Optional[HDXDownloader] = None,\n        reader: Optional[HDXReader] = None,\n        data_store: Optional[DataStore] = None,\n        logger: Optional[logging.Logger] = None,\n        **kwargs,\n    ):\n        self._dataset_name = dataset_name\n        super().__init__(\n            config=config,\n            downloader=downloader,\n            reader=reader,\n            data_store=data_store,\n            logger=logger,\n            **kwargs,\n        )\n\n    def create_config(\n        self, data_store: DataStore, logger: logging.Logger, **kwargs\n    ) -&gt; HDXConfig:\n        \"\"\"Create and return a HDXConfig instance\"\"\"\n        return HDXConfig(\n            dataset_name=self._dataset_name,\n            data_store=data_store,\n            logger=logger,\n            **kwargs,\n        )\n\n    def create_downloader(\n        self,\n        config: HDXConfig,\n        data_store: DataStore,\n        logger: logging.Logger,\n        **kwargs,\n    ) -&gt; HDXDownloader:\n        \"\"\"Create and return a HDXDownloader instance\"\"\"\n        return HDXDownloader(\n            config=config,\n            data_store=data_store,\n            logger=logger,\n            **kwargs,\n        )\n\n    def create_reader(\n        self,\n        config: HDXConfig,\n        data_store: DataStore,\n        logger: logging.Logger,\n        **kwargs,\n    ) -&gt; HDXReader:\n        \"\"\"Create and return a HDXReader instance\"\"\"\n        return HDXReader(\n            config=config,\n            data_store=data_store,\n            logger=logger,\n            **kwargs,\n        )\n</code></pre>"},{"location":"api/handlers/#gigaspatial.handlers.hdx.HDXHandler.create_config","title":"<code>create_config(data_store, logger, **kwargs)</code>","text":"<p>Create and return a HDXConfig instance</p> Source code in <code>gigaspatial/handlers/hdx.py</code> <pre><code>def create_config(\n    self, data_store: DataStore, logger: logging.Logger, **kwargs\n) -&gt; HDXConfig:\n    \"\"\"Create and return a HDXConfig instance\"\"\"\n    return HDXConfig(\n        dataset_name=self._dataset_name,\n        data_store=data_store,\n        logger=logger,\n        **kwargs,\n    )\n</code></pre>"},{"location":"api/handlers/#gigaspatial.handlers.hdx.HDXHandler.create_downloader","title":"<code>create_downloader(config, data_store, logger, **kwargs)</code>","text":"<p>Create and return a HDXDownloader instance</p> Source code in <code>gigaspatial/handlers/hdx.py</code> <pre><code>def create_downloader(\n    self,\n    config: HDXConfig,\n    data_store: DataStore,\n    logger: logging.Logger,\n    **kwargs,\n) -&gt; HDXDownloader:\n    \"\"\"Create and return a HDXDownloader instance\"\"\"\n    return HDXDownloader(\n        config=config,\n        data_store=data_store,\n        logger=logger,\n        **kwargs,\n    )\n</code></pre>"},{"location":"api/handlers/#gigaspatial.handlers.hdx.HDXHandler.create_reader","title":"<code>create_reader(config, data_store, logger, **kwargs)</code>","text":"<p>Create and return a HDXReader instance</p> Source code in <code>gigaspatial/handlers/hdx.py</code> <pre><code>def create_reader(\n    self,\n    config: HDXConfig,\n    data_store: DataStore,\n    logger: logging.Logger,\n    **kwargs,\n) -&gt; HDXReader:\n    \"\"\"Create and return a HDXReader instance\"\"\"\n    return HDXReader(\n        config=config,\n        data_store=data_store,\n        logger=logger,\n        **kwargs,\n    )\n</code></pre>"},{"location":"api/handlers/#gigaspatial.handlers.hdx.HDXReader","title":"<code>HDXReader</code>","text":"<p>               Bases: <code>BaseHandlerReader</code></p> <p>Reader for HDX datasets</p> Source code in <code>gigaspatial/handlers/hdx.py</code> <pre><code>class HDXReader(BaseHandlerReader):\n    \"\"\"Reader for HDX datasets\"\"\"\n\n    def __init__(\n        self,\n        config: Optional[HDXConfig] = None,\n        data_store: Optional[DataStore] = None,\n        logger: Optional[logging.Logger] = None,\n    ):\n        config = config if isinstance(config, HDXConfig) else HDXConfig(**config)\n        super().__init__(config=config, data_store=data_store, logger=logger)\n\n    def load_from_paths(\n        self, source_data_path: List[Union[str, Path]], **kwargs\n    ) -&gt; Any:\n        \"\"\"Load data from paths\"\"\"\n        if len(source_data_path) == 1:\n            return read_dataset(self.data_store, source_data_path[0])\n\n        all_data = {}\n        for file_path in source_data_path:\n            try:\n                all_data[file_path] = read_dataset(self.data_store, file_path)\n            except Exception as e:\n                raise ValueError(f\"Could not read file {file_path}: {str(e)}\")\n        return all_data\n\n    def load_all_resources(self):\n        resources = self.config.list_resources()\n        return self.load_from_paths(resources)\n</code></pre>"},{"location":"api/handlers/#gigaspatial.handlers.hdx.HDXReader.load_from_paths","title":"<code>load_from_paths(source_data_path, **kwargs)</code>","text":"<p>Load data from paths</p> Source code in <code>gigaspatial/handlers/hdx.py</code> <pre><code>def load_from_paths(\n    self, source_data_path: List[Union[str, Path]], **kwargs\n) -&gt; Any:\n    \"\"\"Load data from paths\"\"\"\n    if len(source_data_path) == 1:\n        return read_dataset(self.data_store, source_data_path[0])\n\n    all_data = {}\n    for file_path in source_data_path:\n        try:\n            all_data[file_path] = read_dataset(self.data_store, file_path)\n        except Exception as e:\n            raise ValueError(f\"Could not read file {file_path}: {str(e)}\")\n    return all_data\n</code></pre>"},{"location":"api/handlers/#gigaspatial.handlers.healthsites","title":"<code>healthsites</code>","text":""},{"location":"api/handlers/#gigaspatial.handlers.healthsites.HealthSitesFetcher","title":"<code>HealthSitesFetcher</code>","text":"<p>Fetch and process health facility location data from the Healthsites.io API.</p> Source code in <code>gigaspatial/handlers/healthsites.py</code> <pre><code>@dataclass(config=ConfigDict(arbitrary_types_allowed=True))\nclass HealthSitesFetcher:\n    \"\"\"\n    Fetch and process health facility location data from the Healthsites.io API.\n    \"\"\"\n\n    country: Optional[str] = Field(default=None, description=\"Country to filter\")\n    api_url: str = Field(\n        default=\"https://healthsites.io/api/v3/facilities/\",\n        description=\"Base URL for the Healthsites API\",\n    )\n    api_key: str = config.HEALTHSITES_API_KEY\n    extent: Optional[Tuple[float, float, float, float]] = Field(\n        default=None, description=\"Bounding box as (minLng, minLat, maxLng, maxLat)\"\n    )\n    page_size: int = Field(default=100, description=\"Number of records per API page\")\n    flat_properties: bool = Field(\n        default=True, description=\"Show properties in flat format\"\n    )\n    tag_format: str = Field(default=\"osm\", description=\"Tag format (osm/hxl)\")\n    output_format: str = Field(\n        default=\"geojson\", description=\"Output format (json/geojson)\"\n    )\n    sleep_time: float = Field(\n        default=0.2, description=\"Sleep time between API requests\"\n    )\n\n    def __post_init__(self):\n        self.logger = config.get_logger(self.__class__.__name__)\n        # Convert country code to OSM English name if provided\n        if self.country:\n            self.country = self._convert_country(self.country)\n\n    def fetch_facilities(self, **kwargs) -&gt; Union[pd.DataFrame, gpd.GeoDataFrame]:\n        \"\"\"\n        Fetch and process health facility locations.\n\n        Args:\n            **kwargs: Additional parameters for customization\n                - country: Override country filter\n                - extent: Override extent filter\n                - from_date: Get data modified from this timestamp (datetime or string)\n                - to_date: Get data modified to this timestamp (datetime or string)\n                - page_size: Override default page size\n                - sleep_time: Override default sleep time between requests\n                - max_pages: Limit the number of pages to fetch\n                - output_format: Override output format ('json' or 'geojson')\n                - flat_properties: Override flat properties setting\n\n        Returns:\n            Union[pd.DataFrame, gpd.GeoDataFrame]: Health facilities data.\n                Returns GeoDataFrame for geojson format, DataFrame for json format.\n        \"\"\"\n        # Override defaults with kwargs if provided\n        country = kwargs.get(\"country\", self.country)\n        extent = kwargs.get(\"extent\", self.extent)\n        from_date = kwargs.get(\"from_date\", None)\n        to_date = kwargs.get(\"to_date\", None)\n        page_size = kwargs.get(\"page_size\", self.page_size)\n        sleep_time = kwargs.get(\"sleep_time\", self.sleep_time)\n        max_pages = kwargs.get(\"max_pages\", None)\n        output_format = kwargs.get(\"output_format\", self.output_format)\n        flat_properties = kwargs.get(\"flat_properties\", self.flat_properties)\n\n        # Convert country if provided in kwargs\n        if country:\n            country = self._convert_country(country)\n\n        # Prepare base parameters\n        base_params = {\n            \"api-key\": self.api_key,\n            \"tag-format\": self.tag_format,\n            \"output\": output_format,\n        }\n\n        # Only add flat-properties if True (don't send it as false, as that makes it flat anyway)\n        if flat_properties:\n            base_params[\"flat-properties\"] = \"true\"\n\n        # Add optional filters\n        if country:\n            base_params[\"country\"] = country\n\n        if extent:\n            if len(extent) != 4:\n                raise ValueError(\n                    \"Extent must be a tuple of 4 values: (minLng, minLat, maxLng, maxLat)\"\n                )\n            base_params[\"extent\"] = \",\".join(map(str, extent))\n\n        if from_date:\n            base_params[\"from\"] = self._format_timestamp(from_date)\n\n        if to_date:\n            base_params[\"to\"] = self._format_timestamp(to_date)\n\n        all_data = []\n        page = 1\n\n        self.logger.info(\n            f\"Starting to fetch health facilities for country: {country or 'all countries'}\"\n        )\n        self.logger.info(\n            f\"Output format: {output_format}, Flat properties: {flat_properties}\"\n        )\n\n        while True:\n            # Check if we've reached max_pages limit\n            if max_pages and page &gt; max_pages:\n                self.logger.info(f\"Reached maximum pages limit: {max_pages}\")\n                break\n\n            # Add page parameter\n            params = base_params.copy()\n            params[\"page\"] = page\n\n            try:\n                self.logger.debug(f\"Fetching page {page} with params: {params}\")\n                response = requests.get(self.api_url, params=params)\n                response.raise_for_status()\n\n                parsed = response.json()\n\n                # Handle different response structures based on output format\n                if output_format == \"geojson\":\n                    # GeoJSON returns FeatureCollection with features list\n                    data = parsed.get(\"features\", [])\n                else:\n                    # JSON returns direct list\n                    data = parsed if isinstance(parsed, list) else []\n\n            except requests.exceptions.RequestException as e:\n                self.logger.error(f\"Request failed on page {page}: {e}\")\n                break\n            except ValueError as e:\n                self.logger.error(f\"Failed to parse JSON response on page {page}: {e}\")\n                break\n\n            # Check if we got any data\n            if not data or not isinstance(data, list):\n                self.logger.info(f\"No data on page {page}. Stopping.\")\n                break\n\n            all_data.extend(data)\n            self.logger.info(f\"Fetched page {page} with {len(data)} records\")\n\n            # If we got fewer records than page_size, we've reached the end\n            if len(data) &lt; page_size:\n                self.logger.info(\"Reached end of data (partial page received)\")\n                break\n\n            page += 1\n\n            # Sleep to be respectful to the API\n            if sleep_time &gt; 0:\n                time.sleep(sleep_time)\n\n        self.logger.info(f\"Finished fetching. Total records: {len(all_data)}\")\n\n        # Convert to DataFrame/GeoDataFrame based on format\n        if not all_data:\n            self.logger.warning(\"No data fetched, returning empty DataFrame\")\n            if output_format == \"geojson\":\n                return gpd.GeoDataFrame()\n            return pd.DataFrame()\n\n        if output_format == \"geojson\":\n            # Use GeoDataFrame.from_features for GeoJSON format\n            gdf = gpd.GeoDataFrame.from_features(all_data, crs=\"EPSG:4326\")\n            self.logger.info(f\"Created GeoDataFrame with {len(gdf)} records\")\n            return gdf\n        else:\n            # For JSON format, handle nested structure if flat_properties is False\n            if not flat_properties:\n                df = self._process_json_with_centroid(all_data)\n            else:\n                df = pd.DataFrame(all_data)\n\n            self.logger.info(f\"Created DataFrame with {len(df)} records\")\n            return df\n\n    def fetch_statistics(self, **kwargs) -&gt; dict:\n        \"\"\"\n        Fetch statistics for health facilities.\n\n        Args:\n            **kwargs: Same filtering parameters as fetch_facilities\n\n        Returns:\n            dict: Statistics data\n        \"\"\"\n        country = kwargs.get(\"country\", self.country)\n        extent = kwargs.get(\"extent\", self.extent)\n        from_date = kwargs.get(\"from_date\", None)\n        to_date = kwargs.get(\"to_date\", None)\n\n        # Convert country if provided\n        if country:\n            country = self._convert_country(country)\n\n        params = {\n            \"api-key\": self.api_key,\n        }\n\n        # Add optional filters\n        if country:\n            params[\"country\"] = country\n        if extent:\n            params[\"extent\"] = \",\".join(map(str, extent))\n        if from_date:\n            params[\"from\"] = self._format_timestamp(from_date)\n        if to_date:\n            params[\"to\"] = self._format_timestamp(to_date)\n\n        try:\n            response = requests.get(f\"{self.api_url}/statistic/\", params=params)\n            response.raise_for_status()\n            return response.json()\n        except requests.exceptions.RequestException as e:\n            self.logger.error(f\"Request failed for statistics: {e}\")\n            raise\n\n    def fetch_facility_by_id(self, osm_type: str, osm_id: str) -&gt; dict:\n        \"\"\"\n        Fetch a specific facility by OSM type and ID.\n\n        Args:\n            osm_type: OSM type (node, way, relation)\n            osm_id: OSM ID\n\n        Returns:\n            dict: Facility details\n        \"\"\"\n        params = {\"api-key\": self.api_key}\n\n        try:\n            url = f\"{self.api_url}/{osm_type}/{osm_id}\"\n            response = requests.get(url, params=params)\n            response.raise_for_status()\n            return response.json()\n        except requests.exceptions.RequestException as e:\n            self.logger.error(f\"Request failed for facility {osm_type}/{osm_id}: {e}\")\n            raise\n\n    def _create_dataframe(self, data: List[dict]) -&gt; pd.DataFrame:\n        \"\"\"\n        Create DataFrame from API response data.\n\n        Args:\n            data: List of facility records\n\n        Returns:\n            pd.DataFrame: Processed DataFrame\n        \"\"\"\n        if self.output_format == \"geojson\":\n            # Handle GeoJSON format\n            records = []\n            for feature in data:\n                record = feature.get(\"properties\", {}).copy()\n                geometry = feature.get(\"geometry\", {})\n                coordinates = geometry.get(\"coordinates\", [])\n\n                if coordinates and len(coordinates) &gt;= 2:\n                    record[\"longitude\"] = coordinates[0]\n                    record[\"latitude\"] = coordinates[1]\n\n                records.append(record)\n            return pd.DataFrame(records)\n        else:\n            # Handle regular JSON format\n            return pd.DataFrame(data)\n\n    def _process_json_with_centroid(self, data: List[dict]) -&gt; pd.DataFrame:\n        \"\"\"\n        Process JSON data to flatten 'attributes' and 'centroid' fields,\n        and extract longitude/latitude from centroid.\n\n        Args:\n            data: List of facility records, where each record might contain\n                  nested 'attributes' and 'centroid' dictionaries.\n\n        Returns:\n            pd.DataFrame: Processed DataFrame with flattened data.\n        \"\"\"\n        processed_records = []\n        for record in data:\n            new_record = {}\n\n            # Flatten top-level keys\n            for key, value in record.items():\n                if key not in [\"attributes\", \"centroid\"]:\n                    new_record[key] = value\n\n            # Flatten 'attributes'\n            attributes = record.get(\"attributes\", {})\n            for attr_key, attr_value in attributes.items():\n                new_record[f\"{attr_key}\"] = attr_value\n\n            # Extract centroid coordinates\n            centroid = record.get(\"centroid\", {})\n            coordinates = centroid.get(\"coordinates\", [])\n            if coordinates and len(coordinates) == 2:\n                new_record[\"longitude\"] = coordinates[0]\n                new_record[\"latitude\"] = coordinates[1]\n            else:\n                new_record[\"longitude\"] = None\n                new_record[\"latitude\"] = None\n\n            processed_records.append(new_record)\n\n        return pd.DataFrame(processed_records)\n\n    def _convert_country(self, country: str) -&gt; str:\n        try:\n            # First convert to ISO3 format if needed\n            country_obj = pycountry.countries.lookup(country)\n            iso3_code = country_obj.alpha_3\n\n            # Get OSM English name using OSMLocationFetcher\n            osm_data = OSMLocationFetcher.get_osm_countries(iso3_code=iso3_code)\n            osm_name_en = osm_data.get(\"name:en\")\n\n            if not osm_name_en:\n                raise ValueError(\n                    f\"Could not find OSM English name for country: {country}\"\n                )\n\n            self.logger.info(\n                f\"Converted country code to OSM English name: {osm_name_en}\"\n            )\n\n            return osm_name_en\n\n        except LookupError:\n            raise ValueError(f\"Invalid country code provided: {country}\")\n        except Exception as e:\n            raise ValueError(f\"Failed to get OSM English name: {e}\")\n</code></pre>"},{"location":"api/handlers/#gigaspatial.handlers.healthsites.HealthSitesFetcher.fetch_facilities","title":"<code>fetch_facilities(**kwargs)</code>","text":"<p>Fetch and process health facility locations.</p> <p>Parameters:</p> Name Type Description Default <code>**kwargs</code> <p>Additional parameters for customization - country: Override country filter - extent: Override extent filter - from_date: Get data modified from this timestamp (datetime or string) - to_date: Get data modified to this timestamp (datetime or string) - page_size: Override default page size - sleep_time: Override default sleep time between requests - max_pages: Limit the number of pages to fetch - output_format: Override output format ('json' or 'geojson') - flat_properties: Override flat properties setting</p> <code>{}</code> <p>Returns:</p> Type Description <code>Union[DataFrame, GeoDataFrame]</code> <p>Union[pd.DataFrame, gpd.GeoDataFrame]: Health facilities data. Returns GeoDataFrame for geojson format, DataFrame for json format.</p> Source code in <code>gigaspatial/handlers/healthsites.py</code> <pre><code>def fetch_facilities(self, **kwargs) -&gt; Union[pd.DataFrame, gpd.GeoDataFrame]:\n    \"\"\"\n    Fetch and process health facility locations.\n\n    Args:\n        **kwargs: Additional parameters for customization\n            - country: Override country filter\n            - extent: Override extent filter\n            - from_date: Get data modified from this timestamp (datetime or string)\n            - to_date: Get data modified to this timestamp (datetime or string)\n            - page_size: Override default page size\n            - sleep_time: Override default sleep time between requests\n            - max_pages: Limit the number of pages to fetch\n            - output_format: Override output format ('json' or 'geojson')\n            - flat_properties: Override flat properties setting\n\n    Returns:\n        Union[pd.DataFrame, gpd.GeoDataFrame]: Health facilities data.\n            Returns GeoDataFrame for geojson format, DataFrame for json format.\n    \"\"\"\n    # Override defaults with kwargs if provided\n    country = kwargs.get(\"country\", self.country)\n    extent = kwargs.get(\"extent\", self.extent)\n    from_date = kwargs.get(\"from_date\", None)\n    to_date = kwargs.get(\"to_date\", None)\n    page_size = kwargs.get(\"page_size\", self.page_size)\n    sleep_time = kwargs.get(\"sleep_time\", self.sleep_time)\n    max_pages = kwargs.get(\"max_pages\", None)\n    output_format = kwargs.get(\"output_format\", self.output_format)\n    flat_properties = kwargs.get(\"flat_properties\", self.flat_properties)\n\n    # Convert country if provided in kwargs\n    if country:\n        country = self._convert_country(country)\n\n    # Prepare base parameters\n    base_params = {\n        \"api-key\": self.api_key,\n        \"tag-format\": self.tag_format,\n        \"output\": output_format,\n    }\n\n    # Only add flat-properties if True (don't send it as false, as that makes it flat anyway)\n    if flat_properties:\n        base_params[\"flat-properties\"] = \"true\"\n\n    # Add optional filters\n    if country:\n        base_params[\"country\"] = country\n\n    if extent:\n        if len(extent) != 4:\n            raise ValueError(\n                \"Extent must be a tuple of 4 values: (minLng, minLat, maxLng, maxLat)\"\n            )\n        base_params[\"extent\"] = \",\".join(map(str, extent))\n\n    if from_date:\n        base_params[\"from\"] = self._format_timestamp(from_date)\n\n    if to_date:\n        base_params[\"to\"] = self._format_timestamp(to_date)\n\n    all_data = []\n    page = 1\n\n    self.logger.info(\n        f\"Starting to fetch health facilities for country: {country or 'all countries'}\"\n    )\n    self.logger.info(\n        f\"Output format: {output_format}, Flat properties: {flat_properties}\"\n    )\n\n    while True:\n        # Check if we've reached max_pages limit\n        if max_pages and page &gt; max_pages:\n            self.logger.info(f\"Reached maximum pages limit: {max_pages}\")\n            break\n\n        # Add page parameter\n        params = base_params.copy()\n        params[\"page\"] = page\n\n        try:\n            self.logger.debug(f\"Fetching page {page} with params: {params}\")\n            response = requests.get(self.api_url, params=params)\n            response.raise_for_status()\n\n            parsed = response.json()\n\n            # Handle different response structures based on output format\n            if output_format == \"geojson\":\n                # GeoJSON returns FeatureCollection with features list\n                data = parsed.get(\"features\", [])\n            else:\n                # JSON returns direct list\n                data = parsed if isinstance(parsed, list) else []\n\n        except requests.exceptions.RequestException as e:\n            self.logger.error(f\"Request failed on page {page}: {e}\")\n            break\n        except ValueError as e:\n            self.logger.error(f\"Failed to parse JSON response on page {page}: {e}\")\n            break\n\n        # Check if we got any data\n        if not data or not isinstance(data, list):\n            self.logger.info(f\"No data on page {page}. Stopping.\")\n            break\n\n        all_data.extend(data)\n        self.logger.info(f\"Fetched page {page} with {len(data)} records\")\n\n        # If we got fewer records than page_size, we've reached the end\n        if len(data) &lt; page_size:\n            self.logger.info(\"Reached end of data (partial page received)\")\n            break\n\n        page += 1\n\n        # Sleep to be respectful to the API\n        if sleep_time &gt; 0:\n            time.sleep(sleep_time)\n\n    self.logger.info(f\"Finished fetching. Total records: {len(all_data)}\")\n\n    # Convert to DataFrame/GeoDataFrame based on format\n    if not all_data:\n        self.logger.warning(\"No data fetched, returning empty DataFrame\")\n        if output_format == \"geojson\":\n            return gpd.GeoDataFrame()\n        return pd.DataFrame()\n\n    if output_format == \"geojson\":\n        # Use GeoDataFrame.from_features for GeoJSON format\n        gdf = gpd.GeoDataFrame.from_features(all_data, crs=\"EPSG:4326\")\n        self.logger.info(f\"Created GeoDataFrame with {len(gdf)} records\")\n        return gdf\n    else:\n        # For JSON format, handle nested structure if flat_properties is False\n        if not flat_properties:\n            df = self._process_json_with_centroid(all_data)\n        else:\n            df = pd.DataFrame(all_data)\n\n        self.logger.info(f\"Created DataFrame with {len(df)} records\")\n        return df\n</code></pre>"},{"location":"api/handlers/#gigaspatial.handlers.healthsites.HealthSitesFetcher.fetch_facility_by_id","title":"<code>fetch_facility_by_id(osm_type, osm_id)</code>","text":"<p>Fetch a specific facility by OSM type and ID.</p> <p>Parameters:</p> Name Type Description Default <code>osm_type</code> <code>str</code> <p>OSM type (node, way, relation)</p> required <code>osm_id</code> <code>str</code> <p>OSM ID</p> required <p>Returns:</p> Name Type Description <code>dict</code> <code>dict</code> <p>Facility details</p> Source code in <code>gigaspatial/handlers/healthsites.py</code> <pre><code>def fetch_facility_by_id(self, osm_type: str, osm_id: str) -&gt; dict:\n    \"\"\"\n    Fetch a specific facility by OSM type and ID.\n\n    Args:\n        osm_type: OSM type (node, way, relation)\n        osm_id: OSM ID\n\n    Returns:\n        dict: Facility details\n    \"\"\"\n    params = {\"api-key\": self.api_key}\n\n    try:\n        url = f\"{self.api_url}/{osm_type}/{osm_id}\"\n        response = requests.get(url, params=params)\n        response.raise_for_status()\n        return response.json()\n    except requests.exceptions.RequestException as e:\n        self.logger.error(f\"Request failed for facility {osm_type}/{osm_id}: {e}\")\n        raise\n</code></pre>"},{"location":"api/handlers/#gigaspatial.handlers.healthsites.HealthSitesFetcher.fetch_statistics","title":"<code>fetch_statistics(**kwargs)</code>","text":"<p>Fetch statistics for health facilities.</p> <p>Parameters:</p> Name Type Description Default <code>**kwargs</code> <p>Same filtering parameters as fetch_facilities</p> <code>{}</code> <p>Returns:</p> Name Type Description <code>dict</code> <code>dict</code> <p>Statistics data</p> Source code in <code>gigaspatial/handlers/healthsites.py</code> <pre><code>def fetch_statistics(self, **kwargs) -&gt; dict:\n    \"\"\"\n    Fetch statistics for health facilities.\n\n    Args:\n        **kwargs: Same filtering parameters as fetch_facilities\n\n    Returns:\n        dict: Statistics data\n    \"\"\"\n    country = kwargs.get(\"country\", self.country)\n    extent = kwargs.get(\"extent\", self.extent)\n    from_date = kwargs.get(\"from_date\", None)\n    to_date = kwargs.get(\"to_date\", None)\n\n    # Convert country if provided\n    if country:\n        country = self._convert_country(country)\n\n    params = {\n        \"api-key\": self.api_key,\n    }\n\n    # Add optional filters\n    if country:\n        params[\"country\"] = country\n    if extent:\n        params[\"extent\"] = \",\".join(map(str, extent))\n    if from_date:\n        params[\"from\"] = self._format_timestamp(from_date)\n    if to_date:\n        params[\"to\"] = self._format_timestamp(to_date)\n\n    try:\n        response = requests.get(f\"{self.api_url}/statistic/\", params=params)\n        response.raise_for_status()\n        return response.json()\n    except requests.exceptions.RequestException as e:\n        self.logger.error(f\"Request failed for statistics: {e}\")\n        raise\n</code></pre>"},{"location":"api/handlers/#gigaspatial.handlers.mapbox_image","title":"<code>mapbox_image</code>","text":""},{"location":"api/handlers/#gigaspatial.handlers.mapbox_image.MapboxImageDownloader","title":"<code>MapboxImageDownloader</code>","text":"<p>Class to download images from Mapbox Static Images API using a specific style</p> Source code in <code>gigaspatial/handlers/mapbox_image.py</code> <pre><code>class MapboxImageDownloader:\n    \"\"\"Class to download images from Mapbox Static Images API using a specific style\"\"\"\n\n    BASE_URL = \"https://api.mapbox.com/styles/v1\"\n\n    def __init__(\n        self,\n        access_token: str = config.MAPBOX_ACCESS_TOKEN,\n        style_id: Optional[str] = None,\n        data_store: Optional[DataStore] = None,\n    ):\n        \"\"\"\n        Initialize the downloader with Mapbox credentials\n\n        Args:\n            access_token: Mapbox access token\n            style_id: Mapbox style ID to use for image download\n            data_store: Instance of DataStore for accessing data storage\n        \"\"\"\n        self.access_token = access_token\n        self.style_id = style_id if style_id else \"mapbox/satellite-v9\"\n        self.data_store = data_store or LocalDataStore()\n        self.logger = config.get_logger(self.__class__.__name__)\n\n    def _construct_url(self, bounds: Iterable[float], image_size: str) -&gt; str:\n        \"\"\"Construct the Mapbox Static Images API URL\"\"\"\n        bounds_str = f\"[{','.join(map(str, bounds))}]\"\n\n        return (\n            f\"{self.BASE_URL}/{self.style_id}/static/{bounds_str}/{image_size}\"\n            f\"?access_token={self.access_token}&amp;attribution=false&amp;logo=false\"\n        )\n\n    def _download_single_image(self, url: str, output_path: Path) -&gt; bool:\n        \"\"\"Download a single image from URL\"\"\"\n        try:\n            response = requests.get(url)\n            response.raise_for_status()\n\n            with self.data_store.open(str(output_path), \"wb\") as f:\n                f.write(response.content)\n            return True\n        except Exception as e:\n            self.logger.warning(f\"Error downloading {output_path.name}: {str(e)}\")\n            return False\n\n    def download_images_by_tiles(\n        self,\n        mercator_tiles: \"MercatorTiles\",\n        output_dir: Union[str, Path],\n        image_size: Tuple[int, int] = (512, 512),\n        max_workers: int = 4,\n        image_prefix: str = \"image_\",\n    ) -&gt; None:\n        \"\"\"\n        Download images for given mercator tiles using the specified style\n\n        Args:\n            mercator_tiles: MercatorTiles instance containing quadkeys\n            output_dir: Directory to save images\n            image_size: Tuple of (width, height) for output images\n            max_workers: Maximum number of concurrent downloads\n            image_prefix: Prefix for output image names\n        \"\"\"\n        output_dir = Path(output_dir)\n        # self.data_store.makedirs(str(output_dir), exist_ok=True)\n\n        image_size_str = f\"{image_size[0]}x{image_size[1]}\"\n        total_tiles = len(mercator_tiles.quadkeys)\n\n        self.logger.info(\n            f\"Downloading {total_tiles} tiles with size {image_size_str}...\"\n        )\n\n        def _get_tile_bounds(quadkey: str) -&gt; List[float]:\n            \"\"\"Get tile bounds from quadkey\"\"\"\n            tile = mercantile.quadkey_to_tile(quadkey)\n            bounds = mercantile.bounds(tile)\n            return [bounds.west, bounds.south, bounds.east, bounds.north]\n\n        def download_image(quadkey: str) -&gt; bool:\n            bounds = _get_tile_bounds(quadkey)\n            file_name = f\"{image_prefix}{quadkey}.png\"\n\n            url = self._construct_url(bounds, image_size_str)\n            success = self._download_single_image(url, output_dir / file_name)\n\n            return success\n\n        with ThreadPoolExecutor(max_workers=max_workers) as executor:\n            futures = [\n                executor.submit(download_image, quadkey)\n                for quadkey in mercator_tiles.quadkeys\n            ]\n\n            successful_downloads = 0\n            with tqdm(total=total_tiles) as pbar:\n                for future in as_completed(futures):\n                    if future.result():\n                        successful_downloads += 1\n                    pbar.update(1)\n\n        self.logger.info(\n            f\"Successfully downloaded {successful_downloads}/{total_tiles} images!\"\n        )\n\n    def download_images_by_bounds(\n        self,\n        gdf: gpd.GeoDataFrame,\n        output_dir: Union[str, Path],\n        image_size: Tuple[int, int] = (512, 512),\n        max_workers: int = 4,\n        image_prefix: str = \"image_\",\n    ) -&gt; None:\n        \"\"\"\n        Download images for given points using the specified style\n\n        Args:\n            gdf_points: GeoDataFrame containing bounding box polygons\n            output_dir: Directory to save images\n            image_size: Tuple of (width, height) for output images\n            max_workers: Maximum number of concurrent downloads\n            image_prefix: Prefix for output image names\n        \"\"\"\n        output_dir = Path(output_dir)\n        # self.data_store.makedirs(str(output_dir), exist_ok=True)\n\n        image_size_str = f\"{image_size[0]}x{image_size[1]}\"\n        total_images = len(gdf)\n\n        self.logger.info(\n            f\"Downloading {total_images} images with size {image_size_str}...\"\n        )\n\n        def download_image(idx: Any, bounds: Tuple[float, float, float, float]) -&gt; bool:\n            file_name = f\"{image_prefix}{idx}.png\"\n            url = self._construct_url(bounds, image_size_str)\n            success = self._download_single_image(url, output_dir / file_name)\n            return success\n\n        with ThreadPoolExecutor(max_workers=max_workers) as executor:\n            futures = [\n                executor.submit(download_image, row.Index, row.geometry.bounds)\n                for row in gdf.itertuples()\n            ]\n\n            successful_downloads = 0\n            with tqdm(total=total_images) as pbar:\n                for future in as_completed(futures):\n                    if future.result():\n                        successful_downloads += 1\n                    pbar.update(1)\n\n        self.logger.info(\n            f\"Successfully downloaded {successful_downloads}/{total_images} images!\"\n        )\n\n    def download_images_by_coordinates(\n        self,\n        data: Union[pd.DataFrame, List[Tuple[float, float]]],\n        res_meters_pixel: float,\n        output_dir: Union[str, Path],\n        image_size: Tuple[int, int] = (512, 512),\n        max_workers: int = 4,\n        image_prefix: str = \"image_\",\n    ) -&gt; None:\n        \"\"\"\n        Download images for given coordinates by creating bounded boxes around points\n\n        Args:\n            data: Either a DataFrame with either latitude/longitude columns or a geometry column or a list of (lat, lon) tuples\n            res_meters_pixel: Size of the bounding box in meters (creates a square)\n            output_dir: Directory to save images\n            image_size: Tuple of (width, height) for output images\n            max_workers: Maximum number of concurrent downloads\n            image_prefix: Prefix for output image names\n        \"\"\"\n\n        if isinstance(data, pd.DataFrame):\n            coordinates_df = data\n        else:\n            coordinates_df = pd.DataFrame(data, columns=[\"latitude\", \"longitude\"])\n\n        gdf = convert_to_geodataframe(coordinates_df)\n\n        buffered_gdf = buffer_geodataframe(\n            gdf, res_meters_pixel / 2, cap_style=\"square\"\n        )\n\n        self.download_images_by_bounds(\n            buffered_gdf, output_dir, image_size, max_workers, image_prefix\n        )\n</code></pre>"},{"location":"api/handlers/#gigaspatial.handlers.mapbox_image.MapboxImageDownloader.__init__","title":"<code>__init__(access_token=config.MAPBOX_ACCESS_TOKEN, style_id=None, data_store=None)</code>","text":"<p>Initialize the downloader with Mapbox credentials</p> <p>Parameters:</p> Name Type Description Default <code>access_token</code> <code>str</code> <p>Mapbox access token</p> <code>MAPBOX_ACCESS_TOKEN</code> <code>style_id</code> <code>Optional[str]</code> <p>Mapbox style ID to use for image download</p> <code>None</code> <code>data_store</code> <code>Optional[DataStore]</code> <p>Instance of DataStore for accessing data storage</p> <code>None</code> Source code in <code>gigaspatial/handlers/mapbox_image.py</code> <pre><code>def __init__(\n    self,\n    access_token: str = config.MAPBOX_ACCESS_TOKEN,\n    style_id: Optional[str] = None,\n    data_store: Optional[DataStore] = None,\n):\n    \"\"\"\n    Initialize the downloader with Mapbox credentials\n\n    Args:\n        access_token: Mapbox access token\n        style_id: Mapbox style ID to use for image download\n        data_store: Instance of DataStore for accessing data storage\n    \"\"\"\n    self.access_token = access_token\n    self.style_id = style_id if style_id else \"mapbox/satellite-v9\"\n    self.data_store = data_store or LocalDataStore()\n    self.logger = config.get_logger(self.__class__.__name__)\n</code></pre>"},{"location":"api/handlers/#gigaspatial.handlers.mapbox_image.MapboxImageDownloader.download_images_by_bounds","title":"<code>download_images_by_bounds(gdf, output_dir, image_size=(512, 512), max_workers=4, image_prefix='image_')</code>","text":"<p>Download images for given points using the specified style</p> <p>Parameters:</p> Name Type Description Default <code>gdf_points</code> <p>GeoDataFrame containing bounding box polygons</p> required <code>output_dir</code> <code>Union[str, Path]</code> <p>Directory to save images</p> required <code>image_size</code> <code>Tuple[int, int]</code> <p>Tuple of (width, height) for output images</p> <code>(512, 512)</code> <code>max_workers</code> <code>int</code> <p>Maximum number of concurrent downloads</p> <code>4</code> <code>image_prefix</code> <code>str</code> <p>Prefix for output image names</p> <code>'image_'</code> Source code in <code>gigaspatial/handlers/mapbox_image.py</code> <pre><code>def download_images_by_bounds(\n    self,\n    gdf: gpd.GeoDataFrame,\n    output_dir: Union[str, Path],\n    image_size: Tuple[int, int] = (512, 512),\n    max_workers: int = 4,\n    image_prefix: str = \"image_\",\n) -&gt; None:\n    \"\"\"\n    Download images for given points using the specified style\n\n    Args:\n        gdf_points: GeoDataFrame containing bounding box polygons\n        output_dir: Directory to save images\n        image_size: Tuple of (width, height) for output images\n        max_workers: Maximum number of concurrent downloads\n        image_prefix: Prefix for output image names\n    \"\"\"\n    output_dir = Path(output_dir)\n    # self.data_store.makedirs(str(output_dir), exist_ok=True)\n\n    image_size_str = f\"{image_size[0]}x{image_size[1]}\"\n    total_images = len(gdf)\n\n    self.logger.info(\n        f\"Downloading {total_images} images with size {image_size_str}...\"\n    )\n\n    def download_image(idx: Any, bounds: Tuple[float, float, float, float]) -&gt; bool:\n        file_name = f\"{image_prefix}{idx}.png\"\n        url = self._construct_url(bounds, image_size_str)\n        success = self._download_single_image(url, output_dir / file_name)\n        return success\n\n    with ThreadPoolExecutor(max_workers=max_workers) as executor:\n        futures = [\n            executor.submit(download_image, row.Index, row.geometry.bounds)\n            for row in gdf.itertuples()\n        ]\n\n        successful_downloads = 0\n        with tqdm(total=total_images) as pbar:\n            for future in as_completed(futures):\n                if future.result():\n                    successful_downloads += 1\n                pbar.update(1)\n\n    self.logger.info(\n        f\"Successfully downloaded {successful_downloads}/{total_images} images!\"\n    )\n</code></pre>"},{"location":"api/handlers/#gigaspatial.handlers.mapbox_image.MapboxImageDownloader.download_images_by_coordinates","title":"<code>download_images_by_coordinates(data, res_meters_pixel, output_dir, image_size=(512, 512), max_workers=4, image_prefix='image_')</code>","text":"<p>Download images for given coordinates by creating bounded boxes around points</p> <p>Parameters:</p> Name Type Description Default <code>data</code> <code>Union[DataFrame, List[Tuple[float, float]]]</code> <p>Either a DataFrame with either latitude/longitude columns or a geometry column or a list of (lat, lon) tuples</p> required <code>res_meters_pixel</code> <code>float</code> <p>Size of the bounding box in meters (creates a square)</p> required <code>output_dir</code> <code>Union[str, Path]</code> <p>Directory to save images</p> required <code>image_size</code> <code>Tuple[int, int]</code> <p>Tuple of (width, height) for output images</p> <code>(512, 512)</code> <code>max_workers</code> <code>int</code> <p>Maximum number of concurrent downloads</p> <code>4</code> <code>image_prefix</code> <code>str</code> <p>Prefix for output image names</p> <code>'image_'</code> Source code in <code>gigaspatial/handlers/mapbox_image.py</code> <pre><code>def download_images_by_coordinates(\n    self,\n    data: Union[pd.DataFrame, List[Tuple[float, float]]],\n    res_meters_pixel: float,\n    output_dir: Union[str, Path],\n    image_size: Tuple[int, int] = (512, 512),\n    max_workers: int = 4,\n    image_prefix: str = \"image_\",\n) -&gt; None:\n    \"\"\"\n    Download images for given coordinates by creating bounded boxes around points\n\n    Args:\n        data: Either a DataFrame with either latitude/longitude columns or a geometry column or a list of (lat, lon) tuples\n        res_meters_pixel: Size of the bounding box in meters (creates a square)\n        output_dir: Directory to save images\n        image_size: Tuple of (width, height) for output images\n        max_workers: Maximum number of concurrent downloads\n        image_prefix: Prefix for output image names\n    \"\"\"\n\n    if isinstance(data, pd.DataFrame):\n        coordinates_df = data\n    else:\n        coordinates_df = pd.DataFrame(data, columns=[\"latitude\", \"longitude\"])\n\n    gdf = convert_to_geodataframe(coordinates_df)\n\n    buffered_gdf = buffer_geodataframe(\n        gdf, res_meters_pixel / 2, cap_style=\"square\"\n    )\n\n    self.download_images_by_bounds(\n        buffered_gdf, output_dir, image_size, max_workers, image_prefix\n    )\n</code></pre>"},{"location":"api/handlers/#gigaspatial.handlers.mapbox_image.MapboxImageDownloader.download_images_by_tiles","title":"<code>download_images_by_tiles(mercator_tiles, output_dir, image_size=(512, 512), max_workers=4, image_prefix='image_')</code>","text":"<p>Download images for given mercator tiles using the specified style</p> <p>Parameters:</p> Name Type Description Default <code>mercator_tiles</code> <code>MercatorTiles</code> <p>MercatorTiles instance containing quadkeys</p> required <code>output_dir</code> <code>Union[str, Path]</code> <p>Directory to save images</p> required <code>image_size</code> <code>Tuple[int, int]</code> <p>Tuple of (width, height) for output images</p> <code>(512, 512)</code> <code>max_workers</code> <code>int</code> <p>Maximum number of concurrent downloads</p> <code>4</code> <code>image_prefix</code> <code>str</code> <p>Prefix for output image names</p> <code>'image_'</code> Source code in <code>gigaspatial/handlers/mapbox_image.py</code> <pre><code>def download_images_by_tiles(\n    self,\n    mercator_tiles: \"MercatorTiles\",\n    output_dir: Union[str, Path],\n    image_size: Tuple[int, int] = (512, 512),\n    max_workers: int = 4,\n    image_prefix: str = \"image_\",\n) -&gt; None:\n    \"\"\"\n    Download images for given mercator tiles using the specified style\n\n    Args:\n        mercator_tiles: MercatorTiles instance containing quadkeys\n        output_dir: Directory to save images\n        image_size: Tuple of (width, height) for output images\n        max_workers: Maximum number of concurrent downloads\n        image_prefix: Prefix for output image names\n    \"\"\"\n    output_dir = Path(output_dir)\n    # self.data_store.makedirs(str(output_dir), exist_ok=True)\n\n    image_size_str = f\"{image_size[0]}x{image_size[1]}\"\n    total_tiles = len(mercator_tiles.quadkeys)\n\n    self.logger.info(\n        f\"Downloading {total_tiles} tiles with size {image_size_str}...\"\n    )\n\n    def _get_tile_bounds(quadkey: str) -&gt; List[float]:\n        \"\"\"Get tile bounds from quadkey\"\"\"\n        tile = mercantile.quadkey_to_tile(quadkey)\n        bounds = mercantile.bounds(tile)\n        return [bounds.west, bounds.south, bounds.east, bounds.north]\n\n    def download_image(quadkey: str) -&gt; bool:\n        bounds = _get_tile_bounds(quadkey)\n        file_name = f\"{image_prefix}{quadkey}.png\"\n\n        url = self._construct_url(bounds, image_size_str)\n        success = self._download_single_image(url, output_dir / file_name)\n\n        return success\n\n    with ThreadPoolExecutor(max_workers=max_workers) as executor:\n        futures = [\n            executor.submit(download_image, quadkey)\n            for quadkey in mercator_tiles.quadkeys\n        ]\n\n        successful_downloads = 0\n        with tqdm(total=total_tiles) as pbar:\n            for future in as_completed(futures):\n                if future.result():\n                    successful_downloads += 1\n                pbar.update(1)\n\n    self.logger.info(\n        f\"Successfully downloaded {successful_downloads}/{total_tiles} images!\"\n    )\n</code></pre>"},{"location":"api/handlers/#gigaspatial.handlers.maxar_image","title":"<code>maxar_image</code>","text":""},{"location":"api/handlers/#gigaspatial.handlers.maxar_image.MaxarConfig","title":"<code>MaxarConfig</code>","text":"<p>               Bases: <code>BaseModel</code></p> <p>Configuration for Maxar Image Downloader using Pydantic</p> Source code in <code>gigaspatial/handlers/maxar_image.py</code> <pre><code>class MaxarConfig(BaseModel):\n    \"\"\"Configuration for Maxar Image Downloader using Pydantic\"\"\"\n\n    username: str = Field(\n        default=global_config.MAXAR_USERNAME, description=\"Maxar API username\"\n    )\n    password: str = Field(\n        default=global_config.MAXAR_PASSWORD, description=\"Maxar API password\"\n    )\n    connection_string: str = Field(\n        default=global_config.MAXAR_CONNECTION_STRING,\n        description=\"Maxar WMS connection string\",\n    )\n\n    base_url: HttpUrl = Field(\n        default=\"https://evwhs.digitalglobe.com/mapservice/wmsaccess?\",\n        description=\"Base URL for Maxar WMS service\",\n    )\n\n    layers: List[Literal[\"DigitalGlobe:ImageryFootprint\", \"DigitalGlobe:Imagery\"]] = (\n        Field(\n            default=[\"DigitalGlobe:Imagery\"],\n            description=\"List of layers to request from WMS\",\n        )\n    )\n\n    feature_profile: str = Field(\n        default=\"Most_Aesthetic_Mosaic_Profile\",\n        description=\"Feature profile to use for WMS requests\",\n    )\n\n    coverage_cql_filter: str = Field(\n        default=\"\", description=\"CQL filter for coverage selection\"\n    )\n\n    exceptions: str = Field(\n        default=\"application/vnd.ogc.se_xml\",\n        description=\"Exception handling format for WMS\",\n    )\n\n    transparent: bool = Field(\n        default=True,\n        description=\"Whether the requested images should have transparency\",\n    )\n\n    image_format: Literal[\"image/png\", \"image/jpeg\", \"image/geotiff\"] = Field(\n        default=\"image/png\",\n    )\n\n    data_crs: Literal[\"EPSG:4326\", \"EPSG:3395\", \"EPSG:3857\", \"CAR:42004\"] = Field(\n        default=\"EPSG:4326\"\n    )\n\n    max_retries: int = Field(\n        default=3, description=\"Number of retries for failed image downloads\"\n    )\n\n    retry_delay: int = Field(default=5, description=\"Delay in seconds between retries\")\n\n    @field_validator(\"username\", \"password\", \"connection_string\")\n    @classmethod\n    def validate_non_empty(cls, value: str, field) -&gt; str:\n        \"\"\"Ensure required credentials are provided\"\"\"\n        if not value or value.strip() == \"\":\n            raise ValueError(\n                f\"{field.name} cannot be empty. Please provide a valid {field.name}.\"\n            )\n        return value\n\n    @property\n    def wms_url(self) -&gt; str:\n        \"\"\"Generate the full WMS URL with connection string\"\"\"\n        return f\"{self.base_url}connectid={self.connection_string}\"\n\n    @property\n    def suffix(self) -&gt; str:\n        return f\".{self.image_format.split('/')[1]}\"\n</code></pre>"},{"location":"api/handlers/#gigaspatial.handlers.maxar_image.MaxarConfig.wms_url","title":"<code>wms_url: str</code>  <code>property</code>","text":"<p>Generate the full WMS URL with connection string</p>"},{"location":"api/handlers/#gigaspatial.handlers.maxar_image.MaxarConfig.validate_non_empty","title":"<code>validate_non_empty(value, field)</code>  <code>classmethod</code>","text":"<p>Ensure required credentials are provided</p> Source code in <code>gigaspatial/handlers/maxar_image.py</code> <pre><code>@field_validator(\"username\", \"password\", \"connection_string\")\n@classmethod\ndef validate_non_empty(cls, value: str, field) -&gt; str:\n    \"\"\"Ensure required credentials are provided\"\"\"\n    if not value or value.strip() == \"\":\n        raise ValueError(\n            f\"{field.name} cannot be empty. Please provide a valid {field.name}.\"\n        )\n    return value\n</code></pre>"},{"location":"api/handlers/#gigaspatial.handlers.maxar_image.MaxarImageDownloader","title":"<code>MaxarImageDownloader</code>","text":"<p>Class to download images from Maxar</p> Source code in <code>gigaspatial/handlers/maxar_image.py</code> <pre><code>class MaxarImageDownloader:\n    \"\"\"Class to download images from Maxar\"\"\"\n\n    def __init__(\n        self,\n        config: Optional[MaxarConfig] = None,\n        data_store: Optional[DataStore] = None,\n    ):\n        \"\"\"\n        Initialize the downloader with Maxar config.\n\n        Args:\n            config: MaxarConfig instance containing credentials and settings\n            data_store: Instance of DataStore for accessing data storage\n        \"\"\"\n        self.config = config or MaxarConfig()\n        self.wms = WebMapService(\n            self.config.wms_url,\n            username=self.config.username,\n            password=self.config.password,\n        )\n        self.data_store = data_store or LocalDataStore()\n        self.logger = global_config.get_logger(self.__class__.__name__)\n\n    def _download_single_image(self, bbox, output_path: Union[Path, str], size) -&gt; bool:\n        \"\"\"Download a single image from bbox and pixel size\"\"\"\n        for attempt in range(self.config.max_retries):\n            try:\n                img_data = self.wms.getmap(\n                    bbox=bbox,\n                    layers=self.config.layers,\n                    srs=self.config.data_crs,\n                    size=size,\n                    featureProfile=self.config.feature_profile,\n                    coverage_cql_filter=self.config.coverage_cql_filter,\n                    exceptions=self.config.exceptions,\n                    transparent=self.config.transparent,\n                    format=self.config.image_format,\n                )\n                self.data_store.write_file(str(output_path), img_data.read())\n                return True\n            except Exception as e:\n                self.logger.warning(\n                    f\"Attempt {attempt + 1} of downloading {output_path.name} failed: {str(e)}\"\n                )\n                if attempt &lt; self.config.max_retries - 1:\n                    sleep(self.config.retry_delay)\n                else:\n                    self.logger.warning(\n                        f\"Failed to download {output_path.name} after {self.config.max_retries} attemps: {str(e)}\"\n                    )\n                    return False\n\n    def download_images_by_tiles(\n        self,\n        mercator_tiles: \"MercatorTiles\",\n        output_dir: Union[str, Path],\n        image_size: Tuple[int, int] = (512, 512),\n        image_prefix: str = \"maxar_image_\",\n    ) -&gt; None:\n        \"\"\"\n        Download images for given mercator tiles using the specified style\n\n        Args:\n            mercator_tiles: MercatorTiles instance containing quadkeys\n            output_dir: Directory to save images\n            image_size: Tuple of (width, height) for output images\n            image_prefix: Prefix for output image names\n        \"\"\"\n        output_dir = Path(output_dir)\n\n        image_size_str = f\"{image_size[0]}x{image_size[1]}\"\n        total_tiles = len(mercator_tiles.quadkeys)\n\n        self.logger.info(\n            f\"Downloading {total_tiles} tiles with size {image_size_str}...\"\n        )\n\n        def _get_tile_bounds(quadkey: str) -&gt; Tuple[float]:\n            \"\"\"Get tile bounds from quadkey\"\"\"\n            tile = mercantile.quadkey_to_tile(quadkey)\n            bounds = mercantile.bounds(tile)\n            return (bounds.west, bounds.south, bounds.east, bounds.north)\n\n        def download_image(\n            quadkey: str, image_size: Tuple[int, int], suffix: str = self.config.suffix\n        ) -&gt; bool:\n            bounds = _get_tile_bounds(quadkey)\n            file_name = f\"{image_prefix}{quadkey}{suffix}\"\n\n            success = self._download_single_image(\n                bounds, output_dir / file_name, image_size\n            )\n\n            return success\n\n        successful_downloads = 0\n        with tqdm(total=total_tiles) as pbar:\n            for quadkey in mercator_tiles.quadkeys:\n                if download_image(quadkey, image_size):\n                    successful_downloads += 1\n                pbar.update(1)\n\n        self.logger.info(\n            f\"Successfully downloaded {successful_downloads}/{total_tiles} images!\"\n        )\n\n    def download_images_by_bounds(\n        self,\n        gdf: gpd.GeoDataFrame,\n        output_dir: Union[str, Path],\n        image_size: Tuple[int, int] = (512, 512),\n        image_prefix: str = \"maxar_image_\",\n    ) -&gt; None:\n        \"\"\"\n        Download images for given points using the specified style\n\n        Args:\n            gdf_points: GeoDataFrame containing bounding box polygons\n            output_dir: Directory to save images\n            image_size: Tuple of (width, height) for output images\n            image_prefix: Prefix for output image names\n        \"\"\"\n        output_dir = Path(output_dir)\n\n        image_size_str = f\"{image_size[0]}x{image_size[1]}\"\n        total_images = len(gdf)\n\n        self.logger.info(\n            f\"Downloading {total_images} images with size {image_size_str}...\"\n        )\n\n        def download_image(\n            idx: Any,\n            bounds: Tuple[float, float, float, float],\n            image_size,\n            suffix: str = self.config.suffix,\n        ) -&gt; bool:\n            file_name = f\"{image_prefix}{idx}{suffix}\"\n            success = self._download_single_image(\n                bounds, output_dir / file_name, image_size\n            )\n            return success\n\n        gdf = gdf.to_crs(self.config.data_crs)\n\n        successful_downloads = 0\n        with tqdm(total=total_images) as pbar:\n            for row in gdf.itertuples():\n                if download_image(row.Index, tuple(row.geometry.bounds), image_size):\n                    successful_downloads += 1\n                pbar.update(1)\n\n        self.logger.info(\n            f\"Successfully downloaded {successful_downloads}/{total_images} images!\"\n        )\n\n    def download_images_by_coordinates(\n        self,\n        data: Union[pd.DataFrame, List[Tuple[float, float]]],\n        res_meters_pixel: float,\n        output_dir: Union[str, Path],\n        image_size: Tuple[int, int] = (512, 512),\n        image_prefix: str = \"maxar_image_\",\n    ) -&gt; None:\n        \"\"\"\n        Download images for given coordinates by creating bounded boxes around points\n\n        Args:\n            data: Either a DataFrame with either latitude/longitude columns or a geometry column or a list of (lat, lon) tuples\n            res_meters_pixel: resolution in meters per pixel\n            output_dir: Directory to save images\n            image_size: Tuple of (width, height) for output images\n            image_prefix: Prefix for output image names\n        \"\"\"\n\n        if isinstance(data, pd.DataFrame):\n            coordinates_df = data\n        else:\n            coordinates_df = pd.DataFrame(data, columns=[\"latitude\", \"longitude\"])\n\n        gdf = convert_to_geodataframe(coordinates_df)\n\n        buffered_gdf = buffer_geodataframe(\n            gdf, res_meters_pixel / 2, cap_style=\"square\"\n        )\n\n        buffered_gdf = buffered_gdf.to_crs(self.config.data_crs)\n\n        self.download_images_by_bounds(\n            buffered_gdf, output_dir, image_size, image_prefix\n        )\n</code></pre>"},{"location":"api/handlers/#gigaspatial.handlers.maxar_image.MaxarImageDownloader.__init__","title":"<code>__init__(config=None, data_store=None)</code>","text":"<p>Initialize the downloader with Maxar config.</p> <p>Parameters:</p> Name Type Description Default <code>config</code> <code>Optional[MaxarConfig]</code> <p>MaxarConfig instance containing credentials and settings</p> <code>None</code> <code>data_store</code> <code>Optional[DataStore]</code> <p>Instance of DataStore for accessing data storage</p> <code>None</code> Source code in <code>gigaspatial/handlers/maxar_image.py</code> <pre><code>def __init__(\n    self,\n    config: Optional[MaxarConfig] = None,\n    data_store: Optional[DataStore] = None,\n):\n    \"\"\"\n    Initialize the downloader with Maxar config.\n\n    Args:\n        config: MaxarConfig instance containing credentials and settings\n        data_store: Instance of DataStore for accessing data storage\n    \"\"\"\n    self.config = config or MaxarConfig()\n    self.wms = WebMapService(\n        self.config.wms_url,\n        username=self.config.username,\n        password=self.config.password,\n    )\n    self.data_store = data_store or LocalDataStore()\n    self.logger = global_config.get_logger(self.__class__.__name__)\n</code></pre>"},{"location":"api/handlers/#gigaspatial.handlers.maxar_image.MaxarImageDownloader.download_images_by_bounds","title":"<code>download_images_by_bounds(gdf, output_dir, image_size=(512, 512), image_prefix='maxar_image_')</code>","text":"<p>Download images for given points using the specified style</p> <p>Parameters:</p> Name Type Description Default <code>gdf_points</code> <p>GeoDataFrame containing bounding box polygons</p> required <code>output_dir</code> <code>Union[str, Path]</code> <p>Directory to save images</p> required <code>image_size</code> <code>Tuple[int, int]</code> <p>Tuple of (width, height) for output images</p> <code>(512, 512)</code> <code>image_prefix</code> <code>str</code> <p>Prefix for output image names</p> <code>'maxar_image_'</code> Source code in <code>gigaspatial/handlers/maxar_image.py</code> <pre><code>def download_images_by_bounds(\n    self,\n    gdf: gpd.GeoDataFrame,\n    output_dir: Union[str, Path],\n    image_size: Tuple[int, int] = (512, 512),\n    image_prefix: str = \"maxar_image_\",\n) -&gt; None:\n    \"\"\"\n    Download images for given points using the specified style\n\n    Args:\n        gdf_points: GeoDataFrame containing bounding box polygons\n        output_dir: Directory to save images\n        image_size: Tuple of (width, height) for output images\n        image_prefix: Prefix for output image names\n    \"\"\"\n    output_dir = Path(output_dir)\n\n    image_size_str = f\"{image_size[0]}x{image_size[1]}\"\n    total_images = len(gdf)\n\n    self.logger.info(\n        f\"Downloading {total_images} images with size {image_size_str}...\"\n    )\n\n    def download_image(\n        idx: Any,\n        bounds: Tuple[float, float, float, float],\n        image_size,\n        suffix: str = self.config.suffix,\n    ) -&gt; bool:\n        file_name = f\"{image_prefix}{idx}{suffix}\"\n        success = self._download_single_image(\n            bounds, output_dir / file_name, image_size\n        )\n        return success\n\n    gdf = gdf.to_crs(self.config.data_crs)\n\n    successful_downloads = 0\n    with tqdm(total=total_images) as pbar:\n        for row in gdf.itertuples():\n            if download_image(row.Index, tuple(row.geometry.bounds), image_size):\n                successful_downloads += 1\n            pbar.update(1)\n\n    self.logger.info(\n        f\"Successfully downloaded {successful_downloads}/{total_images} images!\"\n    )\n</code></pre>"},{"location":"api/handlers/#gigaspatial.handlers.maxar_image.MaxarImageDownloader.download_images_by_coordinates","title":"<code>download_images_by_coordinates(data, res_meters_pixel, output_dir, image_size=(512, 512), image_prefix='maxar_image_')</code>","text":"<p>Download images for given coordinates by creating bounded boxes around points</p> <p>Parameters:</p> Name Type Description Default <code>data</code> <code>Union[DataFrame, List[Tuple[float, float]]]</code> <p>Either a DataFrame with either latitude/longitude columns or a geometry column or a list of (lat, lon) tuples</p> required <code>res_meters_pixel</code> <code>float</code> <p>resolution in meters per pixel</p> required <code>output_dir</code> <code>Union[str, Path]</code> <p>Directory to save images</p> required <code>image_size</code> <code>Tuple[int, int]</code> <p>Tuple of (width, height) for output images</p> <code>(512, 512)</code> <code>image_prefix</code> <code>str</code> <p>Prefix for output image names</p> <code>'maxar_image_'</code> Source code in <code>gigaspatial/handlers/maxar_image.py</code> <pre><code>def download_images_by_coordinates(\n    self,\n    data: Union[pd.DataFrame, List[Tuple[float, float]]],\n    res_meters_pixel: float,\n    output_dir: Union[str, Path],\n    image_size: Tuple[int, int] = (512, 512),\n    image_prefix: str = \"maxar_image_\",\n) -&gt; None:\n    \"\"\"\n    Download images for given coordinates by creating bounded boxes around points\n\n    Args:\n        data: Either a DataFrame with either latitude/longitude columns or a geometry column or a list of (lat, lon) tuples\n        res_meters_pixel: resolution in meters per pixel\n        output_dir: Directory to save images\n        image_size: Tuple of (width, height) for output images\n        image_prefix: Prefix for output image names\n    \"\"\"\n\n    if isinstance(data, pd.DataFrame):\n        coordinates_df = data\n    else:\n        coordinates_df = pd.DataFrame(data, columns=[\"latitude\", \"longitude\"])\n\n    gdf = convert_to_geodataframe(coordinates_df)\n\n    buffered_gdf = buffer_geodataframe(\n        gdf, res_meters_pixel / 2, cap_style=\"square\"\n    )\n\n    buffered_gdf = buffered_gdf.to_crs(self.config.data_crs)\n\n    self.download_images_by_bounds(\n        buffered_gdf, output_dir, image_size, image_prefix\n    )\n</code></pre>"},{"location":"api/handlers/#gigaspatial.handlers.maxar_image.MaxarImageDownloader.download_images_by_tiles","title":"<code>download_images_by_tiles(mercator_tiles, output_dir, image_size=(512, 512), image_prefix='maxar_image_')</code>","text":"<p>Download images for given mercator tiles using the specified style</p> <p>Parameters:</p> Name Type Description Default <code>mercator_tiles</code> <code>MercatorTiles</code> <p>MercatorTiles instance containing quadkeys</p> required <code>output_dir</code> <code>Union[str, Path]</code> <p>Directory to save images</p> required <code>image_size</code> <code>Tuple[int, int]</code> <p>Tuple of (width, height) for output images</p> <code>(512, 512)</code> <code>image_prefix</code> <code>str</code> <p>Prefix for output image names</p> <code>'maxar_image_'</code> Source code in <code>gigaspatial/handlers/maxar_image.py</code> <pre><code>def download_images_by_tiles(\n    self,\n    mercator_tiles: \"MercatorTiles\",\n    output_dir: Union[str, Path],\n    image_size: Tuple[int, int] = (512, 512),\n    image_prefix: str = \"maxar_image_\",\n) -&gt; None:\n    \"\"\"\n    Download images for given mercator tiles using the specified style\n\n    Args:\n        mercator_tiles: MercatorTiles instance containing quadkeys\n        output_dir: Directory to save images\n        image_size: Tuple of (width, height) for output images\n        image_prefix: Prefix for output image names\n    \"\"\"\n    output_dir = Path(output_dir)\n\n    image_size_str = f\"{image_size[0]}x{image_size[1]}\"\n    total_tiles = len(mercator_tiles.quadkeys)\n\n    self.logger.info(\n        f\"Downloading {total_tiles} tiles with size {image_size_str}...\"\n    )\n\n    def _get_tile_bounds(quadkey: str) -&gt; Tuple[float]:\n        \"\"\"Get tile bounds from quadkey\"\"\"\n        tile = mercantile.quadkey_to_tile(quadkey)\n        bounds = mercantile.bounds(tile)\n        return (bounds.west, bounds.south, bounds.east, bounds.north)\n\n    def download_image(\n        quadkey: str, image_size: Tuple[int, int], suffix: str = self.config.suffix\n    ) -&gt; bool:\n        bounds = _get_tile_bounds(quadkey)\n        file_name = f\"{image_prefix}{quadkey}{suffix}\"\n\n        success = self._download_single_image(\n            bounds, output_dir / file_name, image_size\n        )\n\n        return success\n\n    successful_downloads = 0\n    with tqdm(total=total_tiles) as pbar:\n        for quadkey in mercator_tiles.quadkeys:\n            if download_image(quadkey, image_size):\n                successful_downloads += 1\n            pbar.update(1)\n\n    self.logger.info(\n        f\"Successfully downloaded {successful_downloads}/{total_tiles} images!\"\n    )\n</code></pre>"},{"location":"api/handlers/#gigaspatial.handlers.microsoft_global_buildings","title":"<code>microsoft_global_buildings</code>","text":""},{"location":"api/handlers/#gigaspatial.handlers.microsoft_global_buildings.MSBuildingsConfig","title":"<code>MSBuildingsConfig</code>  <code>dataclass</code>","text":"<p>               Bases: <code>BaseHandlerConfig</code></p> <p>Configuration for Microsoft Global Buildings dataset files.</p> Source code in <code>gigaspatial/handlers/microsoft_global_buildings.py</code> <pre><code>@dataclass(config=ConfigDict(arbitrary_types_allowed=True))\nclass MSBuildingsConfig(BaseHandlerConfig):\n    \"\"\"Configuration for Microsoft Global Buildings dataset files.\"\"\"\n\n    TILE_URLS: str = (\n        \"https://minedbuildings.z5.web.core.windows.net/global-buildings/dataset-links.csv\"\n    )\n    MERCATOR_ZOOM_LEVEL: int = 9\n    base_path: Path = global_config.get_path(\"microsoft_global_buildings\", \"bronze\")\n\n    LOCATION_MAPPING_FILE: Path = base_path / \"location_mapping.json\"\n    SIMILARITY_SCORE: float = 0.8\n    DEFAULT_MAPPING: Dict[str, str] = field(\n        default_factory=lambda: {\n            \"Bonaire\": \"BES\",\n            \"Brunei\": \"BRN\",\n            \"IvoryCoast\": \"CIV\",\n            \"CongoDRC\": \"COD\",\n            \"DemocraticRepublicoftheCongo\": \"COD\",\n            \"RepublicoftheCongo\": \"COG\",\n            \"TheGambia\": \"GMB\",\n            \"FYROMakedonija\": \"MKD\",\n            \"SultanateofOman\": \"OMN\",\n            \"StateofQatar\": \"QAT\",\n            \"Russia\": \"RUS\",\n            \"KingdomofSaudiArabia\": \"SAU\",\n            \"Svalbard\": \"SJM\",\n            \"Swaziland\": \"SWZ\",\n            \"StMartin\": \"SXM\",\n            \"leSaint-Martin\": \"MAF\",\n            \"Turkey\": \"TUR\",\n            \"VaticanCity\": \"VAT\",\n            \"BritishVirginIslands\": \"VGB\",\n            \"USVirginIslands\": \"VIR\",\n            \"RepublicofYemen\": \"YEM\",\n            \"CzechRepublic\": \"CZE\",\n            \"French-Martinique\": \"MTQ\",\n            \"French-Guadeloupe\": \"GLP\",\n            \"UnitedStates\": \"USA\",\n        }\n    )\n    CUSTOM_MAPPING: Optional[Dict[str, str]] = None\n\n    def __post_init__(self):\n        \"\"\"Initialize the configuration, load tile URLs, and set up location mapping.\"\"\"\n        super().__post_init__()\n        self._load_tile_urls()\n        self.upload_date = self.df_tiles.upload_date[0]\n        self._setup_location_mapping()\n\n    def _load_tile_urls(self):\n        \"\"\"Load dataset links from csv file.\"\"\"\n        self.df_tiles = pd.read_csv(\n            self.TILE_URLS,\n            names=[\"location\", \"quadkey\", \"url\", \"size\", \"upload_date\"],\n            dtype={\"quadkey\": str},\n            header=0,\n        )\n\n    def _setup_location_mapping(self):\n        \"\"\"Load or create the mapping between dataset locations and ISO country codes.\"\"\"\n        from gigaspatial.core.io.readers import read_json\n        from gigaspatial.core.io.writers import write_json\n\n        if self.data_store.file_exists(str(self.LOCATION_MAPPING_FILE)):\n            self.location_mapping = read_json(\n                self.data_store, str(self.LOCATION_MAPPING_FILE)\n            )\n        else:\n            self.location_mapping = self.create_location_mapping(\n                similarity_score_threshold=self.SIMILARITY_SCORE\n            )\n            self.location_mapping.update(self.DEFAULT_MAPPING)\n            write_json(\n                self.location_mapping, self.data_store, str(self.LOCATION_MAPPING_FILE)\n            )\n\n        self.location_mapping.update(self.CUSTOM_MAPPING or {})\n        self._map_locations()\n        self.df_tiles.loc[self.df_tiles.country.isnull(), \"country\"] = None\n\n    def _map_locations(self):\n        \"\"\"Map the 'location' column in the tiles DataFrame to ISO country codes.\"\"\"\n        self.df_tiles[\"country\"] = self.df_tiles.location.map(self.location_mapping)\n\n    def create_location_mapping(self, similarity_score_threshold: float = 0.8):\n        \"\"\"\n        Create a mapping between the dataset's location names and ISO 3166-1 alpha-3 country codes.\n\n        This function iterates through known countries and attempts to find matching\n        locations in the dataset based on string similarity.\n\n        Args:\n            similarity_score_threshold: The minimum similarity score (between 0 and 1)\n                                        for a dataset location to be considered a match\n                                        for a country. Defaults to 0.8.\n\n        Returns:\n            A dictionary where keys are dataset location names and values are\n            the corresponding ISO 3166-1 alpha-3 country codes.\n        \"\"\"\n\n        def similar(a, b):\n            return SequenceMatcher(None, a, b).ratio()\n\n        location_mapping = dict()\n\n        for country in pycountry.countries:\n            if country.name not in self.df_tiles.location.unique():\n                try:\n                    country_quadkey = CountryMercatorTiles.create(\n                        country.alpha_3, self.MERCATOR_ZOOM_LEVEL\n                    )\n                except:\n                    self.logger.warning(f\"{country.name} is not mapped.\")\n                    continue\n                country_datasets = country_quadkey.filter_quadkeys(\n                    self.df_tiles.quadkey\n                )\n                matching_locations = self.df_tiles[\n                    self.df_tiles.quadkey.isin(country_datasets.quadkeys)\n                ].location.unique()\n                scores = np.array(\n                    [\n                        (\n                            similar(c, country.common_name)\n                            if hasattr(country, \"common_name\")\n                            else similar(c, country.name)\n                        )\n                        for c in matching_locations\n                    ]\n                )\n                if any(scores &gt; similarity_score_threshold):\n                    matched = matching_locations[scores &gt; similarity_score_threshold]\n                    if len(matched) &gt; 2:\n                        self.logger.warning(\n                            f\"Multiple matches exist for {country.name}. {country.name} is not mapped.\"\n                        )\n                    location_mapping[matched[0]] = country.alpha_3\n                    self.logger.debug(f\"{country.name} matched with {matched[0]}!\")\n                else:\n                    self.logger.warning(\n                        f\"No direct matches for {country.name}. {country.name} is not mapped.\"\n                    )\n                    self.logger.debug(\"Possible matches are: \")\n                    for c, score in zip(matching_locations, scores):\n                        self.logger.debug(c, score)\n            else:\n                location_mapping[country.name] = country.alpha_3\n\n        return location_mapping\n\n    def get_relevant_data_units_by_geometry(\n        self, geometry: Union[BaseGeometry, gpd.GeoDataFrame], **kwargs\n    ) -&gt; pd.DataFrame:\n        \"\"\"\n        Get the DataFrame of Microsoft Buildings tiles that intersect with a given source spatial geometry.\n\n        In case country given, this method first tries to find tiles directly mapped to the given country.\n        If no directly mapped tiles are found and the country is not in the location\n        mapping, it attempts to find overlapping tiles by creating Mercator tiles\n        for the country and filtering the dataset's tiles.\n        \"\"\"\n        source = geometry\n\n        if isinstance(source, str):\n            try:\n                country_code = pycountry.countries.lookup(source).alpha_3\n            except:\n                raise ValueError(\"Invalid`country` value!\")\n\n            mask = self.df_tiles[\"country\"] == country_code\n\n            if any(mask):\n                return self.df_tiles.loc[\n                    mask, [\"quadkey\", \"url\", \"country\", \"location\"]\n                ].to_dict(\"records\")\n\n            self.logger.warning(\n                f\"The country code '{country_code}' is not directly in the location mapping. \"\n                \"Manually checking for overlapping locations with the country boundary.\"\n            )\n\n            source_tiles = CountryMercatorTiles.create(\n                country_code, self.MERCATOR_ZOOM_LEVEL\n            )\n        else:\n            source_tiles = MercatorTiles.from_spatial(\n                source=source, zoom_level=self.MERCATOR_ZOOM_LEVEL\n            )\n\n        filtered_tiles = source_tiles.filter_quadkeys(self.df_tiles.quadkey)\n\n        mask = self.df_tiles.quadkey.isin(filtered_tiles.quadkeys)\n\n        return self.df_tiles.loc[\n            mask, [\"quadkey\", \"url\", \"country\", \"location\"]\n        ].to_dict(\"records\")\n\n    def get_data_unit_path(self, unit: Union[pd.Series, dict], **kwargs) -&gt; Path:\n\n        tile_location = unit[\"country\"] if unit[\"country\"] else unit[\"location\"]\n\n        return (\n            self.base_path\n            / tile_location\n            / self.upload_date\n            / f'{unit[\"quadkey\"]}.csv.gz'\n        )\n\n    def get_data_unit_paths(\n        self, units: Union[pd.DataFrame, Iterable[dict]], **kwargs\n    ) -&gt; List:\n        if isinstance(units, pd.DataFrame):\n            return [self.get_data_unit_path(row) for _, row in units.iterrows()]\n        return super().get_data_unit_paths(units)\n\n    def extract_search_geometry(self, source, **kwargs):\n        \"\"\"Override the method since geometry will be extracted by MercatorTiles\"\"\"\n        return source\n</code></pre>"},{"location":"api/handlers/#gigaspatial.handlers.microsoft_global_buildings.MSBuildingsConfig.__post_init__","title":"<code>__post_init__()</code>","text":"<p>Initialize the configuration, load tile URLs, and set up location mapping.</p> Source code in <code>gigaspatial/handlers/microsoft_global_buildings.py</code> <pre><code>def __post_init__(self):\n    \"\"\"Initialize the configuration, load tile URLs, and set up location mapping.\"\"\"\n    super().__post_init__()\n    self._load_tile_urls()\n    self.upload_date = self.df_tiles.upload_date[0]\n    self._setup_location_mapping()\n</code></pre>"},{"location":"api/handlers/#gigaspatial.handlers.microsoft_global_buildings.MSBuildingsConfig.create_location_mapping","title":"<code>create_location_mapping(similarity_score_threshold=0.8)</code>","text":"<p>Create a mapping between the dataset's location names and ISO 3166-1 alpha-3 country codes.</p> <p>This function iterates through known countries and attempts to find matching locations in the dataset based on string similarity.</p> <p>Parameters:</p> Name Type Description Default <code>similarity_score_threshold</code> <code>float</code> <p>The minimum similarity score (between 0 and 1)                         for a dataset location to be considered a match                         for a country. Defaults to 0.8.</p> <code>0.8</code> <p>Returns:</p> Type Description <p>A dictionary where keys are dataset location names and values are</p> <p>the corresponding ISO 3166-1 alpha-3 country codes.</p> Source code in <code>gigaspatial/handlers/microsoft_global_buildings.py</code> <pre><code>def create_location_mapping(self, similarity_score_threshold: float = 0.8):\n    \"\"\"\n    Create a mapping between the dataset's location names and ISO 3166-1 alpha-3 country codes.\n\n    This function iterates through known countries and attempts to find matching\n    locations in the dataset based on string similarity.\n\n    Args:\n        similarity_score_threshold: The minimum similarity score (between 0 and 1)\n                                    for a dataset location to be considered a match\n                                    for a country. Defaults to 0.8.\n\n    Returns:\n        A dictionary where keys are dataset location names and values are\n        the corresponding ISO 3166-1 alpha-3 country codes.\n    \"\"\"\n\n    def similar(a, b):\n        return SequenceMatcher(None, a, b).ratio()\n\n    location_mapping = dict()\n\n    for country in pycountry.countries:\n        if country.name not in self.df_tiles.location.unique():\n            try:\n                country_quadkey = CountryMercatorTiles.create(\n                    country.alpha_3, self.MERCATOR_ZOOM_LEVEL\n                )\n            except:\n                self.logger.warning(f\"{country.name} is not mapped.\")\n                continue\n            country_datasets = country_quadkey.filter_quadkeys(\n                self.df_tiles.quadkey\n            )\n            matching_locations = self.df_tiles[\n                self.df_tiles.quadkey.isin(country_datasets.quadkeys)\n            ].location.unique()\n            scores = np.array(\n                [\n                    (\n                        similar(c, country.common_name)\n                        if hasattr(country, \"common_name\")\n                        else similar(c, country.name)\n                    )\n                    for c in matching_locations\n                ]\n            )\n            if any(scores &gt; similarity_score_threshold):\n                matched = matching_locations[scores &gt; similarity_score_threshold]\n                if len(matched) &gt; 2:\n                    self.logger.warning(\n                        f\"Multiple matches exist for {country.name}. {country.name} is not mapped.\"\n                    )\n                location_mapping[matched[0]] = country.alpha_3\n                self.logger.debug(f\"{country.name} matched with {matched[0]}!\")\n            else:\n                self.logger.warning(\n                    f\"No direct matches for {country.name}. {country.name} is not mapped.\"\n                )\n                self.logger.debug(\"Possible matches are: \")\n                for c, score in zip(matching_locations, scores):\n                    self.logger.debug(c, score)\n        else:\n            location_mapping[country.name] = country.alpha_3\n\n    return location_mapping\n</code></pre>"},{"location":"api/handlers/#gigaspatial.handlers.microsoft_global_buildings.MSBuildingsConfig.extract_search_geometry","title":"<code>extract_search_geometry(source, **kwargs)</code>","text":"<p>Override the method since geometry will be extracted by MercatorTiles</p> Source code in <code>gigaspatial/handlers/microsoft_global_buildings.py</code> <pre><code>def extract_search_geometry(self, source, **kwargs):\n    \"\"\"Override the method since geometry will be extracted by MercatorTiles\"\"\"\n    return source\n</code></pre>"},{"location":"api/handlers/#gigaspatial.handlers.microsoft_global_buildings.MSBuildingsConfig.get_relevant_data_units_by_geometry","title":"<code>get_relevant_data_units_by_geometry(geometry, **kwargs)</code>","text":"<p>Get the DataFrame of Microsoft Buildings tiles that intersect with a given source spatial geometry.</p> <p>In case country given, this method first tries to find tiles directly mapped to the given country. If no directly mapped tiles are found and the country is not in the location mapping, it attempts to find overlapping tiles by creating Mercator tiles for the country and filtering the dataset's tiles.</p> Source code in <code>gigaspatial/handlers/microsoft_global_buildings.py</code> <pre><code>def get_relevant_data_units_by_geometry(\n    self, geometry: Union[BaseGeometry, gpd.GeoDataFrame], **kwargs\n) -&gt; pd.DataFrame:\n    \"\"\"\n    Get the DataFrame of Microsoft Buildings tiles that intersect with a given source spatial geometry.\n\n    In case country given, this method first tries to find tiles directly mapped to the given country.\n    If no directly mapped tiles are found and the country is not in the location\n    mapping, it attempts to find overlapping tiles by creating Mercator tiles\n    for the country and filtering the dataset's tiles.\n    \"\"\"\n    source = geometry\n\n    if isinstance(source, str):\n        try:\n            country_code = pycountry.countries.lookup(source).alpha_3\n        except:\n            raise ValueError(\"Invalid`country` value!\")\n\n        mask = self.df_tiles[\"country\"] == country_code\n\n        if any(mask):\n            return self.df_tiles.loc[\n                mask, [\"quadkey\", \"url\", \"country\", \"location\"]\n            ].to_dict(\"records\")\n\n        self.logger.warning(\n            f\"The country code '{country_code}' is not directly in the location mapping. \"\n            \"Manually checking for overlapping locations with the country boundary.\"\n        )\n\n        source_tiles = CountryMercatorTiles.create(\n            country_code, self.MERCATOR_ZOOM_LEVEL\n        )\n    else:\n        source_tiles = MercatorTiles.from_spatial(\n            source=source, zoom_level=self.MERCATOR_ZOOM_LEVEL\n        )\n\n    filtered_tiles = source_tiles.filter_quadkeys(self.df_tiles.quadkey)\n\n    mask = self.df_tiles.quadkey.isin(filtered_tiles.quadkeys)\n\n    return self.df_tiles.loc[\n        mask, [\"quadkey\", \"url\", \"country\", \"location\"]\n    ].to_dict(\"records\")\n</code></pre>"},{"location":"api/handlers/#gigaspatial.handlers.microsoft_global_buildings.MSBuildingsDownloader","title":"<code>MSBuildingsDownloader</code>","text":"<p>               Bases: <code>BaseHandlerDownloader</code></p> <p>A class to handle downloads of Microsoft's Global ML Building Footprints dataset.</p> Source code in <code>gigaspatial/handlers/microsoft_global_buildings.py</code> <pre><code>class MSBuildingsDownloader(BaseHandlerDownloader):\n    \"\"\"A class to handle downloads of Microsoft's Global ML Building Footprints dataset.\"\"\"\n\n    def __init__(\n        self,\n        config: Optional[MSBuildingsConfig] = None,\n        data_store: Optional[DataStore] = None,\n        logger: Optional[logging.Logger] = None,\n    ):\n        \"\"\"\n        Initialize the downloader.\n\n        Args:\n            config: Optional configuration for customizing download behavior and file paths.\n                    If None, a default `MSBuildingsConfig` is used.\n            data_store: Optional instance of a `DataStore` for managing data storage.\n                        If provided, it overrides the `data_store` in the `config`.\n                        If None, the `data_store` from the `config` is used.\n            logger: Optional custom logger instance. If None, a default logger\n                    named after the module is created and used.\n        \"\"\"\n        config = config or MSBuildingsConfig()\n        super().__init__(config=config, data_store=data_store, logger=logger)\n\n    def download_data_unit(\n        self,\n        tile_info: Union[pd.Series, dict],\n        **kwargs,\n    ) -&gt; Optional[str]:\n        \"\"\"Download data file for a single tile.\"\"\"\n\n        tile_url = tile_info[\"url\"]\n\n        try:\n            response = requests.get(tile_url, stream=True)\n            response.raise_for_status()\n\n            file_path = str(self.config.get_data_unit_path(tile_info))\n\n            with self.data_store.open(file_path, \"wb\") as file:\n                for chunk in response.iter_content(chunk_size=8192):\n                    file.write(chunk)\n\n                self.logger.debug(\n                    f\"Successfully downloaded tile: {tile_info['quadkey']}\"\n                )\n                return file_path\n\n        except requests.exceptions.RequestException as e:\n            self.logger.error(\n                f\"Failed to download tile {tile_info['quadkey']}: {str(e)}\"\n            )\n            return None\n        except Exception as e:\n            self.logger.error(f\"Unexpected error downloading dataset: {str(e)}\")\n            return None\n\n    def download_data_units(\n        self,\n        tiles: Union[pd.DataFrame, List[dict]],\n        **kwargs,\n    ) -&gt; List[str]:\n        \"\"\"Download data files for multiple tiles.\"\"\"\n\n        if len(tiles) == 0:\n            self.logger.warning(f\"There is no matching data\")\n            return []\n\n        with multiprocessing.Pool(self.config.n_workers) as pool:\n            download_func = functools.partial(self.download_data_unit)\n            file_paths = list(\n                tqdm(\n                    pool.imap(\n                        download_func,\n                        (\n                            [row for _, row in tiles.iterrows()]\n                            if isinstance(tiles, pd.DataFrame)\n                            else tiles\n                        ),\n                    ),\n                    total=len(tiles),\n                    desc=f\"Downloading polygons data\",\n                )\n            )\n\n        return [path for path in file_paths if path is not None]\n\n    def download_by_country(\n        self,\n        country: str,\n        data_store: Optional[DataStore] = None,\n        country_geom_path: Optional[Union[str, Path]] = None,\n    ) -&gt; List[str]:\n        \"\"\"\n        Download Microsoft Global ML Building Footprints data for a specific country.\n\n        This is a convenience method to download data for an entire country\n        using its code or name.\n\n        Args:\n            country: The country code (e.g., 'USA', 'GBR') or name.\n            data_store: Optional instance of a `DataStore` to be used by\n                `AdminBoundaries` for loading country boundaries. If None,\n                `AdminBoundaries` will use its default data loading.\n            country_geom_path: Optional path to a GeoJSON file containing the\n                country boundary. If provided, this boundary is used\n                instead of the default from `AdminBoundaries`.\n\n        Returns:\n            A list of local file paths for the successfully downloaded tiles.\n            Returns an empty list if no data is found for the country or if\n            all downloads fail.\n        \"\"\"\n        return self.download(\n            source=country, data_store=data_store, path=country_geom_path\n        )\n</code></pre>"},{"location":"api/handlers/#gigaspatial.handlers.microsoft_global_buildings.MSBuildingsDownloader.__init__","title":"<code>__init__(config=None, data_store=None, logger=None)</code>","text":"<p>Initialize the downloader.</p> <p>Parameters:</p> Name Type Description Default <code>config</code> <code>Optional[MSBuildingsConfig]</code> <p>Optional configuration for customizing download behavior and file paths.     If None, a default <code>MSBuildingsConfig</code> is used.</p> <code>None</code> <code>data_store</code> <code>Optional[DataStore]</code> <p>Optional instance of a <code>DataStore</code> for managing data storage.         If provided, it overrides the <code>data_store</code> in the <code>config</code>.         If None, the <code>data_store</code> from the <code>config</code> is used.</p> <code>None</code> <code>logger</code> <code>Optional[Logger]</code> <p>Optional custom logger instance. If None, a default logger     named after the module is created and used.</p> <code>None</code> Source code in <code>gigaspatial/handlers/microsoft_global_buildings.py</code> <pre><code>def __init__(\n    self,\n    config: Optional[MSBuildingsConfig] = None,\n    data_store: Optional[DataStore] = None,\n    logger: Optional[logging.Logger] = None,\n):\n    \"\"\"\n    Initialize the downloader.\n\n    Args:\n        config: Optional configuration for customizing download behavior and file paths.\n                If None, a default `MSBuildingsConfig` is used.\n        data_store: Optional instance of a `DataStore` for managing data storage.\n                    If provided, it overrides the `data_store` in the `config`.\n                    If None, the `data_store` from the `config` is used.\n        logger: Optional custom logger instance. If None, a default logger\n                named after the module is created and used.\n    \"\"\"\n    config = config or MSBuildingsConfig()\n    super().__init__(config=config, data_store=data_store, logger=logger)\n</code></pre>"},{"location":"api/handlers/#gigaspatial.handlers.microsoft_global_buildings.MSBuildingsDownloader.download_by_country","title":"<code>download_by_country(country, data_store=None, country_geom_path=None)</code>","text":"<p>Download Microsoft Global ML Building Footprints data for a specific country.</p> <p>This is a convenience method to download data for an entire country using its code or name.</p> <p>Parameters:</p> Name Type Description Default <code>country</code> <code>str</code> <p>The country code (e.g., 'USA', 'GBR') or name.</p> required <code>data_store</code> <code>Optional[DataStore]</code> <p>Optional instance of a <code>DataStore</code> to be used by <code>AdminBoundaries</code> for loading country boundaries. If None, <code>AdminBoundaries</code> will use its default data loading.</p> <code>None</code> <code>country_geom_path</code> <code>Optional[Union[str, Path]]</code> <p>Optional path to a GeoJSON file containing the country boundary. If provided, this boundary is used instead of the default from <code>AdminBoundaries</code>.</p> <code>None</code> <p>Returns:</p> Type Description <code>List[str]</code> <p>A list of local file paths for the successfully downloaded tiles.</p> <code>List[str]</code> <p>Returns an empty list if no data is found for the country or if</p> <code>List[str]</code> <p>all downloads fail.</p> Source code in <code>gigaspatial/handlers/microsoft_global_buildings.py</code> <pre><code>def download_by_country(\n    self,\n    country: str,\n    data_store: Optional[DataStore] = None,\n    country_geom_path: Optional[Union[str, Path]] = None,\n) -&gt; List[str]:\n    \"\"\"\n    Download Microsoft Global ML Building Footprints data for a specific country.\n\n    This is a convenience method to download data for an entire country\n    using its code or name.\n\n    Args:\n        country: The country code (e.g., 'USA', 'GBR') or name.\n        data_store: Optional instance of a `DataStore` to be used by\n            `AdminBoundaries` for loading country boundaries. If None,\n            `AdminBoundaries` will use its default data loading.\n        country_geom_path: Optional path to a GeoJSON file containing the\n            country boundary. If provided, this boundary is used\n            instead of the default from `AdminBoundaries`.\n\n    Returns:\n        A list of local file paths for the successfully downloaded tiles.\n        Returns an empty list if no data is found for the country or if\n        all downloads fail.\n    \"\"\"\n    return self.download(\n        source=country, data_store=data_store, path=country_geom_path\n    )\n</code></pre>"},{"location":"api/handlers/#gigaspatial.handlers.microsoft_global_buildings.MSBuildingsDownloader.download_data_unit","title":"<code>download_data_unit(tile_info, **kwargs)</code>","text":"<p>Download data file for a single tile.</p> Source code in <code>gigaspatial/handlers/microsoft_global_buildings.py</code> <pre><code>def download_data_unit(\n    self,\n    tile_info: Union[pd.Series, dict],\n    **kwargs,\n) -&gt; Optional[str]:\n    \"\"\"Download data file for a single tile.\"\"\"\n\n    tile_url = tile_info[\"url\"]\n\n    try:\n        response = requests.get(tile_url, stream=True)\n        response.raise_for_status()\n\n        file_path = str(self.config.get_data_unit_path(tile_info))\n\n        with self.data_store.open(file_path, \"wb\") as file:\n            for chunk in response.iter_content(chunk_size=8192):\n                file.write(chunk)\n\n            self.logger.debug(\n                f\"Successfully downloaded tile: {tile_info['quadkey']}\"\n            )\n            return file_path\n\n    except requests.exceptions.RequestException as e:\n        self.logger.error(\n            f\"Failed to download tile {tile_info['quadkey']}: {str(e)}\"\n        )\n        return None\n    except Exception as e:\n        self.logger.error(f\"Unexpected error downloading dataset: {str(e)}\")\n        return None\n</code></pre>"},{"location":"api/handlers/#gigaspatial.handlers.microsoft_global_buildings.MSBuildingsDownloader.download_data_units","title":"<code>download_data_units(tiles, **kwargs)</code>","text":"<p>Download data files for multiple tiles.</p> Source code in <code>gigaspatial/handlers/microsoft_global_buildings.py</code> <pre><code>def download_data_units(\n    self,\n    tiles: Union[pd.DataFrame, List[dict]],\n    **kwargs,\n) -&gt; List[str]:\n    \"\"\"Download data files for multiple tiles.\"\"\"\n\n    if len(tiles) == 0:\n        self.logger.warning(f\"There is no matching data\")\n        return []\n\n    with multiprocessing.Pool(self.config.n_workers) as pool:\n        download_func = functools.partial(self.download_data_unit)\n        file_paths = list(\n            tqdm(\n                pool.imap(\n                    download_func,\n                    (\n                        [row for _, row in tiles.iterrows()]\n                        if isinstance(tiles, pd.DataFrame)\n                        else tiles\n                    ),\n                ),\n                total=len(tiles),\n                desc=f\"Downloading polygons data\",\n            )\n        )\n\n    return [path for path in file_paths if path is not None]\n</code></pre>"},{"location":"api/handlers/#gigaspatial.handlers.microsoft_global_buildings.MSBuildingsHandler","title":"<code>MSBuildingsHandler</code>","text":"<p>               Bases: <code>BaseHandler</code></p> <p>Handler for Microsoft Global Buildings dataset.</p> <p>This class provides a unified interface for downloading and loading Microsoft Global Buildings data. It manages the lifecycle of configuration, downloading, and reading components.</p> Source code in <code>gigaspatial/handlers/microsoft_global_buildings.py</code> <pre><code>class MSBuildingsHandler(BaseHandler):\n    \"\"\"\n    Handler for Microsoft Global Buildings dataset.\n\n    This class provides a unified interface for downloading and loading Microsoft Global Buildings data.\n    It manages the lifecycle of configuration, downloading, and reading components.\n    \"\"\"\n\n    def create_config(\n        self, data_store: DataStore, logger: logging.Logger, **kwargs\n    ) -&gt; MSBuildingsConfig:\n        \"\"\"\n        Create and return a MSBuildingsConfig instance.\n\n        Args:\n            data_store: The data store instance to use\n            logger: The logger instance to use\n            **kwargs: Additional configuration parameters\n\n        Returns:\n            Configured MSBuildingsConfig instance\n        \"\"\"\n        return MSBuildingsConfig(data_store=data_store, logger=logger, **kwargs)\n\n    def create_downloader(\n        self,\n        config: MSBuildingsConfig,\n        data_store: DataStore,\n        logger: logging.Logger,\n        **kwargs,\n    ) -&gt; MSBuildingsDownloader:\n        \"\"\"\n        Create and return a MSBuildingsDownloader instance.\n\n        Args:\n            config: The configuration object\n            data_store: The data store instance to use\n            logger: The logger instance to use\n            **kwargs: Additional downloader parameters\n\n        Returns:\n            Configured MSBuildingsDownloader instance\n        \"\"\"\n        return MSBuildingsDownloader(\n            config=config, data_store=data_store, logger=logger, **kwargs\n        )\n\n    def create_reader(\n        self,\n        config: MSBuildingsConfig,\n        data_store: DataStore,\n        logger: logging.Logger,\n        **kwargs,\n    ) -&gt; MSBuildingsReader:\n        \"\"\"\n        Create and return a MSBuildingsReader instance.\n\n        Args:\n            config: The configuration object\n            data_store: The data store instance to use\n            logger: The logger instance to use\n            **kwargs: Additional reader parameters\n\n        Returns:\n            Configured MSBuildingsReader instance\n        \"\"\"\n        return MSBuildingsReader(\n            config=config, data_store=data_store, logger=logger, **kwargs\n        )\n</code></pre>"},{"location":"api/handlers/#gigaspatial.handlers.microsoft_global_buildings.MSBuildingsHandler.create_config","title":"<code>create_config(data_store, logger, **kwargs)</code>","text":"<p>Create and return a MSBuildingsConfig instance.</p> <p>Parameters:</p> Name Type Description Default <code>data_store</code> <code>DataStore</code> <p>The data store instance to use</p> required <code>logger</code> <code>Logger</code> <p>The logger instance to use</p> required <code>**kwargs</code> <p>Additional configuration parameters</p> <code>{}</code> <p>Returns:</p> Type Description <code>MSBuildingsConfig</code> <p>Configured MSBuildingsConfig instance</p> Source code in <code>gigaspatial/handlers/microsoft_global_buildings.py</code> <pre><code>def create_config(\n    self, data_store: DataStore, logger: logging.Logger, **kwargs\n) -&gt; MSBuildingsConfig:\n    \"\"\"\n    Create and return a MSBuildingsConfig instance.\n\n    Args:\n        data_store: The data store instance to use\n        logger: The logger instance to use\n        **kwargs: Additional configuration parameters\n\n    Returns:\n        Configured MSBuildingsConfig instance\n    \"\"\"\n    return MSBuildingsConfig(data_store=data_store, logger=logger, **kwargs)\n</code></pre>"},{"location":"api/handlers/#gigaspatial.handlers.microsoft_global_buildings.MSBuildingsHandler.create_downloader","title":"<code>create_downloader(config, data_store, logger, **kwargs)</code>","text":"<p>Create and return a MSBuildingsDownloader instance.</p> <p>Parameters:</p> Name Type Description Default <code>config</code> <code>MSBuildingsConfig</code> <p>The configuration object</p> required <code>data_store</code> <code>DataStore</code> <p>The data store instance to use</p> required <code>logger</code> <code>Logger</code> <p>The logger instance to use</p> required <code>**kwargs</code> <p>Additional downloader parameters</p> <code>{}</code> <p>Returns:</p> Type Description <code>MSBuildingsDownloader</code> <p>Configured MSBuildingsDownloader instance</p> Source code in <code>gigaspatial/handlers/microsoft_global_buildings.py</code> <pre><code>def create_downloader(\n    self,\n    config: MSBuildingsConfig,\n    data_store: DataStore,\n    logger: logging.Logger,\n    **kwargs,\n) -&gt; MSBuildingsDownloader:\n    \"\"\"\n    Create and return a MSBuildingsDownloader instance.\n\n    Args:\n        config: The configuration object\n        data_store: The data store instance to use\n        logger: The logger instance to use\n        **kwargs: Additional downloader parameters\n\n    Returns:\n        Configured MSBuildingsDownloader instance\n    \"\"\"\n    return MSBuildingsDownloader(\n        config=config, data_store=data_store, logger=logger, **kwargs\n    )\n</code></pre>"},{"location":"api/handlers/#gigaspatial.handlers.microsoft_global_buildings.MSBuildingsHandler.create_reader","title":"<code>create_reader(config, data_store, logger, **kwargs)</code>","text":"<p>Create and return a MSBuildingsReader instance.</p> <p>Parameters:</p> Name Type Description Default <code>config</code> <code>MSBuildingsConfig</code> <p>The configuration object</p> required <code>data_store</code> <code>DataStore</code> <p>The data store instance to use</p> required <code>logger</code> <code>Logger</code> <p>The logger instance to use</p> required <code>**kwargs</code> <p>Additional reader parameters</p> <code>{}</code> <p>Returns:</p> Type Description <code>MSBuildingsReader</code> <p>Configured MSBuildingsReader instance</p> Source code in <code>gigaspatial/handlers/microsoft_global_buildings.py</code> <pre><code>def create_reader(\n    self,\n    config: MSBuildingsConfig,\n    data_store: DataStore,\n    logger: logging.Logger,\n    **kwargs,\n) -&gt; MSBuildingsReader:\n    \"\"\"\n    Create and return a MSBuildingsReader instance.\n\n    Args:\n        config: The configuration object\n        data_store: The data store instance to use\n        logger: The logger instance to use\n        **kwargs: Additional reader parameters\n\n    Returns:\n        Configured MSBuildingsReader instance\n    \"\"\"\n    return MSBuildingsReader(\n        config=config, data_store=data_store, logger=logger, **kwargs\n    )\n</code></pre>"},{"location":"api/handlers/#gigaspatial.handlers.microsoft_global_buildings.MSBuildingsReader","title":"<code>MSBuildingsReader</code>","text":"<p>               Bases: <code>BaseHandlerReader</code></p> <p>Reader for Microsoft Global Buildings data, supporting country, points, and geometry-based resolution.</p> Source code in <code>gigaspatial/handlers/microsoft_global_buildings.py</code> <pre><code>class MSBuildingsReader(BaseHandlerReader):\n    \"\"\"\n    Reader for Microsoft Global Buildings data, supporting country, points, and geometry-based resolution.\n    \"\"\"\n\n    def __init__(\n        self,\n        config: Optional[MSBuildingsConfig] = None,\n        data_store: Optional[DataStore] = None,\n        logger: Optional[logging.Logger] = None,\n    ):\n        config = config or MSBuildingsConfig()\n        super().__init__(config=config, data_store=data_store, logger=logger)\n\n    def load_from_paths(\n        self, source_data_path: List[Union[str, Path]], **kwargs\n    ) -&gt; gpd.GeoDataFrame:\n        \"\"\"\n        Load building data from Microsoft Buildings dataset.\n        Args:\n            source_data_path: List of file paths to load\n        Returns:\n            GeoDataFrame containing building data\n        \"\"\"\n        from gigaspatial.core.io.readers import read_gzipped_json_or_csv\n        from shapely.geometry import shape\n\n        def read_ms_dataset(data_store: DataStore, file_path: str):\n            df = read_gzipped_json_or_csv(file_path=file_path, data_store=data_store)\n            df[\"geometry\"] = df[\"geometry\"].apply(shape)\n            return gpd.GeoDataFrame(df, crs=4326)\n\n        result = self._load_tabular_data(\n            file_paths=source_data_path, read_function=read_ms_dataset\n        )\n        return result\n</code></pre>"},{"location":"api/handlers/#gigaspatial.handlers.microsoft_global_buildings.MSBuildingsReader.load_from_paths","title":"<code>load_from_paths(source_data_path, **kwargs)</code>","text":"<p>Load building data from Microsoft Buildings dataset. Args:     source_data_path: List of file paths to load Returns:     GeoDataFrame containing building data</p> Source code in <code>gigaspatial/handlers/microsoft_global_buildings.py</code> <pre><code>def load_from_paths(\n    self, source_data_path: List[Union[str, Path]], **kwargs\n) -&gt; gpd.GeoDataFrame:\n    \"\"\"\n    Load building data from Microsoft Buildings dataset.\n    Args:\n        source_data_path: List of file paths to load\n    Returns:\n        GeoDataFrame containing building data\n    \"\"\"\n    from gigaspatial.core.io.readers import read_gzipped_json_or_csv\n    from shapely.geometry import shape\n\n    def read_ms_dataset(data_store: DataStore, file_path: str):\n        df = read_gzipped_json_or_csv(file_path=file_path, data_store=data_store)\n        df[\"geometry\"] = df[\"geometry\"].apply(shape)\n        return gpd.GeoDataFrame(df, crs=4326)\n\n    result = self._load_tabular_data(\n        file_paths=source_data_path, read_function=read_ms_dataset\n    )\n    return result\n</code></pre>"},{"location":"api/handlers/#gigaspatial.handlers.ookla_speedtest","title":"<code>ookla_speedtest</code>","text":""},{"location":"api/handlers/#gigaspatial.handlers.ookla_speedtest.OoklaSpeedtestConfig","title":"<code>OoklaSpeedtestConfig</code>  <code>dataclass</code>","text":"<p>               Bases: <code>BaseHandlerConfig</code></p> <p>Configuration class for Ookla Speedtest data.</p> <p>This class defines the parameters for accessing and filtering Ookla Speedtest datasets, including available years, quarters, and how dataset URLs are constructed.</p> Source code in <code>gigaspatial/handlers/ookla_speedtest.py</code> <pre><code>@dataclass(config=ConfigDict(arbitrary_types_allowed=True))\nclass OoklaSpeedtestConfig(BaseHandlerConfig):\n    \"\"\"\n    Configuration class for Ookla Speedtest data.\n\n    This class defines the parameters for accessing and filtering Ookla Speedtest datasets,\n    including available years, quarters, and how dataset URLs are constructed.\n    \"\"\"\n\n    MIN_YEAR = 2019\n    MAX_YEAR = datetime.today().year\n    MAX_QUARTER = int(np.floor((datetime.today().month - 1) / 3))\n    if MAX_QUARTER == 0:\n        MAX_YEAR -= 1\n        MAX_QUARTER = 4\n\n    BASE_URL = \"https://ookla-open-data.s3.amazonaws.com/parquet/performance\"\n\n    base_path: Path = Field(default=config.get_path(\"ookla_speedtest\", \"bronze\"))\n\n    type: Literal[\"fixed\", \"mobile\"] = Field(...)\n    year: Optional[int] = Field(default=None, ge=MIN_YEAR, le=MAX_YEAR)\n    quarter: Optional[int] = Field(default=None, ge=0, le=4)\n\n    def __post_init__(self):\n        if self.year is None:\n            self.year = self.MAX_YEAR\n            self.logger.warning(\n                \"Year not provided. Using the latest available data year: %s\", self.year\n            )\n        if self.quarter is None:\n            self.quarter = self.MAX_QUARTER\n            self.logger.warning(\n                \"Quarter not provided. Using the latest available data quarter for year %s: %s\",\n                self.year,\n                self.quarter,\n            )\n\n        super().__post_init__()\n        self.DATASET_URL = self._get_dataset_url(self.type, self.year, self.quarter)\n\n    def _get_dataset_url(self, type, year, quarter):\n        month = [1, 4, 7, 10]\n        quarter_start = datetime(year, month[self.quarter - 1], 1)\n        return f\"{self.BASE_URL}/type={type}/year={quarter_start:%Y}/quarter={quarter}/{quarter_start:%Y-%m-%d}_performance_{type}_tiles.parquet\"\n\n    @staticmethod\n    def get_available_datasets():\n        start_year = 2019  # first data year\n        max_year = datetime.today().year\n        max_quarter = np.floor((datetime.today().month - 1) / 3)\n        if max_quarter == 0:\n            max_year -= 1\n            max_quarter = 4\n\n        ookla_tiles = []\n        for year in range(start_year, max_year + 1):\n            for quarter in range(1, 5):\n                if year == max_year and quarter &gt; max_quarter:\n                    continue\n                for type in [\"fixed\", \"mobile\"]:\n                    ookla_tiles.append(\n                        {\"service_type\": type, \"year\": year, \"quarter\": quarter}\n                    )\n\n        return ookla_tiles\n\n    def get_relevant_data_units(self, source=None, **kwargs):\n        return [self.DATASET_URL]\n\n    def get_relevant_data_units_by_geometry(\n        self, geometry: Union[BaseGeometry, gpd.GeoDataFrame] = None, **kwargs\n    ) -&gt; List[str]:\n        return\n\n    def get_data_unit_path(self, unit: str, **kwargs) -&gt; Path:\n        \"\"\"\n        Given a Ookla Speedtest file url, return the corresponding path.\n        \"\"\"\n        return self.base_path / unit.split(\"/\")[-1]\n</code></pre>"},{"location":"api/handlers/#gigaspatial.handlers.ookla_speedtest.OoklaSpeedtestConfig.get_data_unit_path","title":"<code>get_data_unit_path(unit, **kwargs)</code>","text":"<p>Given a Ookla Speedtest file url, return the corresponding path.</p> Source code in <code>gigaspatial/handlers/ookla_speedtest.py</code> <pre><code>def get_data_unit_path(self, unit: str, **kwargs) -&gt; Path:\n    \"\"\"\n    Given a Ookla Speedtest file url, return the corresponding path.\n    \"\"\"\n    return self.base_path / unit.split(\"/\")[-1]\n</code></pre>"},{"location":"api/handlers/#gigaspatial.handlers.ookla_speedtest.OoklaSpeedtestDownloader","title":"<code>OoklaSpeedtestDownloader</code>","text":"<p>               Bases: <code>BaseHandlerDownloader</code></p> <p>A class to handle the downloading of Ookla Speedtest data.</p> <p>This downloader focuses on fetching parquet files based on the provided configuration and data unit URLs.</p> Source code in <code>gigaspatial/handlers/ookla_speedtest.py</code> <pre><code>class OoklaSpeedtestDownloader(BaseHandlerDownloader):\n    \"\"\"\n    A class to handle the downloading of Ookla Speedtest data.\n\n    This downloader focuses on fetching parquet files based on the provided configuration\n    and data unit URLs.\n    \"\"\"\n\n    def __init__(\n        self,\n        config: Union[OoklaSpeedtestConfig, dict[str, Union[str, int]]],\n        data_store: Optional[DataStore] = None,\n        logger: Optional[logging.Logger] = None,\n    ):\n        config = (\n            config\n            if isinstance(config, OoklaSpeedtestConfig)\n            else OoklaSpeedtestConfig(**config)\n        )\n        super().__init__(config=config, data_store=data_store, logger=logger)\n\n    def download_data_unit(self, url: str, **kwargs) -&gt; Optional[Path]:\n        output_path = self.config.get_data_unit_path(url)\n\n        try:\n            response = requests.get(url, stream=True)\n            response.raise_for_status()\n\n            total_size = int(response.headers.get(\"content-length\", 0))\n\n            with self.data_store.open(str(output_path), \"wb\") as file:\n                for chunk in tqdm(\n                    response.iter_content(chunk_size=8192),\n                    total=total_size // 8192,\n                    unit=\"KB\",\n                    desc=f\"Downloading {output_path.name}\",\n                ):\n                    file.write(chunk)\n\n            self.logger.info(f\"Successfully downloaded: {url} to {output_path}\")\n            return output_path\n\n        except requests.exceptions.RequestException as e:\n            self.logger.error(f\"Failed to download {url}: {str(e)}\")\n            return None\n        except Exception as e:\n            self.logger.error(f\"Unexpected error downloading {url}: {str(e)}\")\n            return None\n\n    def download_data_units(self, urls: List[str], **kwargs) -&gt; List[Optional[Path]]:\n        # Ookla data is not parallelizable in a meaningful way beyond single file, so just iterate.\n        results = [self.download_data_unit(url, **kwargs) for url in urls]\n        return [path for path in results if path is not None]\n\n    def download(\n        self, source: Optional[Union[str, List[str]]] = None, **kwargs\n    ) -&gt; List[Optional[Path]]:\n        urls = self.config.get_relevant_data_units(source)\n        return self.download_data_units(urls, **kwargs)\n</code></pre>"},{"location":"api/handlers/#gigaspatial.handlers.ookla_speedtest.OoklaSpeedtestHandler","title":"<code>OoklaSpeedtestHandler</code>","text":"<p>               Bases: <code>BaseHandler</code></p> <p>Handler for Ookla Speedtest data.</p> <p>This class orchestrates the configuration, downloading, and reading of Ookla Speedtest data, allowing for filtering by geographical sources using Mercator tiles.</p> Source code in <code>gigaspatial/handlers/ookla_speedtest.py</code> <pre><code>class OoklaSpeedtestHandler(BaseHandler):\n    \"\"\"\n    Handler for Ookla Speedtest data.\n\n    This class orchestrates the configuration, downloading, and reading of Ookla Speedtest\n    data, allowing for filtering by geographical sources using Mercator tiles.\n    \"\"\"\n\n    def __init__(\n        self,\n        type: Literal[\"fixed\", \"mobile\"],\n        year: Optional[int] = None,\n        quarter: Optional[int] = None,\n        config: Optional[OoklaSpeedtestConfig] = None,\n        downloader: Optional[OoklaSpeedtestDownloader] = None,\n        reader: Optional[OoklaSpeedtestReader] = None,\n        data_store: Optional[DataStore] = None,\n        logger: Optional[logging.Logger] = None,\n        **kwargs,\n    ):\n        self._type = type\n        self._year = year\n        self._quarter = quarter\n\n        super().__init__(\n            config=config,\n            downloader=downloader,\n            reader=reader,\n            data_store=data_store,\n            logger=logger,\n        )\n\n    def create_config(\n        self, data_store: DataStore, logger: logging.Logger, **kwargs\n    ) -&gt; OoklaSpeedtestConfig:\n        return OoklaSpeedtestConfig(\n            type=self._type,\n            year=self._year,\n            quarter=self._quarter,\n            data_store=data_store,\n            logger=logger,\n            **kwargs,\n        )\n\n    def create_downloader(\n        self,\n        config: OoklaSpeedtestConfig,\n        data_store: DataStore,\n        logger: logging.Logger,\n        **kwargs,\n    ) -&gt; OoklaSpeedtestDownloader:\n        return OoklaSpeedtestDownloader(\n            config=config, data_store=data_store, logger=logger, **kwargs\n        )\n\n    def create_reader(\n        self,\n        config: OoklaSpeedtestConfig,\n        data_store: DataStore,\n        logger: logging.Logger,\n        **kwargs,\n    ) -&gt; OoklaSpeedtestReader:\n        return OoklaSpeedtestReader(\n            config=config, data_store=data_store, logger=logger, **kwargs\n        )\n\n    def load_data(\n        self,\n        source: Union[\n            str,  # country\n            List[Union[Tuple[float, float], Point]],  # points\n            BaseGeometry,  # geometry\n            gpd.GeoDataFrame,  # geodataframe\n            Path,  # path\n            str,  # path\n            List[Union[str, Path]],\n        ] = None,\n        process_geospatial: bool = False,\n        ensure_available: bool = True,\n        **kwargs,\n    ) -&gt; Union[pd.DataFrame, gpd.GeoDataFrame]:\n\n        if source is None or (\n            isinstance(source, (str, Path))\n            and (\n                self.data_store.file_exists(str(source))\n                or str(source).endswith(\".parquet\")\n            )\n            or (\n                isinstance(source, List)\n                and all(isinstance(p, (str, Path)) for p in source)\n            )\n        ):\n            # If no source or source is a direct path, load without filtering\n            result = super().load_data(source, ensure_available, **kwargs)\n        else:\n            # Load the entire dataset and then apply Mercator tile filtering\n            full_dataset = super().load_data(\n                None, ensure_available, **kwargs\n            )  # Load the full dataset (uses DATASET_URL)\n\n            key = self.config._cache_key(source, **kwargs)\n\n            # Check cache unless forced recompute\n            if (\n                not kwargs.get(\"force_recompute\", False)\n                and key in self.config._unit_cache\n            ):\n                self.logger.debug(\n                    f\"Using cached quadkeys for {key[0]}: {key[1][:50]}...\"\n                )\n                quadkeys = self.config._unit_cache[key]\n\n            else:\n\n                if isinstance(source, str):  # country\n                    mercator_tiles = CountryMercatorTiles.create(\n                        source, zoom_level=16, **kwargs\n                    )\n                elif isinstance(source, (BaseGeometry, gpd.GeoDataFrame, List)):\n                    mercator_tiles = MercatorTiles.from_spatial(\n                        source, zoom_level=16, **kwargs\n                    )\n                else:\n                    raise ValueError(\n                        f\"Unsupported source type for filtering: {type(source)}\"\n                    )\n\n                quadkeys = mercator_tiles.quadkeys\n\n                # Cache the result\n                self.config._unit_cache[key] = quadkeys\n\n            result = full_dataset[full_dataset[\"quadkey\"].isin(quadkeys)].reset_index(\n                drop=True\n            )\n\n        if process_geospatial:\n            # Convert 'tile' column from WKT to geometry\n            result[\"geometry\"] = result[\"tile\"].apply(wkt.loads)\n            return gpd.GeoDataFrame(result, geometry=\"geometry\", crs=\"EPSG:4326\")\n\n        return result\n</code></pre>"},{"location":"api/handlers/#gigaspatial.handlers.ookla_speedtest.OoklaSpeedtestReader","title":"<code>OoklaSpeedtestReader</code>","text":"<p>               Bases: <code>BaseHandlerReader</code></p> <p>A class to handle reading Ookla Speedtest data.</p> <p>It loads parquet files into a DataFrame.</p> Source code in <code>gigaspatial/handlers/ookla_speedtest.py</code> <pre><code>class OoklaSpeedtestReader(BaseHandlerReader):\n    \"\"\"\n    A class to handle reading Ookla Speedtest data.\n\n    It loads parquet files into a DataFrame.\n    \"\"\"\n\n    def __init__(\n        self,\n        config: Union[OoklaSpeedtestConfig, dict[str, Union[str, int]]],\n        data_store: Optional[DataStore] = None,\n        logger: Optional[logging.Logger] = None,\n    ):\n        config = (\n            config\n            if isinstance(config, OoklaSpeedtestConfig)\n            else OoklaSpeedtestConfig(**config)\n        )\n        super().__init__(config=config, data_store=data_store, logger=logger)\n\n    def load_from_paths(\n        self, source_data_path: List[Union[str, Path]], **kwargs\n    ) -&gt; pd.DataFrame:\n        result = self._load_tabular_data(file_paths=source_data_path)\n        return result\n\n    def load(\n        self,\n        source: Optional[\n            Union[\n                str,  # country\n                List[Union[Tuple[float, float], Point]],  # points\n                BaseGeometry,  # geometry\n                gpd.GeoDataFrame,  # geodataframe\n                Path,  # path\n                str,  # path\n                List[Union[str, Path]],\n            ]\n        ] = None,\n        **kwargs,\n    ) -&gt; pd.DataFrame:\n        return super().load(source=source, **kwargs)\n</code></pre>"},{"location":"api/handlers/#gigaspatial.handlers.opencellid","title":"<code>opencellid</code>","text":""},{"location":"api/handlers/#gigaspatial.handlers.opencellid.OpenCellIDConfig","title":"<code>OpenCellIDConfig</code>","text":"<p>               Bases: <code>BaseModel</code></p> <p>Configuration for OpenCellID data access</p> Source code in <code>gigaspatial/handlers/opencellid.py</code> <pre><code>class OpenCellIDConfig(BaseModel):\n    \"\"\"Configuration for OpenCellID data access\"\"\"\n\n    # Base URLs\n    BASE_URL: HttpUrl = Field(default=\"https://opencellid.org/\")\n    DOWNLOAD_URL: HttpUrl = Field(default=\"https://opencellid.org/downloads.php?token=\")\n\n    # User configuration\n    country: str = Field(...)\n    api_token: str = Field(\n        default=global_config.OPENCELLID_ACCESS_TOKEN,\n        description=\"OpenCellID API Access Token\",\n    )\n    base_path: Path = Field(default=global_config.get_path(\"opencellid\", \"bronze\"))\n    created_newer: int = Field(\n        default=2003, description=\"Filter out cell towers added before this year\"\n    )\n    created_before: int = Field(\n        default=datetime.now().year,\n        description=\"Filter out cell towers added after this year\",\n    )\n    drop_duplicates: bool = Field(\n        default=True,\n        description=\"Drop cells that are in the exact same location and radio technology\",\n    )\n\n    @field_validator(\"country\")\n    def validate_country(cls, value: str) -&gt; str:\n        try:\n            return pycountry.countries.lookup(value).alpha_3\n        except LookupError:\n            raise ValueError(f\"Invalid country code provided: {value}\")\n\n    @property\n    def output_file_path(self) -&gt; Path:\n        \"\"\"Path to save the downloaded OpenCellID data\"\"\"\n        return self.base_path / f\"opencellid_{self.country.lower()}.csv.gz\"\n\n    def __repr__(self) -&gt; str:\n        return (\n            f\"OpenCellIDConfig(\\n\"\n            f\"  country='{self.country}'\\n\"\n            f\"  created_newer={self.created_newer}\\n\"\n            f\"  created_before={self.created_before}\\n\"\n            f\"  drop_duplicates={self.drop_duplicates}\\n\"\n            f\")\"\n        )\n</code></pre>"},{"location":"api/handlers/#gigaspatial.handlers.opencellid.OpenCellIDConfig.output_file_path","title":"<code>output_file_path: Path</code>  <code>property</code>","text":"<p>Path to save the downloaded OpenCellID data</p>"},{"location":"api/handlers/#gigaspatial.handlers.opencellid.OpenCellIDDownloader","title":"<code>OpenCellIDDownloader</code>","text":"<p>Downloader for OpenCellID data</p> Source code in <code>gigaspatial/handlers/opencellid.py</code> <pre><code>class OpenCellIDDownloader:\n    \"\"\"Downloader for OpenCellID data\"\"\"\n\n    def __init__(\n        self,\n        config: Union[OpenCellIDConfig, dict],\n        data_store: Optional[DataStore] = None,\n        logger: Optional[logging.Logger] = None,\n    ):\n        if isinstance(config, dict):\n            self.config = OpenCellIDConfig(**config)\n        else:\n            self.config = config\n\n        self.data_store = data_store or LocalDataStore()\n        self.logger = logger or global_config.get_logger(self.__class__.__name__)\n\n    @classmethod\n    def from_country(\n        cls,\n        country: str,\n        api_token: str = global_config.OPENCELLID_ACCESS_TOKEN,\n        **kwargs,\n    ):\n        \"\"\"Create a downloader for a specific country\"\"\"\n        config = OpenCellIDConfig(country=country, api_token=api_token, **kwargs)\n        return cls(config=config)\n\n    def get_download_links(self) -&gt; List[str]:\n        \"\"\"Get download links for the country from OpenCellID website\"\"\"\n        url = f\"{self.config.DOWNLOAD_URL}{self.config.api_token}\"\n        country_alpha2 = pycountry.countries.get(\n            alpha_3=self.config.country.upper()\n        ).alpha_2\n\n        try:\n            # Find table with cell tower data links\n            self.logger.info(f\"Fetching download links for {self.config.country}\")\n            html_content = requests.get(url).text\n            soup = BeautifulSoup(html_content, \"lxml\")\n            table = soup.find(\"table\", {\"id\": \"regions\"})\n\n            if not table:\n                raise ValueError(\n                    \"Could not find cell tower data table on OpenCellID website\"\n                )\n\n            # Parse table headers\n            t_headers = []\n            for th in table.find_all(\"th\"):\n                t_headers.append(th.text.replace(\"\\n\", \" \").strip())\n\n            # Parse table data\n            table_data = []\n            for tr in table.tbody.find_all(\"tr\"):\n                t_row = {}\n\n                for td, th in zip(tr.find_all(\"td\"), t_headers):\n                    if \"Files\" in th:\n                        t_row[th] = []\n                        for a in td.find_all(\"a\"):\n                            t_row[th].append(a.get(\"href\"))\n                    else:\n                        t_row[th] = td.text.replace(\"\\n\", \"\").strip()\n\n                table_data.append(t_row)\n\n            cell_dict = pd.DataFrame(table_data)\n\n            # Get links for the country code\n            if country_alpha2 not in cell_dict[\"Country Code\"].values:\n                raise ValueError(\n                    f\"Country code {country_alpha2} not found in OpenCellID database\"\n                )\n            else:\n                links = cell_dict[cell_dict[\"Country Code\"] == country_alpha2][\n                    \"Files (grouped by MCC)\"\n                ].values[0]\n\n            return links\n\n        except Exception as e:\n            self.logger.error(f\"Error fetching download links: {str(e)}\")\n            raise\n\n    def download_and_process(self) -&gt; str:\n        \"\"\"Download and process OpenCellID data for the configured country\"\"\"\n\n        try:\n            links = self.get_download_links()\n            self.logger.info(f\"Found {len(links)} data files for {self.config.country}\")\n\n            dfs = []\n\n            for link in links:\n                self.logger.info(f\"Downloading data from {link}\")\n                response = requests.get(link, stream=True)\n                response.raise_for_status()\n\n                # Use a temporary file for download\n                with tempfile.NamedTemporaryFile(delete=False, suffix=\".gz\") as tmpfile:\n                    for chunk in response.iter_content(chunk_size=1024):\n                        if chunk:\n                            tmpfile.write(chunk)\n                    temp_file = tmpfile.name\n\n                try:\n                    # Read the downloaded gzipped CSV data\n                    with gzip.open(temp_file, \"rt\") as feed_data:\n                        dfs.append(\n                            pd.read_csv(\n                                feed_data,\n                                names=[\n                                    \"radio\",\n                                    \"mcc\",\n                                    \"net\",\n                                    \"area\",\n                                    \"cell\",\n                                    \"unit\",\n                                    \"lon\",\n                                    \"lat\",\n                                    \"range\",\n                                    \"samples\",\n                                    \"changeable\",\n                                    \"created\",\n                                    \"updated\",\n                                    \"average_signal\",\n                                ],\n                            )\n                        )\n                except IOError as e:\n                    with open(temp_file, \"r\") as error_file:\n                        contents = error_file.readline()\n\n                    if \"RATE_LIMITED\" in contents:\n                        raise RuntimeError(\n                            \"API rate limit exceeded. You're rate-limited!\"\n                        )\n                    elif \"INVALID_TOKEN\" in contents:\n                        raise RuntimeError(\"API token rejected by OpenCellID!\")\n                    else:\n                        raise RuntimeError(\n                            f\"Error processing downloaded data: {str(e)}\"\n                        )\n                finally:\n                    # Clean up temporary file\n                    if os.path.exists(temp_file):\n                        os.remove(temp_file)\n\n            df_cell = pd.concat(dfs, ignore_index=True)\n\n            # Process the data\n            if not df_cell.empty:\n                # Convert timestamps to datetime\n                df_cell[\"created\"] = pd.to_datetime(\n                    df_cell[\"created\"], unit=\"s\", origin=\"unix\"\n                )\n                df_cell[\"updated\"] = pd.to_datetime(\n                    df_cell[\"updated\"], unit=\"s\", origin=\"unix\"\n                )\n\n                # Filter by year\n                df_cell = df_cell[\n                    (df_cell.created.dt.year &gt;= self.config.created_newer)\n                    &amp; (df_cell.created.dt.year &lt; self.config.created_before)\n                ]\n\n                # Drop duplicates if configured\n                if self.config.drop_duplicates:\n                    df_cell = (\n                        df_cell.groupby([\"radio\", \"lon\", \"lat\"]).first().reset_index()\n                    )\n\n                # Save processed data using data_store\n                output_path = str(self.config.output_file_path)\n                self.logger.info(f\"Saving processed data to {output_path}\")\n                with self.data_store.open(output_path, \"wb\") as f:\n                    df_cell.to_csv(f, compression=\"gzip\", index=False)\n\n                return output_path\n            else:\n                raise ValueError(f\"No data found for {self.config.country}\")\n\n        except Exception as e:\n            self.logger.error(f\"Error downloading and processing data: {str(e)}\")\n            raise\n</code></pre>"},{"location":"api/handlers/#gigaspatial.handlers.opencellid.OpenCellIDDownloader.download_and_process","title":"<code>download_and_process()</code>","text":"<p>Download and process OpenCellID data for the configured country</p> Source code in <code>gigaspatial/handlers/opencellid.py</code> <pre><code>def download_and_process(self) -&gt; str:\n    \"\"\"Download and process OpenCellID data for the configured country\"\"\"\n\n    try:\n        links = self.get_download_links()\n        self.logger.info(f\"Found {len(links)} data files for {self.config.country}\")\n\n        dfs = []\n\n        for link in links:\n            self.logger.info(f\"Downloading data from {link}\")\n            response = requests.get(link, stream=True)\n            response.raise_for_status()\n\n            # Use a temporary file for download\n            with tempfile.NamedTemporaryFile(delete=False, suffix=\".gz\") as tmpfile:\n                for chunk in response.iter_content(chunk_size=1024):\n                    if chunk:\n                        tmpfile.write(chunk)\n                temp_file = tmpfile.name\n\n            try:\n                # Read the downloaded gzipped CSV data\n                with gzip.open(temp_file, \"rt\") as feed_data:\n                    dfs.append(\n                        pd.read_csv(\n                            feed_data,\n                            names=[\n                                \"radio\",\n                                \"mcc\",\n                                \"net\",\n                                \"area\",\n                                \"cell\",\n                                \"unit\",\n                                \"lon\",\n                                \"lat\",\n                                \"range\",\n                                \"samples\",\n                                \"changeable\",\n                                \"created\",\n                                \"updated\",\n                                \"average_signal\",\n                            ],\n                        )\n                    )\n            except IOError as e:\n                with open(temp_file, \"r\") as error_file:\n                    contents = error_file.readline()\n\n                if \"RATE_LIMITED\" in contents:\n                    raise RuntimeError(\n                        \"API rate limit exceeded. You're rate-limited!\"\n                    )\n                elif \"INVALID_TOKEN\" in contents:\n                    raise RuntimeError(\"API token rejected by OpenCellID!\")\n                else:\n                    raise RuntimeError(\n                        f\"Error processing downloaded data: {str(e)}\"\n                    )\n            finally:\n                # Clean up temporary file\n                if os.path.exists(temp_file):\n                    os.remove(temp_file)\n\n        df_cell = pd.concat(dfs, ignore_index=True)\n\n        # Process the data\n        if not df_cell.empty:\n            # Convert timestamps to datetime\n            df_cell[\"created\"] = pd.to_datetime(\n                df_cell[\"created\"], unit=\"s\", origin=\"unix\"\n            )\n            df_cell[\"updated\"] = pd.to_datetime(\n                df_cell[\"updated\"], unit=\"s\", origin=\"unix\"\n            )\n\n            # Filter by year\n            df_cell = df_cell[\n                (df_cell.created.dt.year &gt;= self.config.created_newer)\n                &amp; (df_cell.created.dt.year &lt; self.config.created_before)\n            ]\n\n            # Drop duplicates if configured\n            if self.config.drop_duplicates:\n                df_cell = (\n                    df_cell.groupby([\"radio\", \"lon\", \"lat\"]).first().reset_index()\n                )\n\n            # Save processed data using data_store\n            output_path = str(self.config.output_file_path)\n            self.logger.info(f\"Saving processed data to {output_path}\")\n            with self.data_store.open(output_path, \"wb\") as f:\n                df_cell.to_csv(f, compression=\"gzip\", index=False)\n\n            return output_path\n        else:\n            raise ValueError(f\"No data found for {self.config.country}\")\n\n    except Exception as e:\n        self.logger.error(f\"Error downloading and processing data: {str(e)}\")\n        raise\n</code></pre>"},{"location":"api/handlers/#gigaspatial.handlers.opencellid.OpenCellIDDownloader.from_country","title":"<code>from_country(country, api_token=global_config.OPENCELLID_ACCESS_TOKEN, **kwargs)</code>  <code>classmethod</code>","text":"<p>Create a downloader for a specific country</p> Source code in <code>gigaspatial/handlers/opencellid.py</code> <pre><code>@classmethod\ndef from_country(\n    cls,\n    country: str,\n    api_token: str = global_config.OPENCELLID_ACCESS_TOKEN,\n    **kwargs,\n):\n    \"\"\"Create a downloader for a specific country\"\"\"\n    config = OpenCellIDConfig(country=country, api_token=api_token, **kwargs)\n    return cls(config=config)\n</code></pre>"},{"location":"api/handlers/#gigaspatial.handlers.opencellid.OpenCellIDDownloader.get_download_links","title":"<code>get_download_links()</code>","text":"<p>Get download links for the country from OpenCellID website</p> Source code in <code>gigaspatial/handlers/opencellid.py</code> <pre><code>def get_download_links(self) -&gt; List[str]:\n    \"\"\"Get download links for the country from OpenCellID website\"\"\"\n    url = f\"{self.config.DOWNLOAD_URL}{self.config.api_token}\"\n    country_alpha2 = pycountry.countries.get(\n        alpha_3=self.config.country.upper()\n    ).alpha_2\n\n    try:\n        # Find table with cell tower data links\n        self.logger.info(f\"Fetching download links for {self.config.country}\")\n        html_content = requests.get(url).text\n        soup = BeautifulSoup(html_content, \"lxml\")\n        table = soup.find(\"table\", {\"id\": \"regions\"})\n\n        if not table:\n            raise ValueError(\n                \"Could not find cell tower data table on OpenCellID website\"\n            )\n\n        # Parse table headers\n        t_headers = []\n        for th in table.find_all(\"th\"):\n            t_headers.append(th.text.replace(\"\\n\", \" \").strip())\n\n        # Parse table data\n        table_data = []\n        for tr in table.tbody.find_all(\"tr\"):\n            t_row = {}\n\n            for td, th in zip(tr.find_all(\"td\"), t_headers):\n                if \"Files\" in th:\n                    t_row[th] = []\n                    for a in td.find_all(\"a\"):\n                        t_row[th].append(a.get(\"href\"))\n                else:\n                    t_row[th] = td.text.replace(\"\\n\", \"\").strip()\n\n            table_data.append(t_row)\n\n        cell_dict = pd.DataFrame(table_data)\n\n        # Get links for the country code\n        if country_alpha2 not in cell_dict[\"Country Code\"].values:\n            raise ValueError(\n                f\"Country code {country_alpha2} not found in OpenCellID database\"\n            )\n        else:\n            links = cell_dict[cell_dict[\"Country Code\"] == country_alpha2][\n                \"Files (grouped by MCC)\"\n            ].values[0]\n\n        return links\n\n    except Exception as e:\n        self.logger.error(f\"Error fetching download links: {str(e)}\")\n        raise\n</code></pre>"},{"location":"api/handlers/#gigaspatial.handlers.opencellid.OpenCellIDReader","title":"<code>OpenCellIDReader</code>","text":"<p>Reader for OpenCellID data</p> Source code in <code>gigaspatial/handlers/opencellid.py</code> <pre><code>class OpenCellIDReader:\n    \"\"\"Reader for OpenCellID data\"\"\"\n\n    def __init__(\n        self,\n        country: str,\n        data_store: Optional[DataStore] = None,\n        base_path: Optional[Path] = None,\n    ):\n        self.country = pycountry.countries.lookup(country).alpha_3\n        self.data_store = data_store or LocalDataStore()\n        self.base_path = base_path or global_config.get_path(\"opencellid\", \"bronze\")\n\n    def read_data(self) -&gt; pd.DataFrame:\n        \"\"\"Read OpenCellID data for the specified country\"\"\"\n        file_path = str(self.base_path / f\"opencellid_{self.country.lower()}.csv.gz\")\n\n        if not self.data_store.file_exists(file_path):\n            raise FileNotFoundError(\n                f\"OpenCellID data for {self.country} not found at {file_path}. \"\n                \"Download the data first using OpenCellIDDownloader.\"\n            )\n\n        return read_dataset(self.data_store, file_path)\n\n    def to_geodataframe(self) -&gt; gpd.GeoDataFrame:\n        \"\"\"Convert OpenCellID data to a GeoDataFrame\"\"\"\n        df = self.read_data()\n        gdf = gpd.GeoDataFrame(\n            df, geometry=gpd.points_from_xy(df.lon, df.lat), crs=\"EPSG:4326\"\n        )\n        return gdf\n</code></pre>"},{"location":"api/handlers/#gigaspatial.handlers.opencellid.OpenCellIDReader.read_data","title":"<code>read_data()</code>","text":"<p>Read OpenCellID data for the specified country</p> Source code in <code>gigaspatial/handlers/opencellid.py</code> <pre><code>def read_data(self) -&gt; pd.DataFrame:\n    \"\"\"Read OpenCellID data for the specified country\"\"\"\n    file_path = str(self.base_path / f\"opencellid_{self.country.lower()}.csv.gz\")\n\n    if not self.data_store.file_exists(file_path):\n        raise FileNotFoundError(\n            f\"OpenCellID data for {self.country} not found at {file_path}. \"\n            \"Download the data first using OpenCellIDDownloader.\"\n        )\n\n    return read_dataset(self.data_store, file_path)\n</code></pre>"},{"location":"api/handlers/#gigaspatial.handlers.opencellid.OpenCellIDReader.to_geodataframe","title":"<code>to_geodataframe()</code>","text":"<p>Convert OpenCellID data to a GeoDataFrame</p> Source code in <code>gigaspatial/handlers/opencellid.py</code> <pre><code>def to_geodataframe(self) -&gt; gpd.GeoDataFrame:\n    \"\"\"Convert OpenCellID data to a GeoDataFrame\"\"\"\n    df = self.read_data()\n    gdf = gpd.GeoDataFrame(\n        df, geometry=gpd.points_from_xy(df.lon, df.lat), crs=\"EPSG:4326\"\n    )\n    return gdf\n</code></pre>"},{"location":"api/handlers/#gigaspatial.handlers.osm","title":"<code>osm</code>","text":""},{"location":"api/handlers/#gigaspatial.handlers.osm.OSMLocationFetcher","title":"<code>OSMLocationFetcher</code>","text":"<p>A class to fetch and process location data from OpenStreetMap using the Overpass API.</p> <p>This class supports fetching various OSM location types including amenities, buildings, shops, and other POI categories.</p> Source code in <code>gigaspatial/handlers/osm.py</code> <pre><code>@dataclass\nclass OSMLocationFetcher:\n    \"\"\"\n    A class to fetch and process location data from OpenStreetMap using the Overpass API.\n\n    This class supports fetching various OSM location types including amenities, buildings,\n    shops, and other POI categories.\n    \"\"\"\n\n    country: Optional[str] = None\n    admin_level: Optional[int] = None\n    admin_value: Optional[str] = None\n    location_types: Union[List[str], Dict[str, List[str]]] = Field(...)\n    base_url: str = \"http://overpass-api.de/api/interpreter\"\n    timeout: int = 600\n    max_retries: int = 3\n    retry_delay: int = 5\n\n    def __post_init__(self):\n        \"\"\"Validate inputs, normalize location_types, and set up logging.\"\"\"\n\n        # Normalize location_types to always be a dictionary\n        if isinstance(self.location_types, list):\n            self.location_types = {\"amenity\": self.location_types}\n        elif not isinstance(self.location_types, dict):\n            raise TypeError(\n                \"location_types must be a list of strings or a dictionary mapping categories to type lists\"\n            )\n\n        self.logger = config.get_logger(self.__class__.__name__)\n\n        # Validate area selection\n        if self.admin_level is not None and self.admin_value is not None:\n            self.area_query = f'area[\"admin_level\"={self.admin_level}][\"name\"=\"{self.admin_value}\"]-&gt;.searchArea;'\n            self.logger.info(\n                f\"Using admin_level={self.admin_level}, name={self.admin_value} for area selection.\"\n            )\n        elif self.country is not None:\n            try:\n                self.country = pycountry.countries.lookup(self.country).alpha_2\n            except LookupError:\n                raise ValueError(f\"Invalid country code provided: {self.country}\")\n            self.area_query = f'area[\"ISO3166-1\"={self.country}]-&gt;.searchArea;'\n            self.logger.info(f\"Using country={self.country} for area selection.\")\n        else:\n            raise ValueError(\n                \"Either country or both admin_level and admin_value must be provided.\"\n            )\n\n    @staticmethod\n    def get_admin_names(\n        admin_level: int, country: Optional[str] = None, timeout: int = 120\n    ) -&gt; List[str]:\n        \"\"\"\n        Fetch all admin area names for a given admin_level (optionally within a country).\n\n        Args:\n            admin_level (int): The OSM admin_level to search for (e.g., 4 for states, 6 for counties).\n            country (str, optional): Country name or ISO code to filter within.\n            timeout (int): Timeout for the Overpass API request.\n\n        Returns:\n            List[str]: List of admin area names.\n        \"\"\"\n\n        # Build area filter for country if provided\n        if country:\n            try:\n                country_code = pycountry.countries.lookup(country).alpha_2\n            except LookupError:\n                raise ValueError(f\"Invalid country code or name: {country}\")\n            area_filter = f'area[\"ISO3166-1\"=\"{country_code}\"]-&gt;.countryArea;'\n            area_ref = \"(area.countryArea)\"\n        else:\n            area_filter = \"\"\n            area_ref = \"\"\n\n        # Overpass QL to get all admin areas at the specified level\n        query = f\"\"\"\n        [out:json][timeout:{timeout}];\n        {area_filter}\n        (\n          relation[\"admin_level\"=\"{admin_level}\"]{area_ref};\n        );\n        out tags;\n        \"\"\"\n\n        url = \"http://overpass-api.de/api/interpreter\"\n        response = requests.get(url, params={\"data\": query}, timeout=timeout)\n        response.raise_for_status()\n        data = response.json()\n\n        names = []\n        for el in data.get(\"elements\", []):\n            tags = el.get(\"tags\", {})\n            name = tags.get(\"name\")\n            if name:\n                names.append(name)\n        return sorted(set(names))\n\n    @staticmethod\n    def get_osm_countries(\n        iso3_code: Optional[str] = None, include_names: bool = True, timeout: int = 1000\n    ) -&gt; Union[str, Dict[str, str], List[str], List[Dict[str, str]]]:\n        \"\"\"\n        Fetch countries from OpenStreetMap database.\n\n        This queries the actual OSM database for country boundaries and returns\n        country names as they appear in OSM, including various name translations.\n\n        Args:\n            iso3_code (str, optional): ISO 3166-1 alpha-3 code to fetch a specific country.\n                                      If provided, returns single country data.\n                                      If None, returns all countries.\n            include_names (bool): If True, return dict with multiple name variants.\n                                 If False, return only the primary name.\n            timeout (int): Timeout for the Overpass API request (default: 1000).\n\n        Returns:\n            When iso3_code is provided:\n                - If include_names=False: Single country name (str)\n                - If include_names=True: Dict with name variants\n            When iso3_code is None:\n                - If include_names=False: List of country names\n                - If include_names=True: List of dicts with name variants including:\n                  name, name:en, ISO3166-1 codes, and other name translations\n\n        Raises:\n            ValueError: If iso3_code is provided but country not found in OSM.\n        \"\"\"\n        if iso3_code:\n            # Filter for the specific ISO3 code provided\n            iso3_upper = iso3_code.upper()\n            country_filter = f'[\"ISO3166-1:alpha3\"=\"{iso3_upper}\"]'\n        else:\n            # Filter for the *existence* of an ISO3 code tag to limit results to actual countries\n            country_filter = '[\"ISO3166-1:alpha3\"]'\n\n        # Query OSM for country-level boundaries\n        query = f\"\"\"\n        [out:json][timeout:{timeout}];\n        (\n          relation[\"boundary\"=\"administrative\"][\"admin_level\"=\"2\"]{country_filter};\n        );\n        out tags;\n        \"\"\"\n\n        url = \"http://overpass-api.de/api/interpreter\"\n        response = requests.get(url, params={\"data\": query}, timeout=timeout)\n        response.raise_for_status()\n        data = response.json()\n\n        countries = []\n        for element in data.get(\"elements\", []):\n            tags = element.get(\"tags\", {})\n\n            if include_names:\n                country_info = {\n                    \"name\": tags.get(\"name\", \"\"),\n                    \"name:en\": tags.get(\"name:en\", \"\"),\n                    \"official_name\": tags.get(\"official_name\", \"\"),\n                    \"official_name:en\": tags.get(\"official_name:en\", \"\"),\n                    \"ISO3166-1\": tags.get(\"ISO3166-1\", \"\"),\n                    \"ISO3166-1:alpha2\": tags.get(\"ISO3166-1:alpha2\", \"\"),\n                    \"ISO3166-1:alpha3\": tags.get(\"ISO3166-1:alpha3\", \"\"),\n                }\n\n                # Add any other name:* tags (translations)\n                for key, value in tags.items():\n                    if key.startswith(\"name:\") and key not in country_info:\n                        country_info[key] = value\n\n                # Remove empty string values\n                country_info = {k: v for k, v in country_info.items() if v}\n\n                if country_info.get(\"name\"):  # Only add if has a name\n                    countries.append(country_info)\n            else:\n                name = tags.get(\"name\")\n                if name:\n                    countries.append(name)\n\n        # If looking for a specific country, return single result or raise error\n        if iso3_code:\n            if not countries:\n                raise ValueError(\n                    f\"Country with ISO3 code '{iso3_code}' not found in OSM database\"\n                )\n            return countries[0]  # Return single country, not a list\n\n        # Return sorted list for all countries\n        return sorted(\n            countries, key=lambda x: x if isinstance(x, str) else x.get(\"name\", \"\")\n        )\n\n    def _make_request(self, query: str) -&gt; Dict:\n        \"\"\"Make HTTP request to Overpass API with retry mechanism.\"\"\"\n        for attempt in range(self.max_retries):\n            try:\n                self.logger.debug(f\"Executing query:\\n{query}\")\n                response = requests.get(\n                    self.base_url, params={\"data\": query}, timeout=self.timeout\n                )\n                response.raise_for_status()\n                return response.json()\n            except RequestException as e:\n                self.logger.warning(f\"Attempt {attempt + 1} failed: {str(e)}\")\n                if attempt &lt; self.max_retries - 1:\n                    sleep(self.retry_delay)\n                else:\n                    raise RuntimeError(\n                        f\"Failed to fetch data after {self.max_retries} attempts\"\n                    ) from e\n\n    def _extract_matching_categories(self, tags: Dict[str, str]) -&gt; Dict[str, str]:\n        \"\"\"\n        Extract all matching categories and their values from the tags.\n        Returns:\n            Dict mapping each matching category to its value\n        \"\"\"\n        matches = {}\n        for category, types in self.location_types.items():\n            if category in tags and tags[category] in types:\n                matches[category] = tags[category]\n        return matches\n\n    def _process_node_relation(self, element: Dict) -&gt; List[Dict[str, any]]:\n        \"\"\"\n        Process a node or relation element.\n        May return multiple processed elements if the element matches multiple categories.\n        \"\"\"\n        try:\n            tags = element.get(\"tags\", {})\n            matching_categories = self._extract_matching_categories(tags)\n\n            if not matching_categories:\n                self.logger.warning(\n                    f\"Element {element['id']} missing or not matching specified category tags\"\n                )\n                return []\n\n            _lat = element.get(\"lat\") or element[\"center\"][\"lat\"]\n            _lon = element.get(\"lon\") or element[\"center\"][\"lon\"]\n            point_geom = Point(_lon, _lat)\n\n            # Extract metadata if available\n            metadata = {}\n            if \"timestamp\" in element:\n                metadata[\"timestamp\"] = element[\"timestamp\"]\n                metadata[\"version\"] = element.get(\"version\")\n                metadata[\"changeset\"] = element.get(\"changeset\")\n                metadata[\"user\"] = element.get(\"user\")\n                metadata[\"uid\"] = element.get(\"uid\")\n\n            # For each matching category, create a separate element\n            results = []\n            for category, value in matching_categories.items():\n                result = {\n                    \"source_id\": element[\"id\"],\n                    \"category\": category,\n                    \"category_value\": value,\n                    \"name\": tags.get(\"name\", \"\"),\n                    \"name_en\": tags.get(\"name:en\", \"\"),\n                    \"type\": element[\"type\"],\n                    \"geometry\": point_geom,\n                    \"latitude\": _lat,\n                    \"longitude\": _lon,\n                    \"matching_categories\": list(matching_categories.keys()),\n                }\n                # Add metadata if available\n                result.update(metadata)\n                results.append(result)\n\n            return results\n\n        except KeyError as e:\n            self.logger.error(f\"Corrupt data received for node/relation: {str(e)}\")\n            return []\n\n    def _process_way(self, element: Dict) -&gt; List[Dict[str, any]]:\n        \"\"\"\n        Process a way element with geometry.\n        May return multiple processed elements if the element matches multiple categories.\n        \"\"\"\n        try:\n            tags = element.get(\"tags\", {})\n            matching_categories = self._extract_matching_categories(tags)\n\n            if not matching_categories:\n                self.logger.warning(\n                    f\"Element {element['id']} missing or not matching specified category tags\"\n                )\n                return []\n\n            # Create polygon from geometry points\n            polygon = Polygon([(p[\"lon\"], p[\"lat\"]) for p in element[\"geometry\"]])\n            centroid = polygon.centroid\n\n            # Extract metadata if available\n            metadata = {}\n            if \"timestamp\" in element:\n                metadata[\"timestamp\"] = element[\"timestamp\"]\n                metadata[\"version\"] = element.get(\"version\")\n                metadata[\"changeset\"] = element.get(\"changeset\")\n                metadata[\"user\"] = element.get(\"user\")\n                metadata[\"uid\"] = element.get(\"uid\")\n\n            # For each matching category, create a separate element\n            results = []\n            for category, value in matching_categories.items():\n                result = {\n                    \"source_id\": element[\"id\"],\n                    \"category\": category,\n                    \"category_value\": value,\n                    \"name\": tags.get(\"name\", \"\"),\n                    \"name_en\": tags.get(\"name:en\", \"\"),\n                    \"type\": element[\"type\"],\n                    \"geometry\": polygon,\n                    \"latitude\": centroid.y,\n                    \"longitude\": centroid.x,\n                    \"matching_categories\": list(matching_categories.keys()),\n                }\n                # Add metadata if available\n                result.update(metadata)\n                results.append(result)\n\n            return results\n        except (KeyError, ValueError) as e:\n            self.logger.error(f\"Error processing way geometry: {str(e)}\")\n            return []\n\n    def _build_queries(\n        self,\n        date_filter_type: Optional[Literal[\"newer\", \"changed\"]] = None,\n        start_date: Optional[str] = None,\n        end_date: Optional[str] = None,\n        include_metadata: bool = False,\n    ) -&gt; List[str]:\n        \"\"\"\n        Construct Overpass QL queries with optional date filtering and metadata.\n\n        Args:\n            date_filter_type: Type of date filter ('newer' or 'changed')\n            start_date: Start date in ISO 8601 format\n            end_date: End date in ISO 8601 format (required for 'changed')\n            include_metadata: If True, include change metadata (timestamp, version, changeset, user)\n\n        Returns:\n            List[str]: List of [nodes_relations_query, ways_query]\n        \"\"\"\n        # Build the date filter based on type\n        if date_filter_type == \"newer\" and start_date:\n            date_filter = f'(newer:\"{start_date}\")'\n        elif date_filter_type == \"changed\" and start_date and end_date:\n            date_filter = f'(changed:\"{start_date}\",\"{end_date}\")'\n        else:\n            date_filter = \"\"\n\n        # Determine output mode\n        output_mode = \"center meta\" if include_metadata else \"center\"\n        output_mode_geom = \"geom meta\" if include_metadata else \"geom\"\n\n        # Query for nodes and relations\n        nodes_relations_queries = []\n        for category, types in self.location_types.items():\n            nodes_relations_queries.extend(\n                [\n                    f\"\"\"node[\"{category}\"~\"^({\"|\".join(types)})\"]{date_filter}(area.searchArea);\"\"\",\n                    f\"\"\"relation[\"{category}\"~\"^({\"|\".join(types)})\"]{date_filter}(area.searchArea);\"\"\",\n                ]\n            )\n\n        nodes_relations_queries = \"\\n\".join(nodes_relations_queries)\n\n        nodes_relations_query = f\"\"\"\n        [out:json][timeout:{self.timeout}];\n        {self.area_query}\n        (\n            {nodes_relations_queries}\n        );\n        out {output_mode};\n        \"\"\"\n\n        # Query for ways\n        ways_queries = []\n        for category, types in self.location_types.items():\n            ways_queries.append(\n                f\"\"\"way[\"{category}\"~\"^({\"|\".join(types)})\"]{date_filter}(area.searchArea);\"\"\"\n            )\n\n        ways_queries = \"\\n\".join(ways_queries)\n\n        ways_query = f\"\"\"\n        [out:json][timeout:{self.timeout}];\n        {self.area_query}\n        (\n            {ways_queries}\n        );\n        out {output_mode_geom};\n        \"\"\"\n\n        return [nodes_relations_query, ways_query]\n\n    def fetch_locations(\n        self,\n        since_date: Optional[Union[str, datetime]] = None,\n        handle_duplicates: Literal[\"separate\", \"combine\", \"primary\"] = \"separate\",\n        include_metadata: bool = False,\n    ) -&gt; pd.DataFrame:\n        \"\"\"\n        Fetch OSM locations, optionally filtered by 'since' date.\n\n        Use this for incremental updates or getting all current locations.\n\n        Args:\n            since_year (int, optional): Filter for locations added/modified since this year.\n            handle_duplicates (str): How to handle objects matching multiple categories:\n                - 'separate': Create separate entries for each category (default)\n                - 'combine': Use a single entry with a list of matching categories\n                - 'primary': Keep only the first matching category\n            include_metadata: If True, include change tracking metadata\n                (timestamp, version, changeset, user, uid)\n\n        Returns:\n            pd.DataFrame: Processed OSM locations\n        \"\"\"\n        if handle_duplicates not in (\"separate\", \"combine\", \"primary\"):\n            raise ValueError(\n                \"handle_duplicates must be one of: 'separate', 'combine', 'primary'\"\n            )\n\n        self.logger.info(\n            f\"Fetching OSM locations from Overpass API for country: {self.country}\"\n        )\n        self.logger.info(f\"Location types: {self.location_types}\")\n\n        # Normalize date if provided\n        since_str = self._normalize_date(since_date) if since_date else None\n\n        if since_str:\n            self.logger.info(f\"Filtering for changes since: {since_str}\")\n\n        queries = self._build_queries(\n            date_filter_type=\"newer\" if since_str else None,\n            start_date=since_str,\n            include_metadata=include_metadata,\n        )\n\n        return self._execute_and_process_queries(queries, handle_duplicates)\n\n    def fetch_locations_changed_between(\n        self,\n        start_date: Union[str, datetime],\n        end_date: Union[str, datetime],\n        handle_duplicates: Literal[\"separate\", \"combine\", \"primary\"] = \"separate\",\n        include_metadata: bool = True,\n    ) -&gt; pd.DataFrame:\n        \"\"\"\n        Fetch OSM locations that changed within a specific date range.\n\n        Use this for historical analysis or tracking changes in a specific period.\n\n        Args:\n            start_date: Start date/time in ISO 8601 format (str: \"YYYY-MM-DDThh:mm:ssZ\")\n                    or datetime object. Changes after this date will be included.\n            end_date: End date/time in ISO 8601 format (str: \"YYYY-MM-DDThh:mm:ssZ\")\n                    or datetime object. Changes before this date will be included.\n            handle_duplicates: How to handle objects matching multiple categories:\n                - 'separate': Create separate entries for each category (default)\n                - 'combine': Use a single entry with a list of matching categories\n                - 'primary': Keep only the first matching category\n            include_metadata: If True, include change tracking metadata\n                (timestamp, version, changeset, user, uid)\n                Defaults to True since change tracking is the main use case.\n\n        Returns:\n            pd.DataFrame: Processed OSM locations that changed within the date range\n\n        Raises:\n            ValueError: If dates are invalid or start_date is after end_date\n        \"\"\"\n        start_str = self._normalize_date(start_date)\n        end_str = self._normalize_date(end_date)\n\n        if start_str &gt;= end_str:\n            raise ValueError(\n                f\"start_date must be before end_date (got {start_str} &gt;= {end_str})\"\n            )\n\n        queries = self._build_queries(\n            date_filter_type=\"changed\",\n            start_date=start_str,\n            end_date=end_str,\n            include_metadata=include_metadata,\n        )\n\n        return self._execute_and_process_queries(queries, handle_duplicates)\n\n    def _normalize_date(self, date_input: Union[str, datetime]) -&gt; str:\n        \"\"\"\n        Convert date input to ISO 8601 format string.\n\n        Args:\n            date_input: Either a string in ISO 8601 format or a datetime object\n\n        Returns:\n            str: Date in format \"YYYY-MM-DDThh:mm:ssZ\"\n\n        Raises:\n            ValueError: If string format is invalid\n        \"\"\"\n        from datetime import datetime\n\n        if isinstance(date_input, datetime):\n            # Convert datetime to ISO 8601 string with Z (UTC) timezone\n            return date_input.strftime(\"%Y-%m-%dT%H:%M:%SZ\")\n\n        elif isinstance(date_input, str):\n            # Validate the string format\n            try:\n                # Try to parse it to ensure it's valid\n                datetime.strptime(date_input, \"%Y-%m-%dT%H:%M:%SZ\")\n                return date_input\n            except ValueError:\n                raise ValueError(\n                    f\"Invalid date format: '{date_input}'. \"\n                    \"Expected format: 'YYYY-MM-DDThh:mm:ssZ' (e.g., '2024-03-15T14:30:00Z')\"\n                )\n        else:\n            raise TypeError(\n                f\"date_input must be str or datetime, got {type(date_input).__name__}\"\n            )\n\n    def _execute_and_process_queries(\n        self, queries: List[str], handle_duplicates: str\n    ) -&gt; pd.DataFrame:\n        \"\"\"\n        Execute queries and process results (extracted from fetch_locations).\n\n        Args:\n            queries: List of [nodes_relations_query, ways_query]\n            handle_duplicates: Strategy for handling duplicate categories\n\n        Returns:\n            pd.DataFrame: Processed locations\n        \"\"\"\n        nodes_relations_query, ways_query = queries\n\n        # Fetch nodes and relations\n        nodes_relations_response = self._make_request(nodes_relations_query)\n        nodes_relations = nodes_relations_response.get(\"elements\", [])\n\n        # Fetch ways\n        ways_response = self._make_request(ways_query)\n        ways = ways_response.get(\"elements\", [])\n\n        if not nodes_relations and not ways:\n            self.logger.warning(\"No locations found for the specified criteria\")\n            return pd.DataFrame()\n\n        self.logger.info(\n            f\"Processing {len(nodes_relations)} nodes/relations and {len(ways)} ways...\"\n        )\n\n        # Process nodes and relations\n        with ThreadPoolExecutor() as executor:\n            processed_nodes_relations = [\n                item\n                for sublist in executor.map(\n                    self._process_node_relation, nodes_relations\n                )\n                for item in sublist\n            ]\n\n        # Process ways\n        with ThreadPoolExecutor() as executor:\n            processed_ways = [\n                item\n                for sublist in executor.map(self._process_way, ways)\n                for item in sublist\n            ]\n\n        # Combine all processed elements\n        all_elements = processed_nodes_relations + processed_ways\n\n        if not all_elements:\n            self.logger.warning(\"No matching elements found after processing\")\n            return pd.DataFrame()\n\n        # Handle duplicates (reuse existing logic from fetch_locations)\n        if handle_duplicates != \"separate\":\n            grouped_elements = {}\n            for elem in all_elements:\n                source_id = elem[\"source_id\"]\n                if source_id not in grouped_elements:\n                    grouped_elements[source_id] = elem\n                elif handle_duplicates == \"combine\":\n                    if grouped_elements[source_id][\"category\"] != elem[\"category\"]:\n                        if isinstance(grouped_elements[source_id][\"category\"], str):\n                            grouped_elements[source_id][\"category\"] = [\n                                grouped_elements[source_id][\"category\"]\n                            ]\n                            grouped_elements[source_id][\"category_value\"] = [\n                                grouped_elements[source_id][\"category_value\"]\n                            ]\n\n                        if (\n                            elem[\"category\"]\n                            not in grouped_elements[source_id][\"category\"]\n                        ):\n                            grouped_elements[source_id][\"category\"].append(\n                                elem[\"category\"]\n                            )\n                            grouped_elements[source_id][\"category_value\"].append(\n                                elem[\"category_value\"]\n                            )\n\n            all_elements = list(grouped_elements.values())\n\n        locations = pd.DataFrame(all_elements)\n\n        # Log statistics\n        type_counts = locations[\"type\"].value_counts()\n        self.logger.info(\"\\nElement type distribution:\")\n        for element_type, count in type_counts.items():\n            self.logger.info(f\"{element_type}: {count}\")\n\n        self.logger.info(f\"Successfully processed {len(locations)} locations\")\n        return locations\n</code></pre>"},{"location":"api/handlers/#gigaspatial.handlers.osm.OSMLocationFetcher.__post_init__","title":"<code>__post_init__()</code>","text":"<p>Validate inputs, normalize location_types, and set up logging.</p> Source code in <code>gigaspatial/handlers/osm.py</code> <pre><code>def __post_init__(self):\n    \"\"\"Validate inputs, normalize location_types, and set up logging.\"\"\"\n\n    # Normalize location_types to always be a dictionary\n    if isinstance(self.location_types, list):\n        self.location_types = {\"amenity\": self.location_types}\n    elif not isinstance(self.location_types, dict):\n        raise TypeError(\n            \"location_types must be a list of strings or a dictionary mapping categories to type lists\"\n        )\n\n    self.logger = config.get_logger(self.__class__.__name__)\n\n    # Validate area selection\n    if self.admin_level is not None and self.admin_value is not None:\n        self.area_query = f'area[\"admin_level\"={self.admin_level}][\"name\"=\"{self.admin_value}\"]-&gt;.searchArea;'\n        self.logger.info(\n            f\"Using admin_level={self.admin_level}, name={self.admin_value} for area selection.\"\n        )\n    elif self.country is not None:\n        try:\n            self.country = pycountry.countries.lookup(self.country).alpha_2\n        except LookupError:\n            raise ValueError(f\"Invalid country code provided: {self.country}\")\n        self.area_query = f'area[\"ISO3166-1\"={self.country}]-&gt;.searchArea;'\n        self.logger.info(f\"Using country={self.country} for area selection.\")\n    else:\n        raise ValueError(\n            \"Either country or both admin_level and admin_value must be provided.\"\n        )\n</code></pre>"},{"location":"api/handlers/#gigaspatial.handlers.osm.OSMLocationFetcher.fetch_locations","title":"<code>fetch_locations(since_date=None, handle_duplicates='separate', include_metadata=False)</code>","text":"<p>Fetch OSM locations, optionally filtered by 'since' date.</p> <p>Use this for incremental updates or getting all current locations.</p> <p>Parameters:</p> Name Type Description Default <code>since_year</code> <code>int</code> <p>Filter for locations added/modified since this year.</p> required <code>handle_duplicates</code> <code>str</code> <p>How to handle objects matching multiple categories: - 'separate': Create separate entries for each category (default) - 'combine': Use a single entry with a list of matching categories - 'primary': Keep only the first matching category</p> <code>'separate'</code> <code>include_metadata</code> <code>bool</code> <p>If True, include change tracking metadata (timestamp, version, changeset, user, uid)</p> <code>False</code> <p>Returns:</p> Type Description <code>DataFrame</code> <p>pd.DataFrame: Processed OSM locations</p> Source code in <code>gigaspatial/handlers/osm.py</code> <pre><code>def fetch_locations(\n    self,\n    since_date: Optional[Union[str, datetime]] = None,\n    handle_duplicates: Literal[\"separate\", \"combine\", \"primary\"] = \"separate\",\n    include_metadata: bool = False,\n) -&gt; pd.DataFrame:\n    \"\"\"\n    Fetch OSM locations, optionally filtered by 'since' date.\n\n    Use this for incremental updates or getting all current locations.\n\n    Args:\n        since_year (int, optional): Filter for locations added/modified since this year.\n        handle_duplicates (str): How to handle objects matching multiple categories:\n            - 'separate': Create separate entries for each category (default)\n            - 'combine': Use a single entry with a list of matching categories\n            - 'primary': Keep only the first matching category\n        include_metadata: If True, include change tracking metadata\n            (timestamp, version, changeset, user, uid)\n\n    Returns:\n        pd.DataFrame: Processed OSM locations\n    \"\"\"\n    if handle_duplicates not in (\"separate\", \"combine\", \"primary\"):\n        raise ValueError(\n            \"handle_duplicates must be one of: 'separate', 'combine', 'primary'\"\n        )\n\n    self.logger.info(\n        f\"Fetching OSM locations from Overpass API for country: {self.country}\"\n    )\n    self.logger.info(f\"Location types: {self.location_types}\")\n\n    # Normalize date if provided\n    since_str = self._normalize_date(since_date) if since_date else None\n\n    if since_str:\n        self.logger.info(f\"Filtering for changes since: {since_str}\")\n\n    queries = self._build_queries(\n        date_filter_type=\"newer\" if since_str else None,\n        start_date=since_str,\n        include_metadata=include_metadata,\n    )\n\n    return self._execute_and_process_queries(queries, handle_duplicates)\n</code></pre>"},{"location":"api/handlers/#gigaspatial.handlers.osm.OSMLocationFetcher.fetch_locations_changed_between","title":"<code>fetch_locations_changed_between(start_date, end_date, handle_duplicates='separate', include_metadata=True)</code>","text":"<p>Fetch OSM locations that changed within a specific date range.</p> <p>Use this for historical analysis or tracking changes in a specific period.</p> <p>Parameters:</p> Name Type Description Default <code>start_date</code> <code>Union[str, datetime]</code> <p>Start date/time in ISO 8601 format (str: \"YYYY-MM-DDThh:mm:ssZ\")     or datetime object. Changes after this date will be included.</p> required <code>end_date</code> <code>Union[str, datetime]</code> <p>End date/time in ISO 8601 format (str: \"YYYY-MM-DDThh:mm:ssZ\")     or datetime object. Changes before this date will be included.</p> required <code>handle_duplicates</code> <code>Literal['separate', 'combine', 'primary']</code> <p>How to handle objects matching multiple categories: - 'separate': Create separate entries for each category (default) - 'combine': Use a single entry with a list of matching categories - 'primary': Keep only the first matching category</p> <code>'separate'</code> <code>include_metadata</code> <code>bool</code> <p>If True, include change tracking metadata (timestamp, version, changeset, user, uid) Defaults to True since change tracking is the main use case.</p> <code>True</code> <p>Returns:</p> Type Description <code>DataFrame</code> <p>pd.DataFrame: Processed OSM locations that changed within the date range</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If dates are invalid or start_date is after end_date</p> Source code in <code>gigaspatial/handlers/osm.py</code> <pre><code>def fetch_locations_changed_between(\n    self,\n    start_date: Union[str, datetime],\n    end_date: Union[str, datetime],\n    handle_duplicates: Literal[\"separate\", \"combine\", \"primary\"] = \"separate\",\n    include_metadata: bool = True,\n) -&gt; pd.DataFrame:\n    \"\"\"\n    Fetch OSM locations that changed within a specific date range.\n\n    Use this for historical analysis or tracking changes in a specific period.\n\n    Args:\n        start_date: Start date/time in ISO 8601 format (str: \"YYYY-MM-DDThh:mm:ssZ\")\n                or datetime object. Changes after this date will be included.\n        end_date: End date/time in ISO 8601 format (str: \"YYYY-MM-DDThh:mm:ssZ\")\n                or datetime object. Changes before this date will be included.\n        handle_duplicates: How to handle objects matching multiple categories:\n            - 'separate': Create separate entries for each category (default)\n            - 'combine': Use a single entry with a list of matching categories\n            - 'primary': Keep only the first matching category\n        include_metadata: If True, include change tracking metadata\n            (timestamp, version, changeset, user, uid)\n            Defaults to True since change tracking is the main use case.\n\n    Returns:\n        pd.DataFrame: Processed OSM locations that changed within the date range\n\n    Raises:\n        ValueError: If dates are invalid or start_date is after end_date\n    \"\"\"\n    start_str = self._normalize_date(start_date)\n    end_str = self._normalize_date(end_date)\n\n    if start_str &gt;= end_str:\n        raise ValueError(\n            f\"start_date must be before end_date (got {start_str} &gt;= {end_str})\"\n        )\n\n    queries = self._build_queries(\n        date_filter_type=\"changed\",\n        start_date=start_str,\n        end_date=end_str,\n        include_metadata=include_metadata,\n    )\n\n    return self._execute_and_process_queries(queries, handle_duplicates)\n</code></pre>"},{"location":"api/handlers/#gigaspatial.handlers.osm.OSMLocationFetcher.get_admin_names","title":"<code>get_admin_names(admin_level, country=None, timeout=120)</code>  <code>staticmethod</code>","text":"<p>Fetch all admin area names for a given admin_level (optionally within a country).</p> <p>Parameters:</p> Name Type Description Default <code>admin_level</code> <code>int</code> <p>The OSM admin_level to search for (e.g., 4 for states, 6 for counties).</p> required <code>country</code> <code>str</code> <p>Country name or ISO code to filter within.</p> <code>None</code> <code>timeout</code> <code>int</code> <p>Timeout for the Overpass API request.</p> <code>120</code> <p>Returns:</p> Type Description <code>List[str]</code> <p>List[str]: List of admin area names.</p> Source code in <code>gigaspatial/handlers/osm.py</code> <pre><code>@staticmethod\ndef get_admin_names(\n    admin_level: int, country: Optional[str] = None, timeout: int = 120\n) -&gt; List[str]:\n    \"\"\"\n    Fetch all admin area names for a given admin_level (optionally within a country).\n\n    Args:\n        admin_level (int): The OSM admin_level to search for (e.g., 4 for states, 6 for counties).\n        country (str, optional): Country name or ISO code to filter within.\n        timeout (int): Timeout for the Overpass API request.\n\n    Returns:\n        List[str]: List of admin area names.\n    \"\"\"\n\n    # Build area filter for country if provided\n    if country:\n        try:\n            country_code = pycountry.countries.lookup(country).alpha_2\n        except LookupError:\n            raise ValueError(f\"Invalid country code or name: {country}\")\n        area_filter = f'area[\"ISO3166-1\"=\"{country_code}\"]-&gt;.countryArea;'\n        area_ref = \"(area.countryArea)\"\n    else:\n        area_filter = \"\"\n        area_ref = \"\"\n\n    # Overpass QL to get all admin areas at the specified level\n    query = f\"\"\"\n    [out:json][timeout:{timeout}];\n    {area_filter}\n    (\n      relation[\"admin_level\"=\"{admin_level}\"]{area_ref};\n    );\n    out tags;\n    \"\"\"\n\n    url = \"http://overpass-api.de/api/interpreter\"\n    response = requests.get(url, params={\"data\": query}, timeout=timeout)\n    response.raise_for_status()\n    data = response.json()\n\n    names = []\n    for el in data.get(\"elements\", []):\n        tags = el.get(\"tags\", {})\n        name = tags.get(\"name\")\n        if name:\n            names.append(name)\n    return sorted(set(names))\n</code></pre>"},{"location":"api/handlers/#gigaspatial.handlers.osm.OSMLocationFetcher.get_osm_countries","title":"<code>get_osm_countries(iso3_code=None, include_names=True, timeout=1000)</code>  <code>staticmethod</code>","text":"<p>Fetch countries from OpenStreetMap database.</p> <p>This queries the actual OSM database for country boundaries and returns country names as they appear in OSM, including various name translations.</p> <p>Parameters:</p> Name Type Description Default <code>iso3_code</code> <code>str</code> <p>ISO 3166-1 alpha-3 code to fetch a specific country.                       If provided, returns single country data.                       If None, returns all countries.</p> <code>None</code> <code>include_names</code> <code>bool</code> <p>If True, return dict with multiple name variants.                  If False, return only the primary name.</p> <code>True</code> <code>timeout</code> <code>int</code> <p>Timeout for the Overpass API request (default: 1000).</p> <code>1000</code> <p>Returns:</p> Type Description <code>Union[str, Dict[str, str], List[str], List[Dict[str, str]]]</code> <p>When iso3_code is provided: - If include_names=False: Single country name (str) - If include_names=True: Dict with name variants</p> <code>Union[str, Dict[str, str], List[str], List[Dict[str, str]]]</code> <p>When iso3_code is None: - If include_names=False: List of country names - If include_names=True: List of dicts with name variants including:   name, name:en, ISO3166-1 codes, and other name translations</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If iso3_code is provided but country not found in OSM.</p> Source code in <code>gigaspatial/handlers/osm.py</code> <pre><code>@staticmethod\ndef get_osm_countries(\n    iso3_code: Optional[str] = None, include_names: bool = True, timeout: int = 1000\n) -&gt; Union[str, Dict[str, str], List[str], List[Dict[str, str]]]:\n    \"\"\"\n    Fetch countries from OpenStreetMap database.\n\n    This queries the actual OSM database for country boundaries and returns\n    country names as they appear in OSM, including various name translations.\n\n    Args:\n        iso3_code (str, optional): ISO 3166-1 alpha-3 code to fetch a specific country.\n                                  If provided, returns single country data.\n                                  If None, returns all countries.\n        include_names (bool): If True, return dict with multiple name variants.\n                             If False, return only the primary name.\n        timeout (int): Timeout for the Overpass API request (default: 1000).\n\n    Returns:\n        When iso3_code is provided:\n            - If include_names=False: Single country name (str)\n            - If include_names=True: Dict with name variants\n        When iso3_code is None:\n            - If include_names=False: List of country names\n            - If include_names=True: List of dicts with name variants including:\n              name, name:en, ISO3166-1 codes, and other name translations\n\n    Raises:\n        ValueError: If iso3_code is provided but country not found in OSM.\n    \"\"\"\n    if iso3_code:\n        # Filter for the specific ISO3 code provided\n        iso3_upper = iso3_code.upper()\n        country_filter = f'[\"ISO3166-1:alpha3\"=\"{iso3_upper}\"]'\n    else:\n        # Filter for the *existence* of an ISO3 code tag to limit results to actual countries\n        country_filter = '[\"ISO3166-1:alpha3\"]'\n\n    # Query OSM for country-level boundaries\n    query = f\"\"\"\n    [out:json][timeout:{timeout}];\n    (\n      relation[\"boundary\"=\"administrative\"][\"admin_level\"=\"2\"]{country_filter};\n    );\n    out tags;\n    \"\"\"\n\n    url = \"http://overpass-api.de/api/interpreter\"\n    response = requests.get(url, params={\"data\": query}, timeout=timeout)\n    response.raise_for_status()\n    data = response.json()\n\n    countries = []\n    for element in data.get(\"elements\", []):\n        tags = element.get(\"tags\", {})\n\n        if include_names:\n            country_info = {\n                \"name\": tags.get(\"name\", \"\"),\n                \"name:en\": tags.get(\"name:en\", \"\"),\n                \"official_name\": tags.get(\"official_name\", \"\"),\n                \"official_name:en\": tags.get(\"official_name:en\", \"\"),\n                \"ISO3166-1\": tags.get(\"ISO3166-1\", \"\"),\n                \"ISO3166-1:alpha2\": tags.get(\"ISO3166-1:alpha2\", \"\"),\n                \"ISO3166-1:alpha3\": tags.get(\"ISO3166-1:alpha3\", \"\"),\n            }\n\n            # Add any other name:* tags (translations)\n            for key, value in tags.items():\n                if key.startswith(\"name:\") and key not in country_info:\n                    country_info[key] = value\n\n            # Remove empty string values\n            country_info = {k: v for k, v in country_info.items() if v}\n\n            if country_info.get(\"name\"):  # Only add if has a name\n                countries.append(country_info)\n        else:\n            name = tags.get(\"name\")\n            if name:\n                countries.append(name)\n\n    # If looking for a specific country, return single result or raise error\n    if iso3_code:\n        if not countries:\n            raise ValueError(\n                f\"Country with ISO3 code '{iso3_code}' not found in OSM database\"\n            )\n        return countries[0]  # Return single country, not a list\n\n    # Return sorted list for all countries\n    return sorted(\n        countries, key=lambda x: x if isinstance(x, str) else x.get(\"name\", \"\")\n    )\n</code></pre>"},{"location":"api/handlers/#gigaspatial.handlers.overture","title":"<code>overture</code>","text":""},{"location":"api/handlers/#gigaspatial.handlers.overture.OvertureAmenityFetcher","title":"<code>OvertureAmenityFetcher</code>","text":"<p>A class to fetch and process amenity locations from Overture.</p> Source code in <code>gigaspatial/handlers/overture.py</code> <pre><code>@dataclass(config=ConfigDict(arbitrary_types_allowed=True))\nclass OvertureAmenityFetcher:\n    \"\"\"\n    A class to fetch and process amenity locations from Overture.\n    \"\"\"\n\n    # constants\n    release: Optional[str] = \"2024-12-18.0\"\n    base_url: Optional[str] = (\n        \"s3://overturemaps-us-west-2/release/{release}/theme=places/*/*\"\n    )\n\n    # user config\n    country: str = Field(...)\n    amenity_types: List[str] = Field(..., description=\"List of amenity types to fetch\")\n    geom: Union[Polygon, MultiPolygon] = None\n\n    # config for country boundary access from data storage\n    # if None GADM boundaries will be used\n    data_store: DataStore = None\n    country_geom_path: Optional[Union[str, Path]] = None\n\n    def __post_init__(self):\n        \"\"\"Validate inputs and set up logging.\"\"\"\n        try:\n            self.country = pycountry.countries.lookup(self.country).alpha_2\n        except LookupError:\n            raise ValueError(f\"Invalid country code provided: {self.country}\")\n\n        self.base_url = self.base_url.format(release=self.release)\n        self.logger = config.get_logger(self.__class__.__name__)\n\n        self.connection = self._set_connection()\n\n    def _set_connection(self):\n        \"\"\"Set the connection to the DB\"\"\"\n        db = duckdb.connect()\n        db.install_extension(\"spatial\")\n        db.load_extension(\"spatial\")\n        return db\n\n    def _load_country_geometry(\n        self,\n    ) -&gt; Union[Polygon, MultiPolygon]:\n        \"\"\"Load country boundary geometry from DataStore or GADM.\"\"\"\n\n        gdf_admin0 = AdminBoundaries.create(\n            country_code=pycountry.countries.lookup(self.country).alpha_3,\n            admin_level=0,\n            data_store=self.data_store,\n            path=self.country_geom_path,\n        ).to_geodataframe()\n\n        return gdf_admin0.geometry.iloc[0]\n\n    def _build_query(self, match_pattern: bool = False, **kwargs) -&gt; str:\n        \"\"\"Constructs and returns the query\"\"\"\n\n        if match_pattern:\n            amenity_query = \" OR \".join(\n                [f\"category ilike '%{amenity}%'\" for amenity in self.amenity_types]\n            )\n        else:\n            amenity_query = \" OR \".join(\n                [f\"category == '{amenity}'\" for amenity in self.amenity_types]\n            )\n\n        query = \"\"\"\n        SELECT id,\n            names.primary AS name,\n            ROUND(confidence,2) as confidence,\n            categories.primary AS category,\n            ST_AsText(geometry) as geometry,\n        FROM read_parquet('s3://overturemaps-us-west-2/release/2024-12-18.0/theme=places/type=place/*',\n            hive_partitioning=1)\n        WHERE bbox.xmin &gt; {}\n            AND bbox.ymin &gt; {} \n            AND bbox.xmax &lt;  {}\n            AND bbox.ymax &lt; {}\n            AND ({})\n        \"\"\"\n\n        if not self.geom:\n            self.geom = self._load_country_geometry()\n\n        return query.format(*self.geom.bounds, amenity_query)\n\n    def fetch_locations(\n        self, match_pattern: bool = False, **kwargs\n    ) -&gt; gpd.GeoDataFrame:\n        \"\"\"Fetch and process amenity locations.\"\"\"\n        self.logger.info(\"Fetching amenity locations from Overture DB...\")\n\n        query = self._build_query(match_pattern=match_pattern, **kwargs)\n\n        df = self.connection.execute(query).df()\n\n        self.logger.info(\"Processing geometries\")\n        gdf = gpd.GeoDataFrame(\n            df, geometry=gpd.GeoSeries.from_wkt(df[\"geometry\"]), crs=\"EPSG:4326\"\n        )\n\n        # filter by geometry boundary\n        s = STRtree(gdf.geometry)\n        result = s.query(self.geom, predicate=\"intersects\")\n\n        locations = gdf.iloc[result].reset_index(drop=True)\n\n        self.logger.info(f\"Successfully processed {len(locations)} amenity locations\")\n        return locations\n</code></pre>"},{"location":"api/handlers/#gigaspatial.handlers.overture.OvertureAmenityFetcher.__post_init__","title":"<code>__post_init__()</code>","text":"<p>Validate inputs and set up logging.</p> Source code in <code>gigaspatial/handlers/overture.py</code> <pre><code>def __post_init__(self):\n    \"\"\"Validate inputs and set up logging.\"\"\"\n    try:\n        self.country = pycountry.countries.lookup(self.country).alpha_2\n    except LookupError:\n        raise ValueError(f\"Invalid country code provided: {self.country}\")\n\n    self.base_url = self.base_url.format(release=self.release)\n    self.logger = config.get_logger(self.__class__.__name__)\n\n    self.connection = self._set_connection()\n</code></pre>"},{"location":"api/handlers/#gigaspatial.handlers.overture.OvertureAmenityFetcher.fetch_locations","title":"<code>fetch_locations(match_pattern=False, **kwargs)</code>","text":"<p>Fetch and process amenity locations.</p> Source code in <code>gigaspatial/handlers/overture.py</code> <pre><code>def fetch_locations(\n    self, match_pattern: bool = False, **kwargs\n) -&gt; gpd.GeoDataFrame:\n    \"\"\"Fetch and process amenity locations.\"\"\"\n    self.logger.info(\"Fetching amenity locations from Overture DB...\")\n\n    query = self._build_query(match_pattern=match_pattern, **kwargs)\n\n    df = self.connection.execute(query).df()\n\n    self.logger.info(\"Processing geometries\")\n    gdf = gpd.GeoDataFrame(\n        df, geometry=gpd.GeoSeries.from_wkt(df[\"geometry\"]), crs=\"EPSG:4326\"\n    )\n\n    # filter by geometry boundary\n    s = STRtree(gdf.geometry)\n    result = s.query(self.geom, predicate=\"intersects\")\n\n    locations = gdf.iloc[result].reset_index(drop=True)\n\n    self.logger.info(f\"Successfully processed {len(locations)} amenity locations\")\n    return locations\n</code></pre>"},{"location":"api/handlers/#gigaspatial.handlers.rwi","title":"<code>rwi</code>","text":""},{"location":"api/handlers/#gigaspatial.handlers.rwi.RWIConfig","title":"<code>RWIConfig</code>  <code>dataclass</code>","text":"<p>               Bases: <code>HDXConfig</code></p> <p>Configuration for Relative Wealth Index data access</p> Source code in <code>gigaspatial/handlers/rwi.py</code> <pre><code>@dataclass(config=ConfigDict(arbitrary_types_allowed=True))\nclass RWIConfig(HDXConfig):\n    \"\"\"Configuration for Relative Wealth Index data access\"\"\"\n\n    # Override dataset_name to be fixed for RWI\n    dataset_name: Literal[\"relative-wealth-index\"] = Field(\n        default=\"relative-wealth-index\"\n    )\n\n    # Additional RWI-specific configurations\n    country: Optional[str] = Field(\n        default=None, description=\"Country ISO code to filter data for\"\n    )\n    latest_only: bool = Field(\n        default=True,\n        description=\"If True, only get the latest resource for each country\",\n    )\n\n    def __post_init__(self):\n        super().__post_init__()\n\n    def get_relevant_data_units(\n        self, source: str, force_recompute: bool = False, **kwargs\n    ):\n        key = self._cache_key(source, **kwargs)\n        resources = super().get_relevant_data_units(source, force_recompute, **kwargs)\n\n        if self.latest_only and len(resources) &gt; 1:\n            # Find the resource with the latest creation date\n            latest_resource = None\n            latest_date = None\n\n            for resource in resources:\n                created = resource.get(\"created\")\n                if created:\n                    try:\n                        created_dt = datetime.fromisoformat(\n                            created.replace(\"Z\", \"+00:00\")\n                        )\n                        if latest_date is None or created_dt &gt; latest_date:\n                            latest_date = created_dt\n                            latest_resource = resource\n                    except ValueError:\n                        self.logger.warning(\n                            f\"Could not parse creation date for resource: {created}\"\n                        )\n\n            if latest_resource:\n                resources = [latest_resource]\n\n            # Update the cache to the latest only\n            self._unit_cache[key] = resources\n            return resources\n\n        return resources\n</code></pre>"},{"location":"api/handlers/#gigaspatial.handlers.rwi.RWIDownloader","title":"<code>RWIDownloader</code>","text":"<p>               Bases: <code>HDXDownloader</code></p> <p>Specialized downloader for the Relative Wealth Index dataset from HDX</p> Source code in <code>gigaspatial/handlers/rwi.py</code> <pre><code>class RWIDownloader(HDXDownloader):\n    \"\"\"Specialized downloader for the Relative Wealth Index dataset from HDX\"\"\"\n\n    def __init__(\n        self,\n        config: Union[RWIConfig, dict] = None,\n        data_store: Optional[DataStore] = None,\n        logger: Optional[logging.Logger] = None,\n    ):\n        config = config if isinstance(config, RWIConfig) else RWIConfig(**config)\n        super().__init__(config=config, data_store=data_store, logger=logger)\n</code></pre>"},{"location":"api/handlers/#gigaspatial.handlers.rwi.RWIHandler","title":"<code>RWIHandler</code>","text":"<p>               Bases: <code>HDXHandler</code></p> <p>Handler for Relative Wealth Index dataset</p> Source code in <code>gigaspatial/handlers/rwi.py</code> <pre><code>class RWIHandler(HDXHandler):\n    \"\"\"Handler for Relative Wealth Index dataset\"\"\"\n\n    def __init__(\n        self,\n        config: Optional[RWIConfig] = None,\n        downloader: Optional[RWIDownloader] = None,\n        reader: Optional[RWIReader] = None,\n        data_store: Optional[DataStore] = None,\n        logger: Optional[logging.Logger] = None,\n        **kwargs,\n    ):\n        super().__init__(\n            dataset_name=\"relative-wealth-index\",\n            config=config,\n            downloader=downloader,\n            reader=reader,\n            data_store=data_store,\n            logger=logger,\n            **kwargs,\n        )\n\n    def create_config(\n        self, data_store: DataStore, logger: logging.Logger, **kwargs\n    ) -&gt; RWIConfig:\n        \"\"\"Create and return a RWIConfig instance\"\"\"\n        return RWIConfig(\n            data_store=data_store,\n            logger=logger,\n            **kwargs,\n        )\n\n    def create_downloader(\n        self,\n        config: RWIConfig,\n        data_store: DataStore,\n        logger: logging.Logger,\n        **kwargs,\n    ) -&gt; RWIDownloader:\n        \"\"\"Create and return a RWIDownloader instance\"\"\"\n        return RWIDownloader(\n            config=config,\n            data_store=data_store,\n            logger=logger,\n            **kwargs,\n        )\n\n    def create_reader(\n        self,\n        config: RWIConfig,\n        data_store: DataStore,\n        logger: logging.Logger,\n        **kwargs,\n    ) -&gt; RWIReader:\n        \"\"\"Create and return a RWIReader instance\"\"\"\n        return RWIReader(\n            config=config,\n            data_store=data_store,\n            logger=logger,\n            **kwargs,\n        )\n</code></pre>"},{"location":"api/handlers/#gigaspatial.handlers.rwi.RWIHandler.create_config","title":"<code>create_config(data_store, logger, **kwargs)</code>","text":"<p>Create and return a RWIConfig instance</p> Source code in <code>gigaspatial/handlers/rwi.py</code> <pre><code>def create_config(\n    self, data_store: DataStore, logger: logging.Logger, **kwargs\n) -&gt; RWIConfig:\n    \"\"\"Create and return a RWIConfig instance\"\"\"\n    return RWIConfig(\n        data_store=data_store,\n        logger=logger,\n        **kwargs,\n    )\n</code></pre>"},{"location":"api/handlers/#gigaspatial.handlers.rwi.RWIHandler.create_downloader","title":"<code>create_downloader(config, data_store, logger, **kwargs)</code>","text":"<p>Create and return a RWIDownloader instance</p> Source code in <code>gigaspatial/handlers/rwi.py</code> <pre><code>def create_downloader(\n    self,\n    config: RWIConfig,\n    data_store: DataStore,\n    logger: logging.Logger,\n    **kwargs,\n) -&gt; RWIDownloader:\n    \"\"\"Create and return a RWIDownloader instance\"\"\"\n    return RWIDownloader(\n        config=config,\n        data_store=data_store,\n        logger=logger,\n        **kwargs,\n    )\n</code></pre>"},{"location":"api/handlers/#gigaspatial.handlers.rwi.RWIHandler.create_reader","title":"<code>create_reader(config, data_store, logger, **kwargs)</code>","text":"<p>Create and return a RWIReader instance</p> Source code in <code>gigaspatial/handlers/rwi.py</code> <pre><code>def create_reader(\n    self,\n    config: RWIConfig,\n    data_store: DataStore,\n    logger: logging.Logger,\n    **kwargs,\n) -&gt; RWIReader:\n    \"\"\"Create and return a RWIReader instance\"\"\"\n    return RWIReader(\n        config=config,\n        data_store=data_store,\n        logger=logger,\n        **kwargs,\n    )\n</code></pre>"},{"location":"api/handlers/#gigaspatial.handlers.rwi.RWIReader","title":"<code>RWIReader</code>","text":"<p>               Bases: <code>HDXReader</code></p> <p>Specialized reader for the Relative Wealth Index dataset from HDX</p> Source code in <code>gigaspatial/handlers/rwi.py</code> <pre><code>class RWIReader(HDXReader):\n    \"\"\"Specialized reader for the Relative Wealth Index dataset from HDX\"\"\"\n\n    def __init__(\n        self,\n        config: Union[RWIConfig, dict] = None,\n        data_store: Optional[DataStore] = None,\n        logger: Optional[logging.Logger] = None,\n    ):\n        config = config if isinstance(config, RWIConfig) else RWIConfig(**config)\n        super().__init__(config=config, data_store=data_store, logger=logger)\n</code></pre>"},{"location":"api/handlers/#gigaspatial.handlers.srtm","title":"<code>srtm</code>","text":""},{"location":"api/handlers/#gigaspatial.handlers.srtm.nasa_srtm","title":"<code>nasa_srtm</code>","text":""},{"location":"api/handlers/#gigaspatial.handlers.srtm.nasa_srtm.NasaSRTMConfig","title":"<code>NasaSRTMConfig</code>  <code>dataclass</code>","text":"<p>               Bases: <code>BaseHandlerConfig</code></p> <p>Configuration for NASA SRTM .hgt tiles (30m or 90m). Creates tile geometries dynamically for 1\u00b0x1\u00b0 grid cells.</p> <p>Each tile file covers 1 degree latitude x 1 degree longitude.</p> Source code in <code>gigaspatial/handlers/srtm/nasa_srtm.py</code> <pre><code>@dataclass(config=ConfigDict(arbitrary_types_allowed=True))\nclass NasaSRTMConfig(BaseHandlerConfig):\n    \"\"\"\n    Configuration for NASA SRTM .hgt tiles (30m or 90m).\n    Creates tile geometries dynamically for 1\u00b0x1\u00b0 grid cells.\n\n    Each tile file covers 1 degree latitude x 1 degree longitude.\n    \"\"\"\n\n    earthdata_username: str = Field(\n        default=global_config.EARTHDATA_USERNAME, description=\"Earthdata Login username\"\n    )\n    earthdata_password: str = Field(\n        default=global_config.EARTHDATA_PASSWORD, description=\"Earthdata Login password\"\n    )\n\n    BASE_URL: str = \"https://e4ftl01.cr.usgs.gov/MEASURES/SRTMGL{}.003/2000.02.11/\"\n\n    # user config\n    base_path: Path = global_config.get_path(\"nasa_srtm\", \"bronze\")\n    resolution: Literal[\"30m\", \"90m\"] = \"30m\"\n\n    def __post_init__(self):\n        super().__post_init__()\n        self._res_arc = 3 if self.resolution == \"90m\" else 1\n        self.BASE_URL = self.BASE_URL.format(self._res_arc)\n        # self.session = self._setup_earthdata_session()\n        self.session = self._create_authenticated_session()\n        self._generate_tile_grid()\n\n    def _create_authenticated_session(self) -&gt; requests.Session:\n        \"\"\"\n        Create a persistent Earthdata-authenticated requests session\n        that keeps Authorization headers through redirects.\n        \"\"\"\n        logging.info(\"Setting up Earthdata session with header redirection...\")\n\n        session = EarthdataSession(\n            username=self.earthdata_username,\n            password=self.earthdata_password,\n        )\n\n        # Optionally verify credentials once (to pre-authenticate cookies)\n        auth_test = \"https://urs.earthdata.nasa.gov\"\n        try:\n            r = session.get(auth_test, timeout=10)\n            logging.debug(f\"Earthdata auth test status: {r.status_code}\")\n        except requests.RequestException as e:\n            logging.warning(f\"Earthdata auth test failed: {e}\")\n\n        return session\n\n    def _generate_tile_grid(self):\n        \"\"\"\n        Generate 1\u00b0x1\u00b0 grid polygons covering global extent.\n        \"\"\"\n\n        lats = range(-90, 90)\n        lons = range(-180, 180)\n\n        grid_records = []\n        for lat, lon in itertools.product(lats, lons):\n            tile_name = self._tile_name(lat, lon)\n            grid_records.append(\n                {\n                    \"tile_id\": tile_name,\n                    \"geometry\": box(lon, lat, lon + 1, lat + 1),\n                    \"tile_url\": f\"{self.BASE_URL}/{tile_name}.SRTMGL{self._res_arc}.hgt.zip\",\n                }\n            )\n\n        self.grid_records = grid_records\n        self.tile_tree = STRtree([r.get(\"geometry\") for r in grid_records])\n\n    def _tile_name(self, lat: int, lon: int) -&gt; str:\n        \"\"\"Return the SRTM tile name like N37E023 or S10W120.\"\"\"\n        ns = \"N\" if lat &gt;= 0 else \"S\"\n        ew = \"E\" if lon &gt;= 0 else \"W\"\n        return f\"{ns}{abs(lat):02d}{ew}{abs(lon):03d}\"\n\n    def get_relevant_data_units(self, source, force_recompute: bool = False, **kwargs):\n        return super().get_relevant_data_units(\n            source, force_recompute, crs=\"EPSG:4326\", **kwargs\n        )\n\n    def get_relevant_data_units_by_geometry(\n        self, geometry: Union[BaseGeometry, gpd.GeoDataFrame], **kwargs\n    ) -&gt; List[dict]:\n        mask = self.tile_tree.query(geometry, predicate=\"intersects\")\n        filtered_grid = [self.grid_records[i] for i in mask]\n\n        return gpd.GeoDataFrame(filtered_grid, crs=\"EPSG:4326\").to_dict(\"records\")\n\n    def get_data_unit_path(self, unit: Union[pd.Series, dict, str], **kwargs) -&gt; Path:\n        \"\"\"\n        Given a tile unit or tile_id, return expected storage path.\n        \"\"\"\n        tile_id = unit[\"tile_id\"] if isinstance(unit, (pd.Series, dict)) else unit\n        return self.base_path / f\"{tile_id}.SRTMGL{self._res_arc}.hgt.zip\"\n\n    def get_data_unit_paths(\n        self, units: Union[pd.DataFrame, Iterable[Union[dict, str]]], **kwargs\n    ) -&gt; list:\n        \"\"\"\n        Given tile identifiers, return list of file paths.\n        \"\"\"\n        if isinstance(units, pd.DataFrame):\n            return [\n                self.get_data_unit_path(row, **kwargs) for _, row in units.iterrows()\n            ]\n        return super().get_data_unit_paths(units, **kwargs)\n</code></pre>"},{"location":"api/handlers/#gigaspatial.handlers.srtm.nasa_srtm.NasaSRTMConfig.get_data_unit_path","title":"<code>get_data_unit_path(unit, **kwargs)</code>","text":"<p>Given a tile unit or tile_id, return expected storage path.</p> Source code in <code>gigaspatial/handlers/srtm/nasa_srtm.py</code> <pre><code>def get_data_unit_path(self, unit: Union[pd.Series, dict, str], **kwargs) -&gt; Path:\n    \"\"\"\n    Given a tile unit or tile_id, return expected storage path.\n    \"\"\"\n    tile_id = unit[\"tile_id\"] if isinstance(unit, (pd.Series, dict)) else unit\n    return self.base_path / f\"{tile_id}.SRTMGL{self._res_arc}.hgt.zip\"\n</code></pre>"},{"location":"api/handlers/#gigaspatial.handlers.srtm.nasa_srtm.NasaSRTMConfig.get_data_unit_paths","title":"<code>get_data_unit_paths(units, **kwargs)</code>","text":"<p>Given tile identifiers, return list of file paths.</p> Source code in <code>gigaspatial/handlers/srtm/nasa_srtm.py</code> <pre><code>def get_data_unit_paths(\n    self, units: Union[pd.DataFrame, Iterable[Union[dict, str]]], **kwargs\n) -&gt; list:\n    \"\"\"\n    Given tile identifiers, return list of file paths.\n    \"\"\"\n    if isinstance(units, pd.DataFrame):\n        return [\n            self.get_data_unit_path(row, **kwargs) for _, row in units.iterrows()\n        ]\n    return super().get_data_unit_paths(units, **kwargs)\n</code></pre>"},{"location":"api/handlers/#gigaspatial.handlers.srtm.nasa_srtm.NasaSRTMDownloader","title":"<code>NasaSRTMDownloader</code>","text":"<p>               Bases: <code>BaseHandlerDownloader</code></p> <p>A class to handle downloads of NASA SRTM elevation data.</p> Source code in <code>gigaspatial/handlers/srtm/nasa_srtm.py</code> <pre><code>class NasaSRTMDownloader(BaseHandlerDownloader):\n    \"\"\"A class to handle downloads of NASA SRTM elevation data.\"\"\"\n\n    def __init__(\n        self,\n        config: Optional[NasaSRTMConfig] = None,\n        data_store: Optional[DataStore] = None,\n        logger: Optional[logging.Logger] = None,\n    ):\n        \"\"\"\n        Initialize the downloader.\n\n        Args:\n            config: Optional configuration for customizing download behavior and file paths.\n                    If None, a default `NasaSRTMConfig` is used.\n            data_store: Optional instance of a `DataStore` for managing data storage.\n                        If provided, it overrides the `data_store` in the `config`.\n                        If None, the `data_store` from the `config` is used.\n            logger: Optional custom logger instance. If None, a default logger\n                    named after the module is created and used.\n        \"\"\"\n        config = config or NasaSRTMConfig()\n        super().__init__(config=config, data_store=data_store, logger=logger)\n\n    def download_data_unit(\n        self,\n        tile_info: Union[pd.Series, dict],\n        **kwargs,\n    ) -&gt; Optional[str]:\n        \"\"\"Download data file for a single SRTM tile.\"\"\"\n\n        tile_url = tile_info[\"tile_url\"]\n\n        try:\n            response = self.config.session.get(tile_url, stream=True)\n            response.raise_for_status()\n\n            file_path = str(self.config.get_data_unit_path(tile_info))\n\n            with self.data_store.open(file_path, \"wb\") as file:\n                for chunk in response.iter_content(chunk_size=8192):\n                    file.write(chunk)\n\n                self.logger.debug(\n                    f\"Successfully downloaded tile: {tile_info['tile_id']}\"\n                )\n                return file_path\n\n        except requests.exceptions.RequestException as e:\n            self.logger.error(\n                f\"Failed to download tile {tile_info['tile_id']}: {str(e)}\"\n            )\n            return None\n        except Exception as e:\n            self.logger.error(f\"Unexpected error downloading dataset: {str(e)}\")\n            return None\n\n    def download_data_units(\n        self,\n        tiles: Union[pd.DataFrame, List[dict]],\n        **kwargs,\n    ) -&gt; List[str]:\n        \"\"\"Download data files for multiple SRTM tiles.\"\"\"\n\n        if len(tiles) == 0:\n            self.logger.warning(f\"There is no matching data\")\n            return []\n\n        with multiprocessing.Pool(self.config.n_workers) as pool:\n            download_func = functools.partial(self.download_data_unit)\n            file_paths = list(\n                tqdm(\n                    pool.imap(\n                        download_func,\n                        (\n                            [row for _, row in tiles.iterrows()]\n                            if isinstance(tiles, pd.DataFrame)\n                            else tiles\n                        ),\n                    ),\n                    total=len(tiles),\n                    desc=f\"Downloading SRTM elevation data\",\n                )\n            )\n\n        return [path for path in file_paths if path is not None]\n</code></pre>"},{"location":"api/handlers/#gigaspatial.handlers.srtm.nasa_srtm.NasaSRTMDownloader.__init__","title":"<code>__init__(config=None, data_store=None, logger=None)</code>","text":"<p>Initialize the downloader.</p> <p>Parameters:</p> Name Type Description Default <code>config</code> <code>Optional[NasaSRTMConfig]</code> <p>Optional configuration for customizing download behavior and file paths.     If None, a default <code>NasaSRTMConfig</code> is used.</p> <code>None</code> <code>data_store</code> <code>Optional[DataStore]</code> <p>Optional instance of a <code>DataStore</code> for managing data storage.         If provided, it overrides the <code>data_store</code> in the <code>config</code>.         If None, the <code>data_store</code> from the <code>config</code> is used.</p> <code>None</code> <code>logger</code> <code>Optional[Logger]</code> <p>Optional custom logger instance. If None, a default logger     named after the module is created and used.</p> <code>None</code> Source code in <code>gigaspatial/handlers/srtm/nasa_srtm.py</code> <pre><code>def __init__(\n    self,\n    config: Optional[NasaSRTMConfig] = None,\n    data_store: Optional[DataStore] = None,\n    logger: Optional[logging.Logger] = None,\n):\n    \"\"\"\n    Initialize the downloader.\n\n    Args:\n        config: Optional configuration for customizing download behavior and file paths.\n                If None, a default `NasaSRTMConfig` is used.\n        data_store: Optional instance of a `DataStore` for managing data storage.\n                    If provided, it overrides the `data_store` in the `config`.\n                    If None, the `data_store` from the `config` is used.\n        logger: Optional custom logger instance. If None, a default logger\n                named after the module is created and used.\n    \"\"\"\n    config = config or NasaSRTMConfig()\n    super().__init__(config=config, data_store=data_store, logger=logger)\n</code></pre>"},{"location":"api/handlers/#gigaspatial.handlers.srtm.nasa_srtm.NasaSRTMDownloader.download_data_unit","title":"<code>download_data_unit(tile_info, **kwargs)</code>","text":"<p>Download data file for a single SRTM tile.</p> Source code in <code>gigaspatial/handlers/srtm/nasa_srtm.py</code> <pre><code>def download_data_unit(\n    self,\n    tile_info: Union[pd.Series, dict],\n    **kwargs,\n) -&gt; Optional[str]:\n    \"\"\"Download data file for a single SRTM tile.\"\"\"\n\n    tile_url = tile_info[\"tile_url\"]\n\n    try:\n        response = self.config.session.get(tile_url, stream=True)\n        response.raise_for_status()\n\n        file_path = str(self.config.get_data_unit_path(tile_info))\n\n        with self.data_store.open(file_path, \"wb\") as file:\n            for chunk in response.iter_content(chunk_size=8192):\n                file.write(chunk)\n\n            self.logger.debug(\n                f\"Successfully downloaded tile: {tile_info['tile_id']}\"\n            )\n            return file_path\n\n    except requests.exceptions.RequestException as e:\n        self.logger.error(\n            f\"Failed to download tile {tile_info['tile_id']}: {str(e)}\"\n        )\n        return None\n    except Exception as e:\n        self.logger.error(f\"Unexpected error downloading dataset: {str(e)}\")\n        return None\n</code></pre>"},{"location":"api/handlers/#gigaspatial.handlers.srtm.nasa_srtm.NasaSRTMDownloader.download_data_units","title":"<code>download_data_units(tiles, **kwargs)</code>","text":"<p>Download data files for multiple SRTM tiles.</p> Source code in <code>gigaspatial/handlers/srtm/nasa_srtm.py</code> <pre><code>def download_data_units(\n    self,\n    tiles: Union[pd.DataFrame, List[dict]],\n    **kwargs,\n) -&gt; List[str]:\n    \"\"\"Download data files for multiple SRTM tiles.\"\"\"\n\n    if len(tiles) == 0:\n        self.logger.warning(f\"There is no matching data\")\n        return []\n\n    with multiprocessing.Pool(self.config.n_workers) as pool:\n        download_func = functools.partial(self.download_data_unit)\n        file_paths = list(\n            tqdm(\n                pool.imap(\n                    download_func,\n                    (\n                        [row for _, row in tiles.iterrows()]\n                        if isinstance(tiles, pd.DataFrame)\n                        else tiles\n                    ),\n                ),\n                total=len(tiles),\n                desc=f\"Downloading SRTM elevation data\",\n            )\n        )\n\n    return [path for path in file_paths if path is not None]\n</code></pre>"},{"location":"api/handlers/#gigaspatial.handlers.srtm.nasa_srtm.NasaSRTMHandler","title":"<code>NasaSRTMHandler</code>","text":"<p>               Bases: <code>BaseHandler</code></p> <p>Main handler class for NASA SRTM elevation data.</p> Source code in <code>gigaspatial/handlers/srtm/nasa_srtm.py</code> <pre><code>class NasaSRTMHandler(BaseHandler):\n    \"\"\"Main handler class for NASA SRTM elevation data.\"\"\"\n\n    def create_config(\n        self, data_store: DataStore, logger: logging.Logger, **kwargs\n    ) -&gt; NasaSRTMConfig:\n        \"\"\"Create and return a NasaSRTMConfig instance.\"\"\"\n        return NasaSRTMConfig(data_store=data_store, logger=logger, **kwargs)\n\n    def create_downloader(\n        self,\n        config: NasaSRTMConfig,\n        data_store: DataStore,\n        logger: logging.Logger,\n        **kwargs,\n    ) -&gt; NasaSRTMDownloader:\n        \"\"\"Create and return a NasaSRTMDownloader instance.\"\"\"\n        return NasaSRTMDownloader(\n            config=config, data_store=data_store, logger=logger, **kwargs\n        )\n\n    def create_reader(\n        self,\n        config: NasaSRTMConfig,\n        data_store: DataStore,\n        logger: logging.Logger,\n        **kwargs,\n    ) -&gt; NasaSRTMReader:\n        \"\"\"Create and return a NasaSRTMReader instance.\"\"\"\n        return NasaSRTMReader(\n            config=config, data_store=data_store, logger=logger, **kwargs\n        )\n</code></pre>"},{"location":"api/handlers/#gigaspatial.handlers.srtm.nasa_srtm.NasaSRTMHandler.create_config","title":"<code>create_config(data_store, logger, **kwargs)</code>","text":"<p>Create and return a NasaSRTMConfig instance.</p> Source code in <code>gigaspatial/handlers/srtm/nasa_srtm.py</code> <pre><code>def create_config(\n    self, data_store: DataStore, logger: logging.Logger, **kwargs\n) -&gt; NasaSRTMConfig:\n    \"\"\"Create and return a NasaSRTMConfig instance.\"\"\"\n    return NasaSRTMConfig(data_store=data_store, logger=logger, **kwargs)\n</code></pre>"},{"location":"api/handlers/#gigaspatial.handlers.srtm.nasa_srtm.NasaSRTMHandler.create_downloader","title":"<code>create_downloader(config, data_store, logger, **kwargs)</code>","text":"<p>Create and return a NasaSRTMDownloader instance.</p> Source code in <code>gigaspatial/handlers/srtm/nasa_srtm.py</code> <pre><code>def create_downloader(\n    self,\n    config: NasaSRTMConfig,\n    data_store: DataStore,\n    logger: logging.Logger,\n    **kwargs,\n) -&gt; NasaSRTMDownloader:\n    \"\"\"Create and return a NasaSRTMDownloader instance.\"\"\"\n    return NasaSRTMDownloader(\n        config=config, data_store=data_store, logger=logger, **kwargs\n    )\n</code></pre>"},{"location":"api/handlers/#gigaspatial.handlers.srtm.nasa_srtm.NasaSRTMHandler.create_reader","title":"<code>create_reader(config, data_store, logger, **kwargs)</code>","text":"<p>Create and return a NasaSRTMReader instance.</p> Source code in <code>gigaspatial/handlers/srtm/nasa_srtm.py</code> <pre><code>def create_reader(\n    self,\n    config: NasaSRTMConfig,\n    data_store: DataStore,\n    logger: logging.Logger,\n    **kwargs,\n) -&gt; NasaSRTMReader:\n    \"\"\"Create and return a NasaSRTMReader instance.\"\"\"\n    return NasaSRTMReader(\n        config=config, data_store=data_store, logger=logger, **kwargs\n    )\n</code></pre>"},{"location":"api/handlers/#gigaspatial.handlers.srtm.nasa_srtm.NasaSRTMReader","title":"<code>NasaSRTMReader</code>","text":"<p>               Bases: <code>BaseHandlerReader</code></p> <p>A class to handle reading of NASA SRTM elevation data.</p> Source code in <code>gigaspatial/handlers/srtm/nasa_srtm.py</code> <pre><code>class NasaSRTMReader(BaseHandlerReader):\n    \"\"\"A class to handle reading of NASA SRTM elevation data.\"\"\"\n\n    def __init__(\n        self,\n        config: Optional[NasaSRTMConfig] = None,\n        data_store: Optional[DataStore] = None,\n        logger: Optional[logging.Logger] = None,\n    ):\n        \"\"\"\n        Initialize the reader.\n\n        Args:\n            config: Optional configuration for customizing reading behavior and file paths.\n                    If None, a default `NasaSRTMConfig` is used.\n            data_store: Optional instance of a `DataStore` for managing data storage.\n                        If provided, it overrides the `data_store` in the `config`.\n                        If None, the `data_store` from the `config` is used.\n            logger: Optional custom logger instance. If None, a default logger\n                    named after the module is created and used.\n        \"\"\"\n        config = config or NasaSRTMConfig()\n        super().__init__(config=config, data_store=data_store, logger=logger)\n\n    def load_from_paths(\n        self, source_data_path: List[Union[str, Path]], **kwargs\n    ) -&gt; Union[pd.DataFrame, List[SRTMParser]]:\n        \"\"\"\n        Load SRTM elevation data from file paths.\n\n        Args:\n            source_data_path: List of SRTM .hgt.zip file paths\n            **kwargs: Additional parameters for data loading\n                - as_dataframe: bool, default=True. If True, return concatenated DataFrame.\n                               If False, return list of SRTMParser objects.\n                - dropna: bool, default=True. If True, drop rows with NaN elevation values.\n\n        Returns:\n            Union[pd.DataFrame, List[SRTMParser]]: Loaded elevation data\n        \"\"\"\n        as_dataframe = kwargs.get(\"as_dataframe\", True)\n        dropna = kwargs.get(\"dropna\", True)\n\n        parsers = []\n        for file_path in source_data_path:\n            try:\n                parser = SRTMParser(file_path, data_store=self.data_store)\n                parsers.append(parser)\n                self.logger.debug(f\"Successfully loaded SRTM tile: {file_path}\")\n            except Exception as e:\n                self.logger.error(f\"Failed to load SRTM tile {file_path}: {str(e)}\")\n                continue\n\n        if not parsers:\n            self.logger.warning(\"No SRTM tiles could be loaded\")\n            return pd.DataFrame() if as_dataframe else []\n\n        if as_dataframe:\n            # Concatenate all tile dataframes\n            dataframes = [parser.to_dataframe(dropna=dropna) for parser in parsers]\n            if dataframes:\n                combined_df = pd.concat(dataframes, ignore_index=True)\n                self.logger.info(\n                    f\"Loaded {len(combined_df)} elevation points from {len(parsers)} tiles\"\n                )\n                return combined_df\n            else:\n                return pd.DataFrame()\n        else:\n            self.logger.info(f\"Loaded {len(parsers)} SRTM tiles\")\n            return parsers\n</code></pre>"},{"location":"api/handlers/#gigaspatial.handlers.srtm.nasa_srtm.NasaSRTMReader.__init__","title":"<code>__init__(config=None, data_store=None, logger=None)</code>","text":"<p>Initialize the reader.</p> <p>Parameters:</p> Name Type Description Default <code>config</code> <code>Optional[NasaSRTMConfig]</code> <p>Optional configuration for customizing reading behavior and file paths.     If None, a default <code>NasaSRTMConfig</code> is used.</p> <code>None</code> <code>data_store</code> <code>Optional[DataStore]</code> <p>Optional instance of a <code>DataStore</code> for managing data storage.         If provided, it overrides the <code>data_store</code> in the <code>config</code>.         If None, the <code>data_store</code> from the <code>config</code> is used.</p> <code>None</code> <code>logger</code> <code>Optional[Logger]</code> <p>Optional custom logger instance. If None, a default logger     named after the module is created and used.</p> <code>None</code> Source code in <code>gigaspatial/handlers/srtm/nasa_srtm.py</code> <pre><code>def __init__(\n    self,\n    config: Optional[NasaSRTMConfig] = None,\n    data_store: Optional[DataStore] = None,\n    logger: Optional[logging.Logger] = None,\n):\n    \"\"\"\n    Initialize the reader.\n\n    Args:\n        config: Optional configuration for customizing reading behavior and file paths.\n                If None, a default `NasaSRTMConfig` is used.\n        data_store: Optional instance of a `DataStore` for managing data storage.\n                    If provided, it overrides the `data_store` in the `config`.\n                    If None, the `data_store` from the `config` is used.\n        logger: Optional custom logger instance. If None, a default logger\n                named after the module is created and used.\n    \"\"\"\n    config = config or NasaSRTMConfig()\n    super().__init__(config=config, data_store=data_store, logger=logger)\n</code></pre>"},{"location":"api/handlers/#gigaspatial.handlers.srtm.nasa_srtm.NasaSRTMReader.load_from_paths","title":"<code>load_from_paths(source_data_path, **kwargs)</code>","text":"<p>Load SRTM elevation data from file paths.</p> <p>Parameters:</p> Name Type Description Default <code>source_data_path</code> <code>List[Union[str, Path]]</code> <p>List of SRTM .hgt.zip file paths</p> required <code>**kwargs</code> <p>Additional parameters for data loading - as_dataframe: bool, default=True. If True, return concatenated DataFrame.                If False, return list of SRTMParser objects. - dropna: bool, default=True. If True, drop rows with NaN elevation values.</p> <code>{}</code> <p>Returns:</p> Type Description <code>Union[DataFrame, List[SRTMParser]]</code> <p>Union[pd.DataFrame, List[SRTMParser]]: Loaded elevation data</p> Source code in <code>gigaspatial/handlers/srtm/nasa_srtm.py</code> <pre><code>def load_from_paths(\n    self, source_data_path: List[Union[str, Path]], **kwargs\n) -&gt; Union[pd.DataFrame, List[SRTMParser]]:\n    \"\"\"\n    Load SRTM elevation data from file paths.\n\n    Args:\n        source_data_path: List of SRTM .hgt.zip file paths\n        **kwargs: Additional parameters for data loading\n            - as_dataframe: bool, default=True. If True, return concatenated DataFrame.\n                           If False, return list of SRTMParser objects.\n            - dropna: bool, default=True. If True, drop rows with NaN elevation values.\n\n    Returns:\n        Union[pd.DataFrame, List[SRTMParser]]: Loaded elevation data\n    \"\"\"\n    as_dataframe = kwargs.get(\"as_dataframe\", True)\n    dropna = kwargs.get(\"dropna\", True)\n\n    parsers = []\n    for file_path in source_data_path:\n        try:\n            parser = SRTMParser(file_path, data_store=self.data_store)\n            parsers.append(parser)\n            self.logger.debug(f\"Successfully loaded SRTM tile: {file_path}\")\n        except Exception as e:\n            self.logger.error(f\"Failed to load SRTM tile {file_path}: {str(e)}\")\n            continue\n\n    if not parsers:\n        self.logger.warning(\"No SRTM tiles could be loaded\")\n        return pd.DataFrame() if as_dataframe else []\n\n    if as_dataframe:\n        # Concatenate all tile dataframes\n        dataframes = [parser.to_dataframe(dropna=dropna) for parser in parsers]\n        if dataframes:\n            combined_df = pd.concat(dataframes, ignore_index=True)\n            self.logger.info(\n                f\"Loaded {len(combined_df)} elevation points from {len(parsers)} tiles\"\n            )\n            return combined_df\n        else:\n            return pd.DataFrame()\n    else:\n        self.logger.info(f\"Loaded {len(parsers)} SRTM tiles\")\n        return parsers\n</code></pre>"},{"location":"api/handlers/#gigaspatial.handlers.srtm.srtm_manager","title":"<code>srtm_manager</code>","text":""},{"location":"api/handlers/#gigaspatial.handlers.srtm.srtm_manager.SRTMManager","title":"<code>SRTMManager</code>","text":"<p>Manager for accessing elevation data across multiple SRTM .hgt.zip files.</p> <p>Implements lazy loading with LRU caching for efficient memory usage. Automatically handles multiple tiles for elevation profiles.</p> Source code in <code>gigaspatial/handlers/srtm/srtm_manager.py</code> <pre><code>class SRTMManager:\n    \"\"\"\n    Manager for accessing elevation data across multiple SRTM .hgt.zip files.\n\n    Implements lazy loading with LRU caching for efficient memory usage.\n    Automatically handles multiple tiles for elevation profiles.\n    \"\"\"\n\n    def __init__(\n        self,\n        srtm_directory: Union[str, Path],\n        downloader: NasaSRTMDownloader = None,\n        cache_size: int = 10,\n        data_store: Optional[DataStore] = None,\n    ):\n        \"\"\"\n        Initialize the SRTM Manager.\n\n        Parameters\n        ----------\n        srtm_directory : str or Path\n            Directory containing .hgt.zip files\n        downloader : optional\n            Downloader instance for auto-downloading missing tiles\n        cache_size : int, default=10\n            Maximum number of SRTM tiles to keep in memory (LRU cache)\n        data_store : DataStore, optional\n            Data store for reading files. Priority: provided data_store &gt;\n            downloader.data_store &gt; LocalDataStore()\n        \"\"\"\n        self.srtm_directory = Path(srtm_directory)\n        self.downloader = downloader\n\n        # Set data_store: use provided, otherwise downloader's, otherwise LocalDataStore\n        if data_store is not None:\n            self.data_store = data_store\n        elif downloader is not None and hasattr(downloader, \"data_store\"):\n            self.data_store = downloader.data_store\n        else:\n            self.data_store = LocalDataStore()\n\n        # Check if directory exists\n        if not self.data_store.is_dir(str(self.srtm_directory)):\n            raise FileNotFoundError(f\"Directory not found: {self.srtm_directory}\")\n\n        # Build index of available tiles\n        self.tile_index = self._build_tile_index()\n\n        # Set up LRU cache for lazy loading\n        self._get_parser_cached = lru_cache(maxsize=cache_size)(self._load_parser)\n\n    def _build_tile_index(self) -&gt; dict:\n        \"\"\"\n        Build an index of available SRTM tiles in the directory.\n\n        Returns\n        -------\n        dict\n            Mapping of (lat, lon) tuple to file path\n        \"\"\"\n        tile_index = {}\n\n        # Pattern to match SRTM filenames: N00E000 or S00W000\n        pattern = re.compile(r\"^([NS])(\\d{2})([EW])(\\d{3})\")\n\n        # List files using DataStore\n        file_list = self.data_store.list_files(str(self.srtm_directory))\n\n        for file_path_str in file_list:\n            if file_path_str.endswith(\".hgt.zip\"):\n                # Extract just the filename for pattern matching\n                file_name = Path(file_path_str).name\n                file_stem = Path(file_name).stem\n\n                match = pattern.match(file_stem)\n\n                if match:\n                    lat_dir, lat_val, lon_dir, lon_val = match.groups()\n\n                    lat = int(lat_val) if lat_dir == \"N\" else -int(lat_val)\n                    lon = int(lon_val) if lon_dir == \"E\" else -int(lon_val)\n\n                    # Use the path as returned by DataStore (will be used for reading)\n                    tile_index[(lat, lon)] = file_path_str\n\n        return tile_index\n\n    def _get_tile_coordinates(\n        self, latitude: float, longitude: float\n    ) -&gt; Tuple[int, int]:\n        \"\"\"\n        Get the tile coordinates (southwest corner) for a given lat/lon.\n\n        Parameters\n        ----------\n        latitude : float\n            Latitude in decimal degrees\n        longitude : float\n            Longitude in decimal degrees\n\n        Returns\n        -------\n        tuple of (lat_tile, lon_tile)\n            Southwest corner coordinates of the tile\n        \"\"\"\n        # SRTM tiles are 1x1 degree, named by their southwest corner\n        lat_tile = int(np.floor(latitude))\n        lon_tile = int(np.floor(longitude))\n\n        return lat_tile, lon_tile\n\n    def _load_parser(self, lat_tile: int, lon_tile: int):\n        \"\"\"\n        Load a SRTMParser for a specific tile (used with LRU cache).\n\n        Parameters\n        ----------\n        lat_tile : int\n            Tile latitude (southwest corner)\n        lon_tile : int\n            Tile longitude (southwest corner)\n\n        Returns\n        -------\n        SRTMParser\n            Parser instance for the tile\n        \"\"\"\n        tile_key = (lat_tile, lon_tile)\n\n        if tile_key not in self.tile_index:\n            if self.downloader:\n                # Auto-download missing tile\n                from shapely.geometry import box\n\n                # Create tile_info following the pattern from NasaSRTMConfig\n                tile_id = self.downloader.config._tile_name(lat_tile, lon_tile)\n                tile_url = f\"{self.downloader.config.BASE_URL}/{tile_id}.SRTMGL{self.downloader.config._res_arc}.hgt.zip\"\n\n                tile_info = {\n                    \"tile_id\": tile_id,\n                    \"geometry\": box(lon_tile, lat_tile, lon_tile + 1, lat_tile + 1),\n                    \"tile_url\": tile_url,\n                }\n\n                # Use download_data_unit for direct download\n                self.downloader.download_data_unit(tile_info)\n\n                # Rebuild index to find new tile\n                self.tile_index = self._build_tile_index()\n\n                # Check if tile is now available\n                if tile_key not in self.tile_index:\n                    raise FileNotFoundError(\n                        f\"SRTM tile for ({lat_tile}, {lon_tile}) could not be downloaded to {self.srtm_directory}\"\n                    )\n            else:\n                raise FileNotFoundError(\n                    f\"SRTM tile for ({lat_tile}, {lon_tile}) not found in {self.srtm_directory}\"\n                )\n\n        return SRTMParser(self.tile_index[tile_key], data_store=self.data_store)\n\n    def get_elevation(self, latitude: float, longitude: float) -&gt; float:\n        \"\"\"\n        Get interpolated elevation for a specific coordinate.\n\n        Automatically finds and loads the correct SRTM tile.\n\n        Parameters\n        ----------\n        latitude : float\n            Latitude in decimal degrees (-90 to 90)\n        longitude : float\n            Longitude in decimal degrees (-180 to 180)\n\n        Returns\n        -------\n        float\n            Interpolated elevation in meters\n\n        Raises\n        ------\n        FileNotFoundError\n            If the required SRTM tile is not available\n        \"\"\"\n        # Get tile coordinates\n        lat_tile, lon_tile = self._get_tile_coordinates(latitude, longitude)\n\n        # Load parser (cached)\n        parser = self._get_parser_cached(lat_tile, lon_tile)\n\n        # Get elevation\n        return parser.get_elevation(latitude, longitude)\n\n    def get_elevation_batch(self, coordinates: np.ndarray) -&gt; np.ndarray:\n        \"\"\"\n        Get elevations for multiple coordinates efficiently.\n\n        Groups coordinates by tile to minimize parser loads.\n\n        Parameters\n        ----------\n        coordinates : np.ndarray of shape (n, 2)\n            Array of (latitude, longitude) pairs\n\n        Returns\n        -------\n        np.ndarray of shape (n,)\n            Elevations in meters\n\n        Raises\n        ------\n        FileNotFoundError\n            If any required SRTM tile is not available\n        \"\"\"\n        elevations = np.zeros(len(coordinates))\n\n        # Group coordinates by tile\n        tile_groups = {}\n        for i, (lat, lon) in enumerate(coordinates):\n            tile_key = self._get_tile_coordinates(lat, lon)\n            if tile_key not in tile_groups:\n                tile_groups[tile_key] = []\n            tile_groups[tile_key].append((i, lat, lon))\n\n        # Process each tile group\n        for tile_key, coords_list in tile_groups.items():\n            parser = self._get_parser_cached(*tile_key)\n\n            # Extract coordinates for this tile\n            indices = [c[0] for c in coords_list]\n            tile_coords = np.array([[c[1], c[2]] for c in coords_list])\n\n            # Get elevations\n            tile_elevations = parser.get_elevation_batch(tile_coords)\n\n            # Store results\n            elevations[indices] = tile_elevations\n\n        return elevations\n\n    def get_elevation_profile(\n        self,\n        start_lat: float,\n        start_lon: float,\n        end_lat: float,\n        end_lon: float,\n        num_points: int = 100,\n    ) -&gt; pd.DataFrame:\n        \"\"\"\n        Get elevation profile between two points.\n\n        Uses linear interpolation between points and automatically handles multiple SRTM tiles.\n        For more accurate great circle paths over long distances, consider using geopy.\n\n        Parameters\n        ----------\n        start_lat : float\n            Starting latitude in decimal degrees\n        start_lon : float\n            Starting longitude in decimal degrees\n        end_lat : float\n            Ending latitude in decimal degrees\n        end_lon : float\n            Ending longitude in decimal degrees\n        num_points : int, default=100\n            Number of sample points along the path\n\n        Returns\n        -------\n        pd.DataFrame\n            DataFrame with columns: distance_km, latitude, longitude, elevation\n\n        Raises\n        ------\n        FileNotFoundError\n            If any required SRTM tile along the path is not available\n        \"\"\"\n        # Generate points along the path (linear interpolation)\n        lats = np.linspace(start_lat, end_lat, num_points)\n        lons = np.linspace(start_lon, end_lon, num_points)\n\n        coordinates = np.column_stack((lats, lons))\n\n        # Get elevations for all points\n        elevations = self.get_elevation_batch(coordinates)\n\n        # Calculate distances using Haversine formula\n        distances = self._calculate_cumulative_distances(lats, lons)\n\n        # Create DataFrame\n        profile = pd.DataFrame(\n            {\n                \"distance_km\": distances,\n                \"latitude\": lats,\n                \"longitude\": lons,\n                \"elevation\": elevations,\n            }\n        )\n\n        return profile\n\n    @staticmethod\n    def _calculate_cumulative_distances(\n        lats: np.ndarray, lons: np.ndarray\n    ) -&gt; np.ndarray:\n        \"\"\"\n        Calculate cumulative distances along a path using Haversine formula.\n\n        Parameters\n        ----------\n        lats : np.ndarray\n            Array of latitudes\n        lons : np.ndarray\n            Array of longitudes\n\n        Returns\n        -------\n        np.ndarray\n            Cumulative distances in kilometers\n        \"\"\"\n        R = 6371.0  # Earth radius in km\n\n        distances = np.zeros(len(lats))\n\n        for i in range(1, len(lats)):\n            # Haversine formula\n            lat1, lon1 = np.radians(lats[i - 1]), np.radians(lons[i - 1])\n            lat2, lon2 = np.radians(lats[i]), np.radians(lons[i])\n\n            dlat = lat2 - lat1\n            dlon = lon2 - lon1\n\n            a = (\n                np.sin(dlat / 2) ** 2\n                + np.cos(lat1) * np.cos(lat2) * np.sin(dlon / 2) ** 2\n            )\n            c = 2 * np.arctan2(np.sqrt(a), np.sqrt(1 - a))\n\n            distances[i] = distances[i - 1] + R * c\n\n        return distances\n\n    def get_available_tiles(self) -&gt; List[Tuple[int, int]]:\n        \"\"\"\n        Get list of available SRTM tiles.\n\n        Returns\n        -------\n        list of tuples\n            List of (lat, lon) tile coordinates\n        \"\"\"\n        return list(self.tile_index.keys())\n\n    def check_coverage(self, latitude: float, longitude: float) -&gt; bool:\n        \"\"\"\n        Check if a specific coordinate has SRTM coverage.\n\n        Parameters\n        ----------\n        latitude : float\n            Latitude in decimal degrees\n        longitude : float\n            Longitude in decimal degrees\n\n        Returns\n        -------\n        bool\n            True if tile is available, False otherwise\n        \"\"\"\n        tile_key = self._get_tile_coordinates(latitude, longitude)\n        return tile_key in self.tile_index\n\n    def clear_cache(self):\n        \"\"\"Clear the LRU cache of loaded parsers.\"\"\"\n        self._get_parser_cached.cache_clear()\n\n    def get_cache_info(self):\n        \"\"\"\n        Get cache statistics.\n\n        Returns\n        -------\n        CacheInfo\n            Named tuple with hits, misses, maxsize, currsize\n        \"\"\"\n        return self._get_parser_cached.cache_info()\n\n    def __repr__(self):\n        return (\n            f\"SRTMManager(directory={self.srtm_directory}, \"\n            f\"tiles={len(self.tile_index)}, \"\n            f\"cache_size={self._get_parser_cached.cache_info().maxsize}, \"\n            f\"data_store={type(self.data_store).__name__})\"\n        )\n</code></pre>"},{"location":"api/handlers/#gigaspatial.handlers.srtm.srtm_manager.SRTMManager.__init__","title":"<code>__init__(srtm_directory, downloader=None, cache_size=10, data_store=None)</code>","text":"<p>Initialize the SRTM Manager.</p>"},{"location":"api/handlers/#gigaspatial.handlers.srtm.srtm_manager.SRTMManager.__init__--parameters","title":"Parameters","text":"<p>srtm_directory : str or Path     Directory containing .hgt.zip files downloader : optional     Downloader instance for auto-downloading missing tiles cache_size : int, default=10     Maximum number of SRTM tiles to keep in memory (LRU cache) data_store : DataStore, optional     Data store for reading files. Priority: provided data_store &gt;     downloader.data_store &gt; LocalDataStore()</p> Source code in <code>gigaspatial/handlers/srtm/srtm_manager.py</code> <pre><code>def __init__(\n    self,\n    srtm_directory: Union[str, Path],\n    downloader: NasaSRTMDownloader = None,\n    cache_size: int = 10,\n    data_store: Optional[DataStore] = None,\n):\n    \"\"\"\n    Initialize the SRTM Manager.\n\n    Parameters\n    ----------\n    srtm_directory : str or Path\n        Directory containing .hgt.zip files\n    downloader : optional\n        Downloader instance for auto-downloading missing tiles\n    cache_size : int, default=10\n        Maximum number of SRTM tiles to keep in memory (LRU cache)\n    data_store : DataStore, optional\n        Data store for reading files. Priority: provided data_store &gt;\n        downloader.data_store &gt; LocalDataStore()\n    \"\"\"\n    self.srtm_directory = Path(srtm_directory)\n    self.downloader = downloader\n\n    # Set data_store: use provided, otherwise downloader's, otherwise LocalDataStore\n    if data_store is not None:\n        self.data_store = data_store\n    elif downloader is not None and hasattr(downloader, \"data_store\"):\n        self.data_store = downloader.data_store\n    else:\n        self.data_store = LocalDataStore()\n\n    # Check if directory exists\n    if not self.data_store.is_dir(str(self.srtm_directory)):\n        raise FileNotFoundError(f\"Directory not found: {self.srtm_directory}\")\n\n    # Build index of available tiles\n    self.tile_index = self._build_tile_index()\n\n    # Set up LRU cache for lazy loading\n    self._get_parser_cached = lru_cache(maxsize=cache_size)(self._load_parser)\n</code></pre>"},{"location":"api/handlers/#gigaspatial.handlers.srtm.srtm_manager.SRTMManager.check_coverage","title":"<code>check_coverage(latitude, longitude)</code>","text":"<p>Check if a specific coordinate has SRTM coverage.</p>"},{"location":"api/handlers/#gigaspatial.handlers.srtm.srtm_manager.SRTMManager.check_coverage--parameters","title":"Parameters","text":"<p>latitude : float     Latitude in decimal degrees longitude : float     Longitude in decimal degrees</p>"},{"location":"api/handlers/#gigaspatial.handlers.srtm.srtm_manager.SRTMManager.check_coverage--returns","title":"Returns","text":"<p>bool     True if tile is available, False otherwise</p> Source code in <code>gigaspatial/handlers/srtm/srtm_manager.py</code> <pre><code>def check_coverage(self, latitude: float, longitude: float) -&gt; bool:\n    \"\"\"\n    Check if a specific coordinate has SRTM coverage.\n\n    Parameters\n    ----------\n    latitude : float\n        Latitude in decimal degrees\n    longitude : float\n        Longitude in decimal degrees\n\n    Returns\n    -------\n    bool\n        True if tile is available, False otherwise\n    \"\"\"\n    tile_key = self._get_tile_coordinates(latitude, longitude)\n    return tile_key in self.tile_index\n</code></pre>"},{"location":"api/handlers/#gigaspatial.handlers.srtm.srtm_manager.SRTMManager.clear_cache","title":"<code>clear_cache()</code>","text":"<p>Clear the LRU cache of loaded parsers.</p> Source code in <code>gigaspatial/handlers/srtm/srtm_manager.py</code> <pre><code>def clear_cache(self):\n    \"\"\"Clear the LRU cache of loaded parsers.\"\"\"\n    self._get_parser_cached.cache_clear()\n</code></pre>"},{"location":"api/handlers/#gigaspatial.handlers.srtm.srtm_manager.SRTMManager.get_available_tiles","title":"<code>get_available_tiles()</code>","text":"<p>Get list of available SRTM tiles.</p>"},{"location":"api/handlers/#gigaspatial.handlers.srtm.srtm_manager.SRTMManager.get_available_tiles--returns","title":"Returns","text":"<p>list of tuples     List of (lat, lon) tile coordinates</p> Source code in <code>gigaspatial/handlers/srtm/srtm_manager.py</code> <pre><code>def get_available_tiles(self) -&gt; List[Tuple[int, int]]:\n    \"\"\"\n    Get list of available SRTM tiles.\n\n    Returns\n    -------\n    list of tuples\n        List of (lat, lon) tile coordinates\n    \"\"\"\n    return list(self.tile_index.keys())\n</code></pre>"},{"location":"api/handlers/#gigaspatial.handlers.srtm.srtm_manager.SRTMManager.get_cache_info","title":"<code>get_cache_info()</code>","text":"<p>Get cache statistics.</p>"},{"location":"api/handlers/#gigaspatial.handlers.srtm.srtm_manager.SRTMManager.get_cache_info--returns","title":"Returns","text":"<p>CacheInfo     Named tuple with hits, misses, maxsize, currsize</p> Source code in <code>gigaspatial/handlers/srtm/srtm_manager.py</code> <pre><code>def get_cache_info(self):\n    \"\"\"\n    Get cache statistics.\n\n    Returns\n    -------\n    CacheInfo\n        Named tuple with hits, misses, maxsize, currsize\n    \"\"\"\n    return self._get_parser_cached.cache_info()\n</code></pre>"},{"location":"api/handlers/#gigaspatial.handlers.srtm.srtm_manager.SRTMManager.get_elevation","title":"<code>get_elevation(latitude, longitude)</code>","text":"<p>Get interpolated elevation for a specific coordinate.</p> <p>Automatically finds and loads the correct SRTM tile.</p>"},{"location":"api/handlers/#gigaspatial.handlers.srtm.srtm_manager.SRTMManager.get_elevation--parameters","title":"Parameters","text":"<p>latitude : float     Latitude in decimal degrees (-90 to 90) longitude : float     Longitude in decimal degrees (-180 to 180)</p>"},{"location":"api/handlers/#gigaspatial.handlers.srtm.srtm_manager.SRTMManager.get_elevation--returns","title":"Returns","text":"<p>float     Interpolated elevation in meters</p>"},{"location":"api/handlers/#gigaspatial.handlers.srtm.srtm_manager.SRTMManager.get_elevation--raises","title":"Raises","text":"<p>FileNotFoundError     If the required SRTM tile is not available</p> Source code in <code>gigaspatial/handlers/srtm/srtm_manager.py</code> <pre><code>def get_elevation(self, latitude: float, longitude: float) -&gt; float:\n    \"\"\"\n    Get interpolated elevation for a specific coordinate.\n\n    Automatically finds and loads the correct SRTM tile.\n\n    Parameters\n    ----------\n    latitude : float\n        Latitude in decimal degrees (-90 to 90)\n    longitude : float\n        Longitude in decimal degrees (-180 to 180)\n\n    Returns\n    -------\n    float\n        Interpolated elevation in meters\n\n    Raises\n    ------\n    FileNotFoundError\n        If the required SRTM tile is not available\n    \"\"\"\n    # Get tile coordinates\n    lat_tile, lon_tile = self._get_tile_coordinates(latitude, longitude)\n\n    # Load parser (cached)\n    parser = self._get_parser_cached(lat_tile, lon_tile)\n\n    # Get elevation\n    return parser.get_elevation(latitude, longitude)\n</code></pre>"},{"location":"api/handlers/#gigaspatial.handlers.srtm.srtm_manager.SRTMManager.get_elevation_batch","title":"<code>get_elevation_batch(coordinates)</code>","text":"<p>Get elevations for multiple coordinates efficiently.</p> <p>Groups coordinates by tile to minimize parser loads.</p>"},{"location":"api/handlers/#gigaspatial.handlers.srtm.srtm_manager.SRTMManager.get_elevation_batch--parameters","title":"Parameters","text":"<p>coordinates : np.ndarray of shape (n, 2)     Array of (latitude, longitude) pairs</p>"},{"location":"api/handlers/#gigaspatial.handlers.srtm.srtm_manager.SRTMManager.get_elevation_batch--returns","title":"Returns","text":"<p>np.ndarray of shape (n,)     Elevations in meters</p>"},{"location":"api/handlers/#gigaspatial.handlers.srtm.srtm_manager.SRTMManager.get_elevation_batch--raises","title":"Raises","text":"<p>FileNotFoundError     If any required SRTM tile is not available</p> Source code in <code>gigaspatial/handlers/srtm/srtm_manager.py</code> <pre><code>def get_elevation_batch(self, coordinates: np.ndarray) -&gt; np.ndarray:\n    \"\"\"\n    Get elevations for multiple coordinates efficiently.\n\n    Groups coordinates by tile to minimize parser loads.\n\n    Parameters\n    ----------\n    coordinates : np.ndarray of shape (n, 2)\n        Array of (latitude, longitude) pairs\n\n    Returns\n    -------\n    np.ndarray of shape (n,)\n        Elevations in meters\n\n    Raises\n    ------\n    FileNotFoundError\n        If any required SRTM tile is not available\n    \"\"\"\n    elevations = np.zeros(len(coordinates))\n\n    # Group coordinates by tile\n    tile_groups = {}\n    for i, (lat, lon) in enumerate(coordinates):\n        tile_key = self._get_tile_coordinates(lat, lon)\n        if tile_key not in tile_groups:\n            tile_groups[tile_key] = []\n        tile_groups[tile_key].append((i, lat, lon))\n\n    # Process each tile group\n    for tile_key, coords_list in tile_groups.items():\n        parser = self._get_parser_cached(*tile_key)\n\n        # Extract coordinates for this tile\n        indices = [c[0] for c in coords_list]\n        tile_coords = np.array([[c[1], c[2]] for c in coords_list])\n\n        # Get elevations\n        tile_elevations = parser.get_elevation_batch(tile_coords)\n\n        # Store results\n        elevations[indices] = tile_elevations\n\n    return elevations\n</code></pre>"},{"location":"api/handlers/#gigaspatial.handlers.srtm.srtm_manager.SRTMManager.get_elevation_profile","title":"<code>get_elevation_profile(start_lat, start_lon, end_lat, end_lon, num_points=100)</code>","text":"<p>Get elevation profile between two points.</p> <p>Uses linear interpolation between points and automatically handles multiple SRTM tiles. For more accurate great circle paths over long distances, consider using geopy.</p>"},{"location":"api/handlers/#gigaspatial.handlers.srtm.srtm_manager.SRTMManager.get_elevation_profile--parameters","title":"Parameters","text":"<p>start_lat : float     Starting latitude in decimal degrees start_lon : float     Starting longitude in decimal degrees end_lat : float     Ending latitude in decimal degrees end_lon : float     Ending longitude in decimal degrees num_points : int, default=100     Number of sample points along the path</p>"},{"location":"api/handlers/#gigaspatial.handlers.srtm.srtm_manager.SRTMManager.get_elevation_profile--returns","title":"Returns","text":"<p>pd.DataFrame     DataFrame with columns: distance_km, latitude, longitude, elevation</p>"},{"location":"api/handlers/#gigaspatial.handlers.srtm.srtm_manager.SRTMManager.get_elevation_profile--raises","title":"Raises","text":"<p>FileNotFoundError     If any required SRTM tile along the path is not available</p> Source code in <code>gigaspatial/handlers/srtm/srtm_manager.py</code> <pre><code>def get_elevation_profile(\n    self,\n    start_lat: float,\n    start_lon: float,\n    end_lat: float,\n    end_lon: float,\n    num_points: int = 100,\n) -&gt; pd.DataFrame:\n    \"\"\"\n    Get elevation profile between two points.\n\n    Uses linear interpolation between points and automatically handles multiple SRTM tiles.\n    For more accurate great circle paths over long distances, consider using geopy.\n\n    Parameters\n    ----------\n    start_lat : float\n        Starting latitude in decimal degrees\n    start_lon : float\n        Starting longitude in decimal degrees\n    end_lat : float\n        Ending latitude in decimal degrees\n    end_lon : float\n        Ending longitude in decimal degrees\n    num_points : int, default=100\n        Number of sample points along the path\n\n    Returns\n    -------\n    pd.DataFrame\n        DataFrame with columns: distance_km, latitude, longitude, elevation\n\n    Raises\n    ------\n    FileNotFoundError\n        If any required SRTM tile along the path is not available\n    \"\"\"\n    # Generate points along the path (linear interpolation)\n    lats = np.linspace(start_lat, end_lat, num_points)\n    lons = np.linspace(start_lon, end_lon, num_points)\n\n    coordinates = np.column_stack((lats, lons))\n\n    # Get elevations for all points\n    elevations = self.get_elevation_batch(coordinates)\n\n    # Calculate distances using Haversine formula\n    distances = self._calculate_cumulative_distances(lats, lons)\n\n    # Create DataFrame\n    profile = pd.DataFrame(\n        {\n            \"distance_km\": distances,\n            \"latitude\": lats,\n            \"longitude\": lons,\n            \"elevation\": elevations,\n        }\n    )\n\n    return profile\n</code></pre>"},{"location":"api/handlers/#gigaspatial.handlers.srtm.srtm_parser","title":"<code>srtm_parser</code>","text":""},{"location":"api/handlers/#gigaspatial.handlers.srtm.srtm_parser.SRTMParser","title":"<code>SRTMParser</code>","text":"<p>Efficient parser for NASA SRTM .hgt.zip files.</p> <p>Supports both SRTM-1 (3601x3601, 1 arc-second) and SRTM-3 (1201x1201, 3 arc-second) formats. Uses memory mapping for efficient handling of large files.</p> Source code in <code>gigaspatial/handlers/srtm/srtm_parser.py</code> <pre><code>class SRTMParser:\n    \"\"\"\n    Efficient parser for NASA SRTM .hgt.zip files.\n\n    Supports both SRTM-1 (3601x3601, 1 arc-second) and SRTM-3 (1201x1201, 3 arc-second) formats.\n    Uses memory mapping for efficient handling of large files.\n    \"\"\"\n\n    def __init__(\n        self, hgt_zip_path: Union[str, Path], data_store: Optional[DataStore] = None\n    ):\n        \"\"\"\n        Initialize the SRTM parser.\n\n        Parameters\n        ----------\n        hgt_zip_path : str or Path\n            Path to the .hgt.zip file (e.g., 'S03E028.SRTMGL1.hgt.zip')\n        data_store : DataStore, optional\n            Data store for reading files. If None, uses LocalDataStore()\n        \"\"\"\n        self.hgt_zip_path = Path(hgt_zip_path)\n        self.data_store = data_store or LocalDataStore()\n\n        # Check if file exists\n        if not self.data_store.file_exists(str(self.hgt_zip_path)):\n            raise FileNotFoundError(f\"File not found: {self.hgt_zip_path}\")\n\n        # Extract tile coordinates from filename (e.g., S03E028)\n        self._parse_filename()\n\n        # Load the elevation data\n        self.data = None\n        self.resolution = None\n        self.size = None\n        self._load_data()\n\n        # Set up interpolator for efficient querying\n        self._setup_interpolator()\n\n    def _parse_filename(self):\n        \"\"\"Extract latitude and longitude from the .hgt filename.\"\"\"\n        filename = self.hgt_zip_path.stem.split(\".\")[\n            0\n        ]  # Get base name without extensions\n\n        # Parse latitude (first 3 characters: N/S + 2 digits)\n        lat_str = filename[:3]\n        lat_dir = lat_str[0]\n        lat_val = int(lat_str[1:])\n        self.lat_corner = lat_val if lat_dir == \"N\" else -lat_val\n\n        # Parse longitude (next 4 characters: E/W + 3 digits)\n        lon_str = filename[3:7]\n        lon_dir = lon_str[0]\n        lon_val = int(lon_str[1:])\n        self.lon_corner = lon_val if lon_dir == \"E\" else -lon_val\n\n    def _load_data(self):\n        \"\"\"Load elevation data from .hgt.zip file using memory-efficient approach.\"\"\"\n        # Read the zip file from DataStore\n        zip_data = self.data_store.read_file(str(self.hgt_zip_path))\n\n        # Create a BytesIO object from the zip data\n        zip_file_obj = io.BytesIO(zip_data)\n\n        # Extract .hgt file from zip\n        with zipfile.ZipFile(zip_file_obj, \"r\") as zip_ref:\n            # Find the .hgt file inside the zip\n            hgt_files = [f for f in zip_ref.namelist() if f.endswith(\".hgt\")]\n\n            if not hgt_files:\n                raise ValueError(f\"No .hgt file found in {self.hgt_zip_path}\")\n\n            hgt_filename = hgt_files[0]\n\n            # Read the binary data\n            with zip_ref.open(hgt_filename) as hgt_file:\n                hgt_data = hgt_file.read()\n\n        # Determine resolution based on file size\n        file_size = len(hgt_data)\n\n        if file_size == 25934402:  # 3601 * 3601 * 2 bytes (SRTM-1, 1 arc-second)\n            self.size = 3601\n            self.resolution = 1 / 3600  # degrees\n        elif file_size == 2884802:  # 1201 * 1201 * 2 bytes (SRTM-3, 3 arc-second)\n            self.size = 1201\n            self.resolution = 3 / 3600  # degrees\n        else:\n            raise ValueError(f\"Unexpected file size: {file_size} bytes\")\n\n        # Parse binary data as big-endian 16-bit signed integers\n        # Using numpy for efficiency\n        self.data = np.frombuffer(hgt_data, dtype=\"&gt;i2\").reshape((self.size, self.size))\n\n        # Replace void values (-32768) with NaN\n        self.data = self.data.astype(np.float32)\n        self.data[self.data == -32768] = np.nan\n\n    def _setup_interpolator(self):\n        \"\"\"Set up RegularGridInterpolator for efficient elevation queries.\"\"\"\n        # Create coordinate arrays\n        # Note: SRTM data is stored from north to south (top to bottom)\n        lats = np.linspace(\n            self.lat_corner + 1, self.lat_corner, self.size  # North edge  # South edge\n        )\n        lons = np.linspace(\n            self.lon_corner, self.lon_corner + 1, self.size  # West edge  # East edge\n        )\n\n        self.interpolator = RegularGridInterpolator(\n            (lats, lons),\n            self.data,\n            method=\"linear\",\n            bounds_error=False,\n            fill_value=np.nan,\n        )\n\n        # Store coordinate arrays for reference\n        self.lats = lats\n        self.lons = lons\n\n    def to_dataframe(self, dropna=True) -&gt; pd.DataFrame:\n        \"\"\"\n        Convert elevation data to a DataFrame with coordinates.\n\n        Returns\n        -------\n        pd.DataFrame\n            DataFrame with columns: latitude, longitude, elevation\n        \"\"\"\n        # Create meshgrid of coordinates\n        lon_grid, lat_grid = np.meshgrid(self.lons, self.lats)\n\n        # Flatten arrays\n        df = pd.DataFrame(\n            {\n                \"latitude\": lat_grid.ravel(),\n                \"longitude\": lon_grid.ravel(),\n                \"elevation\": self.data.ravel(),\n            }\n        )\n\n        return df.dropna(subset=[\"elevation\"]) if dropna else df\n\n    def to_array(self) -&gt; Tuple[np.ndarray, np.ndarray, np.ndarray]:\n        \"\"\"\n        Return elevation data in square array form with coordinate arrays.\n\n        Returns\n        -------\n        tuple of (elevation_array, latitudes, longitudes)\n            elevation_array : np.ndarray of shape (size, size)\n                2D array of elevation values in meters\n            latitudes : np.ndarray of shape (size,)\n                Latitude values for each row (north to south)\n            longitudes : np.ndarray of shape (size,)\n                Longitude values for each column (west to east)\n        \"\"\"\n        return self.data.copy(), self.lats.copy(), self.lons.copy()\n\n    def get_elevation(self, latitude: float, longitude: float) -&gt; float:\n        \"\"\"\n        Get interpolated elevation for a specific coordinate.\n\n        Uses bilinear interpolation for accurate elevation values between grid points.\n\n        Parameters\n        ----------\n        latitude : float\n            Latitude in decimal degrees\n        longitude : float\n            Longitude in decimal degrees\n\n        Returns\n        -------\n        float\n            Interpolated elevation in meters, or np.nan if outside tile bounds\n        \"\"\"\n        # Check if coordinates are within tile bounds\n        if not (self.lat_corner &lt;= latitude &lt;= self.lat_corner + 1):\n            return np.nan\n        if not (self.lon_corner &lt;= longitude &lt;= self.lon_corner + 1):\n            return np.nan\n\n        # Use interpolator for bilinear interpolation\n        elevation = self.interpolator([[latitude, longitude]])[0]\n\n        return float(elevation)\n\n    def get_elevation_batch(self, coordinates: np.ndarray) -&gt; np.ndarray:\n        \"\"\"\n        Get interpolated elevations for multiple coordinates (vectorized).\n\n        Parameters\n        ----------\n        coordinates : np.ndarray of shape (n, 2)\n            Array of (latitude, longitude) pairs\n\n        Returns\n        -------\n        np.ndarray of shape (n,)\n            Interpolated elevations in meters\n        \"\"\"\n        return self.interpolator(coordinates)\n\n    def get_tile_info(self) -&gt; dict:\n        \"\"\"\n        Get information about the SRTM tile.\n\n        Returns\n        -------\n        dict\n            Dictionary containing tile metadata\n        \"\"\"\n        return {\n            \"filename\": self.hgt_zip_path.name,\n            \"lat_corner\": self.lat_corner,\n            \"lon_corner\": self.lon_corner,\n            \"lat_range\": (self.lat_corner, self.lat_corner + 1),\n            \"lon_range\": (self.lon_corner, self.lon_corner + 1),\n            \"resolution_arcsec\": 1 if self.size == 3601 else 3,\n            \"resolution_deg\": self.resolution,\n            \"size\": (self.size, self.size),\n            \"min_elevation\": float(np.nanmin(self.data)),\n            \"max_elevation\": float(np.nanmax(self.data)),\n            \"mean_elevation\": float(np.nanmean(self.data)),\n            \"void_percentage\": float(np.isnan(self.data).sum() / self.data.size * 100),\n        }\n\n    def __repr__(self):\n        return (\n            f\"SRTMParser(tile={self.lat_corner:+03d}{self.lon_corner:+04d}, \"\n            f\"resolution={self.resolution*3600:.0f}arcsec, \"\n            f\"size={self.size}x{self.size}, \"\n            f\"data_store={type(self.data_store).__name__})\"\n        )\n</code></pre>"},{"location":"api/handlers/#gigaspatial.handlers.srtm.srtm_parser.SRTMParser.__init__","title":"<code>__init__(hgt_zip_path, data_store=None)</code>","text":"<p>Initialize the SRTM parser.</p>"},{"location":"api/handlers/#gigaspatial.handlers.srtm.srtm_parser.SRTMParser.__init__--parameters","title":"Parameters","text":"<p>hgt_zip_path : str or Path     Path to the .hgt.zip file (e.g., 'S03E028.SRTMGL1.hgt.zip') data_store : DataStore, optional     Data store for reading files. If None, uses LocalDataStore()</p> Source code in <code>gigaspatial/handlers/srtm/srtm_parser.py</code> <pre><code>def __init__(\n    self, hgt_zip_path: Union[str, Path], data_store: Optional[DataStore] = None\n):\n    \"\"\"\n    Initialize the SRTM parser.\n\n    Parameters\n    ----------\n    hgt_zip_path : str or Path\n        Path to the .hgt.zip file (e.g., 'S03E028.SRTMGL1.hgt.zip')\n    data_store : DataStore, optional\n        Data store for reading files. If None, uses LocalDataStore()\n    \"\"\"\n    self.hgt_zip_path = Path(hgt_zip_path)\n    self.data_store = data_store or LocalDataStore()\n\n    # Check if file exists\n    if not self.data_store.file_exists(str(self.hgt_zip_path)):\n        raise FileNotFoundError(f\"File not found: {self.hgt_zip_path}\")\n\n    # Extract tile coordinates from filename (e.g., S03E028)\n    self._parse_filename()\n\n    # Load the elevation data\n    self.data = None\n    self.resolution = None\n    self.size = None\n    self._load_data()\n\n    # Set up interpolator for efficient querying\n    self._setup_interpolator()\n</code></pre>"},{"location":"api/handlers/#gigaspatial.handlers.srtm.srtm_parser.SRTMParser.get_elevation","title":"<code>get_elevation(latitude, longitude)</code>","text":"<p>Get interpolated elevation for a specific coordinate.</p> <p>Uses bilinear interpolation for accurate elevation values between grid points.</p>"},{"location":"api/handlers/#gigaspatial.handlers.srtm.srtm_parser.SRTMParser.get_elevation--parameters","title":"Parameters","text":"<p>latitude : float     Latitude in decimal degrees longitude : float     Longitude in decimal degrees</p>"},{"location":"api/handlers/#gigaspatial.handlers.srtm.srtm_parser.SRTMParser.get_elevation--returns","title":"Returns","text":"<p>float     Interpolated elevation in meters, or np.nan if outside tile bounds</p> Source code in <code>gigaspatial/handlers/srtm/srtm_parser.py</code> <pre><code>def get_elevation(self, latitude: float, longitude: float) -&gt; float:\n    \"\"\"\n    Get interpolated elevation for a specific coordinate.\n\n    Uses bilinear interpolation for accurate elevation values between grid points.\n\n    Parameters\n    ----------\n    latitude : float\n        Latitude in decimal degrees\n    longitude : float\n        Longitude in decimal degrees\n\n    Returns\n    -------\n    float\n        Interpolated elevation in meters, or np.nan if outside tile bounds\n    \"\"\"\n    # Check if coordinates are within tile bounds\n    if not (self.lat_corner &lt;= latitude &lt;= self.lat_corner + 1):\n        return np.nan\n    if not (self.lon_corner &lt;= longitude &lt;= self.lon_corner + 1):\n        return np.nan\n\n    # Use interpolator for bilinear interpolation\n    elevation = self.interpolator([[latitude, longitude]])[0]\n\n    return float(elevation)\n</code></pre>"},{"location":"api/handlers/#gigaspatial.handlers.srtm.srtm_parser.SRTMParser.get_elevation_batch","title":"<code>get_elevation_batch(coordinates)</code>","text":"<p>Get interpolated elevations for multiple coordinates (vectorized).</p>"},{"location":"api/handlers/#gigaspatial.handlers.srtm.srtm_parser.SRTMParser.get_elevation_batch--parameters","title":"Parameters","text":"<p>coordinates : np.ndarray of shape (n, 2)     Array of (latitude, longitude) pairs</p>"},{"location":"api/handlers/#gigaspatial.handlers.srtm.srtm_parser.SRTMParser.get_elevation_batch--returns","title":"Returns","text":"<p>np.ndarray of shape (n,)     Interpolated elevations in meters</p> Source code in <code>gigaspatial/handlers/srtm/srtm_parser.py</code> <pre><code>def get_elevation_batch(self, coordinates: np.ndarray) -&gt; np.ndarray:\n    \"\"\"\n    Get interpolated elevations for multiple coordinates (vectorized).\n\n    Parameters\n    ----------\n    coordinates : np.ndarray of shape (n, 2)\n        Array of (latitude, longitude) pairs\n\n    Returns\n    -------\n    np.ndarray of shape (n,)\n        Interpolated elevations in meters\n    \"\"\"\n    return self.interpolator(coordinates)\n</code></pre>"},{"location":"api/handlers/#gigaspatial.handlers.srtm.srtm_parser.SRTMParser.get_tile_info","title":"<code>get_tile_info()</code>","text":"<p>Get information about the SRTM tile.</p>"},{"location":"api/handlers/#gigaspatial.handlers.srtm.srtm_parser.SRTMParser.get_tile_info--returns","title":"Returns","text":"<p>dict     Dictionary containing tile metadata</p> Source code in <code>gigaspatial/handlers/srtm/srtm_parser.py</code> <pre><code>def get_tile_info(self) -&gt; dict:\n    \"\"\"\n    Get information about the SRTM tile.\n\n    Returns\n    -------\n    dict\n        Dictionary containing tile metadata\n    \"\"\"\n    return {\n        \"filename\": self.hgt_zip_path.name,\n        \"lat_corner\": self.lat_corner,\n        \"lon_corner\": self.lon_corner,\n        \"lat_range\": (self.lat_corner, self.lat_corner + 1),\n        \"lon_range\": (self.lon_corner, self.lon_corner + 1),\n        \"resolution_arcsec\": 1 if self.size == 3601 else 3,\n        \"resolution_deg\": self.resolution,\n        \"size\": (self.size, self.size),\n        \"min_elevation\": float(np.nanmin(self.data)),\n        \"max_elevation\": float(np.nanmax(self.data)),\n        \"mean_elevation\": float(np.nanmean(self.data)),\n        \"void_percentage\": float(np.isnan(self.data).sum() / self.data.size * 100),\n    }\n</code></pre>"},{"location":"api/handlers/#gigaspatial.handlers.srtm.srtm_parser.SRTMParser.to_array","title":"<code>to_array()</code>","text":"<p>Return elevation data in square array form with coordinate arrays.</p>"},{"location":"api/handlers/#gigaspatial.handlers.srtm.srtm_parser.SRTMParser.to_array--returns","title":"Returns","text":"<p>tuple of (elevation_array, latitudes, longitudes)     elevation_array : np.ndarray of shape (size, size)         2D array of elevation values in meters     latitudes : np.ndarray of shape (size,)         Latitude values for each row (north to south)     longitudes : np.ndarray of shape (size,)         Longitude values for each column (west to east)</p> Source code in <code>gigaspatial/handlers/srtm/srtm_parser.py</code> <pre><code>def to_array(self) -&gt; Tuple[np.ndarray, np.ndarray, np.ndarray]:\n    \"\"\"\n    Return elevation data in square array form with coordinate arrays.\n\n    Returns\n    -------\n    tuple of (elevation_array, latitudes, longitudes)\n        elevation_array : np.ndarray of shape (size, size)\n            2D array of elevation values in meters\n        latitudes : np.ndarray of shape (size,)\n            Latitude values for each row (north to south)\n        longitudes : np.ndarray of shape (size,)\n            Longitude values for each column (west to east)\n    \"\"\"\n    return self.data.copy(), self.lats.copy(), self.lons.copy()\n</code></pre>"},{"location":"api/handlers/#gigaspatial.handlers.srtm.srtm_parser.SRTMParser.to_dataframe","title":"<code>to_dataframe(dropna=True)</code>","text":"<p>Convert elevation data to a DataFrame with coordinates.</p>"},{"location":"api/handlers/#gigaspatial.handlers.srtm.srtm_parser.SRTMParser.to_dataframe--returns","title":"Returns","text":"<p>pd.DataFrame     DataFrame with columns: latitude, longitude, elevation</p> Source code in <code>gigaspatial/handlers/srtm/srtm_parser.py</code> <pre><code>def to_dataframe(self, dropna=True) -&gt; pd.DataFrame:\n    \"\"\"\n    Convert elevation data to a DataFrame with coordinates.\n\n    Returns\n    -------\n    pd.DataFrame\n        DataFrame with columns: latitude, longitude, elevation\n    \"\"\"\n    # Create meshgrid of coordinates\n    lon_grid, lat_grid = np.meshgrid(self.lons, self.lats)\n\n    # Flatten arrays\n    df = pd.DataFrame(\n        {\n            \"latitude\": lat_grid.ravel(),\n            \"longitude\": lon_grid.ravel(),\n            \"elevation\": self.data.ravel(),\n        }\n    )\n\n    return df.dropna(subset=[\"elevation\"]) if dropna else df\n</code></pre>"},{"location":"api/handlers/#gigaspatial.handlers.srtm.utils","title":"<code>utils</code>","text":""},{"location":"api/handlers/#gigaspatial.handlers.srtm.utils.EarthdataSession","title":"<code>EarthdataSession</code>","text":"<p>               Bases: <code>Session</code></p> <p>Custom requests.Session for NASA Earthdata authentication.</p> <p>Maintains Authorization headers through redirects to/from Earthdata hosts. This is required because Earthdata uses multiple redirect domains during authentication.</p> Source code in <code>gigaspatial/handlers/srtm/utils.py</code> <pre><code>class EarthdataSession(requests.Session):\n    \"\"\"\n    Custom requests.Session for NASA Earthdata authentication.\n\n    Maintains Authorization headers through redirects to/from Earthdata hosts.\n    This is required because Earthdata uses multiple redirect domains during authentication.\n    \"\"\"\n\n    AUTH_HOST = \"urs.earthdata.nasa.gov\"\n\n    def __init__(self, username: str, password: str):\n        super().__init__()\n        self.auth = (username, password)\n\n    def rebuild_auth(self, prepared_request, response):\n        \"\"\"Keep auth header on redirects to/from Earthdata host.\"\"\"\n        headers = prepared_request.headers\n        url = prepared_request.url\n\n        if \"Authorization\" in headers:\n            original_parsed = requests.utils.urlparse(response.request.url)\n            redirect_parsed = requests.utils.urlparse(url)\n\n            # remove Authorization only if redirecting *away from* Earthdata\n            if (\n                (original_parsed.hostname != redirect_parsed.hostname)\n                and redirect_parsed.hostname != self.AUTH_HOST\n                and original_parsed.hostname != self.AUTH_HOST\n            ):\n                del headers[\"Authorization\"]\n</code></pre>"},{"location":"api/handlers/#gigaspatial.handlers.srtm.utils.EarthdataSession.rebuild_auth","title":"<code>rebuild_auth(prepared_request, response)</code>","text":"<p>Keep auth header on redirects to/from Earthdata host.</p> Source code in <code>gigaspatial/handlers/srtm/utils.py</code> <pre><code>def rebuild_auth(self, prepared_request, response):\n    \"\"\"Keep auth header on redirects to/from Earthdata host.\"\"\"\n    headers = prepared_request.headers\n    url = prepared_request.url\n\n    if \"Authorization\" in headers:\n        original_parsed = requests.utils.urlparse(response.request.url)\n        redirect_parsed = requests.utils.urlparse(url)\n\n        # remove Authorization only if redirecting *away from* Earthdata\n        if (\n            (original_parsed.hostname != redirect_parsed.hostname)\n            and redirect_parsed.hostname != self.AUTH_HOST\n            and original_parsed.hostname != self.AUTH_HOST\n        ):\n            del headers[\"Authorization\"]\n</code></pre>"},{"location":"api/handlers/#gigaspatial.handlers.unicef_georepo","title":"<code>unicef_georepo</code>","text":""},{"location":"api/handlers/#gigaspatial.handlers.unicef_georepo.GeoRepoClient","title":"<code>GeoRepoClient</code>","text":"<p>A client for interacting with the GeoRepo API.</p> <p>GeoRepo is a platform for managing and accessing geospatial administrative boundary data. This client provides methods to search, retrieve, and work with modules, datasets, views, and administrative entities.</p> <p>Attributes:</p> Name Type Description <code>base_url</code> <code>str</code> <p>The base URL for the GeoRepo API</p> <code>api_key</code> <code>str</code> <p>The API key for authentication</p> <code>email</code> <code>str</code> <p>The email address associated with the API key</p> <code>headers</code> <code>dict</code> <p>HTTP headers used for API requests</p> Source code in <code>gigaspatial/handlers/unicef_georepo.py</code> <pre><code>class GeoRepoClient:\n    \"\"\"\n    A client for interacting with the GeoRepo API.\n\n    GeoRepo is a platform for managing and accessing geospatial administrative\n    boundary data. This client provides methods to search, retrieve, and work\n    with modules, datasets, views, and administrative entities.\n\n    Attributes:\n        base_url (str): The base URL for the GeoRepo API\n        api_key (str): The API key for authentication\n        email (str): The email address associated with the API key\n        headers (dict): HTTP headers used for API requests\n    \"\"\"\n\n    def __init__(self, api_key=None, email=None):\n        \"\"\"\n        Initialize the GeoRepo client.\n\n        Args:\n            api_key (str, optional): GeoRepo API key. If not provided, will use\n                the GEOREPO_API_KEY environment variable from config.\n            email (str, optional): Email address associated with the API key.\n                If not provided, will use the GEOREPO_USER_EMAIL environment\n                variable from config.\n\n        Raises:\n            ValueError: If api_key or email is not provided and cannot be found\n                in environment variables.\n        \"\"\"\n        self.base_url = \"https://georepo.unicef.org/api/v1\"\n        self.api_key = api_key or config.GEOREPO_API_KEY\n        self.email = email or config.GEOREPO_USER_EMAIL\n        self.logger = config.get_logger(self.__class__.__name__)\n\n        if not self.api_key:\n            raise ValueError(\n                \"API Key is required. Provide it as a parameter or set GEOREPO_API_KEY environment variable.\"\n            )\n\n        if not self.email:\n            raise ValueError(\n                \"Email is required. Provide it as a parameter or set GEOREPO_USER_EMAIL environment variable.\"\n            )\n\n        self.headers = {\n            \"Accept\": \"application/json\",\n            \"Authorization\": f\"Token {self.api_key}\",\n            \"GeoRepo-User-Key\": self.email,\n        }\n\n    def _make_request(self, method, endpoint, params=None, data=None):\n        \"\"\"Internal method to handle making HTTP requests.\"\"\"\n        try:\n            response = requests.request(\n                method, endpoint, headers=self.headers, params=params, json=data\n            )\n            response.raise_for_status()\n            return response\n        except requests.exceptions.RequestException as e:\n            raise requests.exceptions.HTTPError(f\"API request failed: {e}\")\n\n    def check_connection(self):\n        \"\"\"\n        Checks if the API connection is valid by making a simple request.\n\n        Returns:\n            bool: True if the connection is valid, False otherwise.\n        \"\"\"\n        endpoint = f\"{self.base_url}/search/module/list/\"\n        try:\n            self._make_request(\"GET\", endpoint)\n            return True\n        except requests.exceptions.HTTPError as e:\n            return False\n        except requests.exceptions.RequestException as e:\n            raise requests.exceptions.RequestException(\n                f\"Connection check encountered a network error: {e}\"\n            )\n\n    def list_modules(self):\n        \"\"\"\n        List all available modules in GeoRepo.\n\n        A module is a top-level organizational unit that contains datasets.\n        Examples include \"Admin Boundaries\", \"Health Facilities\", etc.\n\n        Returns:\n            dict: JSON response containing a list of modules with their metadata.\n                Each module includes 'uuid', 'name', 'description', and other properties.\n\n        Raises:\n            requests.HTTPError: If the API request fails.\n        \"\"\"\n        endpoint = f\"{self.base_url}/search/module/list/\"\n        response = self._make_request(\"GET\", endpoint)\n        return response.json()\n\n    def list_datasets_by_module(self, module_uuid):\n        \"\"\"\n        List all datasets within a specific module.\n\n        A dataset represents a collection of related geographic entities,\n        such as administrative boundaries for a specific country or region.\n\n        Args:\n            module_uuid (str): The UUID of the module to query.\n\n        Returns:\n            dict: JSON response containing a list of datasets with their metadata.\n                Each dataset includes 'uuid', 'name', 'description', creation date, etc.\n\n        Raises:\n            requests.HTTPError: If the API request fails or module_uuid is invalid.\n        \"\"\"\n        endpoint = f\"{self.base_url}/search/module/{module_uuid}/dataset/list/\"\n        response = self._make_request(\"GET\", endpoint)\n        return response.json()\n\n    def get_dataset_details(self, dataset_uuid):\n        \"\"\"\n        Get detailed information about a specific dataset.\n\n        This includes metadata about the dataset and information about\n        available administrative levels (e.g., country, province, district).\n\n        Args:\n            dataset_uuid (str): The UUID of the dataset to query.\n\n        Returns:\n            dict: JSON response containing dataset details including:\n                - Basic metadata (name, description, etc.)\n                - Available administrative levels and their properties\n                - Temporal information and data sources\n\n        Raises:\n            requests.HTTPError: If the API request fails or dataset_uuid is invalid.\n        \"\"\"\n        endpoint = f\"{self.base_url}/search/dataset/{dataset_uuid}/\"\n        response = self._make_request(\"GET\", endpoint)\n        return response.json()\n\n    def list_views_by_dataset(self, dataset_uuid, page=1, page_size=50):\n        \"\"\"\n        List views for a dataset with pagination support.\n\n        A view represents a specific version or subset of a dataset.\n        Views may be tagged as 'latest' or represent different time periods.\n\n        Args:\n            dataset_uuid (str): The UUID of the dataset to query.\n            page (int, optional): Page number for pagination. Defaults to 1.\n            page_size (int, optional): Number of results per page. Defaults to 50.\n\n        Returns:\n            dict: JSON response containing paginated list of views with metadata.\n                Includes 'results', 'total_page', 'current_page', and 'count' fields.\n                Each view includes 'uuid', 'name', 'tags', and other properties.\n\n        Raises:\n            requests.HTTPError: If the API request fails or dataset_uuid is invalid.\n        \"\"\"\n        endpoint = f\"{self.base_url}/search/dataset/{dataset_uuid}/view/list/\"\n        params = {\"page\": page, \"page_size\": page_size}\n        response = self._make_request(\"GET\", endpoint, params=params)\n        return response.json()\n\n    def list_entities_by_admin_level(\n        self,\n        view_uuid,\n        admin_level,\n        geom=\"no_geom\",\n        format=\"json\",\n        page=1,\n        page_size=50,\n    ):\n        \"\"\"\n        List entities at a specific administrative level within a view.\n\n        Administrative levels typically follow a hierarchy:\n        - Level 0: Countries\n        - Level 1: States/Provinces/Regions\n        - Level 2: Districts/Counties\n        - Level 3: Sub-districts/Municipalities\n        - And so on...\n\n        Args:\n            view_uuid (str): The UUID of the view to query.\n            admin_level (int): The administrative level to retrieve (0, 1, 2, etc.).\n            geom (str, optional): Geometry inclusion level. Options:\n                - \"no_geom\": No geometry data\n                - \"centroid\": Only centroid points\n                - \"full_geom\": Complete boundary geometries\n                Defaults to \"no_geom\".\n            format (str, optional): Response format (\"json\" or \"geojson\").\n                Defaults to \"json\".\n            page (int, optional): Page number for pagination. Defaults to 1.\n            page_size (int, optional): Number of results per page. Defaults to 50.\n\n        Returns:\n            tuple: A tuple containing:\n                - dict: JSON/GeoJSON response with entity data\n                - dict: Metadata with pagination info (page, total_page, total_count)\n\n        Raises:\n            requests.HTTPError: If the API request fails or parameters are invalid.\n        \"\"\"\n        endpoint = (\n            f\"{self.base_url}/search/view/{view_uuid}/entity/level/{admin_level}/\"\n        )\n        params = {\"page\": page, \"page_size\": page_size, \"geom\": geom, \"format\": format}\n        response = self._make_request(\"GET\", endpoint, params=params)\n\n        metadata = {\n            \"page\": int(response.headers.get(\"page\", 1)),\n            \"total_page\": int(response.headers.get(\"total_page\", 1)),\n            \"total_count\": int(response.headers.get(\"count\", 0)),\n        }\n\n        return response.json(), metadata\n\n    def get_entity_by_ucode(self, ucode, geom=\"full_geom\", format=\"geojson\"):\n        \"\"\"\n        Get detailed information about a specific entity using its Ucode.\n\n        A Ucode (Universal Code) is a unique identifier for geographic entities\n        within the GeoRepo system, typically in the format \"ISO3_LEVEL_NAME\".\n\n        Args:\n            ucode (str): The unique code identifier for the entity.\n            geom (str, optional): Geometry inclusion level. Options:\n                - \"no_geom\": No geometry data\n                - \"centroid\": Only centroid points\n                - \"full_geom\": Complete boundary geometries\n                Defaults to \"full_geom\".\n            format (str, optional): Response format (\"json\" or \"geojson\").\n                Defaults to \"geojson\".\n\n        Returns:\n            dict: JSON/GeoJSON response containing entity details including\n                geometry, properties, administrative level, and metadata.\n\n        Raises:\n            requests.HTTPError: If the API request fails or ucode is invalid.\n        \"\"\"\n        endpoint = f\"{self.base_url}/search/entity/ucode/{ucode}/\"\n        params = {\"geom\": geom, \"format\": format}\n        response = self._make_request(\"GET\", endpoint, params=params)\n        return response.json()\n\n    def list_entity_children(\n        self, view_uuid, entity_ucode, geom=\"no_geom\", format=\"json\"\n    ):\n        \"\"\"\n        List direct children of an entity in the administrative hierarchy.\n\n        For example, if given a country entity, this will return its states/provinces.\n        If given a state entity, this will return its districts/counties.\n\n        Args:\n            view_uuid (str): The UUID of the view containing the entity.\n            entity_ucode (str): The Ucode of the parent entity.\n            geom (str, optional): Geometry inclusion level. Options:\n                - \"no_geom\": No geometry data\n                - \"centroid\": Only centroid points\n                - \"full_geom\": Complete boundary geometries\n                Defaults to \"no_geom\".\n            format (str, optional): Response format (\"json\" or \"geojson\").\n                Defaults to \"json\".\n\n        Returns:\n            dict: JSON/GeoJSON response containing list of child entities\n                with their properties and optional geometry data.\n\n        Raises:\n            requests.HTTPError: If the API request fails or parameters are invalid.\n        \"\"\"\n        endpoint = (\n            f\"{self.base_url}/search/view/{view_uuid}/entity/{entity_ucode}/children/\"\n        )\n        params = {\"geom\": geom, \"format\": format}\n        response = self._make_request(\"GET\", endpoint, params=params)\n        return response.json()\n\n    def search_entities_by_name(self, view_uuid, name, page=1, page_size=50):\n        \"\"\"\n        Search for entities by name using fuzzy matching.\n\n        This performs a similarity-based search to find entities whose names\n        match or are similar to the provided search term.\n\n        Args:\n            view_uuid (str): The UUID of the view to search within.\n            name (str): The name or partial name to search for.\n            page (int, optional): Page number for pagination. Defaults to 1.\n            page_size (int, optional): Number of results per page. Defaults to 50.\n\n        Returns:\n            dict: JSON response containing paginated search results with\n                matching entities and their similarity scores.\n\n        Raises:\n            requests.HTTPError: If the API request fails or parameters are invalid.\n        \"\"\"\n        endpoint = f\"{self.base_url}/search/view/{view_uuid}/entity/{name}/\"\n        params = {\"page\": page, \"page_size\": page_size}\n        response = self._make_request(\"GET\", endpoint, params=params)\n        return response.json()\n\n    def get_admin_boundaries(\n        self, view_uuid, admin_level=None, geom=\"full_geom\", format=\"geojson\"\n    ):\n        \"\"\"\n        Get administrative boundaries for a specific level or all levels.\n\n        This is a convenience method that can retrieve boundaries for a single\n        administrative level or attempt to fetch all available levels.\n\n        Args:\n            view_uuid (str): The UUID of the view to query.\n            admin_level (int, optional): Administrative level to retrieve\n                (0=country, 1=region, etc.). If None, attempts to fetch all levels.\n            geom (str, optional): Geometry inclusion level. Options:\n                - \"no_geom\": No geometry data\n                - \"centroid\": Only centroid points\n                - \"full_geom\": Complete boundary geometries\n                Defaults to \"full_geom\".\n            format (str, optional): Response format (\"json\" or \"geojson\").\n                Defaults to \"geojson\".\n\n        Returns:\n            dict: JSON/GeoJSON response containing administrative boundaries\n                in the specified format. For GeoJSON, returns a FeatureCollection.\n\n        Raises:\n            requests.HTTPError: If the API request fails or parameters are invalid.\n        \"\"\"\n        # Construct the endpoint based on whether admin_level is provided\n        if admin_level is not None:\n            endpoint = (\n                f\"{self.base_url}/search/view/{view_uuid}/entity/level/{admin_level}/\"\n            )\n        else:\n            # For all levels, we need to fetch level 0 and then get children for each entity\n            endpoint = f\"{self.base_url}/search/view/{view_uuid}/entity/list/\"\n\n        params = {\n            \"geom\": geom,\n            \"format\": format,\n            \"page_size\": 100,\n        }\n\n        response = self._make_request(\"GET\", endpoint, params=params)\n        return response.json()\n\n    def get_vector_tiles_url(self, view_info):\n        \"\"\"\n        Generate an authenticated URL for accessing vector tiles.\n\n        Vector tiles are used for efficient map rendering and can be consumed\n        by mapping libraries like Mapbox GL JS or OpenLayers.\n\n        Args:\n            view_info (dict): Dictionary containing view information that must\n                include a 'vector_tiles' key with the base vector tiles URL.\n\n        Returns:\n            str: Fully authenticated vector tiles URL with API key and user email\n                parameters appended for access control.\n\n        Raises:\n            ValueError: If 'vector_tiles' key is not found in view_info.\n        \"\"\"\n        if \"vector_tiles\" not in view_info:\n            raise ValueError(\"Vector tiles URL not found in view information\")\n\n        vector_tiles_url = view_info[\"vector_tiles\"]\n\n        # Parse out the timestamp parameter if it exists\n        if \"?t=\" in vector_tiles_url:\n            base_url, timestamp = vector_tiles_url.split(\"?t=\")\n            return f\"{base_url}?t={timestamp}&amp;token={self.api_key}&amp;georepo_user_key={self.email}\"\n        else:\n            return (\n                f\"{vector_tiles_url}?token={self.api_key}&amp;georepo_user_key={self.email}\"\n            )\n\n    def find_country_by_iso3(self, view_uuid, iso3_code):\n        \"\"\"\n        Find a country entity using its ISO3 country code.\n\n        This method searches through all level-0 (country) entities to find\n        one that matches the provided ISO3 code. It checks both the entity's\n        Ucode and any external codes stored in the ext_codes field.\n\n        Args:\n            view_uuid (str): The UUID of the view to search within.\n            iso3_code (str): The ISO3 country code to search for (e.g., 'USA', 'KEN', 'BRA').\n\n        Returns:\n            dict or None: Entity information dictionary for the matching country\n                if found, including properties like name, ucode, admin_level, etc.\n                Returns None if no matching country is found.\n\n        Note:\n            This method handles pagination automatically to search through all\n            available countries in the dataset, which may involve multiple API calls.\n\n        Raises:\n            requests.HTTPError: If the API request fails or view_uuid is invalid.\n        \"\"\"\n        # Admin level 0 represents countries\n        endpoint = f\"{self.base_url}/search/view/{view_uuid}/entity/level/0/\"\n        params = {\n            \"page_size\": 100,\n            \"geom\": \"no_geom\",\n        }\n\n        # need to paginate since it can be a large dataset\n        all_countries = []\n        page = 1\n\n        while True:\n            params[\"page\"] = page\n            response = self._make_request(\"GET\", endpoint, params=params)\n            data = response.json()\n\n            countries = data.get(\"results\", [])\n            all_countries.extend(countries)\n\n            # check if there are more pages\n            if page &gt;= data.get(\"total_page\", 1):\n                break\n\n            page += 1\n\n        # Search by ISO3 code\n        for country in all_countries:\n            # Check if ISO3 code is in the ucode (typically at the beginning)\n            if country[\"ucode\"].startswith(iso3_code + \"_\"):\n                return country\n\n            # Also check in ext_codes which may contain the ISO3 code\n            ext_codes = country.get(\"ext_codes\", {})\n            if ext_codes:\n                # Check if ISO3 is directly in ext_codes\n                if (\n                    ext_codes.get(\"PCode\", \"\") == iso3_code\n                    or ext_codes.get(\"default\", \"\") == iso3_code\n                ):\n                    return country\n\n        return None\n</code></pre>"},{"location":"api/handlers/#gigaspatial.handlers.unicef_georepo.GeoRepoClient.__init__","title":"<code>__init__(api_key=None, email=None)</code>","text":"<p>Initialize the GeoRepo client.</p> <p>Parameters:</p> Name Type Description Default <code>api_key</code> <code>str</code> <p>GeoRepo API key. If not provided, will use the GEOREPO_API_KEY environment variable from config.</p> <code>None</code> <code>email</code> <code>str</code> <p>Email address associated with the API key. If not provided, will use the GEOREPO_USER_EMAIL environment variable from config.</p> <code>None</code> <p>Raises:</p> Type Description <code>ValueError</code> <p>If api_key or email is not provided and cannot be found in environment variables.</p> Source code in <code>gigaspatial/handlers/unicef_georepo.py</code> <pre><code>def __init__(self, api_key=None, email=None):\n    \"\"\"\n    Initialize the GeoRepo client.\n\n    Args:\n        api_key (str, optional): GeoRepo API key. If not provided, will use\n            the GEOREPO_API_KEY environment variable from config.\n        email (str, optional): Email address associated with the API key.\n            If not provided, will use the GEOREPO_USER_EMAIL environment\n            variable from config.\n\n    Raises:\n        ValueError: If api_key or email is not provided and cannot be found\n            in environment variables.\n    \"\"\"\n    self.base_url = \"https://georepo.unicef.org/api/v1\"\n    self.api_key = api_key or config.GEOREPO_API_KEY\n    self.email = email or config.GEOREPO_USER_EMAIL\n    self.logger = config.get_logger(self.__class__.__name__)\n\n    if not self.api_key:\n        raise ValueError(\n            \"API Key is required. Provide it as a parameter or set GEOREPO_API_KEY environment variable.\"\n        )\n\n    if not self.email:\n        raise ValueError(\n            \"Email is required. Provide it as a parameter or set GEOREPO_USER_EMAIL environment variable.\"\n        )\n\n    self.headers = {\n        \"Accept\": \"application/json\",\n        \"Authorization\": f\"Token {self.api_key}\",\n        \"GeoRepo-User-Key\": self.email,\n    }\n</code></pre>"},{"location":"api/handlers/#gigaspatial.handlers.unicef_georepo.GeoRepoClient.check_connection","title":"<code>check_connection()</code>","text":"<p>Checks if the API connection is valid by making a simple request.</p> <p>Returns:</p> Name Type Description <code>bool</code> <p>True if the connection is valid, False otherwise.</p> Source code in <code>gigaspatial/handlers/unicef_georepo.py</code> <pre><code>def check_connection(self):\n    \"\"\"\n    Checks if the API connection is valid by making a simple request.\n\n    Returns:\n        bool: True if the connection is valid, False otherwise.\n    \"\"\"\n    endpoint = f\"{self.base_url}/search/module/list/\"\n    try:\n        self._make_request(\"GET\", endpoint)\n        return True\n    except requests.exceptions.HTTPError as e:\n        return False\n    except requests.exceptions.RequestException as e:\n        raise requests.exceptions.RequestException(\n            f\"Connection check encountered a network error: {e}\"\n        )\n</code></pre>"},{"location":"api/handlers/#gigaspatial.handlers.unicef_georepo.GeoRepoClient.find_country_by_iso3","title":"<code>find_country_by_iso3(view_uuid, iso3_code)</code>","text":"<p>Find a country entity using its ISO3 country code.</p> <p>This method searches through all level-0 (country) entities to find one that matches the provided ISO3 code. It checks both the entity's Ucode and any external codes stored in the ext_codes field.</p> <p>Parameters:</p> Name Type Description Default <code>view_uuid</code> <code>str</code> <p>The UUID of the view to search within.</p> required <code>iso3_code</code> <code>str</code> <p>The ISO3 country code to search for (e.g., 'USA', 'KEN', 'BRA').</p> required <p>Returns:</p> Type Description <p>dict or None: Entity information dictionary for the matching country if found, including properties like name, ucode, admin_level, etc. Returns None if no matching country is found.</p> Note <p>This method handles pagination automatically to search through all available countries in the dataset, which may involve multiple API calls.</p> <p>Raises:</p> Type Description <code>HTTPError</code> <p>If the API request fails or view_uuid is invalid.</p> Source code in <code>gigaspatial/handlers/unicef_georepo.py</code> <pre><code>def find_country_by_iso3(self, view_uuid, iso3_code):\n    \"\"\"\n    Find a country entity using its ISO3 country code.\n\n    This method searches through all level-0 (country) entities to find\n    one that matches the provided ISO3 code. It checks both the entity's\n    Ucode and any external codes stored in the ext_codes field.\n\n    Args:\n        view_uuid (str): The UUID of the view to search within.\n        iso3_code (str): The ISO3 country code to search for (e.g., 'USA', 'KEN', 'BRA').\n\n    Returns:\n        dict or None: Entity information dictionary for the matching country\n            if found, including properties like name, ucode, admin_level, etc.\n            Returns None if no matching country is found.\n\n    Note:\n        This method handles pagination automatically to search through all\n        available countries in the dataset, which may involve multiple API calls.\n\n    Raises:\n        requests.HTTPError: If the API request fails or view_uuid is invalid.\n    \"\"\"\n    # Admin level 0 represents countries\n    endpoint = f\"{self.base_url}/search/view/{view_uuid}/entity/level/0/\"\n    params = {\n        \"page_size\": 100,\n        \"geom\": \"no_geom\",\n    }\n\n    # need to paginate since it can be a large dataset\n    all_countries = []\n    page = 1\n\n    while True:\n        params[\"page\"] = page\n        response = self._make_request(\"GET\", endpoint, params=params)\n        data = response.json()\n\n        countries = data.get(\"results\", [])\n        all_countries.extend(countries)\n\n        # check if there are more pages\n        if page &gt;= data.get(\"total_page\", 1):\n            break\n\n        page += 1\n\n    # Search by ISO3 code\n    for country in all_countries:\n        # Check if ISO3 code is in the ucode (typically at the beginning)\n        if country[\"ucode\"].startswith(iso3_code + \"_\"):\n            return country\n\n        # Also check in ext_codes which may contain the ISO3 code\n        ext_codes = country.get(\"ext_codes\", {})\n        if ext_codes:\n            # Check if ISO3 is directly in ext_codes\n            if (\n                ext_codes.get(\"PCode\", \"\") == iso3_code\n                or ext_codes.get(\"default\", \"\") == iso3_code\n            ):\n                return country\n\n    return None\n</code></pre>"},{"location":"api/handlers/#gigaspatial.handlers.unicef_georepo.GeoRepoClient.get_admin_boundaries","title":"<code>get_admin_boundaries(view_uuid, admin_level=None, geom='full_geom', format='geojson')</code>","text":"<p>Get administrative boundaries for a specific level or all levels.</p> <p>This is a convenience method that can retrieve boundaries for a single administrative level or attempt to fetch all available levels.</p> <p>Parameters:</p> Name Type Description Default <code>view_uuid</code> <code>str</code> <p>The UUID of the view to query.</p> required <code>admin_level</code> <code>int</code> <p>Administrative level to retrieve (0=country, 1=region, etc.). If None, attempts to fetch all levels.</p> <code>None</code> <code>geom</code> <code>str</code> <p>Geometry inclusion level. Options: - \"no_geom\": No geometry data - \"centroid\": Only centroid points - \"full_geom\": Complete boundary geometries Defaults to \"full_geom\".</p> <code>'full_geom'</code> <code>format</code> <code>str</code> <p>Response format (\"json\" or \"geojson\"). Defaults to \"geojson\".</p> <code>'geojson'</code> <p>Returns:</p> Name Type Description <code>dict</code> <p>JSON/GeoJSON response containing administrative boundaries in the specified format. For GeoJSON, returns a FeatureCollection.</p> <p>Raises:</p> Type Description <code>HTTPError</code> <p>If the API request fails or parameters are invalid.</p> Source code in <code>gigaspatial/handlers/unicef_georepo.py</code> <pre><code>def get_admin_boundaries(\n    self, view_uuid, admin_level=None, geom=\"full_geom\", format=\"geojson\"\n):\n    \"\"\"\n    Get administrative boundaries for a specific level or all levels.\n\n    This is a convenience method that can retrieve boundaries for a single\n    administrative level or attempt to fetch all available levels.\n\n    Args:\n        view_uuid (str): The UUID of the view to query.\n        admin_level (int, optional): Administrative level to retrieve\n            (0=country, 1=region, etc.). If None, attempts to fetch all levels.\n        geom (str, optional): Geometry inclusion level. Options:\n            - \"no_geom\": No geometry data\n            - \"centroid\": Only centroid points\n            - \"full_geom\": Complete boundary geometries\n            Defaults to \"full_geom\".\n        format (str, optional): Response format (\"json\" or \"geojson\").\n            Defaults to \"geojson\".\n\n    Returns:\n        dict: JSON/GeoJSON response containing administrative boundaries\n            in the specified format. For GeoJSON, returns a FeatureCollection.\n\n    Raises:\n        requests.HTTPError: If the API request fails or parameters are invalid.\n    \"\"\"\n    # Construct the endpoint based on whether admin_level is provided\n    if admin_level is not None:\n        endpoint = (\n            f\"{self.base_url}/search/view/{view_uuid}/entity/level/{admin_level}/\"\n        )\n    else:\n        # For all levels, we need to fetch level 0 and then get children for each entity\n        endpoint = f\"{self.base_url}/search/view/{view_uuid}/entity/list/\"\n\n    params = {\n        \"geom\": geom,\n        \"format\": format,\n        \"page_size\": 100,\n    }\n\n    response = self._make_request(\"GET\", endpoint, params=params)\n    return response.json()\n</code></pre>"},{"location":"api/handlers/#gigaspatial.handlers.unicef_georepo.GeoRepoClient.get_dataset_details","title":"<code>get_dataset_details(dataset_uuid)</code>","text":"<p>Get detailed information about a specific dataset.</p> <p>This includes metadata about the dataset and information about available administrative levels (e.g., country, province, district).</p> <p>Parameters:</p> Name Type Description Default <code>dataset_uuid</code> <code>str</code> <p>The UUID of the dataset to query.</p> required <p>Returns:</p> Name Type Description <code>dict</code> <p>JSON response containing dataset details including: - Basic metadata (name, description, etc.) - Available administrative levels and their properties - Temporal information and data sources</p> <p>Raises:</p> Type Description <code>HTTPError</code> <p>If the API request fails or dataset_uuid is invalid.</p> Source code in <code>gigaspatial/handlers/unicef_georepo.py</code> <pre><code>def get_dataset_details(self, dataset_uuid):\n    \"\"\"\n    Get detailed information about a specific dataset.\n\n    This includes metadata about the dataset and information about\n    available administrative levels (e.g., country, province, district).\n\n    Args:\n        dataset_uuid (str): The UUID of the dataset to query.\n\n    Returns:\n        dict: JSON response containing dataset details including:\n            - Basic metadata (name, description, etc.)\n            - Available administrative levels and their properties\n            - Temporal information and data sources\n\n    Raises:\n        requests.HTTPError: If the API request fails or dataset_uuid is invalid.\n    \"\"\"\n    endpoint = f\"{self.base_url}/search/dataset/{dataset_uuid}/\"\n    response = self._make_request(\"GET\", endpoint)\n    return response.json()\n</code></pre>"},{"location":"api/handlers/#gigaspatial.handlers.unicef_georepo.GeoRepoClient.get_entity_by_ucode","title":"<code>get_entity_by_ucode(ucode, geom='full_geom', format='geojson')</code>","text":"<p>Get detailed information about a specific entity using its Ucode.</p> <p>A Ucode (Universal Code) is a unique identifier for geographic entities within the GeoRepo system, typically in the format \"ISO3_LEVEL_NAME\".</p> <p>Parameters:</p> Name Type Description Default <code>ucode</code> <code>str</code> <p>The unique code identifier for the entity.</p> required <code>geom</code> <code>str</code> <p>Geometry inclusion level. Options: - \"no_geom\": No geometry data - \"centroid\": Only centroid points - \"full_geom\": Complete boundary geometries Defaults to \"full_geom\".</p> <code>'full_geom'</code> <code>format</code> <code>str</code> <p>Response format (\"json\" or \"geojson\"). Defaults to \"geojson\".</p> <code>'geojson'</code> <p>Returns:</p> Name Type Description <code>dict</code> <p>JSON/GeoJSON response containing entity details including geometry, properties, administrative level, and metadata.</p> <p>Raises:</p> Type Description <code>HTTPError</code> <p>If the API request fails or ucode is invalid.</p> Source code in <code>gigaspatial/handlers/unicef_georepo.py</code> <pre><code>def get_entity_by_ucode(self, ucode, geom=\"full_geom\", format=\"geojson\"):\n    \"\"\"\n    Get detailed information about a specific entity using its Ucode.\n\n    A Ucode (Universal Code) is a unique identifier for geographic entities\n    within the GeoRepo system, typically in the format \"ISO3_LEVEL_NAME\".\n\n    Args:\n        ucode (str): The unique code identifier for the entity.\n        geom (str, optional): Geometry inclusion level. Options:\n            - \"no_geom\": No geometry data\n            - \"centroid\": Only centroid points\n            - \"full_geom\": Complete boundary geometries\n            Defaults to \"full_geom\".\n        format (str, optional): Response format (\"json\" or \"geojson\").\n            Defaults to \"geojson\".\n\n    Returns:\n        dict: JSON/GeoJSON response containing entity details including\n            geometry, properties, administrative level, and metadata.\n\n    Raises:\n        requests.HTTPError: If the API request fails or ucode is invalid.\n    \"\"\"\n    endpoint = f\"{self.base_url}/search/entity/ucode/{ucode}/\"\n    params = {\"geom\": geom, \"format\": format}\n    response = self._make_request(\"GET\", endpoint, params=params)\n    return response.json()\n</code></pre>"},{"location":"api/handlers/#gigaspatial.handlers.unicef_georepo.GeoRepoClient.get_vector_tiles_url","title":"<code>get_vector_tiles_url(view_info)</code>","text":"<p>Generate an authenticated URL for accessing vector tiles.</p> <p>Vector tiles are used for efficient map rendering and can be consumed by mapping libraries like Mapbox GL JS or OpenLayers.</p> <p>Parameters:</p> Name Type Description Default <code>view_info</code> <code>dict</code> <p>Dictionary containing view information that must include a 'vector_tiles' key with the base vector tiles URL.</p> required <p>Returns:</p> Name Type Description <code>str</code> <p>Fully authenticated vector tiles URL with API key and user email parameters appended for access control.</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If 'vector_tiles' key is not found in view_info.</p> Source code in <code>gigaspatial/handlers/unicef_georepo.py</code> <pre><code>def get_vector_tiles_url(self, view_info):\n    \"\"\"\n    Generate an authenticated URL for accessing vector tiles.\n\n    Vector tiles are used for efficient map rendering and can be consumed\n    by mapping libraries like Mapbox GL JS or OpenLayers.\n\n    Args:\n        view_info (dict): Dictionary containing view information that must\n            include a 'vector_tiles' key with the base vector tiles URL.\n\n    Returns:\n        str: Fully authenticated vector tiles URL with API key and user email\n            parameters appended for access control.\n\n    Raises:\n        ValueError: If 'vector_tiles' key is not found in view_info.\n    \"\"\"\n    if \"vector_tiles\" not in view_info:\n        raise ValueError(\"Vector tiles URL not found in view information\")\n\n    vector_tiles_url = view_info[\"vector_tiles\"]\n\n    # Parse out the timestamp parameter if it exists\n    if \"?t=\" in vector_tiles_url:\n        base_url, timestamp = vector_tiles_url.split(\"?t=\")\n        return f\"{base_url}?t={timestamp}&amp;token={self.api_key}&amp;georepo_user_key={self.email}\"\n    else:\n        return (\n            f\"{vector_tiles_url}?token={self.api_key}&amp;georepo_user_key={self.email}\"\n        )\n</code></pre>"},{"location":"api/handlers/#gigaspatial.handlers.unicef_georepo.GeoRepoClient.list_datasets_by_module","title":"<code>list_datasets_by_module(module_uuid)</code>","text":"<p>List all datasets within a specific module.</p> <p>A dataset represents a collection of related geographic entities, such as administrative boundaries for a specific country or region.</p> <p>Parameters:</p> Name Type Description Default <code>module_uuid</code> <code>str</code> <p>The UUID of the module to query.</p> required <p>Returns:</p> Name Type Description <code>dict</code> <p>JSON response containing a list of datasets with their metadata. Each dataset includes 'uuid', 'name', 'description', creation date, etc.</p> <p>Raises:</p> Type Description <code>HTTPError</code> <p>If the API request fails or module_uuid is invalid.</p> Source code in <code>gigaspatial/handlers/unicef_georepo.py</code> <pre><code>def list_datasets_by_module(self, module_uuid):\n    \"\"\"\n    List all datasets within a specific module.\n\n    A dataset represents a collection of related geographic entities,\n    such as administrative boundaries for a specific country or region.\n\n    Args:\n        module_uuid (str): The UUID of the module to query.\n\n    Returns:\n        dict: JSON response containing a list of datasets with their metadata.\n            Each dataset includes 'uuid', 'name', 'description', creation date, etc.\n\n    Raises:\n        requests.HTTPError: If the API request fails or module_uuid is invalid.\n    \"\"\"\n    endpoint = f\"{self.base_url}/search/module/{module_uuid}/dataset/list/\"\n    response = self._make_request(\"GET\", endpoint)\n    return response.json()\n</code></pre>"},{"location":"api/handlers/#gigaspatial.handlers.unicef_georepo.GeoRepoClient.list_entities_by_admin_level","title":"<code>list_entities_by_admin_level(view_uuid, admin_level, geom='no_geom', format='json', page=1, page_size=50)</code>","text":"<p>List entities at a specific administrative level within a view.</p> <p>Administrative levels typically follow a hierarchy: - Level 0: Countries - Level 1: States/Provinces/Regions - Level 2: Districts/Counties - Level 3: Sub-districts/Municipalities - And so on...</p> <p>Parameters:</p> Name Type Description Default <code>view_uuid</code> <code>str</code> <p>The UUID of the view to query.</p> required <code>admin_level</code> <code>int</code> <p>The administrative level to retrieve (0, 1, 2, etc.).</p> required <code>geom</code> <code>str</code> <p>Geometry inclusion level. Options: - \"no_geom\": No geometry data - \"centroid\": Only centroid points - \"full_geom\": Complete boundary geometries Defaults to \"no_geom\".</p> <code>'no_geom'</code> <code>format</code> <code>str</code> <p>Response format (\"json\" or \"geojson\"). Defaults to \"json\".</p> <code>'json'</code> <code>page</code> <code>int</code> <p>Page number for pagination. Defaults to 1.</p> <code>1</code> <code>page_size</code> <code>int</code> <p>Number of results per page. Defaults to 50.</p> <code>50</code> <p>Returns:</p> Name Type Description <code>tuple</code> <p>A tuple containing: - dict: JSON/GeoJSON response with entity data - dict: Metadata with pagination info (page, total_page, total_count)</p> <p>Raises:</p> Type Description <code>HTTPError</code> <p>If the API request fails or parameters are invalid.</p> Source code in <code>gigaspatial/handlers/unicef_georepo.py</code> <pre><code>def list_entities_by_admin_level(\n    self,\n    view_uuid,\n    admin_level,\n    geom=\"no_geom\",\n    format=\"json\",\n    page=1,\n    page_size=50,\n):\n    \"\"\"\n    List entities at a specific administrative level within a view.\n\n    Administrative levels typically follow a hierarchy:\n    - Level 0: Countries\n    - Level 1: States/Provinces/Regions\n    - Level 2: Districts/Counties\n    - Level 3: Sub-districts/Municipalities\n    - And so on...\n\n    Args:\n        view_uuid (str): The UUID of the view to query.\n        admin_level (int): The administrative level to retrieve (0, 1, 2, etc.).\n        geom (str, optional): Geometry inclusion level. Options:\n            - \"no_geom\": No geometry data\n            - \"centroid\": Only centroid points\n            - \"full_geom\": Complete boundary geometries\n            Defaults to \"no_geom\".\n        format (str, optional): Response format (\"json\" or \"geojson\").\n            Defaults to \"json\".\n        page (int, optional): Page number for pagination. Defaults to 1.\n        page_size (int, optional): Number of results per page. Defaults to 50.\n\n    Returns:\n        tuple: A tuple containing:\n            - dict: JSON/GeoJSON response with entity data\n            - dict: Metadata with pagination info (page, total_page, total_count)\n\n    Raises:\n        requests.HTTPError: If the API request fails or parameters are invalid.\n    \"\"\"\n    endpoint = (\n        f\"{self.base_url}/search/view/{view_uuid}/entity/level/{admin_level}/\"\n    )\n    params = {\"page\": page, \"page_size\": page_size, \"geom\": geom, \"format\": format}\n    response = self._make_request(\"GET\", endpoint, params=params)\n\n    metadata = {\n        \"page\": int(response.headers.get(\"page\", 1)),\n        \"total_page\": int(response.headers.get(\"total_page\", 1)),\n        \"total_count\": int(response.headers.get(\"count\", 0)),\n    }\n\n    return response.json(), metadata\n</code></pre>"},{"location":"api/handlers/#gigaspatial.handlers.unicef_georepo.GeoRepoClient.list_entity_children","title":"<code>list_entity_children(view_uuid, entity_ucode, geom='no_geom', format='json')</code>","text":"<p>List direct children of an entity in the administrative hierarchy.</p> <p>For example, if given a country entity, this will return its states/provinces. If given a state entity, this will return its districts/counties.</p> <p>Parameters:</p> Name Type Description Default <code>view_uuid</code> <code>str</code> <p>The UUID of the view containing the entity.</p> required <code>entity_ucode</code> <code>str</code> <p>The Ucode of the parent entity.</p> required <code>geom</code> <code>str</code> <p>Geometry inclusion level. Options: - \"no_geom\": No geometry data - \"centroid\": Only centroid points - \"full_geom\": Complete boundary geometries Defaults to \"no_geom\".</p> <code>'no_geom'</code> <code>format</code> <code>str</code> <p>Response format (\"json\" or \"geojson\"). Defaults to \"json\".</p> <code>'json'</code> <p>Returns:</p> Name Type Description <code>dict</code> <p>JSON/GeoJSON response containing list of child entities with their properties and optional geometry data.</p> <p>Raises:</p> Type Description <code>HTTPError</code> <p>If the API request fails or parameters are invalid.</p> Source code in <code>gigaspatial/handlers/unicef_georepo.py</code> <pre><code>def list_entity_children(\n    self, view_uuid, entity_ucode, geom=\"no_geom\", format=\"json\"\n):\n    \"\"\"\n    List direct children of an entity in the administrative hierarchy.\n\n    For example, if given a country entity, this will return its states/provinces.\n    If given a state entity, this will return its districts/counties.\n\n    Args:\n        view_uuid (str): The UUID of the view containing the entity.\n        entity_ucode (str): The Ucode of the parent entity.\n        geom (str, optional): Geometry inclusion level. Options:\n            - \"no_geom\": No geometry data\n            - \"centroid\": Only centroid points\n            - \"full_geom\": Complete boundary geometries\n            Defaults to \"no_geom\".\n        format (str, optional): Response format (\"json\" or \"geojson\").\n            Defaults to \"json\".\n\n    Returns:\n        dict: JSON/GeoJSON response containing list of child entities\n            with their properties and optional geometry data.\n\n    Raises:\n        requests.HTTPError: If the API request fails or parameters are invalid.\n    \"\"\"\n    endpoint = (\n        f\"{self.base_url}/search/view/{view_uuid}/entity/{entity_ucode}/children/\"\n    )\n    params = {\"geom\": geom, \"format\": format}\n    response = self._make_request(\"GET\", endpoint, params=params)\n    return response.json()\n</code></pre>"},{"location":"api/handlers/#gigaspatial.handlers.unicef_georepo.GeoRepoClient.list_modules","title":"<code>list_modules()</code>","text":"<p>List all available modules in GeoRepo.</p> <p>A module is a top-level organizational unit that contains datasets. Examples include \"Admin Boundaries\", \"Health Facilities\", etc.</p> <p>Returns:</p> Name Type Description <code>dict</code> <p>JSON response containing a list of modules with their metadata. Each module includes 'uuid', 'name', 'description', and other properties.</p> <p>Raises:</p> Type Description <code>HTTPError</code> <p>If the API request fails.</p> Source code in <code>gigaspatial/handlers/unicef_georepo.py</code> <pre><code>def list_modules(self):\n    \"\"\"\n    List all available modules in GeoRepo.\n\n    A module is a top-level organizational unit that contains datasets.\n    Examples include \"Admin Boundaries\", \"Health Facilities\", etc.\n\n    Returns:\n        dict: JSON response containing a list of modules with their metadata.\n            Each module includes 'uuid', 'name', 'description', and other properties.\n\n    Raises:\n        requests.HTTPError: If the API request fails.\n    \"\"\"\n    endpoint = f\"{self.base_url}/search/module/list/\"\n    response = self._make_request(\"GET\", endpoint)\n    return response.json()\n</code></pre>"},{"location":"api/handlers/#gigaspatial.handlers.unicef_georepo.GeoRepoClient.list_views_by_dataset","title":"<code>list_views_by_dataset(dataset_uuid, page=1, page_size=50)</code>","text":"<p>List views for a dataset with pagination support.</p> <p>A view represents a specific version or subset of a dataset. Views may be tagged as 'latest' or represent different time periods.</p> <p>Parameters:</p> Name Type Description Default <code>dataset_uuid</code> <code>str</code> <p>The UUID of the dataset to query.</p> required <code>page</code> <code>int</code> <p>Page number for pagination. Defaults to 1.</p> <code>1</code> <code>page_size</code> <code>int</code> <p>Number of results per page. Defaults to 50.</p> <code>50</code> <p>Returns:</p> Name Type Description <code>dict</code> <p>JSON response containing paginated list of views with metadata. Includes 'results', 'total_page', 'current_page', and 'count' fields. Each view includes 'uuid', 'name', 'tags', and other properties.</p> <p>Raises:</p> Type Description <code>HTTPError</code> <p>If the API request fails or dataset_uuid is invalid.</p> Source code in <code>gigaspatial/handlers/unicef_georepo.py</code> <pre><code>def list_views_by_dataset(self, dataset_uuid, page=1, page_size=50):\n    \"\"\"\n    List views for a dataset with pagination support.\n\n    A view represents a specific version or subset of a dataset.\n    Views may be tagged as 'latest' or represent different time periods.\n\n    Args:\n        dataset_uuid (str): The UUID of the dataset to query.\n        page (int, optional): Page number for pagination. Defaults to 1.\n        page_size (int, optional): Number of results per page. Defaults to 50.\n\n    Returns:\n        dict: JSON response containing paginated list of views with metadata.\n            Includes 'results', 'total_page', 'current_page', and 'count' fields.\n            Each view includes 'uuid', 'name', 'tags', and other properties.\n\n    Raises:\n        requests.HTTPError: If the API request fails or dataset_uuid is invalid.\n    \"\"\"\n    endpoint = f\"{self.base_url}/search/dataset/{dataset_uuid}/view/list/\"\n    params = {\"page\": page, \"page_size\": page_size}\n    response = self._make_request(\"GET\", endpoint, params=params)\n    return response.json()\n</code></pre>"},{"location":"api/handlers/#gigaspatial.handlers.unicef_georepo.GeoRepoClient.search_entities_by_name","title":"<code>search_entities_by_name(view_uuid, name, page=1, page_size=50)</code>","text":"<p>Search for entities by name using fuzzy matching.</p> <p>This performs a similarity-based search to find entities whose names match or are similar to the provided search term.</p> <p>Parameters:</p> Name Type Description Default <code>view_uuid</code> <code>str</code> <p>The UUID of the view to search within.</p> required <code>name</code> <code>str</code> <p>The name or partial name to search for.</p> required <code>page</code> <code>int</code> <p>Page number for pagination. Defaults to 1.</p> <code>1</code> <code>page_size</code> <code>int</code> <p>Number of results per page. Defaults to 50.</p> <code>50</code> <p>Returns:</p> Name Type Description <code>dict</code> <p>JSON response containing paginated search results with matching entities and their similarity scores.</p> <p>Raises:</p> Type Description <code>HTTPError</code> <p>If the API request fails or parameters are invalid.</p> Source code in <code>gigaspatial/handlers/unicef_georepo.py</code> <pre><code>def search_entities_by_name(self, view_uuid, name, page=1, page_size=50):\n    \"\"\"\n    Search for entities by name using fuzzy matching.\n\n    This performs a similarity-based search to find entities whose names\n    match or are similar to the provided search term.\n\n    Args:\n        view_uuid (str): The UUID of the view to search within.\n        name (str): The name or partial name to search for.\n        page (int, optional): Page number for pagination. Defaults to 1.\n        page_size (int, optional): Number of results per page. Defaults to 50.\n\n    Returns:\n        dict: JSON response containing paginated search results with\n            matching entities and their similarity scores.\n\n    Raises:\n        requests.HTTPError: If the API request fails or parameters are invalid.\n    \"\"\"\n    endpoint = f\"{self.base_url}/search/view/{view_uuid}/entity/{name}/\"\n    params = {\"page\": page, \"page_size\": page_size}\n    response = self._make_request(\"GET\", endpoint, params=params)\n    return response.json()\n</code></pre>"},{"location":"api/handlers/#gigaspatial.handlers.unicef_georepo.find_admin_boundaries_module","title":"<code>find_admin_boundaries_module()</code>","text":"<p>Find and return the UUID of the Admin Boundaries module.</p> <p>This is a convenience function that searches through all available modules to locate the one named \"Admin Boundaries\", which typically contains administrative boundary datasets.</p> <p>Returns:</p> Name Type Description <code>str</code> <p>The UUID of the Admin Boundaries module.</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If the Admin Boundaries module is not found.</p> Source code in <code>gigaspatial/handlers/unicef_georepo.py</code> <pre><code>def find_admin_boundaries_module():\n    \"\"\"\n    Find and return the UUID of the Admin Boundaries module.\n\n    This is a convenience function that searches through all available modules\n    to locate the one named \"Admin Boundaries\", which typically contains\n    administrative boundary datasets.\n\n    Returns:\n        str: The UUID of the Admin Boundaries module.\n\n    Raises:\n        ValueError: If the Admin Boundaries module is not found.\n    \"\"\"\n    client = GeoRepoClient()\n    modules = client.list_modules()\n\n    for module in modules.get(\"results\", []):\n        if module[\"name\"] == \"Admin Boundaries\":\n            return module[\"uuid\"]\n\n    raise ValueError(\"Admin Boundaries module not found\")\n</code></pre>"},{"location":"api/handlers/#gigaspatial.handlers.unicef_georepo.get_country_boundaries_by_iso3","title":"<code>get_country_boundaries_by_iso3(iso3_code, client=None, admin_level=None)</code>","text":"<p>Get administrative boundaries for a specific country using its ISO3 code.</p> <p>This function provides a high-level interface to retrieve country boundaries by automatically finding the appropriate module, dataset, and view, then fetching the requested administrative boundaries.</p> <p>The function will: 1. Find the Admin Boundaries module 2. Locate a global dataset within that module 3. Find the latest view of that dataset 4. Search for the country using the ISO3 code 5. Look for a country-specific view if available 6. Retrieve boundaries at the specified admin level or all levels</p> <p>Parameters:</p> Name Type Description Default <code>iso3_code</code> <code>str</code> <p>The ISO3 country code (e.g., 'USA', 'KEN', 'BRA').</p> required <code>admin_level</code> <code>int</code> <p>The administrative level to retrieve: - 0: Country level - 1: State/Province/Region level - 2: District/County level - 3: Sub-district/Municipality level - etc. If None, retrieves all available administrative levels.</p> <code>None</code> <p>Returns:</p> Name Type Description <code>dict</code> <p>A GeoJSON FeatureCollection containing the requested boundaries. Each feature includes geometry and properties for the administrative unit.</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If the Admin Boundaries module, datasets, views, or country cannot be found.</p> <code>HTTPError</code> <p>If any API requests fail.</p> Note <p>This function may make multiple API calls and can take some time for countries with many administrative units. It handles pagination automatically and attempts to use country-specific views when available for better performance.</p> Example Source code in <code>gigaspatial/handlers/unicef_georepo.py</code> <pre><code>def get_country_boundaries_by_iso3(\n    iso3_code, client: GeoRepoClient = None, admin_level=None\n):\n    \"\"\"\n    Get administrative boundaries for a specific country using its ISO3 code.\n\n    This function provides a high-level interface to retrieve country boundaries\n    by automatically finding the appropriate module, dataset, and view, then\n    fetching the requested administrative boundaries.\n\n    The function will:\n    1. Find the Admin Boundaries module\n    2. Locate a global dataset within that module\n    3. Find the latest view of that dataset\n    4. Search for the country using the ISO3 code\n    5. Look for a country-specific view if available\n    6. Retrieve boundaries at the specified admin level or all levels\n\n    Args:\n        iso3_code (str): The ISO3 country code (e.g., 'USA', 'KEN', 'BRA').\n        admin_level (int, optional): The administrative level to retrieve:\n            - 0: Country level\n            - 1: State/Province/Region level\n            - 2: District/County level\n            - 3: Sub-district/Municipality level\n            - etc.\n            If None, retrieves all available administrative levels.\n\n    Returns:\n        dict: A GeoJSON FeatureCollection containing the requested boundaries.\n            Each feature includes geometry and properties for the administrative unit.\n\n    Raises:\n        ValueError: If the Admin Boundaries module, datasets, views, or country\n            cannot be found.\n        requests.HTTPError: If any API requests fail.\n\n    Note:\n        This function may make multiple API calls and can take some time for\n        countries with many administrative units. It handles pagination\n        automatically and attempts to use country-specific views when available\n        for better performance.\n\n    Example:\n        &gt;&gt;&gt; # Get all administrative levels for Kenya\n        &gt;&gt;&gt; boundaries = get_country_boundaries_by_iso3('KEN')\n        &gt;&gt;&gt;\n        &gt;&gt;&gt; # Get only province-level boundaries for Kenya\n        &gt;&gt;&gt; provinces = get_country_boundaries_by_iso3('KEN', admin_level=1)\n    \"\"\"\n    client = client or GeoRepoClient()\n\n    client.logger.info(\"Finding Admin Boundaries module...\")\n    modules = client.list_modules()\n    admin_module_uuid = None\n\n    for module in modules.get(\"results\", []):\n        if \"Admin Boundaries\" in module[\"name\"]:\n            admin_module_uuid = module[\"uuid\"]\n            client.logger.info(\n                f\"Found Admin Boundaries module: {module['name']} ({admin_module_uuid})\"\n            )\n            break\n\n    if not admin_module_uuid:\n        raise ValueError(\"Admin Boundaries module not found\")\n\n    client.logger.info(f\"Finding datasets in the Admin Boundaries module...\")\n    datasets = client.list_datasets_by_module(admin_module_uuid)\n    global_dataset_uuid = None\n\n    for dataset in datasets.get(\"results\", []):\n        if any(keyword in dataset[\"name\"].lower() for keyword in [\"global\"]):\n            global_dataset_uuid = dataset[\"uuid\"]\n            client.logger.info(\n                f\"Found global dataset: {dataset['name']} ({global_dataset_uuid})\"\n            )\n            break\n\n    if not global_dataset_uuid:\n        if datasets.get(\"results\"):\n            global_dataset_uuid = datasets[\"results\"][0][\"uuid\"]\n            client.logger.info(\n                f\"Using first available dataset: {datasets['results'][0]['name']} ({global_dataset_uuid})\"\n            )\n        else:\n            raise ValueError(\"No datasets found in the Admin Boundaries module\")\n\n    client.logger.info(f\"Finding views in the dataset...\")\n    views = client.list_views_by_dataset(global_dataset_uuid)\n    latest_view_uuid = None\n\n    for view in views.get(\"results\", []):\n        if \"tags\" in view and \"latest\" in view[\"tags\"]:\n            latest_view_uuid = view[\"uuid\"]\n            client.logger.info(\n                f\"Found latest view: {view['name']} ({latest_view_uuid})\"\n            )\n            break\n\n    if not latest_view_uuid:\n        if views.get(\"results\"):\n            latest_view_uuid = views[\"results\"][0][\"uuid\"]\n            client.logger.info(\n                f\"Using first available view: {views['results'][0]['name']} ({latest_view_uuid})\"\n            )\n        else:\n            raise ValueError(\"No views found in the dataset\")\n\n    # Search for the country by ISO3 code\n    client.logger.info(f\"Searching for country with ISO3 code: {iso3_code}...\")\n    country_entity = client.find_country_by_iso3(latest_view_uuid, iso3_code)\n\n    if not country_entity:\n        raise ValueError(f\"Country with ISO3 code '{iso3_code}' not found\")\n\n    country_ucode = country_entity[\"ucode\"]\n    country_name = country_entity[\"name\"]\n    client.logger.info(f\"Found country: {country_name} (Ucode: {country_ucode})\")\n\n    # Search for country-specific view\n    client.logger.info(f\"Checking for country-specific view...\")\n    country_view_uuid = None\n    all_views = []\n\n    # Need to fetch all pages of views\n    page = 1\n    while True:\n        views_page = client.list_views_by_dataset(global_dataset_uuid, page=page)\n        all_views.extend(views_page.get(\"results\", []))\n        if page &gt;= views_page.get(\"total_page\", 1):\n            break\n        page += 1\n\n    # Look for a view specifically for this country\n    for view in all_views:\n        if country_name.lower() == view[\"name\"].split(\" (\")[\n            0\n        ].lower() and \"latest\" in view.get(\"tags\", []):\n            country_view_uuid = view[\"uuid\"]\n            client.logger.info(\n                f\"Found country-specific view: {view['name']} ({country_view_uuid})\"\n            )\n            break\n\n    # Get boundaries based on admin level\n    if country_view_uuid:\n        client.logger.info(country_view_uuid)\n        # If we found a view specific to this country, use it\n        client.logger.info(f\"Getting admin boundaries from country-specific view...\")\n        if admin_level is not None:\n            client.logger.info(f\"Fetching admin level {admin_level} boundaries...\")\n\n            # Handle pagination for large datasets\n            all_features = []\n            page = 1\n            while True:\n                result, meta = client.list_entities_by_admin_level(\n                    country_view_uuid,\n                    admin_level,\n                    geom=\"full_geom\",\n                    format=\"geojson\",\n                    page=page,\n                    page_size=50,\n                )\n\n                # Add features to our collection\n                if \"features\" in result:\n                    all_features.extend(result[\"features\"])\n                elif \"results\" in result:\n                    # Convert entities to GeoJSON features if needed\n                    for entity in result[\"results\"]:\n                        if \"geometry\" in entity:\n                            feature = {\n                                \"type\": \"Feature\",\n                                \"properties\": {\n                                    k: v for k, v in entity.items() if k != \"geometry\"\n                                },\n                                \"geometry\": entity[\"geometry\"],\n                            }\n                            all_features.append(feature)\n\n                # Check if there are more pages\n                if page &gt;= meta[\"total_page\"]:\n                    break\n\n                page += 1\n\n            boundaries = {\"type\": \"FeatureCollection\", \"features\": all_features}\n        else:\n            # Get all admin levels by fetching each level separately\n            boundaries = {\"type\": \"FeatureCollection\", \"features\": []}\n\n            # Get dataset details to find available admin levels\n            dataset_details = client.get_dataset_details(global_dataset_uuid)\n            max_level = 0\n\n            for level_info in dataset_details.get(\"dataset_levels\", []):\n                if isinstance(level_info.get(\"level\"), int):\n                    max_level = max(max_level, level_info[\"level\"])\n\n            client.logger.info(f\"Dataset has admin levels from 0 to {max_level}\")\n\n            # Fetch each admin level\n            for level in range(max_level + 1):\n                client.logger.info(f\"Fetching admin level {level}...\")\n                try:\n                    level_data, meta = client.list_entities_by_admin_level(\n                        country_view_uuid, level, geom=\"full_geom\", format=\"geojson\"\n                    )\n\n                    if \"features\" in level_data:\n                        boundaries[\"features\"].extend(level_data[\"features\"])\n                    elif \"results\" in level_data:\n                        # Process each page of results\n                        page = 1\n                        while True:\n                            result, meta = client.list_entities_by_admin_level(\n                                country_view_uuid,\n                                level,\n                                geom=\"full_geom\",\n                                format=\"geojson\",\n                                page=page,\n                            )\n\n                            if \"features\" in result:\n                                boundaries[\"features\"].extend(result[\"features\"])\n\n                            # Check for more pages\n                            if page &gt;= meta[\"total_page\"]:\n                                break\n\n                            page += 1\n\n                except Exception as e:\n                    client.logger.warning(f\"Error fetching admin level {level}: {e}\")\n    else:\n        # Use the global view with filtering\n        client.logger.info(f\"Using global view and filtering by country...\")\n\n        # Function to recursively get all descendants\n        def get_all_children(\n            parent_ucode, view_uuid, level=1, max_depth=5, admin_level_filter=None\n        ):\n            \"\"\"\n            Recursively retrieve all child entities of a parent entity.\n\n            Args:\n                parent_ucode (str): The Ucode of the parent entity.\n                view_uuid (str): The UUID of the view to query.\n                level (int): Current recursion level (for depth limiting).\n                max_depth (int): Maximum recursion depth to prevent infinite loops.\n                admin_level_filter (int, optional): If specified, only return\n                    entities at this specific administrative level.\n\n            Returns:\n                list: List of GeoJSON features for all child entities.\n            \"\"\"\n            if level &gt; max_depth:\n                return []\n\n            try:\n                children = client.list_entity_children(view_uuid, parent_ucode)\n                features = []\n\n                for child in children.get(\"results\", []):\n                    # Skip if we're filtering by admin level and this doesn't match\n                    if (\n                        admin_level_filter is not None\n                        and child.get(\"admin_level\") != admin_level_filter\n                    ):\n                        continue\n\n                    # Get the child with full geometry\n                    child_entity = client.get_entity_by_ucode(child[\"ucode\"])\n                    if \"features\" in child_entity:\n                        features.extend(child_entity[\"features\"])\n\n                    # Recursively get grandchildren if not filtering by admin level\n                    if admin_level_filter is None:\n                        features.extend(\n                            get_all_children(\n                                child[\"ucode\"], view_uuid, level + 1, max_depth\n                            )\n                        )\n\n                return features\n            except Exception as e:\n                client.logger.warning(f\"Error getting children for {parent_ucode}: {e}\")\n                return []\n\n        # Start with the country boundaries\n        boundaries = {\"type\": \"FeatureCollection\", \"features\": []}\n\n        # If admin_level is 0, just get the country entity\n        if admin_level == 0:\n            country_entity = client.get_entity_by_ucode(country_ucode)\n            if \"features\" in country_entity:\n                boundaries[\"features\"].extend(country_entity[\"features\"])\n        # If specific admin level requested, get all entities at that level\n        elif admin_level is not None:\n            children_features = get_all_children(\n                country_ucode,\n                latest_view_uuid,\n                max_depth=admin_level + 1,\n                admin_level_filter=admin_level,\n            )\n            boundaries[\"features\"].extend(children_features)\n        # If no admin_level specified, get all levels\n        else:\n            # Start with the country entity\n            country_entity = client.get_entity_by_ucode(country_ucode)\n            if \"features\" in country_entity:\n                boundaries[\"features\"].extend(country_entity[\"features\"])\n\n            # Get all descendants\n            children_features = get_all_children(\n                country_ucode, latest_view_uuid, max_depth=5\n            )\n            boundaries[\"features\"].extend(children_features)\n\n    return boundaries\n</code></pre>"},{"location":"api/handlers/#gigaspatial.handlers.unicef_georepo.get_country_boundaries_by_iso3--get-all-administrative-levels-for-kenya","title":"Get all administrative levels for Kenya","text":"<p>boundaries = get_country_boundaries_by_iso3('KEN')</p>"},{"location":"api/handlers/#gigaspatial.handlers.unicef_georepo.get_country_boundaries_by_iso3--get-only-province-level-boundaries-for-kenya","title":"Get only province-level boundaries for Kenya","text":"<p>provinces = get_country_boundaries_by_iso3('KEN', admin_level=1)</p>"},{"location":"api/handlers/#gigaspatial.handlers.worldpop","title":"<code>worldpop</code>","text":""},{"location":"api/handlers/#gigaspatial.handlers.worldpop.WPPopulationConfig","title":"<code>WPPopulationConfig</code>  <code>dataclass</code>","text":"<p>               Bases: <code>BaseHandlerConfig</code></p> Source code in <code>gigaspatial/handlers/worldpop.py</code> <pre><code>@dataclass(config=ConfigDict(arbitrary_types_allowed=True))\nclass WPPopulationConfig(BaseHandlerConfig):\n\n    client = WorldPopRestClient()\n\n    AVAILABLE_YEARS: List = Field(default=np.append(np.arange(2000, 2021), 2024))\n    AVAILABLE_RESOLUTIONS: List = Field(default=[100, 1000])\n\n    # user config\n    base_path: Path = Field(default=global_config.get_path(\"worldpop\", \"bronze\"))\n    project: Literal[\"pop\", \"age_structures\"] = Field(...)\n    year: int = Field(...)\n    resolution: int = Field(...)\n    un_adjusted: bool = Field(...)\n    constrained: bool = Field(...)\n    school_age: bool = Field(...)\n\n    def _filter_age_sex_paths(self, paths: List[Path], filters: Dict) -&gt; List[Path]:\n        \"\"\"\n        Helper to filter a list of WorldPop age_structures paths based on sex, age, and education level filters.\n        \"\"\"\n        sex_filters = filters.get(\"sex_filters\")\n        level_filters = filters.get(\"level_filters\")  # For school_age=True\n        ages_filter = filters.get(\"ages_filter\")\n        min_age = filters.get(\"min_age\")\n        max_age = filters.get(\"max_age\")\n\n        filtered_paths: List[Path] = []\n\n        for p in paths:\n            # Expected basename patterns:\n            # - School age: DJI_M_SECONDARY_2020_1km.tif (ISO3_SEX_EDUCATIONLEVEL_YEAR_RES.tif)\n            # - Non-school age: RWA_F_25_2020.tif (ISO3_SEX_AGE_YEAR.tif)\n            bn = p.name\n            stem = os.path.splitext(bn)[0]\n            parts = stem.split(\"_\")\n\n            sex_val, age_val, education_level_val = None, None, None\n\n            # Simple heuristic to differentiate school_age vs non-school_age filenames\n            # Check for keywords related to education levels (assuming these are unique to school_age files)\n            is_school_age_filename = any(\n                lvl in stem.upper() for lvl in [\"PRIMARY\", \"SECONDARY\"]\n            )\n\n            if (\n                is_school_age_filename\n            ):  # Filenames like DJI_M_SECONDARY_2020_1km.tif, DJI_F_M_SECONDARY_2020_1km.tif\n                if len(parts) &gt;= 4:\n                    # Determine sex_val and education_level_val based on patterns\n                    if (\n                        len(parts) &gt; 2\n                        and parts[1].upper() == \"F\"\n                        and parts[2].upper() == \"M\"\n                    ):\n                        # Pattern: ISO3_F_M_EDUCATIONLEVEL_YEAR...\n                        sex_val = \"F_M\"\n                        if len(parts) &gt; 3:  # Ensure index exists\n                            education_level_val = parts[3].upper()\n                    elif len(parts) &gt; 1:\n                        # Pattern: ISO3_SEX_EDUCATIONLEVEL_YEAR... (SEX is F or M)\n                        sex_val = parts[1].upper()\n                        if len(parts) &gt; 2:  # Ensure index exists\n                            education_level_val = parts[2].upper()\n            else:  # Filenames like RWA_F_25_2020.tif\n                if len(parts) &gt;= 4:\n                    sex_val = parts[1].upper()\n                    try:\n                        age_val = int(parts[2])\n                    except (ValueError, IndexError):\n                        age_val = None\n\n            # --- Apply sex filter ---\n            if sex_filters:\n                # Explicit matching for all cases\n                sex_ok = False\n                if \"F_M\" in sex_filters and sex_val == \"F_M\":\n                    sex_ok = True\n                elif \"F\" in sex_filters and sex_val == \"F\":\n                    sex_ok = True\n                elif \"M\" in sex_filters and sex_val == \"M\":\n                    sex_ok = True\n\n                if not sex_ok:\n                    continue\n            elif self.project == \"age_structures\" and self.school_age:\n                # Default for school_age=True with no sex filter: only load F_M\n                if sex_val != \"F_M\":\n                    continue\n\n            # --- Apply education level filter (only relevant for school_age filenames) ---\n            if level_filters and is_school_age_filename:\n                if (\n                    education_level_val is None\n                    or education_level_val not in level_filters\n                ):\n                    continue\n\n            # --- Apply age filters (only relevant for non-school_age filenames) ---\n            if (\n                ages_filter is not None or min_age is not None or max_age is not None\n            ) and not is_school_age_filename:\n                if age_val is not None:\n                    if ages_filter is not None and age_val not in ages_filter:\n                        continue\n                    if min_age is not None and age_val &lt; int(min_age):\n                        continue\n                    if max_age is not None and age_val &gt; int(max_age):\n                        continue\n                else:  # If age filters are specified but age_val couldn't be parsed\n                    self.logger.warning(\n                        f\"Could not parse age from filename {p.name} but age filters were applied. Skipping file.\"\n                    )\n                    continue\n\n            filtered_paths.append(p)\n\n        return filtered_paths\n\n    @field_validator(\"year\")\n    def validate_year(cls, value: str) -&gt; int:\n        if value in cls.AVAILABLE_YEARS:\n            return value\n        raise ValueError(\n            f\"No datasets found for the provided year: {value}\\nAvailable years are: {cls.AVAILABLE_YEARS}\"\n        )\n\n    @field_validator(\"resolution\")\n    def validate_resolution(cls, value: str) -&gt; int:\n        if value in cls.AVAILABLE_RESOLUTIONS:\n            return value\n        raise ValueError(\n            f\"No datasets found for the provided resolution: {value}\\nAvailable resolutions are: {cls.AVAILABLE_RESOLUTIONS}\"\n        )\n\n    @model_validator(mode=\"after\")\n    def validate_configuration(self):\n        \"\"\"\n        Validate that the configuration is valid based on dataset availability constraints.\n\n        Specific rules:\n        - For age_structures:\n            - School age data is only available for 2020 at 1km resolution.\n            - Non-school age data is only available at 100m resolution.\n            - Unconstrained, non-school age data is only available without UN adjustment.\n            - Constrained, non-school age data with UN adjustment is only available for 2020.\n            - Constrained, non-school age data without UN adjustment is only available for 2020 and 2024.\n        - For pop:\n            - 2024 data is only available at 100m resolution and without UN adjustment.\n            - Constrained data (other than 2024) is only available for 2020 at 100m resolution.\n            - Unconstrained data at 100m or 1km is available for other years, with or without UN adjustment.\n        \"\"\"\n\n        if self.project == \"age_structures\":\n\n            if self.school_age:\n                if self.resolution == 100:\n                    self.logger.warning(\n                        \"School age population datasets are only available at 1km `resolution`, resolution is set as 1000\"\n                    )\n                    self.resolution = 1000\n\n                if self.year != 2020:\n                    self.logger.warning(\n                        \"School age population datasets are only available for 2020, `year` is set as 2020\"\n                    )\n                    self.year = 2020\n\n                if self.un_adjusted:\n                    self.logger.warning(\n                        \"School age population datasets are only available without UN adjustment, `un_adjusted` is set as False\"\n                    )\n                    self.un_adjusted = False\n\n                if self.constrained:\n                    self.logger.warning(\n                        \"School age population datasets are only available unconstrained, `constrained` is set as False\"\n                    )\n                    self.constrained = False\n\n                self.dataset_category = \"sapya1km\"\n            else:\n                if self.resolution == 1000:\n                    self.logger.warning(\n                        \"Age structures datasets are only available at 100m resolution, `resolution` is set as 100\"\n                    )\n                    self.resolution = 100\n\n                if not self.constrained:\n                    if self.un_adjusted:\n                        self.logger.warning(\n                            \"Age structures unconstrained datasets are only available without UN adjustment, `un_adjusted` is set as False\"\n                        )\n                        self.un_adjusted = False\n\n                    self.dataset_category = (\n                        \"G2_UC_Age_2024_100m\" if self.year == 2024 else \"aswpgp\"\n                    )\n                else:\n                    if self.un_adjusted:\n                        if self.year != 2020:\n                            self.logger.warning(\n                                \"Age structures constrained datasets with UN adjustment are only available for 2020, `year` is set as 2020\"\n                            )\n                            self.year = 2020\n                        self.dataset_category = \"ascicua_2020\"\n                    else:\n                        if self.year == 2024:\n                            self.dataset_category = \"G2_CN_Age_2024_100m\"\n                        elif self.year == 2020:\n                            self.dataset_category = \"ascic_2020\"\n                        else:\n                            raise ValueError(\n                                \"Age structures constrained datasets without UN adjustment are only available for 2020 and 2024, please set `year` to one of the available options: 2020, 2024\"\n                            )\n\n        elif self.project == \"pop\":\n\n            if self.school_age:\n                raise ValueError(\n                    f\"\"\"\n                    Received unexpected value of `{self.school_age}` for project: `{self.project}`.\n                    For school age population datasets, please set project as `age_structures`.\n                    \"\"\"\n                )\n\n            if self.year == 2024:\n                if self.resolution == 1000:\n                    self.logger.warning(\n                        \"2024 datasets are only available at 100m resolution, `resolution` is set as 100m\"\n                    )\n                    self.resolution = 100\n                if self.un_adjusted:\n                    self.logger.warning(\n                        \"2024 datasets are only available without UN adjustment, `un_adjusted` is set as False\"\n                    )\n                    self.un_adjusted = False\n\n                self.dataset_category = (\n                    \"G2_CN_POP_2024_100m\" if self.constrained else \"G2_UC_POP_2024_100m\"\n                )\n            else:\n                if self.constrained:\n                    if self.year != 2020:\n                        self.logger.warning(\n                            \"Population constrained datasets are only available for 2020, `year` is set as 2020\"\n                        )\n                        self.year = 2020\n\n                    if self.resolution != 100:\n                        self.logger.warning(\n                            \"Population constrained datasets are only available at 100m resolution, `resolution` is set as 100\"\n                        )\n                        self.resolution = 100\n\n                    self.dataset_category = (\n                        \"cic2020_UNadj_100m\" if self.un_adjusted else \"cic2020_100m\"\n                    )\n                else:\n                    if self.resolution == 100:\n                        self.dataset_category = (\n                            f\"wpgp{'unadj' if self.un_adjusted else ''}\"\n                        )\n                    else:\n                        self.dataset_category = (\n                            \"wpic1km\" if not self.un_adjusted else \"wpicuadj1km\"\n                        )\n\n    def get_relevant_data_units_by_geometry(\n        self, geometry: str, **kwargs\n    ) -&gt; List[Dict[str, Any]]:\n        datasets = self.client.search_datasets(\n            self.project, self.dataset_category, geometry, self.year\n        )\n\n        if not datasets:\n            raise RuntimeError(\n                f\"No WorldPop datasets found for country: {geometry}, \"\n                f\"project: {self.project}, category: {self.dataset_category}, year: {self.year}. \"\n                \"Please check the configuration parameters.\"\n            )\n\n        files = [\n            file\n            for file in datasets[0].get(\"files\", [])\n            if ((self.dataset_category == \"sapya1km\") or file.endswith(\".tif\"))\n        ]\n\n        return files\n\n    def get_data_unit_path(self, unit: str, **kwargs) -&gt; Path:\n        \"\"\"\n        Given a WP file url, return the corresponding path.\n        \"\"\"\n        return self.base_path / unit.split(\"GIS/\")[1]\n\n    def get_data_unit_paths(self, units: Union[List[str], str], **kwargs) -&gt; list:\n        \"\"\"\n        Given WP file url(s), return the corresponding local file paths.\n\n        - For school_age age_structures (zip resources), if extracted .tif files are present\n        in the target directory, return those; otherwise, return the zip path(s) to allow\n        the downloader to fetch and extract them.\n        - For non-school_age age_structures (individual .tif URLs), you can filter by sex and age\n        using kwargs: sex, ages, min_age, max_age.\n        \"\"\"\n        if not isinstance(units, list):\n            units = [units]\n\n        # Extract optional filters\n        sex = kwargs.get(\"sex\")\n        education_level = kwargs.get(\"education_level\") or kwargs.get(\"level\")\n        ages_filter = kwargs.get(\"ages\")\n        min_age = kwargs.get(\"min_age\")\n        max_age = kwargs.get(\"max_age\")\n\n        def _to_set(v):\n            if v is None:\n                return None\n            if isinstance(v, (list, tuple, set)):\n                return {str(x).upper() for x in v}\n            return {str(v).upper()}\n\n        sex_filters = _to_set(sex)\n        level_filters = _to_set(education_level)\n\n        # 1) School-age branch (zip \u2192 extracted tifs)\n        if self.project == \"age_structures\" and self.school_age:\n            resolved_paths: List[Path] = []\n            for url in units:\n                output_dir = self.get_data_unit_path(url).parent\n\n                if self.data_store.is_dir(str(output_dir)):\n                    try:\n                        all_extracted_tifs = [\n                            Path(f)\n                            for f in self.data_store.list_files(str(output_dir))\n                            if f.lower().endswith(\".tif\")\n                        ]\n                        # Apply filters to extracted tifs\n                        filtered_tifs = self._filter_age_sex_paths(\n                            all_extracted_tifs,\n                            {\n                                \"sex_filters\": sex_filters,\n                                \"level_filters\": level_filters,\n                            },\n                        )\n                        resolved_paths.extend(filtered_tifs)\n                    except Exception:\n                        resolved_paths.append(self.get_data_unit_path(url))  # Fallback\n                else:\n                    resolved_paths.append(\n                        self.get_data_unit_path(url)\n                    )  # Fallback if not extracted yet\n\n            return resolved_paths\n\n        # 2) Non-school_age age_structures (individual tif URLs) with DEFERRED sex/age filters\n        if self.project == \"age_structures\" and not self.school_age:\n            # Store filters in a way that the reader can access them if needed\n            self._temp_age_sex_filters = {\n                \"sex_filters\": sex_filters,\n                \"ages_filter\": ages_filter,\n                \"min_age\": min_age,\n                \"max_age\": max_age,\n            }\n            # Here, we don't apply the filters yet. We return all potential paths.\n            # The actual filtering will happen in the reader or during TifProcessor loading.\n            return [self.get_data_unit_path(unit) for unit in units]\n\n        # Default behavior for all other datasets\n        return [self.get_data_unit_path(unit) for unit in units]\n\n    def extract_search_geometry(self, source, **kwargs):\n        \"\"\"\n        Override the method since geometry extraction does not apply.\n        Returns country iso3 for dataset search\n        \"\"\"\n        if not isinstance(source, str):\n            raise ValueError(\n                f\"Unsupported source type: {type(source)}\"\n                \"Please use country-based (str) filtering.\"\n            )\n\n        return pycountry.countries.lookup(source).alpha_3\n\n    def __repr__(self) -&gt; str:\n\n        return (\n            f\"WPPopulationConfig(\",\n            f\"project={self.project}, \"\n            f\"year={self.year}, \"\n            f\"resolution={self.resolution}, \"\n            f\"un_adjusted={self.un_adjusted}, \"\n            f\"constrained={self.constrained}, \"\n            f\"school_age={self.school_age}, \"\n            f\")\",\n        )\n</code></pre>"},{"location":"api/handlers/#gigaspatial.handlers.worldpop.WPPopulationConfig.extract_search_geometry","title":"<code>extract_search_geometry(source, **kwargs)</code>","text":"<p>Override the method since geometry extraction does not apply. Returns country iso3 for dataset search</p> Source code in <code>gigaspatial/handlers/worldpop.py</code> <pre><code>def extract_search_geometry(self, source, **kwargs):\n    \"\"\"\n    Override the method since geometry extraction does not apply.\n    Returns country iso3 for dataset search\n    \"\"\"\n    if not isinstance(source, str):\n        raise ValueError(\n            f\"Unsupported source type: {type(source)}\"\n            \"Please use country-based (str) filtering.\"\n        )\n\n    return pycountry.countries.lookup(source).alpha_3\n</code></pre>"},{"location":"api/handlers/#gigaspatial.handlers.worldpop.WPPopulationConfig.get_data_unit_path","title":"<code>get_data_unit_path(unit, **kwargs)</code>","text":"<p>Given a WP file url, return the corresponding path.</p> Source code in <code>gigaspatial/handlers/worldpop.py</code> <pre><code>def get_data_unit_path(self, unit: str, **kwargs) -&gt; Path:\n    \"\"\"\n    Given a WP file url, return the corresponding path.\n    \"\"\"\n    return self.base_path / unit.split(\"GIS/\")[1]\n</code></pre>"},{"location":"api/handlers/#gigaspatial.handlers.worldpop.WPPopulationConfig.get_data_unit_paths","title":"<code>get_data_unit_paths(units, **kwargs)</code>","text":"<p>Given WP file url(s), return the corresponding local file paths.</p> <ul> <li>For school_age age_structures (zip resources), if extracted .tif files are present in the target directory, return those; otherwise, return the zip path(s) to allow the downloader to fetch and extract them.</li> <li>For non-school_age age_structures (individual .tif URLs), you can filter by sex and age using kwargs: sex, ages, min_age, max_age.</li> </ul> Source code in <code>gigaspatial/handlers/worldpop.py</code> <pre><code>def get_data_unit_paths(self, units: Union[List[str], str], **kwargs) -&gt; list:\n    \"\"\"\n    Given WP file url(s), return the corresponding local file paths.\n\n    - For school_age age_structures (zip resources), if extracted .tif files are present\n    in the target directory, return those; otherwise, return the zip path(s) to allow\n    the downloader to fetch and extract them.\n    - For non-school_age age_structures (individual .tif URLs), you can filter by sex and age\n    using kwargs: sex, ages, min_age, max_age.\n    \"\"\"\n    if not isinstance(units, list):\n        units = [units]\n\n    # Extract optional filters\n    sex = kwargs.get(\"sex\")\n    education_level = kwargs.get(\"education_level\") or kwargs.get(\"level\")\n    ages_filter = kwargs.get(\"ages\")\n    min_age = kwargs.get(\"min_age\")\n    max_age = kwargs.get(\"max_age\")\n\n    def _to_set(v):\n        if v is None:\n            return None\n        if isinstance(v, (list, tuple, set)):\n            return {str(x).upper() for x in v}\n        return {str(v).upper()}\n\n    sex_filters = _to_set(sex)\n    level_filters = _to_set(education_level)\n\n    # 1) School-age branch (zip \u2192 extracted tifs)\n    if self.project == \"age_structures\" and self.school_age:\n        resolved_paths: List[Path] = []\n        for url in units:\n            output_dir = self.get_data_unit_path(url).parent\n\n            if self.data_store.is_dir(str(output_dir)):\n                try:\n                    all_extracted_tifs = [\n                        Path(f)\n                        for f in self.data_store.list_files(str(output_dir))\n                        if f.lower().endswith(\".tif\")\n                    ]\n                    # Apply filters to extracted tifs\n                    filtered_tifs = self._filter_age_sex_paths(\n                        all_extracted_tifs,\n                        {\n                            \"sex_filters\": sex_filters,\n                            \"level_filters\": level_filters,\n                        },\n                    )\n                    resolved_paths.extend(filtered_tifs)\n                except Exception:\n                    resolved_paths.append(self.get_data_unit_path(url))  # Fallback\n            else:\n                resolved_paths.append(\n                    self.get_data_unit_path(url)\n                )  # Fallback if not extracted yet\n\n        return resolved_paths\n\n    # 2) Non-school_age age_structures (individual tif URLs) with DEFERRED sex/age filters\n    if self.project == \"age_structures\" and not self.school_age:\n        # Store filters in a way that the reader can access them if needed\n        self._temp_age_sex_filters = {\n            \"sex_filters\": sex_filters,\n            \"ages_filter\": ages_filter,\n            \"min_age\": min_age,\n            \"max_age\": max_age,\n        }\n        # Here, we don't apply the filters yet. We return all potential paths.\n        # The actual filtering will happen in the reader or during TifProcessor loading.\n        return [self.get_data_unit_path(unit) for unit in units]\n\n    # Default behavior for all other datasets\n    return [self.get_data_unit_path(unit) for unit in units]\n</code></pre>"},{"location":"api/handlers/#gigaspatial.handlers.worldpop.WPPopulationConfig.validate_configuration","title":"<code>validate_configuration()</code>","text":"<p>Validate that the configuration is valid based on dataset availability constraints.</p> <p>Specific rules: - For age_structures:     - School age data is only available for 2020 at 1km resolution.     - Non-school age data is only available at 100m resolution.     - Unconstrained, non-school age data is only available without UN adjustment.     - Constrained, non-school age data with UN adjustment is only available for 2020.     - Constrained, non-school age data without UN adjustment is only available for 2020 and 2024. - For pop:     - 2024 data is only available at 100m resolution and without UN adjustment.     - Constrained data (other than 2024) is only available for 2020 at 100m resolution.     - Unconstrained data at 100m or 1km is available for other years, with or without UN adjustment.</p> Source code in <code>gigaspatial/handlers/worldpop.py</code> <pre><code>@model_validator(mode=\"after\")\ndef validate_configuration(self):\n    \"\"\"\n    Validate that the configuration is valid based on dataset availability constraints.\n\n    Specific rules:\n    - For age_structures:\n        - School age data is only available for 2020 at 1km resolution.\n        - Non-school age data is only available at 100m resolution.\n        - Unconstrained, non-school age data is only available without UN adjustment.\n        - Constrained, non-school age data with UN adjustment is only available for 2020.\n        - Constrained, non-school age data without UN adjustment is only available for 2020 and 2024.\n    - For pop:\n        - 2024 data is only available at 100m resolution and without UN adjustment.\n        - Constrained data (other than 2024) is only available for 2020 at 100m resolution.\n        - Unconstrained data at 100m or 1km is available for other years, with or without UN adjustment.\n    \"\"\"\n\n    if self.project == \"age_structures\":\n\n        if self.school_age:\n            if self.resolution == 100:\n                self.logger.warning(\n                    \"School age population datasets are only available at 1km `resolution`, resolution is set as 1000\"\n                )\n                self.resolution = 1000\n\n            if self.year != 2020:\n                self.logger.warning(\n                    \"School age population datasets are only available for 2020, `year` is set as 2020\"\n                )\n                self.year = 2020\n\n            if self.un_adjusted:\n                self.logger.warning(\n                    \"School age population datasets are only available without UN adjustment, `un_adjusted` is set as False\"\n                )\n                self.un_adjusted = False\n\n            if self.constrained:\n                self.logger.warning(\n                    \"School age population datasets are only available unconstrained, `constrained` is set as False\"\n                )\n                self.constrained = False\n\n            self.dataset_category = \"sapya1km\"\n        else:\n            if self.resolution == 1000:\n                self.logger.warning(\n                    \"Age structures datasets are only available at 100m resolution, `resolution` is set as 100\"\n                )\n                self.resolution = 100\n\n            if not self.constrained:\n                if self.un_adjusted:\n                    self.logger.warning(\n                        \"Age structures unconstrained datasets are only available without UN adjustment, `un_adjusted` is set as False\"\n                    )\n                    self.un_adjusted = False\n\n                self.dataset_category = (\n                    \"G2_UC_Age_2024_100m\" if self.year == 2024 else \"aswpgp\"\n                )\n            else:\n                if self.un_adjusted:\n                    if self.year != 2020:\n                        self.logger.warning(\n                            \"Age structures constrained datasets with UN adjustment are only available for 2020, `year` is set as 2020\"\n                        )\n                        self.year = 2020\n                    self.dataset_category = \"ascicua_2020\"\n                else:\n                    if self.year == 2024:\n                        self.dataset_category = \"G2_CN_Age_2024_100m\"\n                    elif self.year == 2020:\n                        self.dataset_category = \"ascic_2020\"\n                    else:\n                        raise ValueError(\n                            \"Age structures constrained datasets without UN adjustment are only available for 2020 and 2024, please set `year` to one of the available options: 2020, 2024\"\n                        )\n\n    elif self.project == \"pop\":\n\n        if self.school_age:\n            raise ValueError(\n                f\"\"\"\n                Received unexpected value of `{self.school_age}` for project: `{self.project}`.\n                For school age population datasets, please set project as `age_structures`.\n                \"\"\"\n            )\n\n        if self.year == 2024:\n            if self.resolution == 1000:\n                self.logger.warning(\n                    \"2024 datasets are only available at 100m resolution, `resolution` is set as 100m\"\n                )\n                self.resolution = 100\n            if self.un_adjusted:\n                self.logger.warning(\n                    \"2024 datasets are only available without UN adjustment, `un_adjusted` is set as False\"\n                )\n                self.un_adjusted = False\n\n            self.dataset_category = (\n                \"G2_CN_POP_2024_100m\" if self.constrained else \"G2_UC_POP_2024_100m\"\n            )\n        else:\n            if self.constrained:\n                if self.year != 2020:\n                    self.logger.warning(\n                        \"Population constrained datasets are only available for 2020, `year` is set as 2020\"\n                    )\n                    self.year = 2020\n\n                if self.resolution != 100:\n                    self.logger.warning(\n                        \"Population constrained datasets are only available at 100m resolution, `resolution` is set as 100\"\n                    )\n                    self.resolution = 100\n\n                self.dataset_category = (\n                    \"cic2020_UNadj_100m\" if self.un_adjusted else \"cic2020_100m\"\n                )\n            else:\n                if self.resolution == 100:\n                    self.dataset_category = (\n                        f\"wpgp{'unadj' if self.un_adjusted else ''}\"\n                    )\n                else:\n                    self.dataset_category = (\n                        \"wpic1km\" if not self.un_adjusted else \"wpicuadj1km\"\n                    )\n</code></pre>"},{"location":"api/handlers/#gigaspatial.handlers.worldpop.WPPopulationDownloader","title":"<code>WPPopulationDownloader</code>","text":"<p>               Bases: <code>BaseHandlerDownloader</code></p> Source code in <code>gigaspatial/handlers/worldpop.py</code> <pre><code>class WPPopulationDownloader(BaseHandlerDownloader):\n\n    def __init__(\n        self,\n        config: Union[WPPopulationConfig, dict[str, Union[str, int]]],\n        data_store: Optional[DataStore] = None,\n        logger: Optional[logging.Logger] = None,\n    ):\n        \"\"\"\n        Initialize the downloader.\n\n        Args:\n            config: Configuration for the WorldPop dataset, either as a WPPopulationConfig object or a dictionary of parameters\n            data_store: Optional data storage interface. If not provided, uses LocalDataStore.\n            logger: Optional custom logger. If not provided, uses default logger.\n        \"\"\"\n        config = (\n            config\n            if isinstance(config, WPPopulationConfig)\n            else WPPopulationConfig(**config)\n        )\n        super().__init__(config=config, data_store=data_store, logger=logger)\n\n    def download_data_unit(self, url, **kwargs):\n        \"\"\"Download data file for a url. If a zip, extract contained .tif files.\"\"\"\n        # If the resource is a zip (e.g., school age datasets), download to temp and extract .tif files\n        if url.lower().endswith(\".zip\"):\n            temp_downloaded_path: Optional[Path] = None\n            try:\n                with tempfile.NamedTemporaryFile(\n                    delete=False, suffix=\".zip\"\n                ) as temp_file:\n                    temp_downloaded_path = Path(temp_file.name)\n                    response = self.config.client.session.get(\n                        url, stream=True, timeout=self.config.client.timeout\n                    )\n                    response.raise_for_status()\n\n                    total_size = int(response.headers.get(\"content-length\", 0))\n\n                    with tqdm(\n                        total=total_size,\n                        unit=\"B\",\n                        unit_scale=True,\n                        desc=f\"Downloading {os.path.basename(temp_downloaded_path)}\",\n                    ) as pbar:\n                        for chunk in response.iter_content(chunk_size=8192):\n                            if chunk:\n                                temp_file.write(chunk)\n                                pbar.update(len(chunk))\n\n                extracted_files: List[Path] = []\n                output_dir = self.config.get_data_unit_path(url).parent\n                with zipfile.ZipFile(str(temp_downloaded_path), \"r\") as zip_ref:\n                    members = [\n                        m for m in zip_ref.namelist() if m.lower().endswith(\".tif\")\n                    ]\n                    for member in members:\n                        extracted_path = output_dir / Path(member).name\n                        with zip_ref.open(member) as source:\n                            file_content = source.read()\n                            self.data_store.write_file(\n                                str(extracted_path), file_content\n                            )\n                        extracted_files.append(extracted_path)\n                        self.logger.info(f\"Extracted {member} to {extracted_path}\")\n\n                return extracted_files\n\n            except requests.RequestException as e:\n                self.logger.error(f\"Failed to download {url}: {e}\")\n                return None\n            except zipfile.BadZipFile:\n                self.logger.error(\"Downloaded file is not a valid zip archive.\")\n                return None\n            except Exception as e:\n                self.logger.error(f\"Unexpected error processing zip for {url}: {e}\")\n                return None\n            finally:\n                if temp_downloaded_path and temp_downloaded_path.exists():\n                    try:\n                        temp_downloaded_path.unlink()\n                    except OSError as e:\n                        self.logger.warning(\n                            f\"Could not delete temporary file {temp_downloaded_path}: {e}\"\n                        )\n\n        # Otherwise, download as a regular file (e.g., .tif)\n        try:\n            response = self.config.client.session.get(\n                url, stream=True, timeout=self.config.client.timeout\n            )\n            response.raise_for_status()\n\n            total_size = int(response.headers.get(\"content-length\", 0))\n            file_path = self.config.get_data_unit_path(url)\n\n            with self.data_store.open(str(file_path), \"wb\") as file:\n                with tqdm(\n                    total=total_size,\n                    unit=\"B\",\n                    unit_scale=True,\n                    desc=f\"Downloading {os.path.basename(file_path)}\",\n                ) as pbar:\n                    for chunk in response.iter_content(chunk_size=8192):\n                        if chunk:\n                            file.write(chunk)\n                            pbar.update(len(chunk))\n\n            self.logger.info(f\"Successfully downloaded: {file_path}\")\n            return file_path\n\n        except requests.RequestException as e:\n            self.logger.error(f\"Failed to download {url}: {e}\")\n            return None\n        except Exception as e:\n            self.logger.error(f\"Unexpected error downloading {url}: {e}\")\n            return None\n\n    def download_data_units(\n        self,\n        urls: List[str],\n        **kwargs,\n    ) -&gt; List[str]:\n        \"\"\"Download data files for multiple urls.\"\"\"\n\n        with multiprocessing.Pool(self.config.n_workers) as pool:\n            download_func = functools.partial(self.download_data_unit)\n            results = list(\n                tqdm(\n                    pool.imap(download_func, urls),\n                    total=len(urls),\n                    desc=f\"Downloading data\",\n                )\n            )\n\n        # Flatten results and filter out None\n        flattened: List[Path] = []\n        for item in results:\n            if item is None:\n                continue\n            if isinstance(item, list):\n                flattened.extend(item)\n            else:\n                flattened.append(item)\n\n        return flattened\n</code></pre>"},{"location":"api/handlers/#gigaspatial.handlers.worldpop.WPPopulationDownloader.__init__","title":"<code>__init__(config, data_store=None, logger=None)</code>","text":"<p>Initialize the downloader.</p> <p>Parameters:</p> Name Type Description Default <code>config</code> <code>Union[WPPopulationConfig, dict[str, Union[str, int]]]</code> <p>Configuration for the WorldPop dataset, either as a WPPopulationConfig object or a dictionary of parameters</p> required <code>data_store</code> <code>Optional[DataStore]</code> <p>Optional data storage interface. If not provided, uses LocalDataStore.</p> <code>None</code> <code>logger</code> <code>Optional[Logger]</code> <p>Optional custom logger. If not provided, uses default logger.</p> <code>None</code> Source code in <code>gigaspatial/handlers/worldpop.py</code> <pre><code>def __init__(\n    self,\n    config: Union[WPPopulationConfig, dict[str, Union[str, int]]],\n    data_store: Optional[DataStore] = None,\n    logger: Optional[logging.Logger] = None,\n):\n    \"\"\"\n    Initialize the downloader.\n\n    Args:\n        config: Configuration for the WorldPop dataset, either as a WPPopulationConfig object or a dictionary of parameters\n        data_store: Optional data storage interface. If not provided, uses LocalDataStore.\n        logger: Optional custom logger. If not provided, uses default logger.\n    \"\"\"\n    config = (\n        config\n        if isinstance(config, WPPopulationConfig)\n        else WPPopulationConfig(**config)\n    )\n    super().__init__(config=config, data_store=data_store, logger=logger)\n</code></pre>"},{"location":"api/handlers/#gigaspatial.handlers.worldpop.WPPopulationDownloader.download_data_unit","title":"<code>download_data_unit(url, **kwargs)</code>","text":"<p>Download data file for a url. If a zip, extract contained .tif files.</p> Source code in <code>gigaspatial/handlers/worldpop.py</code> <pre><code>def download_data_unit(self, url, **kwargs):\n    \"\"\"Download data file for a url. If a zip, extract contained .tif files.\"\"\"\n    # If the resource is a zip (e.g., school age datasets), download to temp and extract .tif files\n    if url.lower().endswith(\".zip\"):\n        temp_downloaded_path: Optional[Path] = None\n        try:\n            with tempfile.NamedTemporaryFile(\n                delete=False, suffix=\".zip\"\n            ) as temp_file:\n                temp_downloaded_path = Path(temp_file.name)\n                response = self.config.client.session.get(\n                    url, stream=True, timeout=self.config.client.timeout\n                )\n                response.raise_for_status()\n\n                total_size = int(response.headers.get(\"content-length\", 0))\n\n                with tqdm(\n                    total=total_size,\n                    unit=\"B\",\n                    unit_scale=True,\n                    desc=f\"Downloading {os.path.basename(temp_downloaded_path)}\",\n                ) as pbar:\n                    for chunk in response.iter_content(chunk_size=8192):\n                        if chunk:\n                            temp_file.write(chunk)\n                            pbar.update(len(chunk))\n\n            extracted_files: List[Path] = []\n            output_dir = self.config.get_data_unit_path(url).parent\n            with zipfile.ZipFile(str(temp_downloaded_path), \"r\") as zip_ref:\n                members = [\n                    m for m in zip_ref.namelist() if m.lower().endswith(\".tif\")\n                ]\n                for member in members:\n                    extracted_path = output_dir / Path(member).name\n                    with zip_ref.open(member) as source:\n                        file_content = source.read()\n                        self.data_store.write_file(\n                            str(extracted_path), file_content\n                        )\n                    extracted_files.append(extracted_path)\n                    self.logger.info(f\"Extracted {member} to {extracted_path}\")\n\n            return extracted_files\n\n        except requests.RequestException as e:\n            self.logger.error(f\"Failed to download {url}: {e}\")\n            return None\n        except zipfile.BadZipFile:\n            self.logger.error(\"Downloaded file is not a valid zip archive.\")\n            return None\n        except Exception as e:\n            self.logger.error(f\"Unexpected error processing zip for {url}: {e}\")\n            return None\n        finally:\n            if temp_downloaded_path and temp_downloaded_path.exists():\n                try:\n                    temp_downloaded_path.unlink()\n                except OSError as e:\n                    self.logger.warning(\n                        f\"Could not delete temporary file {temp_downloaded_path}: {e}\"\n                    )\n\n    # Otherwise, download as a regular file (e.g., .tif)\n    try:\n        response = self.config.client.session.get(\n            url, stream=True, timeout=self.config.client.timeout\n        )\n        response.raise_for_status()\n\n        total_size = int(response.headers.get(\"content-length\", 0))\n        file_path = self.config.get_data_unit_path(url)\n\n        with self.data_store.open(str(file_path), \"wb\") as file:\n            with tqdm(\n                total=total_size,\n                unit=\"B\",\n                unit_scale=True,\n                desc=f\"Downloading {os.path.basename(file_path)}\",\n            ) as pbar:\n                for chunk in response.iter_content(chunk_size=8192):\n                    if chunk:\n                        file.write(chunk)\n                        pbar.update(len(chunk))\n\n        self.logger.info(f\"Successfully downloaded: {file_path}\")\n        return file_path\n\n    except requests.RequestException as e:\n        self.logger.error(f\"Failed to download {url}: {e}\")\n        return None\n    except Exception as e:\n        self.logger.error(f\"Unexpected error downloading {url}: {e}\")\n        return None\n</code></pre>"},{"location":"api/handlers/#gigaspatial.handlers.worldpop.WPPopulationDownloader.download_data_units","title":"<code>download_data_units(urls, **kwargs)</code>","text":"<p>Download data files for multiple urls.</p> Source code in <code>gigaspatial/handlers/worldpop.py</code> <pre><code>def download_data_units(\n    self,\n    urls: List[str],\n    **kwargs,\n) -&gt; List[str]:\n    \"\"\"Download data files for multiple urls.\"\"\"\n\n    with multiprocessing.Pool(self.config.n_workers) as pool:\n        download_func = functools.partial(self.download_data_unit)\n        results = list(\n            tqdm(\n                pool.imap(download_func, urls),\n                total=len(urls),\n                desc=f\"Downloading data\",\n            )\n        )\n\n    # Flatten results and filter out None\n    flattened: List[Path] = []\n    for item in results:\n        if item is None:\n            continue\n        if isinstance(item, list):\n            flattened.extend(item)\n        else:\n            flattened.append(item)\n\n    return flattened\n</code></pre>"},{"location":"api/handlers/#gigaspatial.handlers.worldpop.WPPopulationHandler","title":"<code>WPPopulationHandler</code>","text":"<p>               Bases: <code>BaseHandler</code></p> <p>Handler for WorldPop Populations datasets.</p> <p>This class provides a unified interface for downloading and loading WP Population data. It manages the lifecycle of configuration, downloading, and reading components.</p> Source code in <code>gigaspatial/handlers/worldpop.py</code> <pre><code>class WPPopulationHandler(BaseHandler):\n    \"\"\"\n    Handler for WorldPop Populations datasets.\n\n    This class provides a unified interface for downloading and loading WP Population data.\n    It manages the lifecycle of configuration, downloading, and reading components.\n    \"\"\"\n\n    def __init__(\n        self,\n        project: Literal[\"pop\", \"age_structures\"] = \"pop\",\n        year: int = 2020,\n        resolution: int = 1000,\n        un_adjusted: bool = True,\n        constrained: bool = False,\n        school_age: bool = False,\n        config: Optional[WPPopulationConfig] = None,\n        downloader: Optional[WPPopulationDownloader] = None,\n        reader: Optional[WPPopulationReader] = None,\n        data_store: Optional[DataStore] = None,\n        logger: Optional[logging.Logger] = None,\n        **kwargs,\n    ):\n        self._project = project\n        self._year = year\n        self._resolution = resolution\n        self._un_adjusted = un_adjusted\n        self._constrained = constrained\n        self._school_age = school_age\n        super().__init__(\n            config=config,\n            downloader=downloader,\n            reader=reader,\n            data_store=data_store,\n            logger=logger,\n        )\n\n    def create_config(\n        self, data_store: DataStore, logger: logging.Logger, **kwargs\n    ) -&gt; WPPopulationConfig:\n        \"\"\"\n        Create and return a WPPopulationConfig instance.\n\n        Args:\n            data_store: The data store instance to use\n            logger: The logger instance to use\n            **kwargs: Additional configuration parameters\n\n        Returns:\n            Configured WPPopulationConfig instance\n        \"\"\"\n        return WPPopulationConfig(\n            project=self._project,\n            year=self._year,\n            resolution=self._resolution,\n            un_adjusted=self._un_adjusted,\n            constrained=self._constrained,\n            school_age=self._school_age,\n            data_store=data_store,\n            logger=logger,\n            **kwargs,\n        )\n\n    def create_downloader(\n        self,\n        config: WPPopulationConfig,\n        data_store: DataStore,\n        logger: logging.Logger,\n        **kwargs,\n    ) -&gt; WPPopulationDownloader:\n        \"\"\"\n        Create and return a WPPopulationDownloader instance.\n\n        Args:\n            config: The configuration object\n            data_store: The data store instance to use\n            logger: The logger instance to use\n            **kwargs: Additional downloader parameters\n\n        Returns:\n            Configured WPPopulationDownloader instance\n        \"\"\"\n        return WPPopulationDownloader(\n            config=config, data_store=data_store, logger=logger, **kwargs\n        )\n\n    def create_reader(\n        self,\n        config: WPPopulationConfig,\n        data_store: DataStore,\n        logger: logging.Logger,\n        **kwargs,\n    ) -&gt; WPPopulationReader:\n        \"\"\"\n        Create and return a WPPopulationReader instance.\n\n        Args:\n            config: The configuration object\n            data_store: The data store instance to use\n            logger: The logger instance to use\n            **kwargs: Additional reader parameters\n\n        Returns:\n            Configured WPPopulationReader instance\n        \"\"\"\n        return WPPopulationReader(\n            config=config, data_store=data_store, logger=logger, **kwargs\n        )\n\n    def load_into_dataframe(\n        self,\n        source: str,\n        ensure_available: bool = True,\n        **kwargs,\n    ) -&gt; pd.DataFrame:\n        \"\"\"\n        Load GHSL data into a pandas DataFrame.\n\n        Args:\n            source: The data source specification\n            ensure_available: If True, ensure data is downloaded before loading\n            **kwargs: Additional parameters passed to load methods\n\n        Returns:\n            DataFrame containing the GHSL data\n        \"\"\"\n        tif_processors = self.load_data(\n            source=source, ensure_available=ensure_available, **kwargs\n        )\n        if isinstance(tif_processors, TifProcessor):\n            return tif_processors.to_dataframe(**kwargs)\n\n        return pd.concat(\n            [tp.to_dataframe(**kwargs) for tp in tif_processors], ignore_index=True\n        )\n\n    def load_into_geodataframe(\n        self,\n        source: str,\n        ensure_available: bool = True,\n        **kwargs,\n    ) -&gt; gpd.GeoDataFrame:\n        \"\"\"\n        Load GHSL data into a geopandas GeoDataFrame.\n\n        Args:\n            source: The data source specification\n            ensure_available: If True, ensure data is downloaded before loading\n            **kwargs: Additional parameters passed to load methods\n\n        Returns:\n            GeoDataFrame containing the GHSL data\n        \"\"\"\n        tif_processors = self.load_data(\n            source=source, ensure_available=ensure_available, **kwargs\n        )\n        if isinstance(tif_processors, TifProcessor):\n            return tif_processors.to_geodataframe(**kwargs)\n\n        return pd.concat(\n            [tp.to_geodataframe(**kwargs) for tp in tif_processors], ignore_index=True\n        )\n</code></pre>"},{"location":"api/handlers/#gigaspatial.handlers.worldpop.WPPopulationHandler.create_config","title":"<code>create_config(data_store, logger, **kwargs)</code>","text":"<p>Create and return a WPPopulationConfig instance.</p> <p>Parameters:</p> Name Type Description Default <code>data_store</code> <code>DataStore</code> <p>The data store instance to use</p> required <code>logger</code> <code>Logger</code> <p>The logger instance to use</p> required <code>**kwargs</code> <p>Additional configuration parameters</p> <code>{}</code> <p>Returns:</p> Type Description <code>WPPopulationConfig</code> <p>Configured WPPopulationConfig instance</p> Source code in <code>gigaspatial/handlers/worldpop.py</code> <pre><code>def create_config(\n    self, data_store: DataStore, logger: logging.Logger, **kwargs\n) -&gt; WPPopulationConfig:\n    \"\"\"\n    Create and return a WPPopulationConfig instance.\n\n    Args:\n        data_store: The data store instance to use\n        logger: The logger instance to use\n        **kwargs: Additional configuration parameters\n\n    Returns:\n        Configured WPPopulationConfig instance\n    \"\"\"\n    return WPPopulationConfig(\n        project=self._project,\n        year=self._year,\n        resolution=self._resolution,\n        un_adjusted=self._un_adjusted,\n        constrained=self._constrained,\n        school_age=self._school_age,\n        data_store=data_store,\n        logger=logger,\n        **kwargs,\n    )\n</code></pre>"},{"location":"api/handlers/#gigaspatial.handlers.worldpop.WPPopulationHandler.create_downloader","title":"<code>create_downloader(config, data_store, logger, **kwargs)</code>","text":"<p>Create and return a WPPopulationDownloader instance.</p> <p>Parameters:</p> Name Type Description Default <code>config</code> <code>WPPopulationConfig</code> <p>The configuration object</p> required <code>data_store</code> <code>DataStore</code> <p>The data store instance to use</p> required <code>logger</code> <code>Logger</code> <p>The logger instance to use</p> required <code>**kwargs</code> <p>Additional downloader parameters</p> <code>{}</code> <p>Returns:</p> Type Description <code>WPPopulationDownloader</code> <p>Configured WPPopulationDownloader instance</p> Source code in <code>gigaspatial/handlers/worldpop.py</code> <pre><code>def create_downloader(\n    self,\n    config: WPPopulationConfig,\n    data_store: DataStore,\n    logger: logging.Logger,\n    **kwargs,\n) -&gt; WPPopulationDownloader:\n    \"\"\"\n    Create and return a WPPopulationDownloader instance.\n\n    Args:\n        config: The configuration object\n        data_store: The data store instance to use\n        logger: The logger instance to use\n        **kwargs: Additional downloader parameters\n\n    Returns:\n        Configured WPPopulationDownloader instance\n    \"\"\"\n    return WPPopulationDownloader(\n        config=config, data_store=data_store, logger=logger, **kwargs\n    )\n</code></pre>"},{"location":"api/handlers/#gigaspatial.handlers.worldpop.WPPopulationHandler.create_reader","title":"<code>create_reader(config, data_store, logger, **kwargs)</code>","text":"<p>Create and return a WPPopulationReader instance.</p> <p>Parameters:</p> Name Type Description Default <code>config</code> <code>WPPopulationConfig</code> <p>The configuration object</p> required <code>data_store</code> <code>DataStore</code> <p>The data store instance to use</p> required <code>logger</code> <code>Logger</code> <p>The logger instance to use</p> required <code>**kwargs</code> <p>Additional reader parameters</p> <code>{}</code> <p>Returns:</p> Type Description <code>WPPopulationReader</code> <p>Configured WPPopulationReader instance</p> Source code in <code>gigaspatial/handlers/worldpop.py</code> <pre><code>def create_reader(\n    self,\n    config: WPPopulationConfig,\n    data_store: DataStore,\n    logger: logging.Logger,\n    **kwargs,\n) -&gt; WPPopulationReader:\n    \"\"\"\n    Create and return a WPPopulationReader instance.\n\n    Args:\n        config: The configuration object\n        data_store: The data store instance to use\n        logger: The logger instance to use\n        **kwargs: Additional reader parameters\n\n    Returns:\n        Configured WPPopulationReader instance\n    \"\"\"\n    return WPPopulationReader(\n        config=config, data_store=data_store, logger=logger, **kwargs\n    )\n</code></pre>"},{"location":"api/handlers/#gigaspatial.handlers.worldpop.WPPopulationHandler.load_into_dataframe","title":"<code>load_into_dataframe(source, ensure_available=True, **kwargs)</code>","text":"<p>Load GHSL data into a pandas DataFrame.</p> <p>Parameters:</p> Name Type Description Default <code>source</code> <code>str</code> <p>The data source specification</p> required <code>ensure_available</code> <code>bool</code> <p>If True, ensure data is downloaded before loading</p> <code>True</code> <code>**kwargs</code> <p>Additional parameters passed to load methods</p> <code>{}</code> <p>Returns:</p> Type Description <code>DataFrame</code> <p>DataFrame containing the GHSL data</p> Source code in <code>gigaspatial/handlers/worldpop.py</code> <pre><code>def load_into_dataframe(\n    self,\n    source: str,\n    ensure_available: bool = True,\n    **kwargs,\n) -&gt; pd.DataFrame:\n    \"\"\"\n    Load GHSL data into a pandas DataFrame.\n\n    Args:\n        source: The data source specification\n        ensure_available: If True, ensure data is downloaded before loading\n        **kwargs: Additional parameters passed to load methods\n\n    Returns:\n        DataFrame containing the GHSL data\n    \"\"\"\n    tif_processors = self.load_data(\n        source=source, ensure_available=ensure_available, **kwargs\n    )\n    if isinstance(tif_processors, TifProcessor):\n        return tif_processors.to_dataframe(**kwargs)\n\n    return pd.concat(\n        [tp.to_dataframe(**kwargs) for tp in tif_processors], ignore_index=True\n    )\n</code></pre>"},{"location":"api/handlers/#gigaspatial.handlers.worldpop.WPPopulationHandler.load_into_geodataframe","title":"<code>load_into_geodataframe(source, ensure_available=True, **kwargs)</code>","text":"<p>Load GHSL data into a geopandas GeoDataFrame.</p> <p>Parameters:</p> Name Type Description Default <code>source</code> <code>str</code> <p>The data source specification</p> required <code>ensure_available</code> <code>bool</code> <p>If True, ensure data is downloaded before loading</p> <code>True</code> <code>**kwargs</code> <p>Additional parameters passed to load methods</p> <code>{}</code> <p>Returns:</p> Type Description <code>GeoDataFrame</code> <p>GeoDataFrame containing the GHSL data</p> Source code in <code>gigaspatial/handlers/worldpop.py</code> <pre><code>def load_into_geodataframe(\n    self,\n    source: str,\n    ensure_available: bool = True,\n    **kwargs,\n) -&gt; gpd.GeoDataFrame:\n    \"\"\"\n    Load GHSL data into a geopandas GeoDataFrame.\n\n    Args:\n        source: The data source specification\n        ensure_available: If True, ensure data is downloaded before loading\n        **kwargs: Additional parameters passed to load methods\n\n    Returns:\n        GeoDataFrame containing the GHSL data\n    \"\"\"\n    tif_processors = self.load_data(\n        source=source, ensure_available=ensure_available, **kwargs\n    )\n    if isinstance(tif_processors, TifProcessor):\n        return tif_processors.to_geodataframe(**kwargs)\n\n    return pd.concat(\n        [tp.to_geodataframe(**kwargs) for tp in tif_processors], ignore_index=True\n    )\n</code></pre>"},{"location":"api/handlers/#gigaspatial.handlers.worldpop.WPPopulationReader","title":"<code>WPPopulationReader</code>","text":"<p>               Bases: <code>BaseHandlerReader</code></p> Source code in <code>gigaspatial/handlers/worldpop.py</code> <pre><code>class WPPopulationReader(BaseHandlerReader):\n\n    def __init__(\n        self,\n        config: Union[WPPopulationConfig, dict[str, Union[str, int]]],\n        data_store: Optional[DataStore] = None,\n        logger: Optional[logging.Logger] = None,\n    ):\n        \"\"\"\n        Initialize the reader.\n\n        Args:\n            config: Configuration for the WorldPop dataset, either as a WPPopulationConfig object or a dictionary of parameters\n            data_store: Optional data storage interface. If not provided, uses LocalDataStore.\n            logger: Optional custom logger. If not provided, uses default logger.\n        \"\"\"\n        config = (\n            config\n            if isinstance(config, WPPopulationConfig)\n            else WPPopulationConfig(**config)\n        )\n        super().__init__(config=config, data_store=data_store, logger=logger)\n\n    def load_from_paths(\n        self,\n        source_data_path: List[Union[str, Path]],\n        merge_rasters: bool = False,\n        **kwargs,\n    ) -&gt; Union[List[TifProcessor], TifProcessor]:\n        \"\"\"\n        Load TifProcessors of WP datasets.\n        Args:\n            source_data_path: List of file paths to load\n            merge_rasters: If True, all rasters will be merged into a single TifProcessor.\n                           Defaults to False.\n        Returns:\n            Union[List[TifProcessor], TifProcessor]: List of TifProcessor objects for accessing the raster data or a single\n                                                    TifProcessor if merge_rasters is True.\n        \"\"\"\n        # Apply deferred age/sex filters if present and applicable\n        if (\n            hasattr(self.config, \"_temp_age_sex_filters\")\n            and self.config.project == \"age_structures\"\n            and not self.config.school_age\n        ):\n            # Ensure source_data_path is a list of Path objects for consistent filtering\n            source_data_path = [\n                Path(p) if isinstance(p, str) else p for p in source_data_path\n            ]\n            filtered_paths = self.config._filter_age_sex_paths(\n                source_data_path, self.config._temp_age_sex_filters\n            )\n            # Clear the temporary filter after use\n            del self.config._temp_age_sex_filters\n            if not filtered_paths:\n                self.logger.warning(\n                    \"No WorldPop age_structures paths matched the applied filters.\"\n                )\n                return []  # Return empty list if no paths after filtering\n            source_data_path = filtered_paths\n\n        return self._load_raster_data(\n            raster_paths=source_data_path, merge_rasters=merge_rasters\n        )\n\n    def load(self, source, merge_rasters: bool = False, **kwargs):\n        return super().load(source=source, merge_rasters=merge_rasters, **kwargs)\n</code></pre>"},{"location":"api/handlers/#gigaspatial.handlers.worldpop.WPPopulationReader.__init__","title":"<code>__init__(config, data_store=None, logger=None)</code>","text":"<p>Initialize the reader.</p> <p>Parameters:</p> Name Type Description Default <code>config</code> <code>Union[WPPopulationConfig, dict[str, Union[str, int]]]</code> <p>Configuration for the WorldPop dataset, either as a WPPopulationConfig object or a dictionary of parameters</p> required <code>data_store</code> <code>Optional[DataStore]</code> <p>Optional data storage interface. If not provided, uses LocalDataStore.</p> <code>None</code> <code>logger</code> <code>Optional[Logger]</code> <p>Optional custom logger. If not provided, uses default logger.</p> <code>None</code> Source code in <code>gigaspatial/handlers/worldpop.py</code> <pre><code>def __init__(\n    self,\n    config: Union[WPPopulationConfig, dict[str, Union[str, int]]],\n    data_store: Optional[DataStore] = None,\n    logger: Optional[logging.Logger] = None,\n):\n    \"\"\"\n    Initialize the reader.\n\n    Args:\n        config: Configuration for the WorldPop dataset, either as a WPPopulationConfig object or a dictionary of parameters\n        data_store: Optional data storage interface. If not provided, uses LocalDataStore.\n        logger: Optional custom logger. If not provided, uses default logger.\n    \"\"\"\n    config = (\n        config\n        if isinstance(config, WPPopulationConfig)\n        else WPPopulationConfig(**config)\n    )\n    super().__init__(config=config, data_store=data_store, logger=logger)\n</code></pre>"},{"location":"api/handlers/#gigaspatial.handlers.worldpop.WPPopulationReader.load_from_paths","title":"<code>load_from_paths(source_data_path, merge_rasters=False, **kwargs)</code>","text":"<p>Load TifProcessors of WP datasets. Args:     source_data_path: List of file paths to load     merge_rasters: If True, all rasters will be merged into a single TifProcessor.                    Defaults to False. Returns:     Union[List[TifProcessor], TifProcessor]: List of TifProcessor objects for accessing the raster data or a single                                             TifProcessor if merge_rasters is True.</p> Source code in <code>gigaspatial/handlers/worldpop.py</code> <pre><code>def load_from_paths(\n    self,\n    source_data_path: List[Union[str, Path]],\n    merge_rasters: bool = False,\n    **kwargs,\n) -&gt; Union[List[TifProcessor], TifProcessor]:\n    \"\"\"\n    Load TifProcessors of WP datasets.\n    Args:\n        source_data_path: List of file paths to load\n        merge_rasters: If True, all rasters will be merged into a single TifProcessor.\n                       Defaults to False.\n    Returns:\n        Union[List[TifProcessor], TifProcessor]: List of TifProcessor objects for accessing the raster data or a single\n                                                TifProcessor if merge_rasters is True.\n    \"\"\"\n    # Apply deferred age/sex filters if present and applicable\n    if (\n        hasattr(self.config, \"_temp_age_sex_filters\")\n        and self.config.project == \"age_structures\"\n        and not self.config.school_age\n    ):\n        # Ensure source_data_path is a list of Path objects for consistent filtering\n        source_data_path = [\n            Path(p) if isinstance(p, str) else p for p in source_data_path\n        ]\n        filtered_paths = self.config._filter_age_sex_paths(\n            source_data_path, self.config._temp_age_sex_filters\n        )\n        # Clear the temporary filter after use\n        del self.config._temp_age_sex_filters\n        if not filtered_paths:\n            self.logger.warning(\n                \"No WorldPop age_structures paths matched the applied filters.\"\n            )\n            return []  # Return empty list if no paths after filtering\n        source_data_path = filtered_paths\n\n    return self._load_raster_data(\n        raster_paths=source_data_path, merge_rasters=merge_rasters\n    )\n</code></pre>"},{"location":"api/handlers/#gigaspatial.handlers.worldpop.WorldPopRestClient","title":"<code>WorldPopRestClient</code>","text":"<p>REST API client for WorldPop data access.</p> <p>This class provides direct access to the WorldPop REST API without any configuration dependencies, allowing flexible integration patterns.</p> Source code in <code>gigaspatial/handlers/worldpop.py</code> <pre><code>class WorldPopRestClient:\n    \"\"\"\n    REST API client for WorldPop data access.\n\n    This class provides direct access to the WorldPop REST API without any\n    configuration dependencies, allowing flexible integration patterns.\n    \"\"\"\n\n    def __init__(\n        self,\n        base_url: str = \"https://www.worldpop.org/rest/data\",\n        stats_url: str = \"https://api.worldpop.org/v1/services/stats\",\n        api_key: Optional[str] = None,\n        timeout: int = 30,\n        logger: Optional[logging.Logger] = None,\n    ):\n        \"\"\"\n        Initialize the WorldPop REST API client.\n\n        Args:\n            base_url: Base URL for the WorldPop REST API\n            stats_url: URL for the WorldPop statistics API\n            api_key: Optional API key for higher rate limits\n            timeout: Request timeout in seconds\n            logger: Optional logger instance\n        \"\"\"\n        self.base_url = base_url.rstrip(\"/\")\n        self.stats_url = stats_url.rstrip(\"/\")\n        self.api_key = api_key\n        self.timeout = timeout\n        self.logger = logger or logging.getLogger(self.__class__.__name__)\n\n        # Setup session with default headers\n        self.session = requests.Session()\n        self.session.headers.update(\n            {\"Accept\": \"application/json\", \"User-Agent\": \"WorldPop-Python-Client/1.0\"}\n        )\n\n        if self.api_key:\n            self.session.headers[\"X-API-Key\"] = self.api_key\n\n    def get_available_projects(self) -&gt; List[Dict[str, Any]]:\n        \"\"\"\n        Get list of all available projects (e.g., population, births, pregnancies, etc.).\n\n        Returns:\n            List of project dictionaries with alias, name, title, and description\n        \"\"\"\n        try:\n            response = self.session.get(self.base_url, timeout=self.timeout)\n            response.raise_for_status()\n            data = response.json()\n            return data.get(\"data\", [])\n        except requests.RequestException as e:\n            self.logger.error(f\"Failed to fetch available project aliases: {e}\")\n            return []\n\n    def get_project_sources(self, dataset_type: str) -&gt; List[Dict[str, Any]]:\n        \"\"\"\n        Get available sources for a specific project type.\n\n        Args:\n            dataset_type: Project type alias (e.g., 'pop', 'births', 'pregnancies')\n\n        Returns:\n            List of source dictionaries with alias and name\n        \"\"\"\n        try:\n            url = f\"{self.base_url}/{dataset_type}\"\n            response = self.session.get(url, timeout=self.timeout)\n            response.raise_for_status()\n            data = response.json()\n            return data.get(\"data\", [])\n        except requests.RequestException as e:\n            self.logger.error(\n                f\"Failed to fetch project sources for {dataset_type}: {e}\"\n            )\n            return []\n\n    def get_source_entities(\n        self, dataset_type: str, category: str\n    ) -&gt; List[Dict[str, Any]]:\n        \"\"\"\n        Get list of entities (countries, global, continental) available for a specific project type and source.\n\n        Args:\n            dataset_type: Project type alias (e.g., 'pop', 'births')\n            category: Source alias (e.g., 'wpgp', 'pic')\n\n        Returns:\n            List of entity dictionaries with id and iso3 codes (if applicable)\n        \"\"\"\n        try:\n            url = f\"{self.base_url}/{dataset_type}/{category}\"\n            response = self.session.get(url, timeout=self.timeout)\n            response.raise_for_status()\n            data = response.json()\n            return data.get(\"data\", [])\n        except requests.RequestException as e:\n            self.logger.error(\n                f\"Failed to fetch entities for {dataset_type}/{category}: {e}\"\n            )\n            return []\n\n    def get_datasets(self, dataset_type: str, category: str, params: dict):\n        \"\"\"\n        Get all datasets available for the params.\n\n        Args:\n            dataset_type: Dataset type alias (e.g., 'pop', 'births')\n            category: Category alias (e.g., 'wpgp', 'pic')\n            params: Query parameters (e.g., {'iso3`:'RWA'})\n\n        Returns:\n            List of dataset dictionaries with metadata and file information\n        \"\"\"\n        try:\n            url = f\"{self.base_url}/{dataset_type}/{category}\"\n            response = self.session.get(url, params=params, timeout=self.timeout)\n            response.raise_for_status()\n            data = response.json()\n            return data.get(\"data\", [])\n        except requests.RequestException as e:\n            self.logger.error(f\"Failed to fetch datasets for {params}: {e}\")\n            return []\n\n    def get_datasets_by_country(\n        self, dataset_type: str, category: str, iso3: str\n    ) -&gt; List[Dict[str, Any]]:\n        \"\"\"\n        Get all datasets available for a specific country.\n\n        Args:\n            dataset_type: Dataset type alias (e.g., 'pop', 'births')\n            category: Category alias (e.g., 'wpgp', 'pic')\n            iso3: ISO3 country code (e.g., 'USA', 'BRA')\n\n        Returns:\n            List of dataset dictionaries with metadata and file information\n        \"\"\"\n        params = {\"iso3\": iso3}\n        return self.get_datasets(dataset_type, category, params)\n\n    def get_dataset_by_id(\n        self, dataset_type: str, category: str, dataset_id: str\n    ) -&gt; Optional[Dict[str, Any]]:\n        \"\"\"\n        Get dataset information by ID.\n\n        Args:\n            dataset_type: Dataset type alias (e.g., 'pop', 'births')\n            category: Category alias (e.g., 'wpgp', 'pic')\n            dataset_id: Dataset ID\n\n        Returns:\n            Dataset dictionary or None if not found\n        \"\"\"\n        params = {\"id\": dataset_id}\n        return self.get_datasets(dataset_type, category, params)\n\n    def find_dataset(\n        self,\n        dataset_type: str,\n        category: str,\n        iso3: str,\n        year: Union[str, int],\n        **filters,\n    ) -&gt; Optional[Dict[str, Any]]:\n        \"\"\"\n        Find a specific dataset by year and optional filters.\n\n        Args:\n            dataset_type: Dataset type alias\n            category: Category alias\n            iso3: ISO3 country code\n            year: Year to search for\n            **filters: Additional filters (e.g., gender='F', resolution='1km')\n\n        Returns:\n            Dataset dictionary or None if not found\n        \"\"\"\n        datasets = self.get_country_datasets(dataset_type, category, iso3)\n        year_str = str(year)\n\n        for dataset in datasets:\n            if dataset.get(\"popyear\") == year_str:\n                # Check additional filters\n                match = True\n                for key, value in filters.items():\n                    if key in dataset and dataset[key] != value:\n                        match = False\n                        break\n\n                if match:\n                    return dataset\n\n        return None\n\n    def list_years_for_country(\n        self, dataset_type: str, category: str, iso3: str\n    ) -&gt; List[int]:\n        \"\"\"\n        List all available years for a specific country and dataset.\n\n        Args:\n            dataset_type: Dataset type alias\n            category: Category alias\n            iso3: ISO3 country code\n\n        Returns:\n            Sorted list of available years\n        \"\"\"\n        datasets = self.get_datasets_by_country(dataset_type, category, iso3)\n        years = []\n\n        for dataset in datasets:\n            try:\n                year = int(dataset.get(\"popyear\", 0))\n                if year &gt; 0:\n                    years.append(year)\n            except (ValueError, TypeError):\n                continue\n\n        return sorted(years)\n\n    def search_datasets(\n        self,\n        dataset_type: Optional[str] = None,\n        category: Optional[str] = None,\n        iso3: Optional[str] = None,\n        year: Optional[Union[str, int]] = None,\n        **filters,\n    ) -&gt; List[Dict[str, Any]]:\n        \"\"\"\n        Search for datasets with flexible filtering.\n\n        Args:\n            dataset_type: Optional dataset type filter\n            category: Optional category filter\n            iso3: Optional country filter\n            year: Optional year filter\n            **filters: Additional filters\n\n        Returns:\n            List of matching datasets\n        \"\"\"\n        results = []\n\n        if dataset_type:\n            if category:\n                # If we have country-specific filters\n                if iso3:\n                    datasets = self.get_datasets_by_country(\n                        dataset_type, category, iso3\n                    )\n                    for dataset in datasets:\n                        match = True\n\n                        # Check year filter\n                        if year and dataset.get(\"popyear\") != str(year):\n                            match = False\n\n                        # Check additional filters\n                        for key, value in filters.items():\n                            if key in dataset and dataset[key] != value:\n                                match = False\n                                break\n\n                        if match:\n                            results.append(dataset)\n                else:\n                    return self.get_source_entities(dataset_type, category)\n            else:\n                return self.get_project_sources(dataset_type)\n        else:\n            return self.get_available_projects()\n\n        return results\n\n    def get_dataset_info(self, dataset: Dict[str, Any]) -&gt; Dict[str, Any]:\n        \"\"\"\n        Extract useful information from a dataset dictionary.\n\n        Args:\n            dataset: Dataset dictionary from API\n\n        Returns:\n            Cleaned dataset information\n        \"\"\"\n        return {\n            \"id\": dataset.get(\"id\"),\n            \"title\": dataset.get(\"title\"),\n            \"description\": dataset.get(\"desc\"),\n            \"doi\": dataset.get(\"doi\"),\n            \"citation\": dataset.get(\"citation\"),\n            \"data_format\": dataset.get(\"data_format\"),\n            \"year\": dataset.get(\"popyear\"),\n            \"country\": dataset.get(\"country\"),\n            \"iso3\": dataset.get(\"iso3\"),\n            \"continent\": dataset.get(\"continent\"),\n            \"download_urls\": dataset.get(\"files\", []),\n            \"image_url\": dataset.get(\"url_img\"),\n            \"summary_url\": dataset.get(\"url_summary\"),\n            \"license\": dataset.get(\"license\"),\n            \"organization\": dataset.get(\"organisation\"),\n            \"author\": dataset.get(\"author_name\"),\n            \"maintainer\": dataset.get(\"maintainer_name\"),\n            \"project\": dataset.get(\"project\"),\n            \"category\": dataset.get(\"category\"),\n            \"date_created\": dataset.get(\"date\"),\n            \"public\": dataset.get(\"public\") == \"Y\",\n            \"archived\": dataset.get(\"archive\") == \"Y\",\n        }\n\n    def close(self):\n        \"\"\"Close the session.\"\"\"\n        self.session.close()\n\n    def __enter__(self):\n        \"\"\"Context manager entry.\"\"\"\n        return self\n\n    def __exit__(self, exc_type, exc_val, exc_tb):\n        \"\"\"Context manager exit.\"\"\"\n        self.close()\n</code></pre>"},{"location":"api/handlers/#gigaspatial.handlers.worldpop.WorldPopRestClient.__enter__","title":"<code>__enter__()</code>","text":"<p>Context manager entry.</p> Source code in <code>gigaspatial/handlers/worldpop.py</code> <pre><code>def __enter__(self):\n    \"\"\"Context manager entry.\"\"\"\n    return self\n</code></pre>"},{"location":"api/handlers/#gigaspatial.handlers.worldpop.WorldPopRestClient.__exit__","title":"<code>__exit__(exc_type, exc_val, exc_tb)</code>","text":"<p>Context manager exit.</p> Source code in <code>gigaspatial/handlers/worldpop.py</code> <pre><code>def __exit__(self, exc_type, exc_val, exc_tb):\n    \"\"\"Context manager exit.\"\"\"\n    self.close()\n</code></pre>"},{"location":"api/handlers/#gigaspatial.handlers.worldpop.WorldPopRestClient.__init__","title":"<code>__init__(base_url='https://www.worldpop.org/rest/data', stats_url='https://api.worldpop.org/v1/services/stats', api_key=None, timeout=30, logger=None)</code>","text":"<p>Initialize the WorldPop REST API client.</p> <p>Parameters:</p> Name Type Description Default <code>base_url</code> <code>str</code> <p>Base URL for the WorldPop REST API</p> <code>'https://www.worldpop.org/rest/data'</code> <code>stats_url</code> <code>str</code> <p>URL for the WorldPop statistics API</p> <code>'https://api.worldpop.org/v1/services/stats'</code> <code>api_key</code> <code>Optional[str]</code> <p>Optional API key for higher rate limits</p> <code>None</code> <code>timeout</code> <code>int</code> <p>Request timeout in seconds</p> <code>30</code> <code>logger</code> <code>Optional[Logger]</code> <p>Optional logger instance</p> <code>None</code> Source code in <code>gigaspatial/handlers/worldpop.py</code> <pre><code>def __init__(\n    self,\n    base_url: str = \"https://www.worldpop.org/rest/data\",\n    stats_url: str = \"https://api.worldpop.org/v1/services/stats\",\n    api_key: Optional[str] = None,\n    timeout: int = 30,\n    logger: Optional[logging.Logger] = None,\n):\n    \"\"\"\n    Initialize the WorldPop REST API client.\n\n    Args:\n        base_url: Base URL for the WorldPop REST API\n        stats_url: URL for the WorldPop statistics API\n        api_key: Optional API key for higher rate limits\n        timeout: Request timeout in seconds\n        logger: Optional logger instance\n    \"\"\"\n    self.base_url = base_url.rstrip(\"/\")\n    self.stats_url = stats_url.rstrip(\"/\")\n    self.api_key = api_key\n    self.timeout = timeout\n    self.logger = logger or logging.getLogger(self.__class__.__name__)\n\n    # Setup session with default headers\n    self.session = requests.Session()\n    self.session.headers.update(\n        {\"Accept\": \"application/json\", \"User-Agent\": \"WorldPop-Python-Client/1.0\"}\n    )\n\n    if self.api_key:\n        self.session.headers[\"X-API-Key\"] = self.api_key\n</code></pre>"},{"location":"api/handlers/#gigaspatial.handlers.worldpop.WorldPopRestClient.close","title":"<code>close()</code>","text":"<p>Close the session.</p> Source code in <code>gigaspatial/handlers/worldpop.py</code> <pre><code>def close(self):\n    \"\"\"Close the session.\"\"\"\n    self.session.close()\n</code></pre>"},{"location":"api/handlers/#gigaspatial.handlers.worldpop.WorldPopRestClient.find_dataset","title":"<code>find_dataset(dataset_type, category, iso3, year, **filters)</code>","text":"<p>Find a specific dataset by year and optional filters.</p> <p>Parameters:</p> Name Type Description Default <code>dataset_type</code> <code>str</code> <p>Dataset type alias</p> required <code>category</code> <code>str</code> <p>Category alias</p> required <code>iso3</code> <code>str</code> <p>ISO3 country code</p> required <code>year</code> <code>Union[str, int]</code> <p>Year to search for</p> required <code>**filters</code> <p>Additional filters (e.g., gender='F', resolution='1km')</p> <code>{}</code> <p>Returns:</p> Type Description <code>Optional[Dict[str, Any]]</code> <p>Dataset dictionary or None if not found</p> Source code in <code>gigaspatial/handlers/worldpop.py</code> <pre><code>def find_dataset(\n    self,\n    dataset_type: str,\n    category: str,\n    iso3: str,\n    year: Union[str, int],\n    **filters,\n) -&gt; Optional[Dict[str, Any]]:\n    \"\"\"\n    Find a specific dataset by year and optional filters.\n\n    Args:\n        dataset_type: Dataset type alias\n        category: Category alias\n        iso3: ISO3 country code\n        year: Year to search for\n        **filters: Additional filters (e.g., gender='F', resolution='1km')\n\n    Returns:\n        Dataset dictionary or None if not found\n    \"\"\"\n    datasets = self.get_country_datasets(dataset_type, category, iso3)\n    year_str = str(year)\n\n    for dataset in datasets:\n        if dataset.get(\"popyear\") == year_str:\n            # Check additional filters\n            match = True\n            for key, value in filters.items():\n                if key in dataset and dataset[key] != value:\n                    match = False\n                    break\n\n            if match:\n                return dataset\n\n    return None\n</code></pre>"},{"location":"api/handlers/#gigaspatial.handlers.worldpop.WorldPopRestClient.get_available_projects","title":"<code>get_available_projects()</code>","text":"<p>Get list of all available projects (e.g., population, births, pregnancies, etc.).</p> <p>Returns:</p> Type Description <code>List[Dict[str, Any]]</code> <p>List of project dictionaries with alias, name, title, and description</p> Source code in <code>gigaspatial/handlers/worldpop.py</code> <pre><code>def get_available_projects(self) -&gt; List[Dict[str, Any]]:\n    \"\"\"\n    Get list of all available projects (e.g., population, births, pregnancies, etc.).\n\n    Returns:\n        List of project dictionaries with alias, name, title, and description\n    \"\"\"\n    try:\n        response = self.session.get(self.base_url, timeout=self.timeout)\n        response.raise_for_status()\n        data = response.json()\n        return data.get(\"data\", [])\n    except requests.RequestException as e:\n        self.logger.error(f\"Failed to fetch available project aliases: {e}\")\n        return []\n</code></pre>"},{"location":"api/handlers/#gigaspatial.handlers.worldpop.WorldPopRestClient.get_dataset_by_id","title":"<code>get_dataset_by_id(dataset_type, category, dataset_id)</code>","text":"<p>Get dataset information by ID.</p> <p>Parameters:</p> Name Type Description Default <code>dataset_type</code> <code>str</code> <p>Dataset type alias (e.g., 'pop', 'births')</p> required <code>category</code> <code>str</code> <p>Category alias (e.g., 'wpgp', 'pic')</p> required <code>dataset_id</code> <code>str</code> <p>Dataset ID</p> required <p>Returns:</p> Type Description <code>Optional[Dict[str, Any]]</code> <p>Dataset dictionary or None if not found</p> Source code in <code>gigaspatial/handlers/worldpop.py</code> <pre><code>def get_dataset_by_id(\n    self, dataset_type: str, category: str, dataset_id: str\n) -&gt; Optional[Dict[str, Any]]:\n    \"\"\"\n    Get dataset information by ID.\n\n    Args:\n        dataset_type: Dataset type alias (e.g., 'pop', 'births')\n        category: Category alias (e.g., 'wpgp', 'pic')\n        dataset_id: Dataset ID\n\n    Returns:\n        Dataset dictionary or None if not found\n    \"\"\"\n    params = {\"id\": dataset_id}\n    return self.get_datasets(dataset_type, category, params)\n</code></pre>"},{"location":"api/handlers/#gigaspatial.handlers.worldpop.WorldPopRestClient.get_dataset_info","title":"<code>get_dataset_info(dataset)</code>","text":"<p>Extract useful information from a dataset dictionary.</p> <p>Parameters:</p> Name Type Description Default <code>dataset</code> <code>Dict[str, Any]</code> <p>Dataset dictionary from API</p> required <p>Returns:</p> Type Description <code>Dict[str, Any]</code> <p>Cleaned dataset information</p> Source code in <code>gigaspatial/handlers/worldpop.py</code> <pre><code>def get_dataset_info(self, dataset: Dict[str, Any]) -&gt; Dict[str, Any]:\n    \"\"\"\n    Extract useful information from a dataset dictionary.\n\n    Args:\n        dataset: Dataset dictionary from API\n\n    Returns:\n        Cleaned dataset information\n    \"\"\"\n    return {\n        \"id\": dataset.get(\"id\"),\n        \"title\": dataset.get(\"title\"),\n        \"description\": dataset.get(\"desc\"),\n        \"doi\": dataset.get(\"doi\"),\n        \"citation\": dataset.get(\"citation\"),\n        \"data_format\": dataset.get(\"data_format\"),\n        \"year\": dataset.get(\"popyear\"),\n        \"country\": dataset.get(\"country\"),\n        \"iso3\": dataset.get(\"iso3\"),\n        \"continent\": dataset.get(\"continent\"),\n        \"download_urls\": dataset.get(\"files\", []),\n        \"image_url\": dataset.get(\"url_img\"),\n        \"summary_url\": dataset.get(\"url_summary\"),\n        \"license\": dataset.get(\"license\"),\n        \"organization\": dataset.get(\"organisation\"),\n        \"author\": dataset.get(\"author_name\"),\n        \"maintainer\": dataset.get(\"maintainer_name\"),\n        \"project\": dataset.get(\"project\"),\n        \"category\": dataset.get(\"category\"),\n        \"date_created\": dataset.get(\"date\"),\n        \"public\": dataset.get(\"public\") == \"Y\",\n        \"archived\": dataset.get(\"archive\") == \"Y\",\n    }\n</code></pre>"},{"location":"api/handlers/#gigaspatial.handlers.worldpop.WorldPopRestClient.get_datasets","title":"<code>get_datasets(dataset_type, category, params)</code>","text":"<p>Get all datasets available for the params.</p> <p>Parameters:</p> Name Type Description Default <code>dataset_type</code> <code>str</code> <p>Dataset type alias (e.g., 'pop', 'births')</p> required <code>category</code> <code>str</code> <p>Category alias (e.g., 'wpgp', 'pic')</p> required <code>params</code> <code>dict</code> <p>Query parameters (e.g., {'iso3`:'RWA'})</p> required <p>Returns:</p> Type Description <p>List of dataset dictionaries with metadata and file information</p> Source code in <code>gigaspatial/handlers/worldpop.py</code> <pre><code>def get_datasets(self, dataset_type: str, category: str, params: dict):\n    \"\"\"\n    Get all datasets available for the params.\n\n    Args:\n        dataset_type: Dataset type alias (e.g., 'pop', 'births')\n        category: Category alias (e.g., 'wpgp', 'pic')\n        params: Query parameters (e.g., {'iso3`:'RWA'})\n\n    Returns:\n        List of dataset dictionaries with metadata and file information\n    \"\"\"\n    try:\n        url = f\"{self.base_url}/{dataset_type}/{category}\"\n        response = self.session.get(url, params=params, timeout=self.timeout)\n        response.raise_for_status()\n        data = response.json()\n        return data.get(\"data\", [])\n    except requests.RequestException as e:\n        self.logger.error(f\"Failed to fetch datasets for {params}: {e}\")\n        return []\n</code></pre>"},{"location":"api/handlers/#gigaspatial.handlers.worldpop.WorldPopRestClient.get_datasets_by_country","title":"<code>get_datasets_by_country(dataset_type, category, iso3)</code>","text":"<p>Get all datasets available for a specific country.</p> <p>Parameters:</p> Name Type Description Default <code>dataset_type</code> <code>str</code> <p>Dataset type alias (e.g., 'pop', 'births')</p> required <code>category</code> <code>str</code> <p>Category alias (e.g., 'wpgp', 'pic')</p> required <code>iso3</code> <code>str</code> <p>ISO3 country code (e.g., 'USA', 'BRA')</p> required <p>Returns:</p> Type Description <code>List[Dict[str, Any]]</code> <p>List of dataset dictionaries with metadata and file information</p> Source code in <code>gigaspatial/handlers/worldpop.py</code> <pre><code>def get_datasets_by_country(\n    self, dataset_type: str, category: str, iso3: str\n) -&gt; List[Dict[str, Any]]:\n    \"\"\"\n    Get all datasets available for a specific country.\n\n    Args:\n        dataset_type: Dataset type alias (e.g., 'pop', 'births')\n        category: Category alias (e.g., 'wpgp', 'pic')\n        iso3: ISO3 country code (e.g., 'USA', 'BRA')\n\n    Returns:\n        List of dataset dictionaries with metadata and file information\n    \"\"\"\n    params = {\"iso3\": iso3}\n    return self.get_datasets(dataset_type, category, params)\n</code></pre>"},{"location":"api/handlers/#gigaspatial.handlers.worldpop.WorldPopRestClient.get_project_sources","title":"<code>get_project_sources(dataset_type)</code>","text":"<p>Get available sources for a specific project type.</p> <p>Parameters:</p> Name Type Description Default <code>dataset_type</code> <code>str</code> <p>Project type alias (e.g., 'pop', 'births', 'pregnancies')</p> required <p>Returns:</p> Type Description <code>List[Dict[str, Any]]</code> <p>List of source dictionaries with alias and name</p> Source code in <code>gigaspatial/handlers/worldpop.py</code> <pre><code>def get_project_sources(self, dataset_type: str) -&gt; List[Dict[str, Any]]:\n    \"\"\"\n    Get available sources for a specific project type.\n\n    Args:\n        dataset_type: Project type alias (e.g., 'pop', 'births', 'pregnancies')\n\n    Returns:\n        List of source dictionaries with alias and name\n    \"\"\"\n    try:\n        url = f\"{self.base_url}/{dataset_type}\"\n        response = self.session.get(url, timeout=self.timeout)\n        response.raise_for_status()\n        data = response.json()\n        return data.get(\"data\", [])\n    except requests.RequestException as e:\n        self.logger.error(\n            f\"Failed to fetch project sources for {dataset_type}: {e}\"\n        )\n        return []\n</code></pre>"},{"location":"api/handlers/#gigaspatial.handlers.worldpop.WorldPopRestClient.get_source_entities","title":"<code>get_source_entities(dataset_type, category)</code>","text":"<p>Get list of entities (countries, global, continental) available for a specific project type and source.</p> <p>Parameters:</p> Name Type Description Default <code>dataset_type</code> <code>str</code> <p>Project type alias (e.g., 'pop', 'births')</p> required <code>category</code> <code>str</code> <p>Source alias (e.g., 'wpgp', 'pic')</p> required <p>Returns:</p> Type Description <code>List[Dict[str, Any]]</code> <p>List of entity dictionaries with id and iso3 codes (if applicable)</p> Source code in <code>gigaspatial/handlers/worldpop.py</code> <pre><code>def get_source_entities(\n    self, dataset_type: str, category: str\n) -&gt; List[Dict[str, Any]]:\n    \"\"\"\n    Get list of entities (countries, global, continental) available for a specific project type and source.\n\n    Args:\n        dataset_type: Project type alias (e.g., 'pop', 'births')\n        category: Source alias (e.g., 'wpgp', 'pic')\n\n    Returns:\n        List of entity dictionaries with id and iso3 codes (if applicable)\n    \"\"\"\n    try:\n        url = f\"{self.base_url}/{dataset_type}/{category}\"\n        response = self.session.get(url, timeout=self.timeout)\n        response.raise_for_status()\n        data = response.json()\n        return data.get(\"data\", [])\n    except requests.RequestException as e:\n        self.logger.error(\n            f\"Failed to fetch entities for {dataset_type}/{category}: {e}\"\n        )\n        return []\n</code></pre>"},{"location":"api/handlers/#gigaspatial.handlers.worldpop.WorldPopRestClient.list_years_for_country","title":"<code>list_years_for_country(dataset_type, category, iso3)</code>","text":"<p>List all available years for a specific country and dataset.</p> <p>Parameters:</p> Name Type Description Default <code>dataset_type</code> <code>str</code> <p>Dataset type alias</p> required <code>category</code> <code>str</code> <p>Category alias</p> required <code>iso3</code> <code>str</code> <p>ISO3 country code</p> required <p>Returns:</p> Type Description <code>List[int]</code> <p>Sorted list of available years</p> Source code in <code>gigaspatial/handlers/worldpop.py</code> <pre><code>def list_years_for_country(\n    self, dataset_type: str, category: str, iso3: str\n) -&gt; List[int]:\n    \"\"\"\n    List all available years for a specific country and dataset.\n\n    Args:\n        dataset_type: Dataset type alias\n        category: Category alias\n        iso3: ISO3 country code\n\n    Returns:\n        Sorted list of available years\n    \"\"\"\n    datasets = self.get_datasets_by_country(dataset_type, category, iso3)\n    years = []\n\n    for dataset in datasets:\n        try:\n            year = int(dataset.get(\"popyear\", 0))\n            if year &gt; 0:\n                years.append(year)\n        except (ValueError, TypeError):\n            continue\n\n    return sorted(years)\n</code></pre>"},{"location":"api/handlers/#gigaspatial.handlers.worldpop.WorldPopRestClient.search_datasets","title":"<code>search_datasets(dataset_type=None, category=None, iso3=None, year=None, **filters)</code>","text":"<p>Search for datasets with flexible filtering.</p> <p>Parameters:</p> Name Type Description Default <code>dataset_type</code> <code>Optional[str]</code> <p>Optional dataset type filter</p> <code>None</code> <code>category</code> <code>Optional[str]</code> <p>Optional category filter</p> <code>None</code> <code>iso3</code> <code>Optional[str]</code> <p>Optional country filter</p> <code>None</code> <code>year</code> <code>Optional[Union[str, int]]</code> <p>Optional year filter</p> <code>None</code> <code>**filters</code> <p>Additional filters</p> <code>{}</code> <p>Returns:</p> Type Description <code>List[Dict[str, Any]]</code> <p>List of matching datasets</p> Source code in <code>gigaspatial/handlers/worldpop.py</code> <pre><code>def search_datasets(\n    self,\n    dataset_type: Optional[str] = None,\n    category: Optional[str] = None,\n    iso3: Optional[str] = None,\n    year: Optional[Union[str, int]] = None,\n    **filters,\n) -&gt; List[Dict[str, Any]]:\n    \"\"\"\n    Search for datasets with flexible filtering.\n\n    Args:\n        dataset_type: Optional dataset type filter\n        category: Optional category filter\n        iso3: Optional country filter\n        year: Optional year filter\n        **filters: Additional filters\n\n    Returns:\n        List of matching datasets\n    \"\"\"\n    results = []\n\n    if dataset_type:\n        if category:\n            # If we have country-specific filters\n            if iso3:\n                datasets = self.get_datasets_by_country(\n                    dataset_type, category, iso3\n                )\n                for dataset in datasets:\n                    match = True\n\n                    # Check year filter\n                    if year and dataset.get(\"popyear\") != str(year):\n                        match = False\n\n                    # Check additional filters\n                    for key, value in filters.items():\n                        if key in dataset and dataset[key] != value:\n                            match = False\n                            break\n\n                    if match:\n                        results.append(dataset)\n            else:\n                return self.get_source_entities(dataset_type, category)\n        else:\n            return self.get_project_sources(dataset_type)\n    else:\n        return self.get_available_projects()\n\n    return results\n</code></pre>"},{"location":"api/processing/","title":"Processing Module","text":""},{"location":"api/processing/#gigaspatial.processing","title":"<code>gigaspatial.processing</code>","text":""},{"location":"api/processing/#gigaspatial.processing.DataStore","title":"<code>DataStore</code>","text":"<p>               Bases: <code>ABC</code></p> <p>Abstract base class defining the interface for data store implementations. This class serves as a parent for both local and cloud-based storage solutions.</p> Source code in <code>gigaspatial/core/io/data_store.py</code> <pre><code>class DataStore(ABC):\n    \"\"\"\n    Abstract base class defining the interface for data store implementations.\n    This class serves as a parent for both local and cloud-based storage solutions.\n    \"\"\"\n\n    @abstractmethod\n    def read_file(self, path: str) -&gt; Any:\n        \"\"\"\n        Read contents of a file from the data store.\n\n        Args:\n            path: Path to the file to read\n\n        Returns:\n            Contents of the file\n\n        Raises:\n            IOError: If file cannot be read\n        \"\"\"\n        pass\n\n    @abstractmethod\n    def write_file(self, path: str, data: Any) -&gt; None:\n        \"\"\"\n        Write data to a file in the data store.\n\n        Args:\n            path: Path where to write the file\n            data: Data to write to the file\n\n        Raises:\n            IOError: If file cannot be written\n        \"\"\"\n        pass\n\n    @abstractmethod\n    def file_exists(self, path: str) -&gt; bool:\n        \"\"\"\n        Check if a file exists in the data store.\n\n        Args:\n            path: Path to check\n\n        Returns:\n            True if file exists, False otherwise\n        \"\"\"\n        pass\n\n    @abstractmethod\n    def list_files(self, path: str) -&gt; List[str]:\n        \"\"\"\n        List all files in a directory.\n\n        Args:\n            path: Directory path to list\n\n        Returns:\n            List of file paths in the directory\n        \"\"\"\n        pass\n\n    @abstractmethod\n    def walk(self, top: str) -&gt; Generator:\n        \"\"\"\n        Walk through directory tree, similar to os.walk().\n\n        Args:\n            top: Starting directory for the walk\n\n        Returns:\n            Generator yielding tuples of (dirpath, dirnames, filenames)\n        \"\"\"\n        pass\n\n    @abstractmethod\n    def open(self, file: str, mode: str = \"r\") -&gt; Union[str, bytes]:\n        \"\"\"\n        Context manager for file operations.\n\n        Args:\n            file: Path to the file\n            mode: File mode ('r', 'w', 'rb', 'wb')\n\n        Yields:\n            File-like object\n\n        Raises:\n            IOError: If file cannot be opened\n        \"\"\"\n        pass\n\n    @abstractmethod\n    def is_file(self, path: str) -&gt; bool:\n        \"\"\"\n        Check if path points to a file.\n\n        Args:\n            path: Path to check\n\n        Returns:\n            True if path is a file, False otherwise\n        \"\"\"\n        pass\n\n    @abstractmethod\n    def is_dir(self, path: str) -&gt; bool:\n        \"\"\"\n        Check if path points to a directory.\n\n        Args:\n            path: Path to check\n\n        Returns:\n            True if path is a directory, False otherwise\n        \"\"\"\n        pass\n\n    @abstractmethod\n    def remove(self, path: str) -&gt; None:\n        \"\"\"\n        Remove a file.\n\n        Args:\n            path: Path to the file to remove\n\n        Raises:\n            IOError: If file cannot be removed\n        \"\"\"\n        pass\n\n    @abstractmethod\n    def rmdir(self, dir: str) -&gt; None:\n        \"\"\"\n        Remove a directory and all its contents.\n\n        Args:\n            dir: Path to the directory to remove\n\n        Raises:\n            IOError: If directory cannot be removed\n        \"\"\"\n        pass\n</code></pre>"},{"location":"api/processing/#gigaspatial.processing.DataStore.file_exists","title":"<code>file_exists(path)</code>  <code>abstractmethod</code>","text":"<p>Check if a file exists in the data store.</p> <p>Parameters:</p> Name Type Description Default <code>path</code> <code>str</code> <p>Path to check</p> required <p>Returns:</p> Type Description <code>bool</code> <p>True if file exists, False otherwise</p> Source code in <code>gigaspatial/core/io/data_store.py</code> <pre><code>@abstractmethod\ndef file_exists(self, path: str) -&gt; bool:\n    \"\"\"\n    Check if a file exists in the data store.\n\n    Args:\n        path: Path to check\n\n    Returns:\n        True if file exists, False otherwise\n    \"\"\"\n    pass\n</code></pre>"},{"location":"api/processing/#gigaspatial.processing.DataStore.is_dir","title":"<code>is_dir(path)</code>  <code>abstractmethod</code>","text":"<p>Check if path points to a directory.</p> <p>Parameters:</p> Name Type Description Default <code>path</code> <code>str</code> <p>Path to check</p> required <p>Returns:</p> Type Description <code>bool</code> <p>True if path is a directory, False otherwise</p> Source code in <code>gigaspatial/core/io/data_store.py</code> <pre><code>@abstractmethod\ndef is_dir(self, path: str) -&gt; bool:\n    \"\"\"\n    Check if path points to a directory.\n\n    Args:\n        path: Path to check\n\n    Returns:\n        True if path is a directory, False otherwise\n    \"\"\"\n    pass\n</code></pre>"},{"location":"api/processing/#gigaspatial.processing.DataStore.is_file","title":"<code>is_file(path)</code>  <code>abstractmethod</code>","text":"<p>Check if path points to a file.</p> <p>Parameters:</p> Name Type Description Default <code>path</code> <code>str</code> <p>Path to check</p> required <p>Returns:</p> Type Description <code>bool</code> <p>True if path is a file, False otherwise</p> Source code in <code>gigaspatial/core/io/data_store.py</code> <pre><code>@abstractmethod\ndef is_file(self, path: str) -&gt; bool:\n    \"\"\"\n    Check if path points to a file.\n\n    Args:\n        path: Path to check\n\n    Returns:\n        True if path is a file, False otherwise\n    \"\"\"\n    pass\n</code></pre>"},{"location":"api/processing/#gigaspatial.processing.DataStore.list_files","title":"<code>list_files(path)</code>  <code>abstractmethod</code>","text":"<p>List all files in a directory.</p> <p>Parameters:</p> Name Type Description Default <code>path</code> <code>str</code> <p>Directory path to list</p> required <p>Returns:</p> Type Description <code>List[str]</code> <p>List of file paths in the directory</p> Source code in <code>gigaspatial/core/io/data_store.py</code> <pre><code>@abstractmethod\ndef list_files(self, path: str) -&gt; List[str]:\n    \"\"\"\n    List all files in a directory.\n\n    Args:\n        path: Directory path to list\n\n    Returns:\n        List of file paths in the directory\n    \"\"\"\n    pass\n</code></pre>"},{"location":"api/processing/#gigaspatial.processing.DataStore.open","title":"<code>open(file, mode='r')</code>  <code>abstractmethod</code>","text":"<p>Context manager for file operations.</p> <p>Parameters:</p> Name Type Description Default <code>file</code> <code>str</code> <p>Path to the file</p> required <code>mode</code> <code>str</code> <p>File mode ('r', 'w', 'rb', 'wb')</p> <code>'r'</code> <p>Yields:</p> Type Description <code>Union[str, bytes]</code> <p>File-like object</p> <p>Raises:</p> Type Description <code>IOError</code> <p>If file cannot be opened</p> Source code in <code>gigaspatial/core/io/data_store.py</code> <pre><code>@abstractmethod\ndef open(self, file: str, mode: str = \"r\") -&gt; Union[str, bytes]:\n    \"\"\"\n    Context manager for file operations.\n\n    Args:\n        file: Path to the file\n        mode: File mode ('r', 'w', 'rb', 'wb')\n\n    Yields:\n        File-like object\n\n    Raises:\n        IOError: If file cannot be opened\n    \"\"\"\n    pass\n</code></pre>"},{"location":"api/processing/#gigaspatial.processing.DataStore.read_file","title":"<code>read_file(path)</code>  <code>abstractmethod</code>","text":"<p>Read contents of a file from the data store.</p> <p>Parameters:</p> Name Type Description Default <code>path</code> <code>str</code> <p>Path to the file to read</p> required <p>Returns:</p> Type Description <code>Any</code> <p>Contents of the file</p> <p>Raises:</p> Type Description <code>IOError</code> <p>If file cannot be read</p> Source code in <code>gigaspatial/core/io/data_store.py</code> <pre><code>@abstractmethod\ndef read_file(self, path: str) -&gt; Any:\n    \"\"\"\n    Read contents of a file from the data store.\n\n    Args:\n        path: Path to the file to read\n\n    Returns:\n        Contents of the file\n\n    Raises:\n        IOError: If file cannot be read\n    \"\"\"\n    pass\n</code></pre>"},{"location":"api/processing/#gigaspatial.processing.DataStore.remove","title":"<code>remove(path)</code>  <code>abstractmethod</code>","text":"<p>Remove a file.</p> <p>Parameters:</p> Name Type Description Default <code>path</code> <code>str</code> <p>Path to the file to remove</p> required <p>Raises:</p> Type Description <code>IOError</code> <p>If file cannot be removed</p> Source code in <code>gigaspatial/core/io/data_store.py</code> <pre><code>@abstractmethod\ndef remove(self, path: str) -&gt; None:\n    \"\"\"\n    Remove a file.\n\n    Args:\n        path: Path to the file to remove\n\n    Raises:\n        IOError: If file cannot be removed\n    \"\"\"\n    pass\n</code></pre>"},{"location":"api/processing/#gigaspatial.processing.DataStore.rmdir","title":"<code>rmdir(dir)</code>  <code>abstractmethod</code>","text":"<p>Remove a directory and all its contents.</p> <p>Parameters:</p> Name Type Description Default <code>dir</code> <code>str</code> <p>Path to the directory to remove</p> required <p>Raises:</p> Type Description <code>IOError</code> <p>If directory cannot be removed</p> Source code in <code>gigaspatial/core/io/data_store.py</code> <pre><code>@abstractmethod\ndef rmdir(self, dir: str) -&gt; None:\n    \"\"\"\n    Remove a directory and all its contents.\n\n    Args:\n        dir: Path to the directory to remove\n\n    Raises:\n        IOError: If directory cannot be removed\n    \"\"\"\n    pass\n</code></pre>"},{"location":"api/processing/#gigaspatial.processing.DataStore.walk","title":"<code>walk(top)</code>  <code>abstractmethod</code>","text":"<p>Walk through directory tree, similar to os.walk().</p> <p>Parameters:</p> Name Type Description Default <code>top</code> <code>str</code> <p>Starting directory for the walk</p> required <p>Returns:</p> Type Description <code>Generator</code> <p>Generator yielding tuples of (dirpath, dirnames, filenames)</p> Source code in <code>gigaspatial/core/io/data_store.py</code> <pre><code>@abstractmethod\ndef walk(self, top: str) -&gt; Generator:\n    \"\"\"\n    Walk through directory tree, similar to os.walk().\n\n    Args:\n        top: Starting directory for the walk\n\n    Returns:\n        Generator yielding tuples of (dirpath, dirnames, filenames)\n    \"\"\"\n    pass\n</code></pre>"},{"location":"api/processing/#gigaspatial.processing.DataStore.write_file","title":"<code>write_file(path, data)</code>  <code>abstractmethod</code>","text":"<p>Write data to a file in the data store.</p> <p>Parameters:</p> Name Type Description Default <code>path</code> <code>str</code> <p>Path where to write the file</p> required <code>data</code> <code>Any</code> <p>Data to write to the file</p> required <p>Raises:</p> Type Description <code>IOError</code> <p>If file cannot be written</p> Source code in <code>gigaspatial/core/io/data_store.py</code> <pre><code>@abstractmethod\ndef write_file(self, path: str, data: Any) -&gt; None:\n    \"\"\"\n    Write data to a file in the data store.\n\n    Args:\n        path: Path where to write the file\n        data: Data to write to the file\n\n    Raises:\n        IOError: If file cannot be written\n    \"\"\"\n    pass\n</code></pre>"},{"location":"api/processing/#gigaspatial.processing.LocalDataStore","title":"<code>LocalDataStore</code>","text":"<p>               Bases: <code>DataStore</code></p> <p>Implementation for local filesystem storage.</p> Source code in <code>gigaspatial/core/io/local_data_store.py</code> <pre><code>class LocalDataStore(DataStore):\n    \"\"\"Implementation for local filesystem storage.\"\"\"\n\n    def __init__(self, base_path: Union[str, Path] = \"\"):\n        super().__init__()\n        self.base_path = Path(base_path).resolve()\n\n    def _resolve_path(self, path: str) -&gt; Path:\n        \"\"\"Resolve path relative to base directory.\"\"\"\n        return self.base_path / path\n\n    def read_file(self, path: str) -&gt; bytes:\n        full_path = self._resolve_path(path)\n        with open(full_path, \"rb\") as f:\n            return f.read()\n\n    def write_file(self, path: str, data: Union[bytes, str]) -&gt; None:\n        full_path = self._resolve_path(path)\n        self.mkdir(str(full_path.parent), exist_ok=True)\n\n        if isinstance(data, str):\n            mode = \"w\"\n            encoding = \"utf-8\"\n        else:\n            mode = \"wb\"\n            encoding = None\n\n        with open(full_path, mode, encoding=encoding) as f:\n            f.write(data)\n\n    def file_exists(self, path: str) -&gt; bool:\n        return self._resolve_path(path).is_file()\n\n    def list_files(self, path: str) -&gt; List[str]:\n        full_path = self._resolve_path(path)\n        return [\n            str(f.relative_to(self.base_path))\n            for f in full_path.iterdir()\n            if f.is_file()\n        ]\n\n    def walk(self, top: str) -&gt; Generator[Tuple[str, List[str], List[str]], None, None]:\n        full_path = self._resolve_path(top)\n        for root, dirs, files in os.walk(full_path):\n            rel_root = str(Path(root).relative_to(self.base_path))\n            yield rel_root, dirs, files\n\n    def list_directories(self, path: str) -&gt; List[str]:\n        full_path = self._resolve_path(path)\n\n        if not full_path.exists():\n            return []\n\n        if not full_path.is_dir():\n            return []\n\n        return [d.name for d in full_path.iterdir() if d.is_dir()]\n\n    def open(self, path: str, mode: str = \"r\") -&gt; IO:\n        full_path = self._resolve_path(path)\n        self.mkdir(str(full_path.parent), exist_ok=True)\n        return open(full_path, mode)\n\n    def is_file(self, path: str) -&gt; bool:\n        return self._resolve_path(path).is_file()\n\n    def is_dir(self, path: str) -&gt; bool:\n        return self._resolve_path(path).is_dir()\n\n    def remove(self, path: str) -&gt; None:\n        full_path = self._resolve_path(path)\n        if full_path.is_file():\n            os.remove(full_path)\n\n    def copy_file(self, src: str, dst: str) -&gt; None:\n        \"\"\"Copy a file from src to dst.\"\"\"\n        src_path = self._resolve_path(src)\n        dst_path = self._resolve_path(dst)\n        self.mkdir(str(dst_path.parent), exist_ok=True)\n        shutil.copy2(src_path, dst_path)\n\n    def rmdir(self, directory: str) -&gt; None:\n        full_path = self._resolve_path(directory)\n        if full_path.is_dir():\n            os.rmdir(full_path)\n\n    def mkdir(self, path: str, exist_ok: bool = False) -&gt; None:\n        full_path = self._resolve_path(path)\n        full_path.mkdir(parents=True, exist_ok=exist_ok)\n\n    def exists(self, path: str) -&gt; bool:\n        return self._resolve_path(path).exists()\n</code></pre>"},{"location":"api/processing/#gigaspatial.processing.LocalDataStore.copy_file","title":"<code>copy_file(src, dst)</code>","text":"<p>Copy a file from src to dst.</p> Source code in <code>gigaspatial/core/io/local_data_store.py</code> <pre><code>def copy_file(self, src: str, dst: str) -&gt; None:\n    \"\"\"Copy a file from src to dst.\"\"\"\n    src_path = self._resolve_path(src)\n    dst_path = self._resolve_path(dst)\n    self.mkdir(str(dst_path.parent), exist_ok=True)\n    shutil.copy2(src_path, dst_path)\n</code></pre>"},{"location":"api/processing/#gigaspatial.processing.TifProcessor","title":"<code>TifProcessor</code>","text":"<p>A class to handle tif data processing, supporting single-band, RGB, RGBA, and multi-band data. Can merge multiple rasters into one during initialization.</p> Source code in <code>gigaspatial/processing/tif_processor.py</code> <pre><code>@dataclass(config=ConfigDict(arbitrary_types_allowed=True))\nclass TifProcessor:\n    \"\"\"\n    A class to handle tif data processing, supporting single-band, RGB, RGBA, and multi-band data.\n    Can merge multiple rasters into one during initialization.\n    \"\"\"\n\n    dataset_path: Union[Path, str, List[Union[Path, str]]]\n    data_store: Optional[DataStore] = None\n    mode: Literal[\"single\", \"rgb\", \"rgba\", \"multi\"] = \"single\"\n    merge_method: Literal[\"first\", \"last\", \"min\", \"max\", \"mean\"] = \"first\"\n    target_crs: Optional[str] = None  # For reprojection if needed\n    resampling_method: Resampling = Resampling.nearest\n    reprojection_resolution: Optional[Tuple[float, float]] = None\n\n    def __post_init__(self):\n        \"\"\"Validate inputs, merge rasters if needed, and set up logging.\"\"\"\n        self.data_store = self.data_store or LocalDataStore()\n        self.logger = config.get_logger(self.__class__.__name__)\n        self._cache = {}\n        self._temp_dir = tempfile.mkdtemp()\n        self._merged_file_path = None\n        self._reprojected_file_path = None\n        self._clipped_file_path = None\n\n        # Handle multiple dataset paths\n        if isinstance(self.dataset_path, list):\n            if len(self.dataset_path) &gt; 1:\n                self.dataset_paths = [Path(p) for p in self.dataset_path]\n                self._validate_multiple_datasets()\n                self._merge_rasters()\n                self.dataset_path = self._merged_file_path\n        else:\n            self.dataset_paths = [Path(self.dataset_path)]\n            # For absolute paths with LocalDataStore, check file existence directly\n            # to avoid path resolution issues\n            if isinstance(self.data_store, LocalDataStore) and os.path.isabs(\n                str(self.dataset_path)\n            ):\n                if not os.path.exists(str(self.dataset_path)):\n                    raise FileNotFoundError(f\"Dataset not found at {self.dataset_path}\")\n            elif not self.data_store.file_exists(str(self.dataset_path)):\n                raise FileNotFoundError(f\"Dataset not found at {self.dataset_path}\")\n\n            # Reproject single raster during initialization if target_crs is set\n            if self.target_crs:\n                self.logger.info(f\"Reprojecting single raster to {self.target_crs}...\")\n                with self.data_store.open(str(self.dataset_path), \"rb\") as f:\n                    with rasterio.MemoryFile(f.read()) as memfile:\n                        with memfile.open() as src:\n                            self._reprojected_file_path = self._reproject_to_temp_file(\n                                src, self.target_crs\n                            )\n                self.dataset_path = self._reprojected_file_path\n\n        self._load_metadata()\n        self._validate_mode_band_compatibility()\n\n    @contextmanager\n    def open_dataset(self):\n        \"\"\"Context manager for accessing the dataset, handling temporary reprojected files.\"\"\"\n        if self._merged_file_path:\n            with rasterio.open(self._merged_file_path) as src:\n                yield src\n        elif self._reprojected_file_path:\n            with rasterio.open(self._reprojected_file_path) as src:\n                yield src\n        elif self._clipped_file_path:\n            with rasterio.open(self._clipped_file_path) as src:\n                yield src\n        elif isinstance(self.data_store, LocalDataStore):\n            with rasterio.open(str(self.dataset_path)) as src:\n                yield src\n        else:\n            with self.data_store.open(str(self.dataset_path), \"rb\") as f:\n                with rasterio.MemoryFile(f.read()) as memfile:\n                    with memfile.open() as src:\n                        yield src\n\n    def reproject_to(\n        self,\n        target_crs: str,\n        output_path: Optional[Union[str, Path]] = None,\n        resampling_method: Optional[Resampling] = None,\n        resolution: Optional[Tuple[float, float]] = None,\n    ):\n        \"\"\"\n        Reprojects the current raster to a new CRS and optionally saves it.\n\n        Args:\n            target_crs: The CRS to reproject to (e.g., \"EPSG:4326\").\n            output_path: The path to save the reprojected raster. If None,\n                         it is saved to a temporary file.\n            resampling_method: The resampling method to use.\n            resolution: The target resolution (pixel size) in the new CRS.\n        \"\"\"\n        self.logger.info(f\"Reprojecting raster to {target_crs}...\")\n\n        # Use provided or default values\n        resampling_method = resampling_method or self.resampling_method\n        resolution = resolution or self.reprojection_resolution\n\n        with self.open_dataset() as src:\n            if src.crs.to_string() == target_crs:\n                self.logger.info(\n                    \"Raster is already in the target CRS. No reprojection needed.\"\n                )\n                # If output_path is specified, copy the file\n                if output_path:\n                    self.data_store.copy_file(str(self.dataset_path), output_path)\n                return self.dataset_path\n\n            dst_path = output_path or os.path.join(\n                self._temp_dir, f\"reprojected_single_{os.urandom(8).hex()}.tif\"\n            )\n\n            with rasterio.open(\n                dst_path,\n                \"w\",\n                **self._get_reprojection_profile(src, target_crs, resolution),\n            ) as dst:\n                for band_idx in range(1, src.count + 1):\n                    reproject(\n                        source=rasterio.band(src, band_idx),\n                        destination=rasterio.band(dst, band_idx),\n                        src_transform=src.transform,\n                        src_crs=src.crs,\n                        dst_transform=dst.transform,\n                        dst_crs=dst.crs,\n                        resampling=resampling_method,\n                        num_threads=multiprocessing.cpu_count(),\n                    )\n\n            self.logger.info(f\"Reprojection complete. Output saved to {dst_path}\")\n            return Path(dst_path)\n\n    def get_raster_info(self) -&gt; Dict[str, Any]:\n        \"\"\"Get comprehensive raster information.\"\"\"\n        return {\n            \"count\": self.count,\n            \"width\": self.width,\n            \"height\": self.height,\n            \"crs\": self.crs,\n            \"bounds\": self.bounds,\n            \"transform\": self.transform,\n            \"dtypes\": self.dtype,\n            \"nodata\": self.nodata,\n            \"mode\": self.mode,\n            \"is_merged\": self.is_merged,\n            \"source_count\": self.source_count,\n        }\n\n    def _reproject_to_temp_file(\n        self, src: rasterio.DatasetReader, target_crs: str\n    ) -&gt; str:\n        \"\"\"Helper to reproject a raster and save it to a temporary file.\"\"\"\n        dst_path = os.path.join(\n            self._temp_dir, f\"reprojected_temp_{os.urandom(8).hex()}.tif\"\n        )\n        profile = self._get_reprojection_profile(\n            src, target_crs, self.reprojection_resolution\n        )\n\n        with rasterio.open(dst_path, \"w\", **profile) as dst:\n            for band_idx in range(1, src.count + 1):\n                reproject(\n                    source=rasterio.band(src, band_idx),\n                    destination=rasterio.band(dst, band_idx),\n                    src_transform=src.transform,\n                    src_crs=src.crs,\n                    dst_transform=dst.transform,\n                    dst_crs=dst.crs,\n                    resampling=self.resampling_method,\n                )\n        return dst_path\n\n    def _validate_multiple_datasets(self):\n        \"\"\"Validate that all datasets exist and have compatible properties.\"\"\"\n        if len(self.dataset_paths) &lt; 2:\n            raise ValueError(\"Multiple dataset paths required for merging\")\n\n        with self.data_store.open(str(self.dataset_paths[0]), \"rb\") as f:\n            with rasterio.MemoryFile(f.read()) as memfile:\n                with memfile.open() as ref_src:\n                    ref_count = ref_src.count\n                    ref_dtype = ref_src.dtypes[0]\n                    ref_crs = ref_src.crs\n                    ref_transform = ref_src.transform\n                    ref_nodata = ref_src.nodata\n\n        for i, path in enumerate(self.dataset_paths[1:], 1):\n            with self.data_store.open(str(path), \"rb\") as f:\n                with rasterio.MemoryFile(f.read()) as memfile:\n                    with memfile.open() as src:\n                        if src.count != ref_count:\n                            raise ValueError(\n                                f\"Dataset {i} has {src.count} bands, expected {ref_count}\"\n                            )\n                        if src.dtypes[0] != ref_dtype:\n                            raise ValueError(\n                                f\"Dataset {i} has dtype {src.dtypes[0]}, expected {ref_dtype}\"\n                            )\n                        if not self.target_crs and src.crs != ref_crs:\n                            self.logger.warning(\n                                f\"Dataset {i} has CRS {src.crs}, expected {ref_crs}. \"\n                                \"Consider setting target_crs parameter for reprojection before merging.\"\n                            )\n                        if self.target_crs is None and not self._transforms_compatible(\n                            src.transform, ref_transform\n                        ):\n                            self.logger.warning(\n                                f\"Dataset {i} has different resolution. Resampling may be needed.\"\n                            )\n                        if src.nodata != ref_nodata:\n                            self.logger.warning(\n                                f\"Dataset {i} has different nodata value: {src.nodata} vs {ref_nodata}\"\n                            )\n\n    def _get_reprojection_profile(\n        self,\n        src: rasterio.DatasetReader,\n        target_crs: str,\n        resolution: Optional[Tuple[float, float]],\n        compression: str = \"lzw\",\n    ):\n        \"\"\"Calculates and returns the profile for a reprojected raster.\"\"\"\n        if resolution:\n            src_res = (abs(src.transform.a), abs(src.transform.e))\n            self.logger.info(\n                f\"Using target resolution: {resolution}. Source resolution: {src_res}.\"\n            )\n            # Calculate transform and dimensions based on the new resolution\n            dst_transform, width, height = calculate_default_transform(\n                src.crs,\n                target_crs,\n                src.width,\n                src.height,\n                *src.bounds,\n                resolution=resolution,\n            )\n        else:\n            # Keep original resolution but reproject\n            dst_transform, width, height = calculate_default_transform(\n                src.crs, target_crs, src.width, src.height, *src.bounds\n            )\n\n        profile = src.profile.copy()\n        profile.update(\n            {\n                \"crs\": target_crs,\n                \"transform\": dst_transform,\n                \"width\": width,\n                \"height\": height,\n                \"compress\": compression,  # Add compression to save space\n            }\n        )\n        return profile\n\n    def _transforms_compatible(self, transform1, transform2, tolerance=1e-6):\n        \"\"\"Check if two transforms have compatible pixel sizes.\"\"\"\n        return (\n            abs(transform1.a - transform2.a) &lt; tolerance\n            and abs(transform1.e - transform2.e) &lt; tolerance\n        )\n\n    def _merge_rasters(self):\n        \"\"\"Merge multiple rasters into a single raster.\"\"\"\n        self.logger.info(f\"Merging {len(self.dataset_paths)} rasters...\")\n\n        # Open all datasets and handle reprojection if needed\n        datasets_to_merge = []\n        temp_reprojected_files = []\n        try:\n            for path in self.dataset_paths:\n                with self.data_store.open(str(path), \"rb\") as f:\n                    with rasterio.MemoryFile(f.read()) as memfile:\n                        with memfile.open() as src:\n                            if self.target_crs and src.crs != self.target_crs:\n                                self.logger.info(\n                                    f\"Reprojecting {path.name} to {self.target_crs} before merging.\"\n                                )\n                                reprojected_path = self._reproject_to_temp_file(\n                                    src, self.target_crs\n                                )\n                                temp_reprojected_files.append(reprojected_path)\n                                datasets_to_merge.append(\n                                    rasterio.open(reprojected_path)\n                                )\n                            else:\n                                temp_path = os.path.join(\n                                    self._temp_dir,\n                                    f\"temp_{path.stem}_{os.urandom(4).hex()}.tif\",\n                                )\n                                temp_reprojected_files.append(temp_path)\n\n                                profile = src.profile\n                                with rasterio.open(temp_path, \"w\", **profile) as dst:\n                                    dst.write(src.read())\n                                datasets_to_merge.append(rasterio.open(temp_path))\n\n            self._merged_file_path = os.path.join(self._temp_dir, \"merged_raster.tif\")\n\n            if self.merge_method == \"mean\":\n                merged_array, merged_transform = self._merge_with_mean(\n                    datasets_to_merge\n                )\n            else:\n                merged_array, merged_transform = merge(\n                    datasets_to_merge,\n                    method=self.merge_method,\n                    resampling=self.resampling_method,\n                )\n\n            # Get profile from the first file in the list (all should be compatible now)\n            ref_src = datasets_to_merge[0]\n            profile = ref_src.profile.copy()\n            profile.update(\n                {\n                    \"height\": merged_array.shape[-2],\n                    \"width\": merged_array.shape[-1],\n                    \"transform\": merged_transform,\n                    \"crs\": self.target_crs if self.target_crs else ref_src.crs,\n                }\n            )\n\n            with rasterio.open(self._merged_file_path, \"w\", **profile) as dst:\n                dst.write(merged_array)\n        finally:\n            for dataset in datasets_to_merge:\n                if hasattr(dataset, \"close\"):\n                    dataset.close()\n\n            # Clean up temporary files immediately\n            for temp_file in temp_reprojected_files:\n                try:\n                    os.remove(temp_file)\n                except OSError:\n                    pass\n\n        self.logger.info(\"Raster merging completed!\")\n\n    def _merge_with_mean(self, src_files):\n        \"\"\"Merge rasters using mean aggregation.\"\"\"\n        # Get bounds and resolution for merged raster\n        bounds = src_files[0].bounds\n        transform = src_files[0].transform\n\n        for src in src_files[1:]:\n            bounds = rasterio.coords.BoundingBox(\n                min(bounds.left, src.bounds.left),\n                min(bounds.bottom, src.bounds.bottom),\n                max(bounds.right, src.bounds.right),\n                max(bounds.top, src.bounds.top),\n            )\n\n        # Calculate dimensions for merged raster\n        width = int((bounds.right - bounds.left) / abs(transform.a))\n        height = int((bounds.top - bounds.bottom) / abs(transform.e))\n\n        # Create new transform for merged bounds\n        merged_transform = rasterio.transform.from_bounds(\n            bounds.left, bounds.bottom, bounds.right, bounds.top, width, height\n        )\n\n        estimated_memory = height * width * src_files[0].count * 8  # float64\n        if estimated_memory &gt; 1e9:  # 1GB threshold\n            self.logger.warning(\n                f\"Large memory usage expected: {estimated_memory/1e9:.1f}GB\"\n            )\n\n        # Initialize arrays for sum and count\n        sum_array = np.zeros((src_files[0].count, height, width), dtype=np.float64)\n        count_array = np.zeros((height, width), dtype=np.int32)\n\n        # Process each source file\n        for src in src_files:\n            # Read data\n            data = src.read()\n\n            # Calculate offset in merged raster\n            src_bounds = src.bounds\n            col_off = int((src_bounds.left - bounds.left) / abs(transform.a))\n            row_off = int((bounds.top - src_bounds.top) / abs(transform.e))\n\n            # Get valid data mask\n            if src.nodata is not None:\n                valid_mask = data[0] != src.nodata\n            else:\n                valid_mask = np.ones(data[0].shape, dtype=bool)\n\n            # Add to sum and count arrays\n            end_row = row_off + data.shape[1]\n            end_col = col_off + data.shape[2]\n\n            sum_array[:, row_off:end_row, col_off:end_col] += np.where(\n                valid_mask, data, 0\n            )\n            count_array[row_off:end_row, col_off:end_col] += valid_mask.astype(np.int32)\n\n        # Calculate mean\n        mean_array = np.divide(\n            sum_array,\n            count_array,\n            out=np.full_like(\n                sum_array, src_files[0].nodata or 0, dtype=sum_array.dtype\n            ),\n            where=count_array &gt; 0,\n        )\n\n        return mean_array.astype(src_files[0].dtypes[0]), merged_transform\n\n    def _load_metadata(self):\n        \"\"\"Load metadata from the TIF file if not already cached\"\"\"\n        try:\n            with self.open_dataset() as src:\n                self._cache[\"transform\"] = src.transform\n                self._cache[\"crs\"] = src.crs.to_string()\n                self._cache[\"bounds\"] = src.bounds\n                self._cache[\"width\"] = src.width\n                self._cache[\"height\"] = src.height\n                self._cache[\"resolution\"] = (abs(src.transform.a), abs(src.transform.e))\n                self._cache[\"x_transform\"] = src.transform.a\n                self._cache[\"y_transform\"] = src.transform.e\n                self._cache[\"nodata\"] = src.nodata\n                self._cache[\"count\"] = src.count\n                self._cache[\"dtype\"] = src.dtypes[0]\n        except (rasterio.errors.RasterioIOError, FileNotFoundError) as e:\n            raise FileNotFoundError(f\"Could not read raster metadata: {e}\")\n        except Exception as e:\n            raise RuntimeError(f\"Unexpected error loading metadata: {e}\")\n\n    @property\n    def is_merged(self) -&gt; bool:\n        \"\"\"Check if this processor was created from multiple rasters.\"\"\"\n        return len(self.dataset_paths) &gt; 1\n\n    @property\n    def source_count(self) -&gt; int:\n        \"\"\"Get the number of source rasters.\"\"\"\n        return len(self.dataset_paths)\n\n    @property\n    def transform(self):\n        \"\"\"Get the transform from the TIF file\"\"\"\n        return self._cache[\"transform\"]\n\n    @property\n    def crs(self):\n        \"\"\"Get the coordinate reference system from the TIF file\"\"\"\n        return self._cache[\"crs\"]\n\n    @property\n    def bounds(self):\n        \"\"\"Get the bounds of the TIF file\"\"\"\n        return self._cache[\"bounds\"]\n\n    @property\n    def resolution(self) -&gt; Tuple[float, float]:\n        \"\"\"Get the x and y resolution (pixel width and height or pixel size) from the TIF file\"\"\"\n        return self._cache[\"resolution\"]\n\n    @property\n    def x_transform(self) -&gt; float:\n        \"\"\"Get the x transform from the TIF file\"\"\"\n        return self._cache[\"x_transform\"]\n\n    @property\n    def y_transform(self) -&gt; float:\n        \"\"\"Get the y transform from the TIF file\"\"\"\n        return self._cache[\"y_transform\"]\n\n    @property\n    def count(self) -&gt; int:\n        \"\"\"Get the band count from the TIF file\"\"\"\n        return self._cache[\"count\"]\n\n    @property\n    def nodata(self) -&gt; int:\n        \"\"\"Get the value representing no data in the rasters\"\"\"\n        return self._cache[\"nodata\"]\n\n    @property\n    def dtype(self):\n        \"\"\"Get the data types from the TIF file\"\"\"\n        return self._cache.get(\"dtype\", [])\n\n    @property\n    def width(self):\n        return self._cache[\"width\"]\n\n    @property\n    def height(self):\n        return self._cache[\"height\"]\n\n    def to_dataframe(\n        self, drop_nodata=True, check_memory=True, **kwargs\n    ) -&gt; pd.DataFrame:\n        \"\"\"\n        Convert raster to DataFrame.\n\n        Args:\n            drop_nodata: Whether to drop nodata values\n            check_memory: Whether to check memory before operation (default True)\n            **kwargs: Additional arguments\n\n        Returns:\n            pd.DataFrame with raster data\n        \"\"\"\n        # Memory guard check\n        if check_memory:\n            self._memory_guard(\"conversion\", threshold_percent=80.0)\n\n        try:\n            if self.mode == \"single\":\n                return self._to_dataframe(\n                    band_number=kwargs.get(\"band_number\", 1),\n                    drop_nodata=drop_nodata,\n                    band_names=kwargs.get(\"band_names\", None),\n                )\n            else:\n                return self._to_dataframe(\n                    band_number=None,  # All bands\n                    drop_nodata=drop_nodata,\n                    band_names=kwargs.get(\"band_names\", None),\n                )\n        except Exception as e:\n            raise ValueError(\n                f\"Failed to process TIF file in mode '{self.mode}'. \"\n                f\"Please ensure the file is valid and matches the selected mode. \"\n                f\"Original error: {str(e)}\"\n            )\n\n        return df\n\n    def to_geodataframe(self, check_memory=True, **kwargs) -&gt; gpd.GeoDataFrame:\n        \"\"\"\n        Convert the processed TIF data into a GeoDataFrame, where each row represents a pixel zone.\n        Each zone is defined by its bounding box, based on pixel resolution and coordinates.\n\n        Args:\n            check_memory: Whether to check memory before operation\n            **kwargs: Additional arguments passed to to_dataframe()\n\n        Returns:\n            gpd.GeoDataFrame with raster data\n        \"\"\"\n        # Memory guard check\n        if check_memory:\n            self._memory_guard(\"conversion\", threshold_percent=80.0)\n\n        df = self.to_dataframe(check_memory=False, **kwargs)\n\n        x_res, y_res = self.resolution\n\n        # create bounding box for each pixel\n        geometries = [\n            box(lon - x_res / 2, lat - y_res / 2, lon + x_res / 2, lat + y_res / 2)\n            for lon, lat in zip(df[\"lon\"], df[\"lat\"])\n        ]\n\n        gdf = gpd.GeoDataFrame(df, geometry=geometries, crs=self.crs)\n\n        return gdf\n\n    def to_dataframe_chunked(\n        self, drop_nodata=True, chunk_size=None, target_memory_mb=500, **kwargs\n    ):\n        \"\"\"\n        Convert raster to DataFrame using chunked processing for memory efficiency.\n\n        Automatically routes to the appropriate chunked method based on mode.\n        Chunk size is automatically calculated based on target memory usage.\n\n        Args:\n            drop_nodata: Whether to drop nodata values\n            chunk_size: Number of rows per chunk (auto-calculated if None)\n            target_memory_mb: Target memory per chunk in MB (default 500)\n            **kwargs: Additional arguments (band_number, band_names, etc.)\n        \"\"\"\n\n        if chunk_size is None:\n            chunk_size = self._calculate_optimal_chunk_size(\n                \"conversion\", target_memory_mb\n            )\n\n        windows = self._get_chunk_windows(chunk_size)\n\n        # SIMPLE ROUTING\n        if self.mode == \"single\":\n            return self._to_dataframe_chunked(\n                windows,\n                band_number=kwargs.get(\"band_number\", 1),\n                drop_nodata=drop_nodata,\n                band_names=kwargs.get(\"band_names\", None),\n            )\n        else:  # rgb, rgba, multi\n            return self._to_dataframe_chunked(\n                windows,\n                band_number=None,\n                drop_nodata=drop_nodata,\n                band_names=kwargs.get(\"band_names\", None),\n            )\n\n    def clip_to_geometry(\n        self,\n        geometry: Union[\n            Polygon, MultiPolygon, gpd.GeoDataFrame, gpd.GeoSeries, List[dict], dict\n        ],\n        crop: bool = True,\n        all_touched: bool = True,\n        invert: bool = False,\n        nodata: Optional[Union[int, float]] = None,\n        pad: bool = False,\n        pad_width: float = 0.5,\n        return_clipped_processor: bool = True,\n    ) -&gt; Union[\"TifProcessor\", tuple]:\n        \"\"\"\n        Clip raster to geometry boundaries.\n\n        Parameters:\n        -----------\n        geometry : various\n            Geometry to clip to. Can be:\n            - Shapely Polygon or MultiPolygon\n            - GeoDataFrame or GeoSeries\n            - List of GeoJSON-like dicts\n            - Single GeoJSON-like dict\n        crop : bool, default True\n            Whether to crop the raster to the extent of the geometry\n        all_touched : bool, default True\n            Include pixels that touch the geometry boundary\n        invert : bool, default False\n            If True, mask pixels inside geometry instead of outside\n        nodata : int or float, optional\n            Value to use for masked pixels. If None, uses raster's nodata value\n        pad : bool, default False\n            Pad geometry by half pixel before clipping\n        pad_width : float, default 0.5\n            Width of padding in pixels if pad=True\n        return_clipped_processor : bool, default True\n            If True, returns new TifProcessor with clipped data\n            If False, returns (clipped_array, transform, metadata)\n\n        Returns:\n        --------\n        TifProcessor or tuple\n            Either new TifProcessor instance or (array, transform, metadata) tuple\n        \"\"\"\n        # Handle different geometry input types\n        shapes = self._prepare_geometry_for_clipping(geometry)\n\n        # Validate CRS compatibility\n        self._validate_geometry_crs(geometry)\n\n        # Perform the clipping\n        with self.open_dataset() as src:\n            try:\n                clipped_data, clipped_transform = mask(\n                    dataset=src,\n                    shapes=shapes,\n                    crop=crop,\n                    all_touched=all_touched,\n                    invert=invert,\n                    nodata=nodata,\n                    pad=pad,\n                    pad_width=pad_width,\n                    filled=True,\n                )\n\n                # Update metadata for the clipped raster\n                clipped_meta = src.meta.copy()\n                clipped_meta.update(\n                    {\n                        \"height\": clipped_data.shape[1],\n                        \"width\": clipped_data.shape[2],\n                        \"transform\": clipped_transform,\n                        \"nodata\": nodata if nodata is not None else src.nodata,\n                    }\n                )\n\n            except ValueError as e:\n                if \"Input shapes do not overlap raster\" in str(e):\n                    raise ValueError(\n                        \"The geometry does not overlap with the raster. \"\n                        \"Check that both are in the same coordinate reference system.\"\n                    ) from e\n                else:\n                    raise e\n\n        if return_clipped_processor:\n            # Create a new TifProcessor with the clipped data\n            return self._create_clipped_processor(clipped_data, clipped_meta)\n        else:\n            return clipped_data, clipped_transform, clipped_meta\n\n    def clip_to_bounds(\n        self,\n        bounds: tuple,\n        bounds_crs: Optional[str] = None,\n        return_clipped_processor: bool = True,\n    ) -&gt; Union[\"TifProcessor\", tuple]:\n        \"\"\"\n        Clip raster to rectangular bounds.\n\n        Parameters:\n        -----------\n        bounds : tuple\n            Bounding box as (minx, miny, maxx, maxy)\n        bounds_crs : str, optional\n            CRS of the bounds. If None, assumes same as raster CRS\n        return_clipped_processor : bool, default True\n            If True, returns new TifProcessor, else returns (array, transform, metadata)\n\n        Returns:\n        --------\n        TifProcessor or tuple\n            Either new TifProcessor instance or (array, transform, metadata) tuple\n        \"\"\"\n        # Create bounding box geometry\n        bbox_geom = box(*bounds)\n\n        # If bounds_crs is specified and different from raster CRS, create GeoDataFrame for reprojection\n        if bounds_crs is not None:\n            raster_crs = self.crs\n\n            if not self.crs == bounds_crs:\n                # Create GeoDataFrame with bounds CRS and reproject\n                bbox_gdf = gpd.GeoDataFrame([1], geometry=[bbox_geom], crs=bounds_crs)\n                bbox_gdf = bbox_gdf.to_crs(raster_crs)\n                bbox_geom = bbox_gdf.geometry.iloc[0]\n\n        return self.clip_to_geometry(\n            geometry=bbox_geom,\n            crop=True,\n            return_clipped_processor=return_clipped_processor,\n        )\n\n    def to_graph(\n        self,\n        connectivity: Literal[4, 8] = 4,\n        band: Optional[int] = None,\n        include_coordinates: bool = False,\n        graph_type: Literal[\"networkx\", \"sparse\"] = \"networkx\",\n        check_memory: bool = True,\n    ) -&gt; Union[nx.Graph, sp.csr_matrix]:\n        \"\"\"\n        Convert raster to graph based on pixel adjacency.\n\n        Args:\n            connectivity: 4 or 8-connectivity\n            band: Band number (1-indexed)\n            include_coordinates: Include x,y coordinates in nodes\n            graph_type: 'networkx' or 'sparse'\n            check_memory: Whether to check memory before operation\n\n        Returns:\n            Graph representation of raster\n        \"\"\"\n\n        # Memory guard check\n        if check_memory:\n            self._memory_guard(\"graph\", threshold_percent=80.0)\n\n        with self.open_dataset() as src:\n            band_idx = band - 1 if band is not None else 0\n            if band_idx &lt; 0 or band_idx &gt;= src.count:\n                raise ValueError(\n                    f\"Band {band} not available. Raster has {src.count} bands\"\n                )\n\n            data = src.read(band_idx + 1)\n            nodata = src.nodata if src.nodata is not None else self.nodata\n            valid_mask = (\n                data != nodata if nodata is not None else np.ones_like(data, dtype=bool)\n            )\n\n            height, width = data.shape\n\n            # Find all valid pixels\n            valid_rows, valid_cols = np.where(valid_mask)\n            num_valid_pixels = len(valid_rows)\n\n            # Create a sequential mapping from (row, col) to a node ID\n            node_map = np.full(data.shape, -1, dtype=int)\n            node_map[valid_rows, valid_cols] = np.arange(num_valid_pixels)\n\n            # Define neighborhood offsets\n            if connectivity == 4:\n                # von Neumann neighborhood (4-connectivity)\n                offsets = [(-1, 0), (1, 0), (0, -1), (0, 1)]\n            else:  # connectivity == 8\n                # Moore neighborhood (8-connectivity)\n                offsets = [\n                    (-1, -1),\n                    (-1, 0),\n                    (-1, 1),\n                    (0, -1),\n                    (0, 1),\n                    (1, -1),\n                    (1, 0),\n                    (1, 1),\n                ]\n\n            # Collect nodes and edges\n            nodes_to_add = []\n            edges_to_add = []\n\n            for i in range(num_valid_pixels):\n                row, col = valid_rows[i], valid_cols[i]\n                current_node_id = node_map[row, col]\n\n                # Prepare node attributes\n                node_attrs = {\"value\": float(data[row, col])}\n                if include_coordinates:\n                    x, y = src.xy(row, col)\n                    node_attrs[\"x\"] = x\n                    node_attrs[\"y\"] = y\n                nodes_to_add.append((current_node_id, node_attrs))\n\n                # Find neighbors and collect edges\n                for dy, dx in offsets:\n                    neighbor_row, neighbor_col = row + dy, col + dx\n\n                    # Check if neighbor is within bounds and is a valid pixel\n                    if (\n                        0 &lt;= neighbor_row &lt; height\n                        and 0 &lt;= neighbor_col &lt; width\n                        and valid_mask[neighbor_row, neighbor_col]\n                    ):\n                        neighbor_node_id = node_map[neighbor_row, neighbor_col]\n\n                        # Ensure each edge is added only once\n                        if current_node_id &lt; neighbor_node_id:\n                            neighbor_value = float(data[neighbor_row, neighbor_col])\n                            edges_to_add.append(\n                                (current_node_id, neighbor_node_id, neighbor_value)\n                            )\n\n            if graph_type == \"networkx\":\n                G = nx.Graph()\n                G.add_nodes_from(nodes_to_add)\n                G.add_weighted_edges_from(edges_to_add)\n                return G\n            else:  # sparse matrix\n                edges_array = np.array(edges_to_add)\n                row_indices = edges_array[:, 0]\n                col_indices = edges_array[:, 1]\n                weights = edges_array[:, 2]\n\n                # Add reverse edges for symmetric matrix\n                from_idx = np.append(row_indices, col_indices)\n                to_idx = np.append(col_indices, row_indices)\n                weights = np.append(weights, weights)\n\n                return sp.coo_matrix(\n                    (weights, (from_idx, to_idx)),\n                    shape=(num_valid_pixels, num_valid_pixels),\n                ).tocsr()\n\n    def sample_by_coordinates(\n        self, coordinate_list: List[Tuple[float, float]], **kwargs\n    ) -&gt; Union[np.ndarray, dict]:\n        self.logger.info(\"Sampling raster values at the coordinates...\")\n\n        with self.open_dataset() as src:\n            if self.mode == \"rgba\":\n                if self.count != 4:\n                    raise ValueError(\"RGBA mode requires a 4-band TIF file\")\n\n                rgba_values = {\"red\": [], \"green\": [], \"blue\": [], \"alpha\": []}\n\n                for band_idx, color in enumerate([\"red\", \"green\", \"blue\", \"alpha\"], 1):\n                    rgba_values[color] = [\n                        vals[0]\n                        for vals in src.sample(coordinate_list, indexes=band_idx)\n                    ]\n\n                return rgba_values\n\n            elif self.mode == \"rgb\":\n                if self.count != 3:\n                    raise ValueError(\"RGB mode requires a 3-band TIF file\")\n\n                rgb_values = {\"red\": [], \"green\": [], \"blue\": []}\n\n                for band_idx, color in enumerate([\"red\", \"green\", \"blue\"], 1):\n                    rgb_values[color] = [\n                        vals[0]\n                        for vals in src.sample(coordinate_list, indexes=band_idx)\n                    ]\n\n                return rgb_values\n            elif self.count &gt; 1:\n                return np.array(\n                    [vals for vals in src.sample(coordinate_list, **kwargs)]\n                )\n            else:\n                return np.array([vals[0] for vals in src.sample(coordinate_list)])\n\n    def sample_by_polygons(\n        self,\n        polygon_list,\n        stat: Union[str, Callable, List[Union[str, Callable]]] = \"mean\",\n    ):\n        \"\"\"\n        Sample raster values by polygons and compute statistic(s) for each polygon.\n\n        Args:\n            polygon_list: List of shapely Polygon or MultiPolygon objects.\n            stat: Statistic(s) to compute. Can be:\n                - Single string: 'mean', 'median', 'sum', 'min', 'max', 'std', 'count'\n                - Single callable: custom function that takes array and returns scalar\n                - List of strings/callables: multiple statistics to compute\n\n        Returns:\n            If single stat: np.ndarray of computed statistics for each polygon\n            If multiple stats: List of dictionaries with stat names as keys\n        \"\"\"\n        # Determine if single or multiple stats\n        single_stat = not isinstance(stat, list)\n        stats_list = [stat] if single_stat else stat\n\n        # Prepare stat functions\n        stat_funcs = []\n        stat_names = []\n\n        for s in stats_list:\n            if callable(s):\n                stat_funcs.append(s)\n                stat_names.append(\n                    s.__name__\n                    if hasattr(s, \"__name__\")\n                    else f\"custom_{len(stat_names)}\"\n                )\n            else:\n                # Handle string statistics\n                if s == \"count\":\n                    stat_funcs.append(len)\n                else:\n                    stat_funcs.append(getattr(np, s))\n                stat_names.append(s)\n\n        results = []\n\n        with self.open_dataset() as src:\n            for polygon in tqdm(polygon_list):\n                try:\n                    out_image, _ = mask(src, [polygon], crop=True, filled=False)\n\n                    # Use masked arrays for more efficient nodata handling\n                    if hasattr(out_image, \"mask\"):\n                        valid_data = out_image.compressed()\n                    else:\n                        valid_data = (\n                            out_image[out_image != self.nodata]\n                            if self.nodata\n                            else out_image.flatten()\n                        )\n\n                    if len(valid_data) == 0:\n                        if single_stat:\n                            results.append(np.nan)\n                        else:\n                            results.append({name: np.nan for name in stat_names})\n                    else:\n                        if single_stat:\n                            results.append(stat_funcs[0](valid_data))\n                        else:\n                            # Compute all statistics for this polygon\n                            polygon_stats = {}\n                            for func, name in zip(stat_funcs, stat_names):\n                                try:\n                                    polygon_stats[name] = func(valid_data)\n                                except Exception:\n                                    polygon_stats[name] = np.nan\n                            results.append(polygon_stats)\n\n                except Exception:\n                    if single_stat:\n                        results.append(np.nan)\n                    else:\n                        results.append({name: np.nan for name in stat_names})\n\n        return np.array(results) if single_stat else results\n\n    def sample_by_polygons_batched(\n        self,\n        polygon_list: List[Union[Polygon, MultiPolygon]],\n        stat: Union[str, Callable] = \"mean\",\n        batch_size: int = 100,\n        n_workers: int = 4,\n        show_progress: bool = True,\n        check_memory: bool = True,\n        **kwargs,\n    ) -&gt; np.ndarray:\n        \"\"\"\n        Sample raster values by polygons in parallel using batching.\n\n        Args:\n            polygon_list: List of Shapely Polygon or MultiPolygon objects\n            stat: Statistic to compute\n            batch_size: Number of polygons per batch\n            n_workers: Number of worker processes\n            show_progress: Whether to display progress bar\n            check_memory: Whether to check memory before operation\n            **kwargs: Additional arguments\n\n        Returns:\n            np.ndarray of statistics for each polygon\n        \"\"\"\n        import sys\n\n        # Memory guard check with n_workers consideration\n        if check_memory:\n            is_safe = self._memory_guard(\n                \"batched_sampling\",\n                threshold_percent=85.0,\n                n_workers=n_workers,\n                raise_error=False,\n            )\n\n            if not is_safe:\n                # Suggest reducing n_workers\n                memory_info = self._check_available_memory()\n                estimates = self._estimate_memory_usage(\"batched_sampling\", n_workers=1)\n\n                # Calculate optimal workers\n                suggested_workers = max(\n                    1, int(memory_info[\"available\"] * 0.7 / estimates[\"per_worker\"])\n                )\n\n                warnings.warn(\n                    f\"Consider reducing n_workers from {n_workers} to {suggested_workers} \"\n                    f\"to reduce memory pressure.\",\n                    ResourceWarning,\n                )\n\n        # Platform check\n        if sys.platform in [\"win32\", \"darwin\"]:\n            import warnings\n            import multiprocessing as mp\n\n            if mp.get_start_method(allow_none=True) != \"fork\":\n                warnings.warn(\n                    \"Batched sampling may not work on Windows/macOS. \"\n                    \"Use sample_by_polygons() if you encounter errors.\",\n                    RuntimeWarning,\n                )\n\n        def _chunk_list(data_list, chunk_size):\n            \"\"\"Yield successive chunks from data_list.\"\"\"\n            for i in range(0, len(data_list), chunk_size):\n                yield data_list[i : i + chunk_size]\n\n        if len(polygon_list) == 0:\n            return np.array([])\n\n        stat_func = stat if callable(stat) else getattr(np, stat)\n        polygon_chunks = list(_chunk_list(polygon_list, batch_size))\n\n        with multiprocessing.Pool(\n            initializer=self._initializer_worker, processes=n_workers\n        ) as pool:\n            process_func = partial(self._process_polygon_batch, stat_func=stat_func)\n            if show_progress:\n                batched_results = list(\n                    tqdm(\n                        pool.imap(process_func, polygon_chunks),\n                        total=len(polygon_chunks),\n                        desc=f\"Sampling polygons\",\n                    )\n                )\n            else:\n                batched_results = list(pool.imap(process_func, polygon_chunks))\n\n            results = [item for sublist in batched_results for item in sublist]\n\n        return np.array(results)\n\n    def _initializer_worker(self):\n        \"\"\"\n        Initializer function for each worker process.\n        Opens the raster dataset and stores it in a process-local variable.\n        This function runs once per worker, not for every task.\n        \"\"\"\n        global src_handle, memfile_handle\n\n        # Priority: merged &gt; reprojected &gt; original (same as open_dataset)\n        local_file_path = None\n        if self._merged_file_path:\n            # Merged file is a local temp file\n            local_file_path = self._merged_file_path\n        elif self._reprojected_file_path:\n            # Reprojected file is a local temp file\n            local_file_path = self._reprojected_file_path\n        elif isinstance(self.data_store, LocalDataStore):\n            # Local file - can open directly\n            local_file_path = str(self.dataset_path)\n\n        if local_file_path:\n            # Open local file directly\n            with open(local_file_path, \"rb\") as f:\n                memfile_handle = rasterio.MemoryFile(f.read())\n                src_handle = memfile_handle.open()\n        else:\n            # Custom DataStore\n            with self.data_store.open(str(self.dataset_path), \"rb\") as f:\n                memfile_handle = rasterio.MemoryFile(f.read())\n                src_handle = memfile_handle.open()\n\n    def _get_worker_dataset(self):\n        \"\"\"Get dataset handle for worker process.\"\"\"\n        global src_handle\n        if src_handle is None:\n            raise RuntimeError(\"Raster dataset not initialized in this process.\")\n        return src_handle\n\n    def _process_single_polygon(self, polygon, stat_func):\n        \"\"\"\n        Helper function to process a single polygon.\n        This will be run in a separate process.\n        \"\"\"\n        try:\n            src = self._get_worker_dataset()\n            out_image, _ = mask(src, [polygon], crop=True, filled=False)\n\n            if hasattr(out_image, \"mask\"):\n                valid_data = out_image.compressed()\n            else:\n                valid_data = (\n                    out_image[out_image != self.nodata]\n                    if self.nodata\n                    else out_image.flatten()\n                )\n\n            return stat_func(valid_data) if len(valid_data) &gt; 0 else np.nan\n        except RuntimeError as e:\n            self.logger.error(f\"Worker not initialized: {e}\")\n            return np.nan\n        except Exception as e:\n            self.logger.debug(f\"Error processing polygon: {e}\")\n            return np.nan\n\n    def _process_polygon_batch(self, polygon_batch, stat_func):\n        \"\"\"\n        Processes a batch of polygons.\n        \"\"\"\n        return [\n            self._process_single_polygon(polygon, stat_func)\n            for polygon in polygon_batch\n        ]\n\n    def _to_dataframe(\n        self,\n        band_number: Optional[int] = None,\n        drop_nodata: bool = True,\n        band_names: Optional[Union[str, List[str]]] = None,\n    ) -&gt; pd.DataFrame:\n        \"\"\"\n        Process TIF to DataFrame - handles both single-band and multi-band.\n\n        Args:\n            band_number: Specific band to read (1-indexed). If None, reads all bands.\n            drop_no Whether to drop nodata values\n            band_names: Custom names for bands (multi-band only)\n\n        Returns:\n            pd.DataFrame with lon, lat, and band value(s)\n        \"\"\"\n        with self.open_dataset() as src:\n            if band_number is not None:\n                # SINGLE BAND MODE\n                band = src.read(band_number)\n                mask = self._build_data_mask(band, drop_nodata, src.nodata)\n                lons, lats = self._extract_coordinates_with_mask(mask)\n                pixel_values = (\n                    np.extract(mask, band) if mask is not None else band.flatten()\n                )\n                band_name = band_names if isinstance(band_names, str) else \"pixel_value\"\n\n                return pd.DataFrame({\"lon\": lons, \"lat\": lats, band_name: pixel_values})\n            else:\n                # MULTI-BAND MODE (all bands)\n                stack = src.read()\n\n                # Auto-detect band names by mode\n                if band_names is None:\n                    if self.mode == \"rgb\":\n                        band_names = [\"red\", \"green\", \"blue\"]\n                    elif self.mode == \"rgba\":\n                        band_names = [\"red\", \"green\", \"blue\", \"alpha\"]\n                    else:\n                        band_names = [\n                            src.descriptions[i] or f\"band_{i+1}\"\n                            for i in range(self.count)\n                        ]\n\n                # Build mask (checks ALL bands!)\n                mask = self._build_multi_band_mask(stack, drop_nodata, src.nodata)\n\n                # Create DataFrame\n                data_dict = self._bands_to_dict(stack, self.count, band_names, mask)\n                df = pd.DataFrame(data_dict)\n\n                # RGBA: normalize alpha if needed\n                if (\n                    self.mode == \"rgba\"\n                    and \"alpha\" in df.columns\n                    and df[\"alpha\"].max() &gt; 1\n                ):\n                    df[\"alpha\"] = df[\"alpha\"] / 255.0\n\n            return df\n\n    def _to_dataframe_chunked(\n        self,\n        windows: List[rasterio.windows.Window],\n        band_number: Optional[int] = None,\n        drop_nodata: bool = True,\n        band_names: Optional[Union[str, List[str]]] = None,\n        show_progress: bool = True,\n    ) -&gt; pd.DataFrame:\n        \"\"\"Universal chunked converter for ALL modes.\"\"\"\n\n        chunks = []\n        iterator = tqdm(windows, desc=\"Processing chunks\") if show_progress else windows\n\n        with self.open_dataset() as src:\n            # Auto-detect band names ONCE (before loop)\n            if band_number is None and band_names is None:\n                if self.mode == \"rgb\":\n                    band_names = [\"red\", \"green\", \"blue\"]\n                elif self.mode == \"rgba\":\n                    band_names = [\"red\", \"green\", \"blue\", \"alpha\"]\n                else:  # multi\n                    band_names = [\n                        src.descriptions[i] or f\"band_{i+1}\" for i in range(self.count)\n                    ]\n\n            for window in iterator:\n                if band_number is not None:\n                    # SINGLE BAND\n                    band_chunk = src.read(band_number, window=window)\n                    mask = self._build_data_mask(band_chunk, drop_nodata, src.nodata)\n                    lons, lats = self._get_chunk_coordinates(window, src)\n                    band_name = (\n                        band_names if isinstance(band_names, str) else \"pixel_value\"\n                    )\n\n                    # Build chunk DataFrame (could use helper but simple enough)\n                    if mask is not None:\n                        mask_flat = mask.flatten()\n                        chunk_df = pd.DataFrame(\n                            {\n                                \"lon\": lons[mask_flat],\n                                \"lat\": lats[mask_flat],\n                                band_name: band_chunk.flatten()[mask_flat],\n                            }\n                        )\n                    else:\n                        chunk_df = pd.DataFrame(\n                            {\"lon\": lons, \"lat\": lats, band_name: band_chunk.flatten()}\n                        )\n                else:\n                    # MULTI-BAND (includes RGB/RGBA)\n                    stack_chunk = src.read(window=window)\n                    mask = self._build_multi_band_mask(\n                        stack_chunk, drop_nodata, src.nodata\n                    )\n                    lons, lats = self._get_chunk_coordinates(window, src)\n\n                    # Build DataFrame using helper\n                    band_dict = {\n                        band_names[i]: stack_chunk[i] for i in range(self.count)\n                    }\n                    chunk_df = self._build_chunk_dataframe(lons, lats, band_dict, mask)\n\n                    # RGBA: normalize alpha\n                    if self.mode == \"rgba\" and \"alpha\" in chunk_df.columns:\n                        if chunk_df[\"alpha\"].max() &gt; 1:\n                            chunk_df[\"alpha\"] = chunk_df[\"alpha\"] / 255.0\n\n                chunks.append(chunk_df)\n\n        result = pd.concat(chunks, ignore_index=True)\n        return result\n\n    def _prepare_geometry_for_clipping(\n        self,\n        geometry: Union[\n            Polygon,\n            MultiPolygon,\n            MultiPoint,\n            gpd.GeoDataFrame,\n            gpd.GeoSeries,\n            List[dict],\n            dict,\n        ],\n    ) -&gt; List[dict]:\n        \"\"\"Convert various geometry formats to list of GeoJSON-like dicts for rasterio.mask\"\"\"\n\n        if isinstance(geometry, MultiPoint):\n            # Use bounding box of MultiPoint\n            minx, miny, maxx, maxy = geometry.bounds\n            bbox = box(minx, miny, maxx, maxy)\n            return [bbox.__geo_interface__]\n\n        if isinstance(geometry, (Polygon, MultiPolygon)):\n            # Shapely geometry\n            return [geometry.__geo_interface__]\n\n        elif isinstance(geometry, gpd.GeoDataFrame):\n            # GeoDataFrame - use all geometries\n            return [\n                geom.__geo_interface__ for geom in geometry.geometry if geom is not None\n            ]\n\n        elif isinstance(geometry, gpd.GeoSeries):\n            # GeoSeries\n            return [geom.__geo_interface__ for geom in geometry if geom is not None]\n\n        elif isinstance(geometry, dict):\n            # Single GeoJSON-like dict\n            return [geometry]\n\n        elif isinstance(geometry, list):\n            # List of GeoJSON-like dicts\n            return geometry\n\n        else:\n            raise TypeError(\n                f\"Unsupported geometry type: {type(geometry)}. \"\n                \"Supported types: Shapely geometries, GeoDataFrame, GeoSeries, \"\n                \"GeoJSON-like dict, or list of GeoJSON-like dicts.\"\n            )\n\n    def _validate_geometry_crs(\n        self,\n        original_geometry: Any,\n    ) -&gt; None:\n        \"\"\"Validate that geometry CRS matches raster CRS\"\"\"\n\n        # Get raster CRS\n        raster_crs = self.crs\n\n        # Try to get geometry CRS\n        geometry_crs = None\n\n        if isinstance(original_geometry, (gpd.GeoDataFrame, gpd.GeoSeries)):\n            geometry_crs = original_geometry.crs\n        elif hasattr(original_geometry, \"crs\"):\n            geometry_crs = original_geometry.crs\n\n        # Warn if CRS mismatch detected\n        if geometry_crs is not None and raster_crs is not None:\n            if not raster_crs == geometry_crs:\n                self.logger.warning(\n                    f\"CRS mismatch detected! Raster CRS: {raster_crs}, \"\n                    f\"Geometry CRS: {geometry_crs}. \"\n                    \"Consider reprojecting geometry to match raster CRS for accurate clipping.\"\n                )\n\n    def _create_clipped_processor(\n        self, clipped_data: np.ndarray, clipped_meta: dict\n    ) -&gt; \"TifProcessor\":\n        \"\"\"\n        Helper to create a new TifProcessor instance from clipped data.\n        Saves the clipped data to a temporary file and initializes a new TifProcessor.\n        \"\"\"\n        # Create a temporary placeholder file to initialize the processor\n        # This allows us to get the processor's temp_dir\n        placeholder_dir = tempfile.mkdtemp()\n        placeholder_path = os.path.join(\n            placeholder_dir, f\"placeholder_{os.urandom(8).hex()}.tif\"\n        )\n\n        # Create a minimal valid TIF file as placeholder\n        placeholder_transform = rasterio.transform.from_bounds(0, 0, 1, 1, 1, 1)\n        with rasterio.open(\n            placeholder_path,\n            \"w\",\n            driver=\"GTiff\",\n            width=1,\n            height=1,\n            count=1,\n            dtype=\"uint8\",\n            crs=\"EPSG:4326\",\n            transform=placeholder_transform,\n        ) as dst:\n            dst.write(np.zeros((1, 1, 1), dtype=\"uint8\"))\n\n        # Create a new TifProcessor instance with the placeholder\n        # Use LocalDataStore() since the temp file is always a local absolute path\n        new_processor = TifProcessor(\n            dataset_path=placeholder_path,\n            data_store=LocalDataStore(),  # Always use LocalDataStore for temp files\n            mode=self.mode,\n        )\n\n        # Now save the clipped file directly to the new processor's temp directory\n        # Similar to how _reproject_to_temp_file works\n        clipped_file_path = os.path.join(\n            new_processor._temp_dir, f\"clipped_{os.urandom(8).hex()}.tif\"\n        )\n\n        with rasterio.open(clipped_file_path, \"w\", **clipped_meta) as dst:\n            dst.write(clipped_data)\n\n        # Verify file was created successfully\n        if not os.path.exists(clipped_file_path):\n            raise RuntimeError(f\"Failed to create clipped file at {clipped_file_path}\")\n\n        # Set the clipped file path and update processor attributes\n        new_processor._clipped_file_path = clipped_file_path\n        new_processor.dataset_path = clipped_file_path\n        new_processor.dataset_paths = [Path(clipped_file_path)]\n\n        # Clean up placeholder file and directory\n        try:\n            os.remove(placeholder_path)\n            os.rmdir(placeholder_dir)\n        except OSError:\n            pass\n\n        # Reload metadata since the path changed\n        new_processor._load_metadata()\n\n        self.logger.info(f\"Clipped raster saved to temporary file: {clipped_file_path}\")\n\n        return new_processor\n\n    def _get_pixel_coordinates(self):\n        \"\"\"Helper method to generate coordinate arrays for all pixels\"\"\"\n        if \"pixel_coords\" not in self._cache:\n            # use cached values\n            bounds = self._cache[\"bounds\"]\n            width = self._cache[\"width\"]\n            height = self._cache[\"height\"]\n            pixel_size_x = self._cache[\"x_transform\"]\n            pixel_size_y = self._cache[\"y_transform\"]\n\n            self._cache[\"pixel_coords\"] = np.meshgrid(\n                np.linspace(\n                    bounds.left + pixel_size_x / 2,\n                    bounds.right - pixel_size_x / 2,\n                    width,\n                ),\n                np.linspace(\n                    bounds.top + pixel_size_y / 2,\n                    bounds.bottom - pixel_size_y / 2,\n                    height,\n                ),\n            )\n\n        return self._cache[\"pixel_coords\"]\n\n    def _get_chunk_coordinates(self, window, src):\n        \"\"\"Get coordinates for a specific window chunk.\"\"\"\n        transform = src.window_transform(window)\n        rows, cols = np.meshgrid(\n            np.arange(window.height), np.arange(window.width), indexing=\"ij\"\n        )\n        xs, ys = rasterio.transform.xy(transform, rows.flatten(), cols.flatten())\n        return np.array(xs), np.array(ys)\n\n    def _extract_coordinates_with_mask(self, mask=None):\n        \"\"\"Extract flattened coordinates, optionally applying a mask.\"\"\"\n        x_coords, y_coords = self._get_pixel_coordinates()\n\n        if mask is not None:\n            return np.extract(mask, x_coords), np.extract(mask, y_coords)\n\n        return x_coords.flatten(), y_coords.flatten()\n\n    def _build_data_mask(self, data, drop_nodata=True, nodata_value=None):\n        \"\"\"Build a boolean mask for filtering data based on nodata values.\"\"\"\n        if not drop_nodata or nodata_value is None:\n            return None\n\n        return data != nodata_value\n\n    def _build_multi_band_mask(\n        self,\n        bands: np.ndarray,\n        drop_nodata: bool = True,\n        nodata_value: Optional[float] = None,\n    ) -&gt; Optional[np.ndarray]:\n        \"\"\"\n        Build mask for multi-band data - drops pixels where ANY band has nodata.\n\n        Args:\n            bands: 3D array of shape (n_bands, height, width)\n            drop_nodata Whether to drop nodata values\n            nodata_value: The nodata value to check\n\n        Returns:\n            Boolean mask or None if no masking needed\n        \"\"\"\n        if not drop_nodata or nodata_value is None:\n            return None\n\n        # Check if ANY band has nodata at each pixel location\n        has_nodata = np.any(bands == nodata_value, axis=0)\n\n        # Return True where ALL bands are valid\n        valid_mask = ~has_nodata\n\n        return valid_mask if not valid_mask.all() else None\n\n    def _bands_to_dict(self, bands, band_count, band_names, mask=None):\n        \"\"\"Read specified bands and return as a dictionary with optional masking.\"\"\"\n\n        lons, lats = self._extract_coordinates_with_mask(mask)\n        data_dict = {\"lon\": lons, \"lat\": lats}\n\n        for idx, name in enumerate(band_names[:band_count]):\n            band_data = bands[idx]\n            data_dict[name] = (\n                np.extract(mask, band_data) if mask is not None else band_data.flatten()\n            )\n\n        return data_dict\n\n    def _calculate_optimal_chunk_size(\n        self, operation: str = \"conversion\", target_memory_mb: int = 500\n    ) -&gt; int:\n        \"\"\"\n        Calculate optimal chunk size (number of rows) based on target memory usage.\n\n        Args:\n            operation: Type of operation ('conversion', 'graph')\n            target_memory_mb: Target memory per chunk in megabytes\n\n        Returns:\n            Number of rows per chunk\n        \"\"\"\n        bytes_per_element = np.dtype(self.dtype).itemsize\n        n_bands = self.count\n        width = self.width\n\n        # Adjust for operation type\n        if operation == \"conversion\":\n            # DataFrame overhead is roughly 2x\n            bytes_per_row = width * n_bands * bytes_per_element * 2\n        elif operation == \"graph\":\n            # Graph needs additional space for edges\n            bytes_per_row = width * bytes_per_element * 4  # Estimate\n        else:\n            bytes_per_row = width * n_bands * bytes_per_element\n\n        target_bytes = target_memory_mb * 1024 * 1024\n        chunk_rows = max(1, int(target_bytes / bytes_per_row))\n\n        # Ensure chunk size doesn't exceed total height\n        chunk_rows = min(chunk_rows, self.height)\n\n        self.logger.info(\n            f\"Calculated chunk size: {chunk_rows} rows \"\n            f\"(~{self._format_bytes(chunk_rows * bytes_per_row)} per chunk)\"\n        )\n\n        return chunk_rows\n\n    def _get_chunk_windows(self, chunk_size: int) -&gt; List[rasterio.windows.Window]:\n        \"\"\"\n        Generate window objects for chunked reading.\n\n        Args:\n            chunk_size: Number of rows per chunk\n\n        Returns:\n            List of rasterio.windows.Window objects\n        \"\"\"\n        windows = []\n        for row_start in range(0, self.height, chunk_size):\n            row_end = min(row_start + chunk_size, self.height)\n            window = rasterio.windows.Window(\n                col_off=0,\n                row_off=row_start,\n                width=self.width,\n                height=row_end - row_start,\n            )\n            windows.append(window)\n\n        return windows\n\n    def _format_bytes(self, bytes_value: int) -&gt; str:\n        \"\"\"Convert bytes to human-readable format.\"\"\"\n        for unit in [\"B\", \"KB\", \"MB\", \"GB\", \"TB\"]:\n            if bytes_value &lt; 1024.0:\n                return f\"{bytes_value:.2f} {unit}\"\n            bytes_value /= 1024.0\n        return f\"{bytes_value:.2f} PB\"\n\n    def _check_available_memory(self) -&gt; dict:\n        \"\"\"\n        Check available system memory.\n\n        Returns:\n            Dict with total, available, and used memory info\n        \"\"\"\n        import psutil\n\n        memory = psutil.virtual_memory()\n        return {\n            \"total\": memory.total,\n            \"available\": memory.available,\n            \"used\": memory.used,\n            \"percent\": memory.percent,\n            \"available_human\": self._format_bytes(memory.available),\n        }\n\n    def _estimate_memory_usage(\n        self, operation: str = \"conversion\", n_workers: int = 1\n    ) -&gt; dict:\n        \"\"\"\n        Estimate memory usage for various operations.\n\n        Args:\n            operation: Type of operation ('conversion', 'batched_sampling', 'merge', 'graph')\n            n_workers: Number of workers (for batched_sampling)\n\n        Returns:\n            Dict with estimated memory usage in bytes and human-readable format\n        \"\"\"\n        bytes_per_element = np.dtype(self.dtype).itemsize\n        n_pixels = self.width * self.height\n        n_bands = self.count\n\n        estimates = {}\n\n        if operation == \"conversion\":\n            # to_dataframe/to_geodataframe: full raster + DataFrame overhead\n            raster_memory = n_pixels * n_bands * bytes_per_element\n            # DataFrame overhead (roughly 2x for storage + processing)\n            dataframe_memory = (\n                n_pixels * n_bands * 16\n            )  # 16 bytes per value in DataFrame\n            total = raster_memory + dataframe_memory\n            estimates[\"raster\"] = raster_memory\n            estimates[\"dataframe\"] = dataframe_memory\n            estimates[\"total\"] = total\n\n        elif operation == \"batched_sampling\":\n            # Each worker loads full raster into MemoryFile\n            # Need to get file size\n            if self._merged_file_path:\n                file_path = self._merged_file_path\n            elif self._reprojected_file_path:\n                file_path = self._reprojected_file_path\n            else:\n                file_path = str(self.dataset_path)\n\n            try:\n                import os\n\n                file_size = os.path.getsize(file_path)\n            except:\n                # Estimate if can't get file size\n                file_size = n_pixels * n_bands * bytes_per_element * 1.2  # Add overhead\n\n            estimates[\"per_worker\"] = file_size\n            estimates[\"total\"] = file_size * n_workers\n\n        elif operation == \"merge\":\n            # _merge_with_mean uses float64 arrays\n            raster_memory = n_pixels * n_bands * 8  # float64\n            estimates[\"sum_array\"] = raster_memory\n            estimates[\"count_array\"] = n_pixels * 4  # int32\n            estimates[\"total\"] = raster_memory + n_pixels * 4\n\n        elif operation == \"graph\":\n            # to_graph: data + node_map + edges\n            data_memory = n_pixels * bytes_per_element\n            node_map_memory = n_pixels * 4  # int32\n            # Estimate edges (rough: 4-connectivity = 4 edges per pixel)\n            edges_memory = n_pixels * 4 * 3 * 8  # 3 values per edge, float64\n            total = data_memory + node_map_memory + edges_memory\n            estimates[\"data\"] = data_memory\n            estimates[\"node_map\"] = node_map_memory\n            estimates[\"edges\"] = edges_memory\n            estimates[\"total\"] = total\n\n        # Add human-readable format\n        estimates[\"human_readable\"] = self._format_bytes(estimates[\"total\"])\n\n        return estimates\n\n    def _memory_guard(\n        self,\n        operation: str,\n        threshold_percent: float = 80.0,\n        n_workers: Optional[int] = None,\n        raise_error: bool = False,\n    ) -&gt; bool:\n        \"\"\"\n        Check if operation is safe to perform given memory constraints.\n\n        Args:\n            operation: Type of operation to check\n            threshold_percent: Maximum % of available memory to use (default 80%)\n            n_workers: Number of workers (for batched operations)\n            raise_error: If True, raise MemoryError instead of warning\n\n        Returns:\n            True if operation is safe, False otherwise\n\n        Raises:\n            MemoryError: If raise_error=True and memory insufficient\n        \"\"\"\n        import warnings\n\n        estimates = self._estimate_memory_usage(operation, n_workers=n_workers or 1)\n        memory_info = self._check_available_memory()\n\n        estimated_usage = estimates[\"total\"]\n        available = memory_info[\"available\"]\n        threshold = available * (threshold_percent / 100.0)\n\n        is_safe = estimated_usage &lt;= threshold\n\n        if not is_safe:\n            usage_str = self._format_bytes(estimated_usage)\n            available_str = memory_info[\"available_human\"]\n\n            message = (\n                f\"Memory warning: {operation} operation may require {usage_str} \"\n                f\"but only {available_str} is available. \"\n                f\"Current memory usage: {memory_info['percent']:.1f}%\"\n            )\n\n            if raise_error:\n                raise MemoryError(message)\n            else:\n                warnings.warn(message, ResourceWarning)\n                if hasattr(self, \"logger\"):\n                    self.logger.warning(message)\n\n        return is_safe\n\n    def _validate_mode_band_compatibility(self):\n        \"\"\"Validate that mode matches band count.\"\"\"\n        mode_requirements = {\n            \"single\": (1, \"1-band\"),\n            \"rgb\": (3, \"3-band\"),\n            \"rgba\": (4, \"4-band\"),\n        }\n\n        if self.mode in mode_requirements:\n            required_count, description = mode_requirements[self.mode]\n            if self.count != required_count:\n                raise ValueError(\n                    f\"{self.mode.upper()} mode requires a {description} TIF file\"\n                )\n        elif self.mode == \"multi\" and self.count &lt; 2:\n            raise ValueError(\"Multi mode requires a TIF file with 2 or more bands\")\n\n    def __enter__(self):\n        return self\n\n    def __del__(self):\n        \"\"\"Clean up temporary files and directories.\"\"\"\n        if hasattr(self, \"_temp_dir\") and os.path.exists(self._temp_dir):\n            shutil.rmtree(self._temp_dir, ignore_errors=True)\n\n    def cleanup(self):\n        \"\"\"Explicit cleanup method for better control.\"\"\"\n        if hasattr(self, \"_temp_dir\") and os.path.exists(self._temp_dir):\n            shutil.rmtree(self._temp_dir)\n            self.logger.info(\"Cleaned up temporary files\")\n\n    def __exit__(self, exc_type, exc_value, traceback):\n        \"\"\"Proper context manager exit with cleanup.\"\"\"\n        self.cleanup()\n        return False\n</code></pre>"},{"location":"api/processing/#gigaspatial.processing.TifProcessor.bounds","title":"<code>bounds</code>  <code>property</code>","text":"<p>Get the bounds of the TIF file</p>"},{"location":"api/processing/#gigaspatial.processing.TifProcessor.count","title":"<code>count: int</code>  <code>property</code>","text":"<p>Get the band count from the TIF file</p>"},{"location":"api/processing/#gigaspatial.processing.TifProcessor.crs","title":"<code>crs</code>  <code>property</code>","text":"<p>Get the coordinate reference system from the TIF file</p>"},{"location":"api/processing/#gigaspatial.processing.TifProcessor.dtype","title":"<code>dtype</code>  <code>property</code>","text":"<p>Get the data types from the TIF file</p>"},{"location":"api/processing/#gigaspatial.processing.TifProcessor.is_merged","title":"<code>is_merged: bool</code>  <code>property</code>","text":"<p>Check if this processor was created from multiple rasters.</p>"},{"location":"api/processing/#gigaspatial.processing.TifProcessor.nodata","title":"<code>nodata: int</code>  <code>property</code>","text":"<p>Get the value representing no data in the rasters</p>"},{"location":"api/processing/#gigaspatial.processing.TifProcessor.resolution","title":"<code>resolution: Tuple[float, float]</code>  <code>property</code>","text":"<p>Get the x and y resolution (pixel width and height or pixel size) from the TIF file</p>"},{"location":"api/processing/#gigaspatial.processing.TifProcessor.source_count","title":"<code>source_count: int</code>  <code>property</code>","text":"<p>Get the number of source rasters.</p>"},{"location":"api/processing/#gigaspatial.processing.TifProcessor.transform","title":"<code>transform</code>  <code>property</code>","text":"<p>Get the transform from the TIF file</p>"},{"location":"api/processing/#gigaspatial.processing.TifProcessor.x_transform","title":"<code>x_transform: float</code>  <code>property</code>","text":"<p>Get the x transform from the TIF file</p>"},{"location":"api/processing/#gigaspatial.processing.TifProcessor.y_transform","title":"<code>y_transform: float</code>  <code>property</code>","text":"<p>Get the y transform from the TIF file</p>"},{"location":"api/processing/#gigaspatial.processing.TifProcessor.__del__","title":"<code>__del__()</code>","text":"<p>Clean up temporary files and directories.</p> Source code in <code>gigaspatial/processing/tif_processor.py</code> <pre><code>def __del__(self):\n    \"\"\"Clean up temporary files and directories.\"\"\"\n    if hasattr(self, \"_temp_dir\") and os.path.exists(self._temp_dir):\n        shutil.rmtree(self._temp_dir, ignore_errors=True)\n</code></pre>"},{"location":"api/processing/#gigaspatial.processing.TifProcessor.__exit__","title":"<code>__exit__(exc_type, exc_value, traceback)</code>","text":"<p>Proper context manager exit with cleanup.</p> Source code in <code>gigaspatial/processing/tif_processor.py</code> <pre><code>def __exit__(self, exc_type, exc_value, traceback):\n    \"\"\"Proper context manager exit with cleanup.\"\"\"\n    self.cleanup()\n    return False\n</code></pre>"},{"location":"api/processing/#gigaspatial.processing.TifProcessor.__post_init__","title":"<code>__post_init__()</code>","text":"<p>Validate inputs, merge rasters if needed, and set up logging.</p> Source code in <code>gigaspatial/processing/tif_processor.py</code> <pre><code>def __post_init__(self):\n    \"\"\"Validate inputs, merge rasters if needed, and set up logging.\"\"\"\n    self.data_store = self.data_store or LocalDataStore()\n    self.logger = config.get_logger(self.__class__.__name__)\n    self._cache = {}\n    self._temp_dir = tempfile.mkdtemp()\n    self._merged_file_path = None\n    self._reprojected_file_path = None\n    self._clipped_file_path = None\n\n    # Handle multiple dataset paths\n    if isinstance(self.dataset_path, list):\n        if len(self.dataset_path) &gt; 1:\n            self.dataset_paths = [Path(p) for p in self.dataset_path]\n            self._validate_multiple_datasets()\n            self._merge_rasters()\n            self.dataset_path = self._merged_file_path\n    else:\n        self.dataset_paths = [Path(self.dataset_path)]\n        # For absolute paths with LocalDataStore, check file existence directly\n        # to avoid path resolution issues\n        if isinstance(self.data_store, LocalDataStore) and os.path.isabs(\n            str(self.dataset_path)\n        ):\n            if not os.path.exists(str(self.dataset_path)):\n                raise FileNotFoundError(f\"Dataset not found at {self.dataset_path}\")\n        elif not self.data_store.file_exists(str(self.dataset_path)):\n            raise FileNotFoundError(f\"Dataset not found at {self.dataset_path}\")\n\n        # Reproject single raster during initialization if target_crs is set\n        if self.target_crs:\n            self.logger.info(f\"Reprojecting single raster to {self.target_crs}...\")\n            with self.data_store.open(str(self.dataset_path), \"rb\") as f:\n                with rasterio.MemoryFile(f.read()) as memfile:\n                    with memfile.open() as src:\n                        self._reprojected_file_path = self._reproject_to_temp_file(\n                            src, self.target_crs\n                        )\n            self.dataset_path = self._reprojected_file_path\n\n    self._load_metadata()\n    self._validate_mode_band_compatibility()\n</code></pre>"},{"location":"api/processing/#gigaspatial.processing.TifProcessor.cleanup","title":"<code>cleanup()</code>","text":"<p>Explicit cleanup method for better control.</p> Source code in <code>gigaspatial/processing/tif_processor.py</code> <pre><code>def cleanup(self):\n    \"\"\"Explicit cleanup method for better control.\"\"\"\n    if hasattr(self, \"_temp_dir\") and os.path.exists(self._temp_dir):\n        shutil.rmtree(self._temp_dir)\n        self.logger.info(\"Cleaned up temporary files\")\n</code></pre>"},{"location":"api/processing/#gigaspatial.processing.TifProcessor.clip_to_bounds","title":"<code>clip_to_bounds(bounds, bounds_crs=None, return_clipped_processor=True)</code>","text":"<p>Clip raster to rectangular bounds.</p>"},{"location":"api/processing/#gigaspatial.processing.TifProcessor.clip_to_bounds--parameters","title":"Parameters:","text":"<p>bounds : tuple     Bounding box as (minx, miny, maxx, maxy) bounds_crs : str, optional     CRS of the bounds. If None, assumes same as raster CRS return_clipped_processor : bool, default True     If True, returns new TifProcessor, else returns (array, transform, metadata)</p>"},{"location":"api/processing/#gigaspatial.processing.TifProcessor.clip_to_bounds--returns","title":"Returns:","text":"<p>TifProcessor or tuple     Either new TifProcessor instance or (array, transform, metadata) tuple</p> Source code in <code>gigaspatial/processing/tif_processor.py</code> <pre><code>def clip_to_bounds(\n    self,\n    bounds: tuple,\n    bounds_crs: Optional[str] = None,\n    return_clipped_processor: bool = True,\n) -&gt; Union[\"TifProcessor\", tuple]:\n    \"\"\"\n    Clip raster to rectangular bounds.\n\n    Parameters:\n    -----------\n    bounds : tuple\n        Bounding box as (minx, miny, maxx, maxy)\n    bounds_crs : str, optional\n        CRS of the bounds. If None, assumes same as raster CRS\n    return_clipped_processor : bool, default True\n        If True, returns new TifProcessor, else returns (array, transform, metadata)\n\n    Returns:\n    --------\n    TifProcessor or tuple\n        Either new TifProcessor instance or (array, transform, metadata) tuple\n    \"\"\"\n    # Create bounding box geometry\n    bbox_geom = box(*bounds)\n\n    # If bounds_crs is specified and different from raster CRS, create GeoDataFrame for reprojection\n    if bounds_crs is not None:\n        raster_crs = self.crs\n\n        if not self.crs == bounds_crs:\n            # Create GeoDataFrame with bounds CRS and reproject\n            bbox_gdf = gpd.GeoDataFrame([1], geometry=[bbox_geom], crs=bounds_crs)\n            bbox_gdf = bbox_gdf.to_crs(raster_crs)\n            bbox_geom = bbox_gdf.geometry.iloc[0]\n\n    return self.clip_to_geometry(\n        geometry=bbox_geom,\n        crop=True,\n        return_clipped_processor=return_clipped_processor,\n    )\n</code></pre>"},{"location":"api/processing/#gigaspatial.processing.TifProcessor.clip_to_geometry","title":"<code>clip_to_geometry(geometry, crop=True, all_touched=True, invert=False, nodata=None, pad=False, pad_width=0.5, return_clipped_processor=True)</code>","text":"<p>Clip raster to geometry boundaries.</p>"},{"location":"api/processing/#gigaspatial.processing.TifProcessor.clip_to_geometry--parameters","title":"Parameters:","text":"<p>geometry : various     Geometry to clip to. Can be:     - Shapely Polygon or MultiPolygon     - GeoDataFrame or GeoSeries     - List of GeoJSON-like dicts     - Single GeoJSON-like dict crop : bool, default True     Whether to crop the raster to the extent of the geometry all_touched : bool, default True     Include pixels that touch the geometry boundary invert : bool, default False     If True, mask pixels inside geometry instead of outside nodata : int or float, optional     Value to use for masked pixels. If None, uses raster's nodata value pad : bool, default False     Pad geometry by half pixel before clipping pad_width : float, default 0.5     Width of padding in pixels if pad=True return_clipped_processor : bool, default True     If True, returns new TifProcessor with clipped data     If False, returns (clipped_array, transform, metadata)</p>"},{"location":"api/processing/#gigaspatial.processing.TifProcessor.clip_to_geometry--returns","title":"Returns:","text":"<p>TifProcessor or tuple     Either new TifProcessor instance or (array, transform, metadata) tuple</p> Source code in <code>gigaspatial/processing/tif_processor.py</code> <pre><code>def clip_to_geometry(\n    self,\n    geometry: Union[\n        Polygon, MultiPolygon, gpd.GeoDataFrame, gpd.GeoSeries, List[dict], dict\n    ],\n    crop: bool = True,\n    all_touched: bool = True,\n    invert: bool = False,\n    nodata: Optional[Union[int, float]] = None,\n    pad: bool = False,\n    pad_width: float = 0.5,\n    return_clipped_processor: bool = True,\n) -&gt; Union[\"TifProcessor\", tuple]:\n    \"\"\"\n    Clip raster to geometry boundaries.\n\n    Parameters:\n    -----------\n    geometry : various\n        Geometry to clip to. Can be:\n        - Shapely Polygon or MultiPolygon\n        - GeoDataFrame or GeoSeries\n        - List of GeoJSON-like dicts\n        - Single GeoJSON-like dict\n    crop : bool, default True\n        Whether to crop the raster to the extent of the geometry\n    all_touched : bool, default True\n        Include pixels that touch the geometry boundary\n    invert : bool, default False\n        If True, mask pixels inside geometry instead of outside\n    nodata : int or float, optional\n        Value to use for masked pixels. If None, uses raster's nodata value\n    pad : bool, default False\n        Pad geometry by half pixel before clipping\n    pad_width : float, default 0.5\n        Width of padding in pixels if pad=True\n    return_clipped_processor : bool, default True\n        If True, returns new TifProcessor with clipped data\n        If False, returns (clipped_array, transform, metadata)\n\n    Returns:\n    --------\n    TifProcessor or tuple\n        Either new TifProcessor instance or (array, transform, metadata) tuple\n    \"\"\"\n    # Handle different geometry input types\n    shapes = self._prepare_geometry_for_clipping(geometry)\n\n    # Validate CRS compatibility\n    self._validate_geometry_crs(geometry)\n\n    # Perform the clipping\n    with self.open_dataset() as src:\n        try:\n            clipped_data, clipped_transform = mask(\n                dataset=src,\n                shapes=shapes,\n                crop=crop,\n                all_touched=all_touched,\n                invert=invert,\n                nodata=nodata,\n                pad=pad,\n                pad_width=pad_width,\n                filled=True,\n            )\n\n            # Update metadata for the clipped raster\n            clipped_meta = src.meta.copy()\n            clipped_meta.update(\n                {\n                    \"height\": clipped_data.shape[1],\n                    \"width\": clipped_data.shape[2],\n                    \"transform\": clipped_transform,\n                    \"nodata\": nodata if nodata is not None else src.nodata,\n                }\n            )\n\n        except ValueError as e:\n            if \"Input shapes do not overlap raster\" in str(e):\n                raise ValueError(\n                    \"The geometry does not overlap with the raster. \"\n                    \"Check that both are in the same coordinate reference system.\"\n                ) from e\n            else:\n                raise e\n\n    if return_clipped_processor:\n        # Create a new TifProcessor with the clipped data\n        return self._create_clipped_processor(clipped_data, clipped_meta)\n    else:\n        return clipped_data, clipped_transform, clipped_meta\n</code></pre>"},{"location":"api/processing/#gigaspatial.processing.TifProcessor.get_raster_info","title":"<code>get_raster_info()</code>","text":"<p>Get comprehensive raster information.</p> Source code in <code>gigaspatial/processing/tif_processor.py</code> <pre><code>def get_raster_info(self) -&gt; Dict[str, Any]:\n    \"\"\"Get comprehensive raster information.\"\"\"\n    return {\n        \"count\": self.count,\n        \"width\": self.width,\n        \"height\": self.height,\n        \"crs\": self.crs,\n        \"bounds\": self.bounds,\n        \"transform\": self.transform,\n        \"dtypes\": self.dtype,\n        \"nodata\": self.nodata,\n        \"mode\": self.mode,\n        \"is_merged\": self.is_merged,\n        \"source_count\": self.source_count,\n    }\n</code></pre>"},{"location":"api/processing/#gigaspatial.processing.TifProcessor.open_dataset","title":"<code>open_dataset()</code>","text":"<p>Context manager for accessing the dataset, handling temporary reprojected files.</p> Source code in <code>gigaspatial/processing/tif_processor.py</code> <pre><code>@contextmanager\ndef open_dataset(self):\n    \"\"\"Context manager for accessing the dataset, handling temporary reprojected files.\"\"\"\n    if self._merged_file_path:\n        with rasterio.open(self._merged_file_path) as src:\n            yield src\n    elif self._reprojected_file_path:\n        with rasterio.open(self._reprojected_file_path) as src:\n            yield src\n    elif self._clipped_file_path:\n        with rasterio.open(self._clipped_file_path) as src:\n            yield src\n    elif isinstance(self.data_store, LocalDataStore):\n        with rasterio.open(str(self.dataset_path)) as src:\n            yield src\n    else:\n        with self.data_store.open(str(self.dataset_path), \"rb\") as f:\n            with rasterio.MemoryFile(f.read()) as memfile:\n                with memfile.open() as src:\n                    yield src\n</code></pre>"},{"location":"api/processing/#gigaspatial.processing.TifProcessor.reproject_to","title":"<code>reproject_to(target_crs, output_path=None, resampling_method=None, resolution=None)</code>","text":"<p>Reprojects the current raster to a new CRS and optionally saves it.</p> <p>Parameters:</p> Name Type Description Default <code>target_crs</code> <code>str</code> <p>The CRS to reproject to (e.g., \"EPSG:4326\").</p> required <code>output_path</code> <code>Optional[Union[str, Path]]</code> <p>The path to save the reprojected raster. If None,          it is saved to a temporary file.</p> <code>None</code> <code>resampling_method</code> <code>Optional[Resampling]</code> <p>The resampling method to use.</p> <code>None</code> <code>resolution</code> <code>Optional[Tuple[float, float]]</code> <p>The target resolution (pixel size) in the new CRS.</p> <code>None</code> Source code in <code>gigaspatial/processing/tif_processor.py</code> <pre><code>def reproject_to(\n    self,\n    target_crs: str,\n    output_path: Optional[Union[str, Path]] = None,\n    resampling_method: Optional[Resampling] = None,\n    resolution: Optional[Tuple[float, float]] = None,\n):\n    \"\"\"\n    Reprojects the current raster to a new CRS and optionally saves it.\n\n    Args:\n        target_crs: The CRS to reproject to (e.g., \"EPSG:4326\").\n        output_path: The path to save the reprojected raster. If None,\n                     it is saved to a temporary file.\n        resampling_method: The resampling method to use.\n        resolution: The target resolution (pixel size) in the new CRS.\n    \"\"\"\n    self.logger.info(f\"Reprojecting raster to {target_crs}...\")\n\n    # Use provided or default values\n    resampling_method = resampling_method or self.resampling_method\n    resolution = resolution or self.reprojection_resolution\n\n    with self.open_dataset() as src:\n        if src.crs.to_string() == target_crs:\n            self.logger.info(\n                \"Raster is already in the target CRS. No reprojection needed.\"\n            )\n            # If output_path is specified, copy the file\n            if output_path:\n                self.data_store.copy_file(str(self.dataset_path), output_path)\n            return self.dataset_path\n\n        dst_path = output_path or os.path.join(\n            self._temp_dir, f\"reprojected_single_{os.urandom(8).hex()}.tif\"\n        )\n\n        with rasterio.open(\n            dst_path,\n            \"w\",\n            **self._get_reprojection_profile(src, target_crs, resolution),\n        ) as dst:\n            for band_idx in range(1, src.count + 1):\n                reproject(\n                    source=rasterio.band(src, band_idx),\n                    destination=rasterio.band(dst, band_idx),\n                    src_transform=src.transform,\n                    src_crs=src.crs,\n                    dst_transform=dst.transform,\n                    dst_crs=dst.crs,\n                    resampling=resampling_method,\n                    num_threads=multiprocessing.cpu_count(),\n                )\n\n        self.logger.info(f\"Reprojection complete. Output saved to {dst_path}\")\n        return Path(dst_path)\n</code></pre>"},{"location":"api/processing/#gigaspatial.processing.TifProcessor.sample_by_polygons","title":"<code>sample_by_polygons(polygon_list, stat='mean')</code>","text":"<p>Sample raster values by polygons and compute statistic(s) for each polygon.</p> <p>Parameters:</p> Name Type Description Default <code>polygon_list</code> <p>List of shapely Polygon or MultiPolygon objects.</p> required <code>stat</code> <code>Union[str, Callable, List[Union[str, Callable]]]</code> <p>Statistic(s) to compute. Can be: - Single string: 'mean', 'median', 'sum', 'min', 'max', 'std', 'count' - Single callable: custom function that takes array and returns scalar - List of strings/callables: multiple statistics to compute</p> <code>'mean'</code> <p>Returns:</p> Type Description <p>If single stat: np.ndarray of computed statistics for each polygon</p> <p>If multiple stats: List of dictionaries with stat names as keys</p> Source code in <code>gigaspatial/processing/tif_processor.py</code> <pre><code>def sample_by_polygons(\n    self,\n    polygon_list,\n    stat: Union[str, Callable, List[Union[str, Callable]]] = \"mean\",\n):\n    \"\"\"\n    Sample raster values by polygons and compute statistic(s) for each polygon.\n\n    Args:\n        polygon_list: List of shapely Polygon or MultiPolygon objects.\n        stat: Statistic(s) to compute. Can be:\n            - Single string: 'mean', 'median', 'sum', 'min', 'max', 'std', 'count'\n            - Single callable: custom function that takes array and returns scalar\n            - List of strings/callables: multiple statistics to compute\n\n    Returns:\n        If single stat: np.ndarray of computed statistics for each polygon\n        If multiple stats: List of dictionaries with stat names as keys\n    \"\"\"\n    # Determine if single or multiple stats\n    single_stat = not isinstance(stat, list)\n    stats_list = [stat] if single_stat else stat\n\n    # Prepare stat functions\n    stat_funcs = []\n    stat_names = []\n\n    for s in stats_list:\n        if callable(s):\n            stat_funcs.append(s)\n            stat_names.append(\n                s.__name__\n                if hasattr(s, \"__name__\")\n                else f\"custom_{len(stat_names)}\"\n            )\n        else:\n            # Handle string statistics\n            if s == \"count\":\n                stat_funcs.append(len)\n            else:\n                stat_funcs.append(getattr(np, s))\n            stat_names.append(s)\n\n    results = []\n\n    with self.open_dataset() as src:\n        for polygon in tqdm(polygon_list):\n            try:\n                out_image, _ = mask(src, [polygon], crop=True, filled=False)\n\n                # Use masked arrays for more efficient nodata handling\n                if hasattr(out_image, \"mask\"):\n                    valid_data = out_image.compressed()\n                else:\n                    valid_data = (\n                        out_image[out_image != self.nodata]\n                        if self.nodata\n                        else out_image.flatten()\n                    )\n\n                if len(valid_data) == 0:\n                    if single_stat:\n                        results.append(np.nan)\n                    else:\n                        results.append({name: np.nan for name in stat_names})\n                else:\n                    if single_stat:\n                        results.append(stat_funcs[0](valid_data))\n                    else:\n                        # Compute all statistics for this polygon\n                        polygon_stats = {}\n                        for func, name in zip(stat_funcs, stat_names):\n                            try:\n                                polygon_stats[name] = func(valid_data)\n                            except Exception:\n                                polygon_stats[name] = np.nan\n                        results.append(polygon_stats)\n\n            except Exception:\n                if single_stat:\n                    results.append(np.nan)\n                else:\n                    results.append({name: np.nan for name in stat_names})\n\n    return np.array(results) if single_stat else results\n</code></pre>"},{"location":"api/processing/#gigaspatial.processing.TifProcessor.sample_by_polygons_batched","title":"<code>sample_by_polygons_batched(polygon_list, stat='mean', batch_size=100, n_workers=4, show_progress=True, check_memory=True, **kwargs)</code>","text":"<p>Sample raster values by polygons in parallel using batching.</p> <p>Parameters:</p> Name Type Description Default <code>polygon_list</code> <code>List[Union[Polygon, MultiPolygon]]</code> <p>List of Shapely Polygon or MultiPolygon objects</p> required <code>stat</code> <code>Union[str, Callable]</code> <p>Statistic to compute</p> <code>'mean'</code> <code>batch_size</code> <code>int</code> <p>Number of polygons per batch</p> <code>100</code> <code>n_workers</code> <code>int</code> <p>Number of worker processes</p> <code>4</code> <code>show_progress</code> <code>bool</code> <p>Whether to display progress bar</p> <code>True</code> <code>check_memory</code> <code>bool</code> <p>Whether to check memory before operation</p> <code>True</code> <code>**kwargs</code> <p>Additional arguments</p> <code>{}</code> <p>Returns:</p> Type Description <code>ndarray</code> <p>np.ndarray of statistics for each polygon</p> Source code in <code>gigaspatial/processing/tif_processor.py</code> <pre><code>def sample_by_polygons_batched(\n    self,\n    polygon_list: List[Union[Polygon, MultiPolygon]],\n    stat: Union[str, Callable] = \"mean\",\n    batch_size: int = 100,\n    n_workers: int = 4,\n    show_progress: bool = True,\n    check_memory: bool = True,\n    **kwargs,\n) -&gt; np.ndarray:\n    \"\"\"\n    Sample raster values by polygons in parallel using batching.\n\n    Args:\n        polygon_list: List of Shapely Polygon or MultiPolygon objects\n        stat: Statistic to compute\n        batch_size: Number of polygons per batch\n        n_workers: Number of worker processes\n        show_progress: Whether to display progress bar\n        check_memory: Whether to check memory before operation\n        **kwargs: Additional arguments\n\n    Returns:\n        np.ndarray of statistics for each polygon\n    \"\"\"\n    import sys\n\n    # Memory guard check with n_workers consideration\n    if check_memory:\n        is_safe = self._memory_guard(\n            \"batched_sampling\",\n            threshold_percent=85.0,\n            n_workers=n_workers,\n            raise_error=False,\n        )\n\n        if not is_safe:\n            # Suggest reducing n_workers\n            memory_info = self._check_available_memory()\n            estimates = self._estimate_memory_usage(\"batched_sampling\", n_workers=1)\n\n            # Calculate optimal workers\n            suggested_workers = max(\n                1, int(memory_info[\"available\"] * 0.7 / estimates[\"per_worker\"])\n            )\n\n            warnings.warn(\n                f\"Consider reducing n_workers from {n_workers} to {suggested_workers} \"\n                f\"to reduce memory pressure.\",\n                ResourceWarning,\n            )\n\n    # Platform check\n    if sys.platform in [\"win32\", \"darwin\"]:\n        import warnings\n        import multiprocessing as mp\n\n        if mp.get_start_method(allow_none=True) != \"fork\":\n            warnings.warn(\n                \"Batched sampling may not work on Windows/macOS. \"\n                \"Use sample_by_polygons() if you encounter errors.\",\n                RuntimeWarning,\n            )\n\n    def _chunk_list(data_list, chunk_size):\n        \"\"\"Yield successive chunks from data_list.\"\"\"\n        for i in range(0, len(data_list), chunk_size):\n            yield data_list[i : i + chunk_size]\n\n    if len(polygon_list) == 0:\n        return np.array([])\n\n    stat_func = stat if callable(stat) else getattr(np, stat)\n    polygon_chunks = list(_chunk_list(polygon_list, batch_size))\n\n    with multiprocessing.Pool(\n        initializer=self._initializer_worker, processes=n_workers\n    ) as pool:\n        process_func = partial(self._process_polygon_batch, stat_func=stat_func)\n        if show_progress:\n            batched_results = list(\n                tqdm(\n                    pool.imap(process_func, polygon_chunks),\n                    total=len(polygon_chunks),\n                    desc=f\"Sampling polygons\",\n                )\n            )\n        else:\n            batched_results = list(pool.imap(process_func, polygon_chunks))\n\n        results = [item for sublist in batched_results for item in sublist]\n\n    return np.array(results)\n</code></pre>"},{"location":"api/processing/#gigaspatial.processing.TifProcessor.to_dataframe","title":"<code>to_dataframe(drop_nodata=True, check_memory=True, **kwargs)</code>","text":"<p>Convert raster to DataFrame.</p> <p>Parameters:</p> Name Type Description Default <code>drop_nodata</code> <p>Whether to drop nodata values</p> <code>True</code> <code>check_memory</code> <p>Whether to check memory before operation (default True)</p> <code>True</code> <code>**kwargs</code> <p>Additional arguments</p> <code>{}</code> <p>Returns:</p> Type Description <code>DataFrame</code> <p>pd.DataFrame with raster data</p> Source code in <code>gigaspatial/processing/tif_processor.py</code> <pre><code>def to_dataframe(\n    self, drop_nodata=True, check_memory=True, **kwargs\n) -&gt; pd.DataFrame:\n    \"\"\"\n    Convert raster to DataFrame.\n\n    Args:\n        drop_nodata: Whether to drop nodata values\n        check_memory: Whether to check memory before operation (default True)\n        **kwargs: Additional arguments\n\n    Returns:\n        pd.DataFrame with raster data\n    \"\"\"\n    # Memory guard check\n    if check_memory:\n        self._memory_guard(\"conversion\", threshold_percent=80.0)\n\n    try:\n        if self.mode == \"single\":\n            return self._to_dataframe(\n                band_number=kwargs.get(\"band_number\", 1),\n                drop_nodata=drop_nodata,\n                band_names=kwargs.get(\"band_names\", None),\n            )\n        else:\n            return self._to_dataframe(\n                band_number=None,  # All bands\n                drop_nodata=drop_nodata,\n                band_names=kwargs.get(\"band_names\", None),\n            )\n    except Exception as e:\n        raise ValueError(\n            f\"Failed to process TIF file in mode '{self.mode}'. \"\n            f\"Please ensure the file is valid and matches the selected mode. \"\n            f\"Original error: {str(e)}\"\n        )\n\n    return df\n</code></pre>"},{"location":"api/processing/#gigaspatial.processing.TifProcessor.to_dataframe_chunked","title":"<code>to_dataframe_chunked(drop_nodata=True, chunk_size=None, target_memory_mb=500, **kwargs)</code>","text":"<p>Convert raster to DataFrame using chunked processing for memory efficiency.</p> <p>Automatically routes to the appropriate chunked method based on mode. Chunk size is automatically calculated based on target memory usage.</p> <p>Parameters:</p> Name Type Description Default <code>drop_nodata</code> <p>Whether to drop nodata values</p> <code>True</code> <code>chunk_size</code> <p>Number of rows per chunk (auto-calculated if None)</p> <code>None</code> <code>target_memory_mb</code> <p>Target memory per chunk in MB (default 500)</p> <code>500</code> <code>**kwargs</code> <p>Additional arguments (band_number, band_names, etc.)</p> <code>{}</code> Source code in <code>gigaspatial/processing/tif_processor.py</code> <pre><code>def to_dataframe_chunked(\n    self, drop_nodata=True, chunk_size=None, target_memory_mb=500, **kwargs\n):\n    \"\"\"\n    Convert raster to DataFrame using chunked processing for memory efficiency.\n\n    Automatically routes to the appropriate chunked method based on mode.\n    Chunk size is automatically calculated based on target memory usage.\n\n    Args:\n        drop_nodata: Whether to drop nodata values\n        chunk_size: Number of rows per chunk (auto-calculated if None)\n        target_memory_mb: Target memory per chunk in MB (default 500)\n        **kwargs: Additional arguments (band_number, band_names, etc.)\n    \"\"\"\n\n    if chunk_size is None:\n        chunk_size = self._calculate_optimal_chunk_size(\n            \"conversion\", target_memory_mb\n        )\n\n    windows = self._get_chunk_windows(chunk_size)\n\n    # SIMPLE ROUTING\n    if self.mode == \"single\":\n        return self._to_dataframe_chunked(\n            windows,\n            band_number=kwargs.get(\"band_number\", 1),\n            drop_nodata=drop_nodata,\n            band_names=kwargs.get(\"band_names\", None),\n        )\n    else:  # rgb, rgba, multi\n        return self._to_dataframe_chunked(\n            windows,\n            band_number=None,\n            drop_nodata=drop_nodata,\n            band_names=kwargs.get(\"band_names\", None),\n        )\n</code></pre>"},{"location":"api/processing/#gigaspatial.processing.TifProcessor.to_geodataframe","title":"<code>to_geodataframe(check_memory=True, **kwargs)</code>","text":"<p>Convert the processed TIF data into a GeoDataFrame, where each row represents a pixel zone. Each zone is defined by its bounding box, based on pixel resolution and coordinates.</p> <p>Parameters:</p> Name Type Description Default <code>check_memory</code> <p>Whether to check memory before operation</p> <code>True</code> <code>**kwargs</code> <p>Additional arguments passed to to_dataframe()</p> <code>{}</code> <p>Returns:</p> Type Description <code>GeoDataFrame</code> <p>gpd.GeoDataFrame with raster data</p> Source code in <code>gigaspatial/processing/tif_processor.py</code> <pre><code>def to_geodataframe(self, check_memory=True, **kwargs) -&gt; gpd.GeoDataFrame:\n    \"\"\"\n    Convert the processed TIF data into a GeoDataFrame, where each row represents a pixel zone.\n    Each zone is defined by its bounding box, based on pixel resolution and coordinates.\n\n    Args:\n        check_memory: Whether to check memory before operation\n        **kwargs: Additional arguments passed to to_dataframe()\n\n    Returns:\n        gpd.GeoDataFrame with raster data\n    \"\"\"\n    # Memory guard check\n    if check_memory:\n        self._memory_guard(\"conversion\", threshold_percent=80.0)\n\n    df = self.to_dataframe(check_memory=False, **kwargs)\n\n    x_res, y_res = self.resolution\n\n    # create bounding box for each pixel\n    geometries = [\n        box(lon - x_res / 2, lat - y_res / 2, lon + x_res / 2, lat + y_res / 2)\n        for lon, lat in zip(df[\"lon\"], df[\"lat\"])\n    ]\n\n    gdf = gpd.GeoDataFrame(df, geometry=geometries, crs=self.crs)\n\n    return gdf\n</code></pre>"},{"location":"api/processing/#gigaspatial.processing.TifProcessor.to_graph","title":"<code>to_graph(connectivity=4, band=None, include_coordinates=False, graph_type='networkx', check_memory=True)</code>","text":"<p>Convert raster to graph based on pixel adjacency.</p> <p>Parameters:</p> Name Type Description Default <code>connectivity</code> <code>Literal[4, 8]</code> <p>4 or 8-connectivity</p> <code>4</code> <code>band</code> <code>Optional[int]</code> <p>Band number (1-indexed)</p> <code>None</code> <code>include_coordinates</code> <code>bool</code> <p>Include x,y coordinates in nodes</p> <code>False</code> <code>graph_type</code> <code>Literal['networkx', 'sparse']</code> <p>'networkx' or 'sparse'</p> <code>'networkx'</code> <code>check_memory</code> <code>bool</code> <p>Whether to check memory before operation</p> <code>True</code> <p>Returns:</p> Type Description <code>Union[Graph, csr_matrix]</code> <p>Graph representation of raster</p> Source code in <code>gigaspatial/processing/tif_processor.py</code> <pre><code>def to_graph(\n    self,\n    connectivity: Literal[4, 8] = 4,\n    band: Optional[int] = None,\n    include_coordinates: bool = False,\n    graph_type: Literal[\"networkx\", \"sparse\"] = \"networkx\",\n    check_memory: bool = True,\n) -&gt; Union[nx.Graph, sp.csr_matrix]:\n    \"\"\"\n    Convert raster to graph based on pixel adjacency.\n\n    Args:\n        connectivity: 4 or 8-connectivity\n        band: Band number (1-indexed)\n        include_coordinates: Include x,y coordinates in nodes\n        graph_type: 'networkx' or 'sparse'\n        check_memory: Whether to check memory before operation\n\n    Returns:\n        Graph representation of raster\n    \"\"\"\n\n    # Memory guard check\n    if check_memory:\n        self._memory_guard(\"graph\", threshold_percent=80.0)\n\n    with self.open_dataset() as src:\n        band_idx = band - 1 if band is not None else 0\n        if band_idx &lt; 0 or band_idx &gt;= src.count:\n            raise ValueError(\n                f\"Band {band} not available. Raster has {src.count} bands\"\n            )\n\n        data = src.read(band_idx + 1)\n        nodata = src.nodata if src.nodata is not None else self.nodata\n        valid_mask = (\n            data != nodata if nodata is not None else np.ones_like(data, dtype=bool)\n        )\n\n        height, width = data.shape\n\n        # Find all valid pixels\n        valid_rows, valid_cols = np.where(valid_mask)\n        num_valid_pixels = len(valid_rows)\n\n        # Create a sequential mapping from (row, col) to a node ID\n        node_map = np.full(data.shape, -1, dtype=int)\n        node_map[valid_rows, valid_cols] = np.arange(num_valid_pixels)\n\n        # Define neighborhood offsets\n        if connectivity == 4:\n            # von Neumann neighborhood (4-connectivity)\n            offsets = [(-1, 0), (1, 0), (0, -1), (0, 1)]\n        else:  # connectivity == 8\n            # Moore neighborhood (8-connectivity)\n            offsets = [\n                (-1, -1),\n                (-1, 0),\n                (-1, 1),\n                (0, -1),\n                (0, 1),\n                (1, -1),\n                (1, 0),\n                (1, 1),\n            ]\n\n        # Collect nodes and edges\n        nodes_to_add = []\n        edges_to_add = []\n\n        for i in range(num_valid_pixels):\n            row, col = valid_rows[i], valid_cols[i]\n            current_node_id = node_map[row, col]\n\n            # Prepare node attributes\n            node_attrs = {\"value\": float(data[row, col])}\n            if include_coordinates:\n                x, y = src.xy(row, col)\n                node_attrs[\"x\"] = x\n                node_attrs[\"y\"] = y\n            nodes_to_add.append((current_node_id, node_attrs))\n\n            # Find neighbors and collect edges\n            for dy, dx in offsets:\n                neighbor_row, neighbor_col = row + dy, col + dx\n\n                # Check if neighbor is within bounds and is a valid pixel\n                if (\n                    0 &lt;= neighbor_row &lt; height\n                    and 0 &lt;= neighbor_col &lt; width\n                    and valid_mask[neighbor_row, neighbor_col]\n                ):\n                    neighbor_node_id = node_map[neighbor_row, neighbor_col]\n\n                    # Ensure each edge is added only once\n                    if current_node_id &lt; neighbor_node_id:\n                        neighbor_value = float(data[neighbor_row, neighbor_col])\n                        edges_to_add.append(\n                            (current_node_id, neighbor_node_id, neighbor_value)\n                        )\n\n        if graph_type == \"networkx\":\n            G = nx.Graph()\n            G.add_nodes_from(nodes_to_add)\n            G.add_weighted_edges_from(edges_to_add)\n            return G\n        else:  # sparse matrix\n            edges_array = np.array(edges_to_add)\n            row_indices = edges_array[:, 0]\n            col_indices = edges_array[:, 1]\n            weights = edges_array[:, 2]\n\n            # Add reverse edges for symmetric matrix\n            from_idx = np.append(row_indices, col_indices)\n            to_idx = np.append(col_indices, row_indices)\n            weights = np.append(weights, weights)\n\n            return sp.coo_matrix(\n                (weights, (from_idx, to_idx)),\n                shape=(num_valid_pixels, num_valid_pixels),\n            ).tocsr()\n</code></pre>"},{"location":"api/processing/#gigaspatial.processing.add_area_in_meters","title":"<code>add_area_in_meters(gdf, area_column_name='area_in_meters')</code>","text":"<p>Calculate the area of (Multi)Polygon geometries in square meters and add it as a new column.</p>"},{"location":"api/processing/#gigaspatial.processing.add_area_in_meters--parameters","title":"Parameters:","text":"<p>gdf : geopandas.GeoDataFrame     GeoDataFrame containing (Multi)Polygon geometries. area_column_name : str, optional     Name of the new column to store the area values. Default is \"area_m2\".</p>"},{"location":"api/processing/#gigaspatial.processing.add_area_in_meters--returns","title":"Returns:","text":"<p>geopandas.GeoDataFrame     The input GeoDataFrame with an additional column for the area in square meters.</p>"},{"location":"api/processing/#gigaspatial.processing.add_area_in_meters--raises","title":"Raises:","text":"<p>ValueError     If the input GeoDataFrame does not contain (Multi)Polygon geometries.</p> Source code in <code>gigaspatial/processing/geo.py</code> <pre><code>def add_area_in_meters(\n    gdf: gpd.GeoDataFrame, area_column_name: str = \"area_in_meters\"\n) -&gt; gpd.GeoDataFrame:\n    \"\"\"\n    Calculate the area of (Multi)Polygon geometries in square meters and add it as a new column.\n\n    Parameters:\n    ----------\n    gdf : geopandas.GeoDataFrame\n        GeoDataFrame containing (Multi)Polygon geometries.\n    area_column_name : str, optional\n        Name of the new column to store the area values. Default is \"area_m2\".\n\n    Returns:\n    -------\n    geopandas.GeoDataFrame\n        The input GeoDataFrame with an additional column for the area in square meters.\n\n    Raises:\n    ------\n    ValueError\n        If the input GeoDataFrame does not contain (Multi)Polygon geometries.\n    \"\"\"\n    # Validate input geometries\n    if not all(gdf.geometry.geom_type.isin([\"Polygon\", \"MultiPolygon\"])):\n        raise ValueError(\n            \"Input GeoDataFrame must contain only Polygon or MultiPolygon geometries.\"\n        )\n\n    # Create a copy of the GeoDataFrame to avoid modifying the original\n    gdf_with_area = gdf.copy()\n\n    # Calculate the UTM CRS for accurate area calculation\n    try:\n        utm_crs = gdf_with_area.estimate_utm_crs()\n    except Exception as e:\n        LOGGER.warning(\n            f\"Warning: UTM CRS estimation failed, using Web Mercator. Error: {e}\"\n        )\n        utm_crs = \"EPSG:3857\"  # Fallback to Web Mercator\n\n    # Transform to UTM CRS and calculate the area in square meters\n    gdf_with_area[area_column_name] = gdf_with_area.to_crs(utm_crs).geometry.area\n\n    return gdf_with_area\n</code></pre>"},{"location":"api/processing/#gigaspatial.processing.add_spatial_jitter","title":"<code>add_spatial_jitter(df, columns=['latitude', 'longitude'], amount=0.0001, seed=None, copy=True)</code>","text":"<p>Add random jitter to duplicated geographic coordinates to create slight separation between overlapping points.</p>"},{"location":"api/processing/#gigaspatial.processing.add_spatial_jitter--parameters","title":"Parameters:","text":"<p>df : pandas.DataFrame     DataFrame containing geographic coordinates. columns : list of str, optional     Column names containing coordinates to jitter. Default is ['latitude', 'longitude']. amount : float or dict, optional     Amount of jitter to add. If float, same amount used for all columns.     If dict, specify amount per column, e.g., {'lat': 0.0001, 'lon': 0.0002}.     Default is 0.0001 (approximately 11 meters at the equator). seed : int, optional     Random seed for reproducibility. Default is None. copy : bool, optional     Whether to create a copy of the input DataFrame. Default is True.</p>"},{"location":"api/processing/#gigaspatial.processing.add_spatial_jitter--returns","title":"Returns:","text":"<p>pandas.DataFrame     DataFrame with jittered coordinates for previously duplicated points.</p>"},{"location":"api/processing/#gigaspatial.processing.add_spatial_jitter--raises","title":"Raises:","text":"<p>ValueError     If columns don't exist or jitter amount is invalid. TypeError     If input types are incorrect.</p> Source code in <code>gigaspatial/processing/geo.py</code> <pre><code>def add_spatial_jitter(\n    df: pd.DataFrame,\n    columns: List[str] = [\"latitude\", \"longitude\"],\n    amount: float = 0.0001,\n    seed=None,\n    copy=True,\n) -&gt; pd.DataFrame:\n    \"\"\"\n    Add random jitter to duplicated geographic coordinates to create slight separation\n    between overlapping points.\n\n    Parameters:\n    ----------\n    df : pandas.DataFrame\n        DataFrame containing geographic coordinates.\n    columns : list of str, optional\n        Column names containing coordinates to jitter. Default is ['latitude', 'longitude'].\n    amount : float or dict, optional\n        Amount of jitter to add. If float, same amount used for all columns.\n        If dict, specify amount per column, e.g., {'lat': 0.0001, 'lon': 0.0002}.\n        Default is 0.0001 (approximately 11 meters at the equator).\n    seed : int, optional\n        Random seed for reproducibility. Default is None.\n    copy : bool, optional\n        Whether to create a copy of the input DataFrame. Default is True.\n\n    Returns:\n    -------\n    pandas.DataFrame\n        DataFrame with jittered coordinates for previously duplicated points.\n\n    Raises:\n    ------\n    ValueError\n        If columns don't exist or jitter amount is invalid.\n    TypeError\n        If input types are incorrect.\n    \"\"\"\n\n    # Input validation\n    if not isinstance(df, pd.DataFrame):\n        raise TypeError(\"Input must be a pandas DataFrame\")\n\n    if not all(col in df.columns for col in columns):\n        raise ValueError(f\"Not all columns {columns} found in DataFrame\")\n\n    # Handle jitter amounts\n    if isinstance(amount, (int, float)):\n        if amount &lt;= 0:\n            raise ValueError(\"Jitter amount must be positive\")\n        jitter_amounts = {col: amount for col in columns}\n    elif isinstance(amount, dict):\n        if not all(col in amount for col in columns):\n            raise ValueError(\"Must specify jitter amount for each column\")\n        if not all(amt &gt; 0 for amt in amount.values()):\n            raise ValueError(\"All jitter amounts must be positive\")\n        jitter_amounts = amount\n    else:\n        raise TypeError(\"amount must be a number or dictionary\")\n\n    # Create copy if requested\n    df_work = df.copy() if copy else df\n\n    # Set random seed if provided\n    if seed is not None:\n        np.random.seed(seed)\n\n    try:\n        # Find duplicated coordinates\n        duplicate_mask = df_work.duplicated(subset=columns, keep=False)\n        n_duplicates = duplicate_mask.sum()\n\n        if n_duplicates &gt; 0:\n            # Add jitter to each column separately\n            for col in columns:\n                jitter = np.random.uniform(\n                    low=-jitter_amounts[col],\n                    high=jitter_amounts[col],\n                    size=n_duplicates,\n                )\n                df_work.loc[duplicate_mask, col] += jitter\n\n            # Validate results (ensure no remaining duplicates)\n            if df_work.duplicated(subset=columns, keep=False).any():\n                # If duplicates remain, recursively add more jitter\n                df_work = add_spatial_jitter(\n                    df_work,\n                    columns=columns,\n                    amount={col: amt * 2 for col, amt in jitter_amounts.items()},\n                    seed=seed,\n                    copy=False,\n                )\n\n        return df_work\n\n    except Exception as e:\n        raise RuntimeError(f\"Error during jittering operation: {str(e)}\")\n</code></pre>"},{"location":"api/processing/#gigaspatial.processing.aggregate_points_to_zones","title":"<code>aggregate_points_to_zones(points, zones, value_columns=None, aggregation='count', point_zone_predicate='within', zone_id_column='zone_id', output_suffix='', drop_geometry=False)</code>","text":"<p>Aggregate point data to zones with flexible aggregation methods.</p> <p>Parameters:</p> Name Type Description Default <code>points</code> <code>Union[DataFrame, GeoDataFrame]</code> <p>Point data to aggregate</p> required <code>zones</code> <code>GeoDataFrame</code> <p>Zones to aggregate points to</p> required <code>value_columns</code> <code>Optional[Union[str, List[str]]]</code> <p>Column(s) containing values to aggregate If None, only counts will be performed.</p> <code>None</code> <code>aggregation</code> <code>Union[str, Dict[str, str]]</code> <p>Aggregation method(s) to use: - Single string: Use same method for all columns (\"count\", \"mean\", \"sum\", \"min\", \"max\") - Dict: Map column names to aggregation methods</p> <code>'count'</code> <code>point_zone_predicate</code> <code>str</code> <p>Spatial predicate for point-to-zone relationship Options: \"within\", \"intersects\"</p> <code>'within'</code> <code>zone_id_column</code> <code>str</code> <p>Column in zones containing zone identifiers</p> <code>'zone_id'</code> <code>output_suffix</code> <code>str</code> <p>Suffix to add to output column names</p> <code>''</code> <code>drop_geometry</code> <code>bool</code> <p>Whether to drop the geometry column from output</p> <code>False</code> <p>Returns:</p> Type Description <code>GeoDataFrame</code> <p>gpd.GeoDataFrame: Zones with aggregated point values</p> Example <p>poi_counts = aggregate_points_to_zones(pois, zones, aggregation=\"count\") poi_value_mean = aggregate_points_to_zones( ...     pois, zones, value_columns=\"score\", aggregation=\"mean\" ... ) poi_multiple = aggregate_points_to_zones( ...     pois, zones, ...     value_columns=[\"score\", \"visits\"], ...     aggregation={\"score\": \"mean\", \"visits\": \"sum\"} ... )</p> Source code in <code>gigaspatial/processing/geo.py</code> <pre><code>def aggregate_points_to_zones(\n    points: Union[pd.DataFrame, gpd.GeoDataFrame],\n    zones: gpd.GeoDataFrame,\n    value_columns: Optional[Union[str, List[str]]] = None,\n    aggregation: Union[str, Dict[str, str]] = \"count\",\n    point_zone_predicate: str = \"within\",\n    zone_id_column: str = \"zone_id\",\n    output_suffix: str = \"\",\n    drop_geometry: bool = False,\n) -&gt; gpd.GeoDataFrame:\n    \"\"\"\n    Aggregate point data to zones with flexible aggregation methods.\n\n    Args:\n        points (Union[pd.DataFrame, gpd.GeoDataFrame]): Point data to aggregate\n        zones (gpd.GeoDataFrame): Zones to aggregate points to\n        value_columns (Optional[Union[str, List[str]]]): Column(s) containing values to aggregate\n            If None, only counts will be performed.\n        aggregation (Union[str, Dict[str, str]]): Aggregation method(s) to use:\n            - Single string: Use same method for all columns (\"count\", \"mean\", \"sum\", \"min\", \"max\")\n            - Dict: Map column names to aggregation methods\n        point_zone_predicate (str): Spatial predicate for point-to-zone relationship\n            Options: \"within\", \"intersects\"\n        zone_id_column (str): Column in zones containing zone identifiers\n        output_suffix (str): Suffix to add to output column names\n        drop_geometry (bool): Whether to drop the geometry column from output\n\n    Returns:\n        gpd.GeoDataFrame: Zones with aggregated point values\n\n    Example:\n        &gt;&gt;&gt; poi_counts = aggregate_points_to_zones(pois, zones, aggregation=\"count\")\n        &gt;&gt;&gt; poi_value_mean = aggregate_points_to_zones(\n        ...     pois, zones, value_columns=\"score\", aggregation=\"mean\"\n        ... )\n        &gt;&gt;&gt; poi_multiple = aggregate_points_to_zones(\n        ...     pois, zones,\n        ...     value_columns=[\"score\", \"visits\"],\n        ...     aggregation={\"score\": \"mean\", \"visits\": \"sum\"}\n        ... )\n    \"\"\"\n    # Input validation\n    if not isinstance(zones, gpd.GeoDataFrame):\n        raise TypeError(\"zones must be a GeoDataFrame\")\n\n    if zone_id_column not in zones.columns:\n        raise ValueError(f\"Zone ID column '{zone_id_column}' not found in zones\")\n\n    # Convert points to GeoDataFrame if necessary\n    if not isinstance(points, gpd.GeoDataFrame):\n        points_gdf = convert_to_geodataframe(points)\n    else:\n        points_gdf = points.copy()\n\n    # Ensure CRS match\n    if points_gdf.crs != zones.crs:\n        points_gdf = points_gdf.to_crs(zones.crs)\n\n    # Handle value columns\n    if value_columns is not None:\n        if isinstance(value_columns, str):\n            value_columns = [value_columns]\n\n        # Validate that all value columns exist\n        missing_cols = [col for col in value_columns if col not in points_gdf.columns]\n        if missing_cols:\n            raise ValueError(f\"Value columns not found in points data: {missing_cols}\")\n\n    # Handle aggregation method\n    agg_funcs = {}\n\n    if isinstance(aggregation, str):\n        if aggregation == \"count\":\n            # Special case for count (doesn't need value columns)\n            agg_funcs[\"__count\"] = \"count\"\n        elif value_columns is not None:\n            # Apply the same aggregation to all value columns\n            agg_funcs = {col: aggregation for col in value_columns}\n        else:\n            raise ValueError(\n                \"Value columns must be specified for aggregation methods other than 'count'\"\n            )\n    elif isinstance(aggregation, dict):\n        # Validate dictionary keys\n        if value_columns is None:\n            raise ValueError(\n                \"Value columns must be specified when using a dictionary of aggregation methods\"\n            )\n\n        missing_aggs = [col for col in value_columns if col not in aggregation]\n        extra_aggs = [col for col in aggregation if col not in value_columns]\n\n        if missing_aggs:\n            raise ValueError(f\"Missing aggregation methods for columns: {missing_aggs}\")\n        if extra_aggs:\n            raise ValueError(\n                f\"Aggregation methods specified for non-existent columns: {extra_aggs}\"\n            )\n\n        agg_funcs = aggregation\n    else:\n        raise TypeError(\"aggregation must be a string or dictionary\")\n\n    # Create a copy of the zones\n    result = zones.copy()\n\n    # Spatial join\n    joined = gpd.sjoin(points_gdf, zones, how=\"inner\", predicate=point_zone_predicate)\n\n    # Perform aggregation\n    if \"geometry\" in joined.columns and not all(\n        value == \"count\" for value in agg_funcs.values()\n    ):\n        # Drop geometry for non-count aggregations to avoid errors\n        joined = joined.drop(columns=[\"geometry\"])\n\n    if \"__count\" in agg_funcs:\n        # Count points per zone\n        counts = (\n            joined.groupby(zone_id_column)\n            .size()\n            .reset_index(name=f\"point_count{output_suffix}\")\n        )\n        result = result.merge(counts, on=zone_id_column, how=\"left\")\n        result[f\"point_count{output_suffix}\"] = (\n            result[f\"point_count{output_suffix}\"].fillna(0).astype(int)\n        )\n    else:\n        # Aggregate values\n        aggregated = joined.groupby(zone_id_column).agg(agg_funcs).reset_index()\n\n        # Rename columns to include aggregation method\n        if len(value_columns) &gt; 0:\n            # Handle MultiIndex columns from pandas aggregation\n            if isinstance(aggregated.columns, pd.MultiIndex):\n                aggregated.columns = [\n                    (\n                        f\"{col[0]}_{col[1]}{output_suffix}\"\n                        if col[0] != zone_id_column\n                        else zone_id_column\n                    )\n                    for col in aggregated.columns\n                ]\n\n            # Merge back to zones\n            result = result.merge(aggregated, on=zone_id_column, how=\"left\")\n\n            # Fill NaN values with zeros\n            for col in result.columns:\n                if (\n                    col != zone_id_column\n                    and col != \"geometry\"\n                    and pd.api.types.is_numeric_dtype(result[col])\n                ):\n                    result[col] = result[col].fillna(0)\n\n    if drop_geometry:\n        result = result.drop(columns=[\"geometry\"])\n        return result\n\n    return result\n</code></pre>"},{"location":"api/processing/#gigaspatial.processing.aggregate_polygons_to_zones","title":"<code>aggregate_polygons_to_zones(polygons, zones, value_columns, aggregation='sum', predicate='intersects', zone_id_column='zone_id', output_suffix='', drop_geometry=False)</code>","text":"<p>Aggregates polygon data to zones based on a specified spatial relationship.</p> <p>This function performs a spatial join between polygons and zones and then aggregates values from the polygons to their corresponding zones. The aggregation method depends on the <code>predicate</code> parameter, which determines the nature of the spatial relationship.</p> <p>Parameters:</p> Name Type Description Default <code>polygons</code> <code>Union[DataFrame, GeoDataFrame]</code> <p>Polygon data to aggregate. Must be a GeoDataFrame or convertible to one.</p> required <code>zones</code> <code>GeoDataFrame</code> <p>The target zones to which the polygon data will be aggregated.</p> required <code>value_columns</code> <code>Union[str, List[str]]</code> <p>The column(s) in <code>polygons</code> containing the numeric values to aggregate.</p> required <code>aggregation</code> <code>Union[str, Dict[str, str]]</code> <p>The aggregation method(s) to use. Can be a single string (e.g., \"sum\", \"mean\", \"max\") to apply the same method to all columns, or a dictionary mapping column names to aggregation methods (e.g., <code>{'population': 'sum'}</code>). Defaults to \"sum\".</p> <code>'sum'</code> <code>predicate</code> <code>Literal['intersects', 'within', 'fractional']</code> <p>The spatial relationship to use for aggregation: - \"intersects\": Aggregates values for any polygon that intersects a zone. - \"within\": Aggregates values for polygons entirely contained within a zone. - \"fractional\": Performs area-weighted aggregation. The value of a polygon   is distributed proportionally to the area of its overlap with each zone.   This requires calculating a UTM CRS for accurate area measurements. Defaults to \"intersects\".</p> <code>'intersects'</code> <code>zone_id_column</code> <code>str</code> <p>The name of the column in <code>zones</code> that contains the unique zone identifiers. Defaults to \"zone_id\".</p> <code>'zone_id'</code> <code>output_suffix</code> <code>str</code> <p>A suffix to add to the names of the new aggregated columns in the output GeoDataFrame. Defaults to \"\".</p> <code>''</code> <code>drop_geometry</code> <code>bool</code> <p>If True, the geometry column will be dropped from the output GeoDataFrame. Defaults to False.</p> <code>False</code> <p>Returns:</p> Type Description <code>GeoDataFrame</code> <p>gpd.GeoDataFrame: The <code>zones</code> GeoDataFrame with new columns containing the aggregated values. Zones with no intersecting or contained polygons will have <code>0</code> values.</p> <p>Raises:</p> Type Description <code>TypeError</code> <p>If <code>zones</code> is not a GeoDataFrame or <code>polygons</code> cannot be converted.</p> <code>ValueError</code> <p>If <code>zone_id_column</code> or any <code>value_columns</code> are not found, or         if the geometry types in <code>polygons</code> are not polygons.</p> <code>RuntimeError</code> <p>If an error occurs during the area-weighted aggregation process.</p> Example <p>import geopandas as gpd</p> Source code in <code>gigaspatial/processing/geo.py</code> <pre><code>def aggregate_polygons_to_zones(\n    polygons: Union[pd.DataFrame, gpd.GeoDataFrame],\n    zones: gpd.GeoDataFrame,\n    value_columns: Union[str, List[str]],\n    aggregation: Union[str, Dict[str, str]] = \"sum\",\n    predicate: Literal[\"intersects\", \"within\", \"fractional\"] = \"intersects\",\n    zone_id_column: str = \"zone_id\",\n    output_suffix: str = \"\",\n    drop_geometry: bool = False,\n) -&gt; gpd.GeoDataFrame:\n    \"\"\"\n    Aggregates polygon data to zones based on a specified spatial relationship.\n\n    This function performs a spatial join between polygons and zones and then\n    aggregates values from the polygons to their corresponding zones. The aggregation\n    method depends on the `predicate` parameter, which determines the nature of the\n    spatial relationship.\n\n    Args:\n        polygons (Union[pd.DataFrame, gpd.GeoDataFrame]):\n            Polygon data to aggregate. Must be a GeoDataFrame or convertible to one.\n        zones (gpd.GeoDataFrame):\n            The target zones to which the polygon data will be aggregated.\n        value_columns (Union[str, List[str]]):\n            The column(s) in `polygons` containing the numeric values to aggregate.\n        aggregation (Union[str, Dict[str, str]], optional):\n            The aggregation method(s) to use. Can be a single string (e.g., \"sum\",\n            \"mean\", \"max\") to apply the same method to all columns, or a dictionary\n            mapping column names to aggregation methods (e.g., `{'population': 'sum'}`).\n            Defaults to \"sum\".\n        predicate (Literal[\"intersects\", \"within\", \"fractional\"], optional):\n            The spatial relationship to use for aggregation:\n            - \"intersects\": Aggregates values for any polygon that intersects a zone.\n            - \"within\": Aggregates values for polygons entirely contained within a zone.\n            - \"fractional\": Performs area-weighted aggregation. The value of a polygon\n              is distributed proportionally to the area of its overlap with each zone.\n              This requires calculating a UTM CRS for accurate area measurements.\n            Defaults to \"intersects\".\n        zone_id_column (str, optional):\n            The name of the column in `zones` that contains the unique zone identifiers.\n            Defaults to \"zone_id\".\n        output_suffix (str, optional):\n            A suffix to add to the names of the new aggregated columns in the output\n            GeoDataFrame. Defaults to \"\".\n        drop_geometry (bool, optional):\n            If True, the geometry column will be dropped from the output GeoDataFrame.\n            Defaults to False.\n\n    Returns:\n        gpd.GeoDataFrame:\n            The `zones` GeoDataFrame with new columns containing the aggregated values.\n            Zones with no intersecting or contained polygons will have `0` values.\n\n    Raises:\n        TypeError: If `zones` is not a GeoDataFrame or `polygons` cannot be converted.\n        ValueError: If `zone_id_column` or any `value_columns` are not found, or\n                    if the geometry types in `polygons` are not polygons.\n        RuntimeError: If an error occurs during the area-weighted aggregation process.\n\n    Example:\n        &gt;&gt;&gt; import geopandas as gpd\n        &gt;&gt;&gt; # Assuming 'landuse_polygons' and 'grid_zones' are GeoDataFrames\n        &gt;&gt;&gt; # Aggregate total population within each grid zone using area-weighting\n        &gt;&gt;&gt; pop_by_zone = aggregate_polygons_to_zones(\n        ...     landuse_polygons,\n        ...     grid_zones,\n        ...     value_columns=\"population\",\n        ...     predicate=\"fractional\",\n        ...     aggregation=\"sum\",\n        ...     output_suffix=\"_pop\"\n        ... )\n        &gt;&gt;&gt; # Aggregate the count of landuse parcels intersecting each zone\n        &gt;&gt;&gt; count_by_zone = aggregate_polygons_to_zones(\n        ...     landuse_polygons,\n        ...     grid_zones,\n        ...     value_columns=\"parcel_id\",\n        ...     predicate=\"intersects\",\n        ...     aggregation=\"count\"\n        ... )\n    \"\"\"\n    # Input validation\n    if not isinstance(zones, gpd.GeoDataFrame):\n        raise TypeError(\"zones must be a GeoDataFrame\")\n\n    if zones.empty:\n        raise ValueError(\"zones GeoDataFrame is empty\")\n\n    if zone_id_column not in zones.columns:\n        raise ValueError(f\"Zone ID column '{zone_id_column}' not found in zones\")\n\n    if predicate not in [\"intersects\", \"within\", \"fractional\"]:\n        raise ValueError(\n            f\"Unsupported predicate: {predicate}. Predicate can be one of `intersects`, `within`, `fractional`\"\n        )\n\n    # Convert polygons to GeoDataFrame if necessary\n    if not isinstance(polygons, gpd.GeoDataFrame):\n        try:\n            polygons_gdf = convert_to_geodataframe(polygons)\n        except Exception as e:\n            raise TypeError(\n                f\"polygons must be a GeoDataFrame or convertible to one: {e}\"\n            )\n    else:\n        polygons_gdf = polygons.copy()\n\n    if polygons_gdf.empty:\n        LOGGER.warning(\"Empty polygons GeoDataFrame provided\")\n        return zones\n\n    # Validate geometry types\n    non_polygon_geoms = [\n        geom_type\n        for geom_type in polygons_gdf.geometry.geom_type.unique()\n        if geom_type not in [\"Polygon\", \"MultiPolygon\"]\n    ]\n    if non_polygon_geoms:\n        raise ValueError(\n            f\"Input contains non-polygon geometries: {non_polygon_geoms}. \"\n            \"Use aggregate_points_to_zones for point data.\"\n        )\n\n    # Process value columns\n    if isinstance(value_columns, str):\n        value_columns = [value_columns]\n\n    # Validate that all value columns exist\n    missing_cols = [col for col in value_columns if col not in polygons_gdf.columns]\n    if missing_cols:\n        raise ValueError(f\"Value columns not found in polygons data: {missing_cols}\")\n\n    # Check for column name conflicts with zone_id_column\n    if zone_id_column in polygons_gdf.columns:\n        raise ValueError(\n            f\"Column name conflict: polygons DataFrame contains column '{zone_id_column}' \"\n            f\"which conflicts with the zone identifier column. Please rename this column \"\n            f\"in the polygons data to avoid confusion.\"\n        )\n\n    # Ensure CRS match\n    if polygons_gdf.crs != zones.crs:\n        polygons_gdf = polygons_gdf.to_crs(zones.crs)\n\n    # Handle aggregation method\n    agg_funcs = _process_aggregation_methods(aggregation, value_columns)\n\n    # Prepare minimal zones for spatial operations (only zone_id_column and geometry)\n    minimal_zones = zones[[zone_id_column, \"geometry\"]].copy()\n\n    if predicate == \"fractional\":\n        aggregated_data = _fractional_aggregation(\n            polygons_gdf, minimal_zones, value_columns, agg_funcs, zone_id_column\n        )\n    else:\n        aggregated_data = _simple_aggregation(\n            polygons_gdf,\n            minimal_zones,\n            value_columns,\n            agg_funcs,\n            zone_id_column,\n            predicate,\n        )\n\n    # Merge aggregated results back to complete zones data\n    result = zones.merge(\n        aggregated_data[[col for col in aggregated_data.columns if col != \"geometry\"]],\n        on=zone_id_column,\n        how=\"left\",\n    )\n\n    # Fill NaN values with zeros for the newly aggregated columns only\n    aggregated_cols = [col for col in result.columns if col not in zones.columns]\n    for col in aggregated_cols:\n        if pd.api.types.is_numeric_dtype(result[col]):\n            result[col] = result[col].fillna(0)\n\n    # Apply output suffix consistently to result columns only\n    if output_suffix:\n        rename_dict = {col: f\"{col}{output_suffix}\" for col in aggregated_cols}\n        result = result.rename(columns=rename_dict)\n\n    if drop_geometry:\n        result = result.drop(columns=[\"geometry\"])\n\n    return result\n</code></pre>"},{"location":"api/processing/#gigaspatial.processing.aggregate_polygons_to_zones--assuming-landuse_polygons-and-grid_zones-are-geodataframes","title":"Assuming 'landuse_polygons' and 'grid_zones' are GeoDataFrames","text":""},{"location":"api/processing/#gigaspatial.processing.aggregate_polygons_to_zones--aggregate-total-population-within-each-grid-zone-using-area-weighting","title":"Aggregate total population within each grid zone using area-weighting","text":"<p>pop_by_zone = aggregate_polygons_to_zones( ...     landuse_polygons, ...     grid_zones, ...     value_columns=\"population\", ...     predicate=\"fractional\", ...     aggregation=\"sum\", ...     output_suffix=\"_pop\" ... )</p>"},{"location":"api/processing/#gigaspatial.processing.aggregate_polygons_to_zones--aggregate-the-count-of-landuse-parcels-intersecting-each-zone","title":"Aggregate the count of landuse parcels intersecting each zone","text":"<p>count_by_zone = aggregate_polygons_to_zones( ...     landuse_polygons, ...     grid_zones, ...     value_columns=\"parcel_id\", ...     predicate=\"intersects\", ...     aggregation=\"count\" ... )</p>"},{"location":"api/processing/#gigaspatial.processing.annotate_with_admin_regions","title":"<code>annotate_with_admin_regions(gdf, country_code, data_store=None, admin_id_column_suffix='_giga')</code>","text":"<p>Annotate a GeoDataFrame with administrative region information.</p> <p>Performs a spatial join between the input points and administrative boundaries at levels 1 and 2, resolving conflicts when points intersect multiple admin regions.</p> <p>Parameters:</p> Name Type Description Default <code>gdf</code> <code>GeoDataFrame</code> <p>GeoDataFrame containing points to annotate</p> required <code>country_code</code> <code>str</code> <p>Country code for administrative boundaries</p> required <code>data_store</code> <code>Optional[DataStore]</code> <p>Optional DataStore for loading admin boundary data</p> <code>None</code> <p>Returns:</p> Type Description <code>GeoDataFrame</code> <p>GeoDataFrame with added administrative region columns</p> Source code in <code>gigaspatial/processing/geo.py</code> <pre><code>def annotate_with_admin_regions(\n    gdf: gpd.GeoDataFrame,\n    country_code: str,\n    data_store: Optional[DataStore] = None,\n    admin_id_column_suffix=\"_giga\",\n) -&gt; gpd.GeoDataFrame:\n    \"\"\"\n    Annotate a GeoDataFrame with administrative region information.\n\n    Performs a spatial join between the input points and administrative boundaries\n    at levels 1 and 2, resolving conflicts when points intersect multiple admin regions.\n\n    Args:\n        gdf: GeoDataFrame containing points to annotate\n        country_code: Country code for administrative boundaries\n        data_store: Optional DataStore for loading admin boundary data\n\n    Returns:\n        GeoDataFrame with added administrative region columns\n    \"\"\"\n    from gigaspatial.handlers.boundaries import AdminBoundaries\n\n    if not isinstance(gdf, gpd.GeoDataFrame):\n        raise TypeError(\"gdf must be a GeoDataFrame\")\n\n    if gdf.empty:\n        LOGGER.warning(\"Empty GeoDataFrame provided, returning as-is\")\n        return gdf\n\n    # read country admin data\n    admin1_data = AdminBoundaries.create(\n        country_code=country_code, admin_level=1, data_store=data_store\n    ).to_geodataframe()\n\n    admin1_data.rename(\n        columns={\"id\": f\"admin1_id{admin_id_column_suffix}\", \"name\": \"admin1\"},\n        inplace=True,\n    )\n    admin1_data.drop(columns=[\"name_en\", \"parent_id\", \"country_code\"], inplace=True)\n\n    admin2_data = AdminBoundaries.create(\n        country_code=country_code, admin_level=2, data_store=data_store\n    ).to_geodataframe()\n\n    admin2_data.rename(\n        columns={\n            \"id\": f\"admin2_id{admin_id_column_suffix}\",\n            \"parent_id\": f\"admin1_id{admin_id_column_suffix}\",\n            \"name\": \"admin2\",\n        },\n        inplace=True,\n    )\n    admin2_data.drop(columns=[\"name_en\", \"country_code\"], inplace=True)\n\n    # Join dataframes based on 'admin1_id_giga'\n    admin_data = admin2_data.merge(\n        admin1_data[[f\"admin1_id{admin_id_column_suffix}\", \"admin1\", \"geometry\"]],\n        left_on=f\"admin1_id{admin_id_column_suffix}\",\n        right_on=f\"admin1_id{admin_id_column_suffix}\",\n        how=\"outer\",\n    )\n\n    admin_data[\"geometry\"] = admin_data.apply(\n        lambda x: x.geometry_x if x.geometry_x else x.geometry_y, axis=1\n    )\n\n    admin_data = gpd.GeoDataFrame(\n        admin_data.drop(columns=[\"geometry_x\", \"geometry_y\"]),\n        geometry=\"geometry\",\n        crs=4326,\n    )\n\n    admin_data[\"admin2\"].fillna(\"Unknown\", inplace=True)\n    admin_data[f\"admin2_id{admin_id_column_suffix}\"] = admin_data[\n        f\"admin2_id{admin_id_column_suffix}\"\n    ].replace({np.nan: None})\n\n    if gdf.crs is None:\n        LOGGER.warning(\"Input GeoDataFrame has no CRS, assuming EPSG:4326\")\n        gdf.set_crs(epsg=4326, inplace=True)\n    elif gdf.crs != \"EPSG:4326\":\n        LOGGER.info(f\"Reprojecting from {gdf.crs} to EPSG:4326\")\n        gdf = gdf.to_crs(epsg=4326)\n\n    # spatial join gdf to admins\n    gdf_w_admins = gdf.copy().sjoin(\n        admin_data,\n        how=\"left\",\n        predicate=\"intersects\",\n    )\n\n    # Check for duplicates caused by points intersecting multiple polygons\n    if len(gdf_w_admins) != len(gdf):\n        LOGGER.warning(\n            \"Some points intersect multiple administrative boundaries. Resolving conflicts...\"\n        )\n\n        # Group by original index and select the closest admin area for ties\n        gdf_w_admins[\"distance\"] = gdf_w_admins.apply(\n            lambda row: row.geometry.distance(\n                admin_data.loc[row.index_right, \"geometry\"].centroid\n            ),\n            axis=1,\n        )\n\n        # For points with multiple matches, keep the closest polygon\n        gdf_w_admins = gdf_w_admins.loc[\n            gdf_w_admins.groupby(gdf.index)[\"distance\"].idxmin()\n        ].drop(columns=\"distance\")\n\n    # Drop unnecessary columns and reset the index\n    gdf_w_admins = gdf_w_admins.drop(columns=\"index_right\").reset_index(drop=True)\n\n    return gdf_w_admins\n</code></pre>"},{"location":"api/processing/#gigaspatial.processing.assign_id","title":"<code>assign_id(df, required_columns, id_column='id')</code>","text":"<p>Generate IDs for any entity type in a pandas DataFrame.</p> <p>Parameters:</p> Name Type Description Default <code>df</code> <code>DataFrame</code> <p>Input DataFrame containing entity data</p> required <code>required_columns</code> <code>List[str]</code> <p>List of column names required for ID generation</p> required <code>id_column</code> <code>str</code> <p>Name for the id column that will be generated</p> <code>'id'</code> <p>Returns:</p> Type Description <code>DataFrame</code> <p>pd.DataFrame: DataFrame with generated id column</p> Source code in <code>gigaspatial/processing/utils.py</code> <pre><code>def assign_id(\n    df: pd.DataFrame, required_columns: List[str], id_column: str = \"id\"\n) -&gt; pd.DataFrame:\n    \"\"\"\n    Generate IDs for any entity type in a pandas DataFrame.\n\n    Args:\n        df (pd.DataFrame): Input DataFrame containing entity data\n        required_columns (List[str]): List of column names required for ID generation\n        id_column (str): Name for the id column that will be generated\n\n    Returns:\n        pd.DataFrame: DataFrame with generated id column\n    \"\"\"\n    # Create a copy to avoid modifying the original DataFrame\n    df = df.copy()\n\n    # Check if ID column exists, if not create it with None values\n    if id_column not in df.columns:\n        df[id_column] = None\n\n    # Check required columns exist\n    if not all(col in df.columns for col in required_columns):\n        return df\n\n    # Create identifier concat for UUID generation\n    df[\"identifier_concat\"] = (\n        df[required_columns].astype(str).fillna(\"\").agg(\"\".join, axis=1)\n    )\n\n    # Generate UUIDs only where all required fields are present and no existing ID\n    mask = df[id_column].isna()\n    for col in required_columns:\n        mask &amp;= df[col].notna()\n\n    # Apply UUID generation only where mask is True\n    df.loc[mask, id_column] = df.loc[mask, \"identifier_concat\"].apply(\n        lambda x: str(uuid.uuid3(uuid.NAMESPACE_DNS, x))\n    )\n\n    # Drop temporary column\n    df = df.drop(columns=[\"identifier_concat\"])\n\n    return df\n</code></pre>"},{"location":"api/processing/#gigaspatial.processing.buffer_geodataframe","title":"<code>buffer_geodataframe(gdf, buffer_distance_meters, cap_style='round', copy=True)</code>","text":"<p>Buffers a GeoDataFrame with a given buffer distance in meters.</p> <ul> <li>gdf : geopandas.GeoDataFrame     The GeoDataFrame to be buffered.</li> <li>buffer_distance_meters : float     The buffer distance in meters.</li> <li>cap_style : str, optional     The style of caps. round, flat, square. Default is round.</li> </ul> <ul> <li>geopandas.GeoDataFrame     The buffered GeoDataFrame.</li> </ul> Source code in <code>gigaspatial/processing/geo.py</code> <pre><code>def buffer_geodataframe(\n    gdf: gpd.GeoDataFrame,\n    buffer_distance_meters: Union[float, np.array, pd.Series],\n    cap_style: Literal[\"round\", \"square\", \"flat\"] = \"round\",\n    copy=True,\n) -&gt; gpd.GeoDataFrame:\n    \"\"\"\n    Buffers a GeoDataFrame with a given buffer distance in meters.\n\n    Parameters:\n    - gdf : geopandas.GeoDataFrame\n        The GeoDataFrame to be buffered.\n    - buffer_distance_meters : float\n        The buffer distance in meters.\n    - cap_style : str, optional\n        The style of caps. round, flat, square. Default is round.\n\n    Returns:\n    - geopandas.GeoDataFrame\n        The buffered GeoDataFrame.\n    \"\"\"\n\n    # Input validation\n    if not isinstance(gdf, gpd.GeoDataFrame):\n        raise TypeError(\"Input must be a GeoDataFrame\")\n\n    if cap_style not in [\"round\", \"square\", \"flat\"]:\n        raise ValueError(\"cap_style must be round, flat or square.\")\n\n    if gdf.crs is None:\n        raise ValueError(\"Input GeoDataFrame must have a defined CRS\")\n\n    # Create a copy if requested\n    gdf_work = gdf.copy() if copy else gdf\n\n    # Store input CRS\n    input_crs = gdf_work.crs\n\n    try:\n        try:\n            utm_crs = gdf_work.estimate_utm_crs()\n        except Exception as e:\n            LOGGER.warning(\n                f\"Warning: UTM CRS estimation failed, using Web Mercator. Error: {e}\"\n            )\n            utm_crs = \"EPSG:3857\"  # Fallback to Web Mercator\n\n        # Transform to UTM, create buffer, and transform back\n        gdf_work = gdf_work.to_crs(utm_crs)\n        gdf_work[\"geometry\"] = gdf_work[\"geometry\"].buffer(\n            distance=buffer_distance_meters, cap_style=cap_style\n        )\n        gdf_work = gdf_work.to_crs(input_crs)\n\n        return gdf_work\n\n    except Exception as e:\n        raise RuntimeError(f\"Error during buffering operation: {str(e)}\")\n</code></pre>"},{"location":"api/processing/#gigaspatial.processing.calculate_pixels_at_location","title":"<code>calculate_pixels_at_location(gdf, resolution, bbox_size=300, crs='EPSG:3857')</code>","text":"<p>Calculates the number of pixels required to cover a given bounding box around a geographic coordinate, given a resolution in meters per pixel.</p> <p>Parameters:</p> Name Type Description Default <code>gdf</code> <p>a geodataframe with Point geometries that are geographic coordinates</p> required <code>resolution</code> <code>float</code> <p>Desired resolution (meters per pixel).</p> required <code>bbox_size</code> <code>float</code> <p>Bounding box size in meters (default 300m x 300m).</p> <code>300</code> <code>crs</code> <code>str</code> <p>Target projection (default is EPSG:3857).</p> <code>'EPSG:3857'</code> <p>Returns:</p> Name Type Description <code>int</code> <p>Number of pixels per side (width and height).</p> Source code in <code>gigaspatial/processing/sat_images.py</code> <pre><code>def calculate_pixels_at_location(gdf, resolution, bbox_size=300, crs=\"EPSG:3857\"):\n    \"\"\"\n    Calculates the number of pixels required to cover a given bounding box\n    around a geographic coordinate, given a resolution in meters per pixel.\n\n    Parameters:\n        gdf: a geodataframe with Point geometries that are geographic coordinates\n        resolution (float): Desired resolution (meters per pixel).\n        bbox_size (float): Bounding box size in meters (default 300m x 300m).\n        crs (str): Target projection (default is EPSG:3857).\n\n    Returns:\n        int: Number of pixels per side (width and height).\n    \"\"\"\n\n    # Calculate avg lat and lon\n    lon = gdf.geometry.x.mean()\n    lat = gdf.geometry.y.mean()\n\n    # Define projections\n    wgs84 = pyproj.CRS(\"EPSG:4326\")  # Geographic coordinate system\n    mercator = pyproj.CRS(crs)  # Target CRS (EPSG:3857)\n\n    # Transform the center coordinate to EPSG:3857\n    transformer = pyproj.Transformer.from_crs(wgs84, mercator, always_xy=True)\n    x, y = transformer.transform(lon, lat)\n\n    # Calculate scale factor (distortion) at given latitude\n    scale_factor = np.cos(np.radians(lat))  # Mercator scale correction\n\n    # Adjust the effective resolution\n    effective_resolution = resolution * scale_factor\n\n    # Compute number of pixels per side\n    pixels = bbox_size / effective_resolution\n    return int(round(pixels))\n</code></pre>"},{"location":"api/processing/#gigaspatial.processing.convert_to_geodataframe","title":"<code>convert_to_geodataframe(data, lat_col=None, lon_col=None, crs='EPSG:4326')</code>","text":"<p>Convert a pandas DataFrame to a GeoDataFrame, either from latitude/longitude columns or from a WKT geometry column.</p>"},{"location":"api/processing/#gigaspatial.processing.convert_to_geodataframe--parameters","title":"Parameters:","text":"<p>data : pandas.DataFrame     Input DataFrame containing either lat/lon columns or a geometry column. lat_col : str, optional     Name of the latitude column. Default is 'lat'. lon_col : str, optional     Name of the longitude column. Default is 'lon'. crs : str or pyproj.CRS, optional     Coordinate Reference System of the geometry data. Default is 'EPSG:4326'.</p>"},{"location":"api/processing/#gigaspatial.processing.convert_to_geodataframe--returns","title":"Returns:","text":"<p>geopandas.GeoDataFrame     A GeoDataFrame containing the input data with a geometry column.</p>"},{"location":"api/processing/#gigaspatial.processing.convert_to_geodataframe--raises","title":"Raises:","text":"<p>TypeError     If input is not a pandas DataFrame. ValueError     If required columns are missing or contain invalid data.</p> Source code in <code>gigaspatial/processing/geo.py</code> <pre><code>def convert_to_geodataframe(\n    data: pd.DataFrame, lat_col: str = None, lon_col: str = None, crs=\"EPSG:4326\"\n) -&gt; gpd.GeoDataFrame:\n    \"\"\"\n    Convert a pandas DataFrame to a GeoDataFrame, either from latitude/longitude columns\n    or from a WKT geometry column.\n\n    Parameters:\n    ----------\n    data : pandas.DataFrame\n        Input DataFrame containing either lat/lon columns or a geometry column.\n    lat_col : str, optional\n        Name of the latitude column. Default is 'lat'.\n    lon_col : str, optional\n        Name of the longitude column. Default is 'lon'.\n    crs : str or pyproj.CRS, optional\n        Coordinate Reference System of the geometry data. Default is 'EPSG:4326'.\n\n    Returns:\n    -------\n    geopandas.GeoDataFrame\n        A GeoDataFrame containing the input data with a geometry column.\n\n    Raises:\n    ------\n    TypeError\n        If input is not a pandas DataFrame.\n    ValueError\n        If required columns are missing or contain invalid data.\n    \"\"\"\n\n    # Input validation\n    if not isinstance(data, pd.DataFrame):\n        raise TypeError(\"Input 'data' must be a pandas DataFrame\")\n\n    # Create a copy to avoid modifying the input\n    df = data.copy()\n\n    try:\n        if \"geometry\" not in df.columns:\n            # If column names not provided, try to detect them\n            if lat_col is None or lon_col is None:\n                try:\n                    detected_lat, detected_lon = detect_coordinate_columns(df)\n                    lat_col = lat_col or detected_lat\n                    lon_col = lon_col or detected_lon\n                except ValueError as e:\n                    raise ValueError(\n                        f\"Could not automatically detect coordinate columns and no \"\n                        f\"'geometry' column found. Error: {str(e)}\"\n                    )\n\n            # Validate latitude/longitude columns exist\n            if lat_col not in df.columns or lon_col not in df.columns:\n                raise ValueError(\n                    f\"Could not find columns: {lat_col} and/or {lon_col} in the DataFrame\"\n                )\n\n            # Check for missing values\n            if df[lat_col].isna().any() or df[lon_col].isna().any():\n                raise ValueError(\n                    f\"Missing values found in {lat_col} and/or {lon_col} columns\"\n                )\n\n            # Create geometry from lat/lon\n            geometry = gpd.points_from_xy(x=df[lon_col], y=df[lat_col])\n\n        else:\n            # Check if geometry column already contains valid geometries\n            if df[\"geometry\"].apply(lambda x: isinstance(x, base.BaseGeometry)).all():\n                geometry = df[\"geometry\"]\n            elif df[\"geometry\"].apply(lambda x: isinstance(x, str)).all():\n                # Convert WKT strings to geometry objects\n                geometry = df[\"geometry\"].apply(wkt.loads)\n            else:\n                raise ValueError(\n                    \"Invalid geometry format: contains mixed or unsupported types\"\n                )\n\n        # drop the WKT column if conversion was done\n        if (\n            \"geometry\" in df.columns\n            and not df[\"geometry\"]\n            .apply(lambda x: isinstance(x, base.BaseGeometry))\n            .all()\n        ):\n            df = df.drop(columns=[\"geometry\"])\n\n        return gpd.GeoDataFrame(df, geometry=geometry, crs=crs)\n\n    except Exception as e:\n        raise RuntimeError(f\"Error converting to GeoDataFrame: {str(e)}\")\n</code></pre>"},{"location":"api/processing/#gigaspatial.processing.detect_coordinate_columns","title":"<code>detect_coordinate_columns(data, lat_keywords=None, lon_keywords=None, case_sensitive=False)</code>","text":"<p>Detect latitude and longitude columns in a DataFrame using keyword matching.</p>"},{"location":"api/processing/#gigaspatial.processing.detect_coordinate_columns--parameters","title":"Parameters:","text":"<p>data : pandas.DataFrame     DataFrame to search for coordinate columns. lat_keywords : list of str, optional     Keywords for identifying latitude columns. If None, uses default keywords. lon_keywords : list of str, optional     Keywords for identifying longitude columns. If None, uses default keywords. case_sensitive : bool, optional     Whether to perform case-sensitive matching. Default is False.</p>"},{"location":"api/processing/#gigaspatial.processing.detect_coordinate_columns--returns","title":"Returns:","text":"<p>tuple[str, str]     Names of detected (latitude, longitude) columns.</p>"},{"location":"api/processing/#gigaspatial.processing.detect_coordinate_columns--raises","title":"Raises:","text":"<p>ValueError     If no unique pair of latitude/longitude columns can be found. TypeError     If input data is not a pandas DataFrame.</p> Source code in <code>gigaspatial/processing/geo.py</code> <pre><code>def detect_coordinate_columns(\n    data, lat_keywords=None, lon_keywords=None, case_sensitive=False\n) -&gt; Tuple[str, str]:\n    \"\"\"\n    Detect latitude and longitude columns in a DataFrame using keyword matching.\n\n    Parameters:\n    ----------\n    data : pandas.DataFrame\n        DataFrame to search for coordinate columns.\n    lat_keywords : list of str, optional\n        Keywords for identifying latitude columns. If None, uses default keywords.\n    lon_keywords : list of str, optional\n        Keywords for identifying longitude columns. If None, uses default keywords.\n    case_sensitive : bool, optional\n        Whether to perform case-sensitive matching. Default is False.\n\n    Returns:\n    -------\n    tuple[str, str]\n        Names of detected (latitude, longitude) columns.\n\n    Raises:\n    ------\n    ValueError\n        If no unique pair of latitude/longitude columns can be found.\n    TypeError\n        If input data is not a pandas DataFrame.\n    \"\"\"\n\n    # Default keywords if none provided\n    default_lat = [\n        \"latitude\",\n        \"lat\",\n        \"y\",\n        \"lat_\",\n        \"lat(s)\",\n        \"_lat\",\n        \"ylat\",\n        \"latitude_y\",\n    ]\n    default_lon = [\n        \"longitude\",\n        \"lon\",\n        \"long\",\n        \"x\",\n        \"lon_\",\n        \"lon(e)\",\n        \"long(e)\",\n        \"_lon\",\n        \"xlon\",\n        \"longitude_x\",\n    ]\n\n    lat_keywords = lat_keywords or default_lat\n    lon_keywords = lon_keywords or default_lon\n\n    # Input validation\n    if not isinstance(data, pd.DataFrame):\n        raise TypeError(\"Input must be a pandas DataFrame\")\n\n    if not data.columns.is_unique:\n        raise ValueError(\"DataFrame contains duplicate column names\")\n\n    def create_pattern(keywords):\n        \"\"\"Create regex pattern from keywords.\"\"\"\n        return \"|\".join(rf\"\\b{re.escape(keyword)}\\b\" for keyword in keywords)\n\n    def find_matching_columns(columns, pattern, case_sensitive) -&gt; List:\n        \"\"\"Find columns matching the pattern.\"\"\"\n        flags = 0 if case_sensitive else re.IGNORECASE\n        return [col for col in columns if re.search(pattern, col, flags=flags)]\n\n    try:\n        # Create patterns\n        lat_pattern = create_pattern(lat_keywords)\n        lon_pattern = create_pattern(lon_keywords)\n\n        # Find matching columns\n        lat_cols = find_matching_columns(data.columns, lat_pattern, case_sensitive)\n        lon_cols = find_matching_columns(data.columns, lon_pattern, case_sensitive)\n\n        # Remove any longitude matches from latitude columns and vice versa\n        lat_cols = [col for col in lat_cols if col not in lon_cols]\n        lon_cols = [col for col in lon_cols if col not in lat_cols]\n\n        # Detailed error messages based on what was found\n        if not lat_cols and not lon_cols:\n            columns_list = \"\\n\".join(f\"- {col}\" for col in data.columns)\n            raise ValueError(\n                f\"No latitude or longitude columns found. Available columns are:\\n{columns_list}\\n\"\n                f\"Consider adding more keywords or checking column names.\"\n            )\n\n        if not lat_cols:\n            found_lons = \", \".join(lon_cols)\n            raise ValueError(\n                f\"Found longitude columns ({found_lons}) but no latitude columns. \"\n                \"Check latitude keywords or column names.\"\n            )\n\n        if not lon_cols:\n            found_lats = \", \".join(lat_cols)\n            raise ValueError(\n                f\"Found latitude columns ({found_lats}) but no longitude columns. \"\n                \"Check longitude keywords or column names.\"\n            )\n\n        if len(lat_cols) &gt; 1 or len(lon_cols) &gt; 1:\n            raise ValueError(\n                f\"Multiple possible coordinate columns found:\\n\"\n                f\"Latitude candidates: {lat_cols}\\n\"\n                f\"Longitude candidates: {lon_cols}\\n\"\n                \"Please specify more precise keywords.\"\n            )\n\n        return lat_cols[0], lon_cols[0]\n\n    except Exception as e:\n        if isinstance(e, ValueError):\n            raise\n        raise RuntimeError(f\"Error detecting coordinate columns: {str(e)}\")\n</code></pre>"},{"location":"api/processing/#gigaspatial.processing.get_centroids","title":"<code>get_centroids(gdf)</code>","text":"<p>Calculate the centroids of a (Multi)Polygon GeoDataFrame.</p>"},{"location":"api/processing/#gigaspatial.processing.get_centroids--parameters","title":"Parameters:","text":"<p>gdf : geopandas.GeoDataFrame     GeoDataFrame containing (Multi)Polygon geometries.</p>"},{"location":"api/processing/#gigaspatial.processing.get_centroids--returns","title":"Returns:","text":"<p>geopandas.GeoDataFrame     A new GeoDataFrame with Point geometries representing the centroids.</p>"},{"location":"api/processing/#gigaspatial.processing.get_centroids--raises","title":"Raises:","text":"<p>ValueError     If the input GeoDataFrame does not contain (Multi)Polygon geometries.</p> Source code in <code>gigaspatial/processing/geo.py</code> <pre><code>def get_centroids(gdf: gpd.GeoDataFrame) -&gt; gpd.GeoDataFrame:\n    \"\"\"\n    Calculate the centroids of a (Multi)Polygon GeoDataFrame.\n\n    Parameters:\n    ----------\n    gdf : geopandas.GeoDataFrame\n        GeoDataFrame containing (Multi)Polygon geometries.\n\n    Returns:\n    -------\n    geopandas.GeoDataFrame\n        A new GeoDataFrame with Point geometries representing the centroids.\n\n    Raises:\n    ------\n    ValueError\n        If the input GeoDataFrame does not contain (Multi)Polygon geometries.\n    \"\"\"\n    # Validate input geometries\n    if not all(gdf.geometry.geom_type.isin([\"Polygon\", \"MultiPolygon\"])):\n        raise ValueError(\n            \"Input GeoDataFrame must contain only Polygon or MultiPolygon geometries.\"\n        )\n\n    # Calculate centroids\n    centroids = gdf.copy()\n    centroids[\"geometry\"] = centroids.geometry.centroid\n\n    return centroids\n</code></pre>"},{"location":"api/processing/#gigaspatial.processing.map_points_within_polygons","title":"<code>map_points_within_polygons(base_points_gdf, polygon_gdf)</code>","text":"<p>Maps whether each point in <code>base_points_gdf</code> is within any polygon in <code>polygon_gdf</code>.</p>"},{"location":"api/processing/#gigaspatial.processing.map_points_within_polygons--parameters","title":"Parameters:","text":"<p>base_points_gdf : geopandas.GeoDataFrame     GeoDataFrame containing point geometries to check. polygon_gdf : geopandas.GeoDataFrame     GeoDataFrame containing polygon geometries.</p>"},{"location":"api/processing/#gigaspatial.processing.map_points_within_polygons--returns","title":"Returns:","text":"<p>geopandas.GeoDataFrame     The <code>base_points_gdf</code> with an additional column <code>is_within</code> (True/False).</p>"},{"location":"api/processing/#gigaspatial.processing.map_points_within_polygons--raises","title":"Raises:","text":"<p>ValueError     If the geometries in either GeoDataFrame are invalid or not of the expected type.</p> Source code in <code>gigaspatial/processing/geo.py</code> <pre><code>def map_points_within_polygons(base_points_gdf, polygon_gdf):\n    \"\"\"\n    Maps whether each point in `base_points_gdf` is within any polygon in `polygon_gdf`.\n\n    Parameters:\n    ----------\n    base_points_gdf : geopandas.GeoDataFrame\n        GeoDataFrame containing point geometries to check.\n    polygon_gdf : geopandas.GeoDataFrame\n        GeoDataFrame containing polygon geometries.\n\n    Returns:\n    -------\n    geopandas.GeoDataFrame\n        The `base_points_gdf` with an additional column `is_within` (True/False).\n\n    Raises:\n    ------\n    ValueError\n        If the geometries in either GeoDataFrame are invalid or not of the expected type.\n    \"\"\"\n    # Validate input GeoDataFrames\n    if not all(base_points_gdf.geometry.geom_type == \"Point\"):\n        raise ValueError(\"`base_points_gdf` must contain only Point geometries.\")\n    if not all(polygon_gdf.geometry.geom_type.isin([\"Polygon\", \"MultiPolygon\"])):\n        raise ValueError(\n            \"`polygon_gdf` must contain only Polygon or MultiPolygon geometries.\"\n        )\n\n    if not base_points_gdf.crs == polygon_gdf.crs:\n        raise ValueError(\"CRS of `base_points_gdf` and `polygon_gdf` must match.\")\n\n    # Perform spatial join to check if points fall within any polygon\n    joined_gdf = gpd.sjoin(\n        base_points_gdf, polygon_gdf[[\"geometry\"]], how=\"left\", predicate=\"within\"\n    )\n\n    # Add `is_within` column to base_points_gdf\n    base_points_gdf[\"is_within\"] = base_points_gdf.index.isin(\n        set(joined_gdf.index[~joined_gdf.index_right.isna()])\n    )\n\n    return base_points_gdf\n</code></pre>"},{"location":"api/processing/#gigaspatial.processing.simplify_geometries","title":"<code>simplify_geometries(gdf, tolerance=0.01, preserve_topology=True, geometry_column='geometry')</code>","text":"<p>Simplify geometries in a GeoDataFrame to reduce file size and improve visualization performance.</p>"},{"location":"api/processing/#gigaspatial.processing.simplify_geometries--parameters","title":"Parameters","text":"<p>gdf : geopandas.GeoDataFrame     GeoDataFrame containing geometries to simplify. tolerance : float, optional     Tolerance for simplification. Larger values simplify more but reduce detail (default is 0.01). preserve_topology : bool, optional     Whether to preserve topology while simplifying. Preserving topology prevents invalid geometries (default is True). geometry_column : str, optional     Name of the column containing geometries (default is \"geometry\").</p>"},{"location":"api/processing/#gigaspatial.processing.simplify_geometries--returns","title":"Returns","text":"<p>geopandas.GeoDataFrame     A new GeoDataFrame with simplified geometries.</p>"},{"location":"api/processing/#gigaspatial.processing.simplify_geometries--raises","title":"Raises","text":"<p>ValueError     If the specified geometry column does not exist or contains invalid geometries. TypeError     If the geometry column does not contain valid geometries.</p>"},{"location":"api/processing/#gigaspatial.processing.simplify_geometries--examples","title":"Examples","text":"<p>Simplify geometries in a GeoDataFrame:</p> <p>simplified_gdf = simplify_geometries(gdf, tolerance=0.05)</p> Source code in <code>gigaspatial/processing/geo.py</code> <pre><code>def simplify_geometries(\n    gdf: gpd.GeoDataFrame,\n    tolerance: float = 0.01,\n    preserve_topology: bool = True,\n    geometry_column: str = \"geometry\",\n) -&gt; gpd.GeoDataFrame:\n    \"\"\"\n    Simplify geometries in a GeoDataFrame to reduce file size and improve visualization performance.\n\n    Parameters\n    ----------\n    gdf : geopandas.GeoDataFrame\n        GeoDataFrame containing geometries to simplify.\n    tolerance : float, optional\n        Tolerance for simplification. Larger values simplify more but reduce detail (default is 0.01).\n    preserve_topology : bool, optional\n        Whether to preserve topology while simplifying. Preserving topology prevents invalid geometries (default is True).\n    geometry_column : str, optional\n        Name of the column containing geometries (default is \"geometry\").\n\n    Returns\n    -------\n    geopandas.GeoDataFrame\n        A new GeoDataFrame with simplified geometries.\n\n    Raises\n    ------\n    ValueError\n        If the specified geometry column does not exist or contains invalid geometries.\n    TypeError\n        If the geometry column does not contain valid geometries.\n\n    Examples\n    --------\n    Simplify geometries in a GeoDataFrame:\n    &gt;&gt;&gt; simplified_gdf = simplify_geometries(gdf, tolerance=0.05)\n    \"\"\"\n\n    # Check if the specified geometry column exists\n    if geometry_column not in gdf.columns:\n        raise ValueError(\n            f\"Geometry column '{geometry_column}' not found in the GeoDataFrame.\"\n        )\n\n    # Check if the specified column contains geometries\n    if not gpd.GeoSeries(gdf[geometry_column]).is_valid.all():\n        raise TypeError(\n            f\"Geometry column '{geometry_column}' contains invalid geometries.\"\n        )\n\n    # Simplify geometries (non-destructive)\n    gdf_simplified = gdf.copy()\n    gdf_simplified[geometry_column] = gdf_simplified[geometry_column].simplify(\n        tolerance=tolerance, preserve_topology=preserve_topology\n    )\n\n    return gdf_simplified\n</code></pre>"},{"location":"api/processing/#gigaspatial.processing.algorithms","title":"<code>algorithms</code>","text":""},{"location":"api/processing/#gigaspatial.processing.algorithms.build_distance_graph","title":"<code>build_distance_graph(left_df, right_df, distance_threshold, max_k=100, return_dataframe=False, verbose=True, exclude_same_index=None)</code>","text":"<p>Build a graph of spatial matches between two dataframes using KD-tree.</p> <p>Parameters:</p> Name Type Description Default <code>left_df</code> <code>Union[DataFrame, GeoDataFrame]</code> <p>Left dataframe to match from</p> required <code>right_df</code> <code>Union[DataFrame, GeoDataFrame]</code> <p>Right dataframe to match to</p> required <code>distance_threshold</code> <code>float</code> <p>Maximum distance for matching (in meters)</p> required <code>max_k</code> <code>int</code> <p>Maximum number of neighbors to consider per point (default: 100)</p> <code>100</code> <code>return_dataframe</code> <code>bool</code> <p>If True, also return the matches DataFrame</p> <code>False</code> <code>verbose</code> <code>bool</code> <p>If True, print statistics about the graph</p> <code>True</code> <code>exclude_same_index</code> <code>Optional[bool]</code> <p>If True, exclude self-matches. If None, auto-detect based on df equality</p> <code>None</code> <p>Returns:</p> Type Description <code>Union[Graph, Tuple[Graph, DataFrame]]</code> <p>NetworkX Graph, or tuple of (Graph, DataFrame) if return_dataframe=True</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If distance_threshold is negative or max_k is not positive</p> Source code in <code>gigaspatial/processing/algorithms.py</code> <pre><code>def build_distance_graph(\n    left_df: Union[pd.DataFrame, gpd.GeoDataFrame],\n    right_df: Union[pd.DataFrame, gpd.GeoDataFrame],\n    distance_threshold: float,\n    max_k: int = 100,\n    return_dataframe: bool = False,\n    verbose: bool = True,\n    exclude_same_index: Optional[bool] = None,\n) -&gt; Union[nx.Graph, Tuple[nx.Graph, pd.DataFrame]]:\n    \"\"\"\n    Build a graph of spatial matches between two dataframes using KD-tree.\n\n    Args:\n        left_df: Left dataframe to match from\n        right_df: Right dataframe to match to\n        distance_threshold: Maximum distance for matching (in meters)\n        max_k: Maximum number of neighbors to consider per point (default: 100)\n        return_dataframe: If True, also return the matches DataFrame\n        verbose: If True, print statistics about the graph\n        exclude_same_index: If True, exclude self-matches. If None, auto-detect based on df equality\n\n    Returns:\n        NetworkX Graph, or tuple of (Graph, DataFrame) if return_dataframe=True\n\n    Raises:\n        ValueError: If distance_threshold is negative or max_k is not positive\n    \"\"\"\n\n    # Input validation\n    if distance_threshold &lt; 0:\n        raise ValueError(\"distance_threshold must be non-negative\")\n\n    if max_k &lt;= 0:\n        raise ValueError(\"max_k must be positive\")\n\n    if left_df.empty or right_df.empty:\n        if verbose:\n            LOGGER.warning(\"Warning: One or both dataframes are empty\")\n        G = nx.Graph()\n        return (G, pd.DataFrame()) if return_dataframe else G\n\n    def get_utm_coordinates(df: Union[pd.DataFrame, gpd.GeoDataFrame]) -&gt; np.ndarray:\n        \"\"\"Extract coordinates as numpy array in UTM projection.\"\"\"\n        if isinstance(df, pd.DataFrame):\n            gdf = convert_to_geodataframe(df)\n        else:\n            gdf = df.copy()\n\n        # More robust UTM CRS estimation\n        try:\n            gdf_utm = gdf.to_crs(gdf.estimate_utm_crs())\n        except Exception as e:\n            if verbose:\n                LOGGER.warning(\n                    f\"Warning: UTM CRS estimation failed, using Web Mercator. Error: {e}\"\n                )\n            gdf_utm = gdf.to_crs(\"EPSG:3857\")  # Fallback to Web Mercator\n\n        return gdf_utm.get_coordinates().to_numpy()\n\n    # Auto-detect same dataframe case\n    if exclude_same_index is None:\n        exclude_same_index = left_df.equals(right_df)\n        if verbose and exclude_same_index:\n            LOGGER.info(\"Auto-detected same dataframe - excluding self-matches\")\n\n    # Get coordinates\n    left_coords = get_utm_coordinates(left_df)\n    right_coords = (\n        get_utm_coordinates(right_df) if not exclude_same_index else left_coords\n    )\n\n    # Build KD-tree and query\n    kdtree = cKDTree(right_coords)\n\n    # Use the provided max_k parameter, but don't exceed available points\n    k_to_use = min(max_k, len(right_coords))\n\n    if verbose and k_to_use &lt; max_k:\n        LOGGER.info(\n            f\"Note: max_k ({max_k}) reduced to {k_to_use} (number of available points)\"\n        )\n\n    # Note: Distance calculations here are based on Euclidean distance in UTM projection.\n    # This can introduce errors up to ~50 cm for a 50 meter threshold, especially near the poles where distortion increases.\n    distances, indices = kdtree.query(\n        left_coords, k=k_to_use, distance_upper_bound=distance_threshold\n    )\n\n    # Handle single k case (when k_to_use = 1, results are 1D)\n    if distances.ndim == 1:\n        distances = distances.reshape(-1, 1)\n        indices = indices.reshape(-1, 1)\n\n    # Extract valid pairs using vectorized operations\n    left_indices = np.arange(len(distances))[:, np.newaxis]\n    left_indices = np.broadcast_to(left_indices, distances.shape)\n    valid_mask = np.isfinite(distances)\n\n    if exclude_same_index:\n        same_index_mask = left_indices == indices\n        valid_mask = valid_mask &amp; ~same_index_mask\n\n    valid_left = left_indices[valid_mask]\n    valid_right = indices[valid_mask]\n    valid_distances = distances[valid_mask]\n\n    # Map back to original indices\n    valid_left_indices = left_df.index.values[valid_left]\n    valid_right_indices = right_df.index.values[valid_right]\n\n    # Create matches DataFrame\n    matches_df = pd.DataFrame(\n        {\n            \"left_idx\": valid_left_indices,\n            \"right_idx\": valid_right_indices,\n            \"distance\": valid_distances,\n        }\n    )\n\n    # Build graph more efficiently\n    G = nx.from_pandas_edgelist(\n        matches_df,\n        source=\"left_idx\",\n        target=\"right_idx\",\n        edge_attr=\"distance\",\n        create_using=nx.Graph(),\n    )\n\n    # Add isolated nodes (nodes without any matches within threshold)\n    # This ensures all original indices are represented in the graph\n    all_left_nodes = set(left_df.index.values)\n    all_right_nodes = set(right_df.index.values)\n\n    if not exclude_same_index:\n        all_nodes = all_left_nodes | all_right_nodes\n    else:\n        all_nodes = all_left_nodes  # Same dataframe, so same node set\n\n    # Add nodes that don't have edges\n    existing_nodes = set(G.nodes())\n    isolated_nodes = all_nodes - existing_nodes\n    G.add_nodes_from(isolated_nodes)\n\n    # Print statistics\n    if verbose:\n        print(\n            f\"Total potential matches: {len(left_df)} \u00d7 {len(right_df)} = {len(left_df) * len(right_df):,}\"\n        )\n        print(f\"Matches found within {distance_threshold}m: {len(matches_df):,}\")\n        print(f\"Graph nodes: {G.number_of_nodes():,}\")\n        print(f\"Graph edges: {G.number_of_edges():,}\")\n\n        components = list(nx.connected_components(G))\n        print(f\"Connected components: {len(components):,}\")\n\n        if len(components) &gt; 1:\n            component_sizes = [len(c) for c in components]\n            print(f\"Largest component size: {max(component_sizes):,}\")\n            print(\n                f\"Isolated nodes: {sum(1 for size in component_sizes if size == 1):,}\"\n            )\n\n        if len(matches_df) &gt; 0:\n            print(\n                f\"Distance stats - min: {matches_df['distance'].min():.1f}m, \"\n                f\"max: {matches_df['distance'].max():.1f}m, \"\n                f\"mean: {matches_df['distance'].mean():.1f}m\"\n            )\n\n    return (G, matches_df) if return_dataframe else G\n</code></pre>"},{"location":"api/processing/#gigaspatial.processing.geo","title":"<code>geo</code>","text":""},{"location":"api/processing/#gigaspatial.processing.geo.add_area_in_meters","title":"<code>add_area_in_meters(gdf, area_column_name='area_in_meters')</code>","text":"<p>Calculate the area of (Multi)Polygon geometries in square meters and add it as a new column.</p>"},{"location":"api/processing/#gigaspatial.processing.geo.add_area_in_meters--parameters","title":"Parameters:","text":"<p>gdf : geopandas.GeoDataFrame     GeoDataFrame containing (Multi)Polygon geometries. area_column_name : str, optional     Name of the new column to store the area values. Default is \"area_m2\".</p>"},{"location":"api/processing/#gigaspatial.processing.geo.add_area_in_meters--returns","title":"Returns:","text":"<p>geopandas.GeoDataFrame     The input GeoDataFrame with an additional column for the area in square meters.</p>"},{"location":"api/processing/#gigaspatial.processing.geo.add_area_in_meters--raises","title":"Raises:","text":"<p>ValueError     If the input GeoDataFrame does not contain (Multi)Polygon geometries.</p> Source code in <code>gigaspatial/processing/geo.py</code> <pre><code>def add_area_in_meters(\n    gdf: gpd.GeoDataFrame, area_column_name: str = \"area_in_meters\"\n) -&gt; gpd.GeoDataFrame:\n    \"\"\"\n    Calculate the area of (Multi)Polygon geometries in square meters and add it as a new column.\n\n    Parameters:\n    ----------\n    gdf : geopandas.GeoDataFrame\n        GeoDataFrame containing (Multi)Polygon geometries.\n    area_column_name : str, optional\n        Name of the new column to store the area values. Default is \"area_m2\".\n\n    Returns:\n    -------\n    geopandas.GeoDataFrame\n        The input GeoDataFrame with an additional column for the area in square meters.\n\n    Raises:\n    ------\n    ValueError\n        If the input GeoDataFrame does not contain (Multi)Polygon geometries.\n    \"\"\"\n    # Validate input geometries\n    if not all(gdf.geometry.geom_type.isin([\"Polygon\", \"MultiPolygon\"])):\n        raise ValueError(\n            \"Input GeoDataFrame must contain only Polygon or MultiPolygon geometries.\"\n        )\n\n    # Create a copy of the GeoDataFrame to avoid modifying the original\n    gdf_with_area = gdf.copy()\n\n    # Calculate the UTM CRS for accurate area calculation\n    try:\n        utm_crs = gdf_with_area.estimate_utm_crs()\n    except Exception as e:\n        LOGGER.warning(\n            f\"Warning: UTM CRS estimation failed, using Web Mercator. Error: {e}\"\n        )\n        utm_crs = \"EPSG:3857\"  # Fallback to Web Mercator\n\n    # Transform to UTM CRS and calculate the area in square meters\n    gdf_with_area[area_column_name] = gdf_with_area.to_crs(utm_crs).geometry.area\n\n    return gdf_with_area\n</code></pre>"},{"location":"api/processing/#gigaspatial.processing.geo.add_spatial_jitter","title":"<code>add_spatial_jitter(df, columns=['latitude', 'longitude'], amount=0.0001, seed=None, copy=True)</code>","text":"<p>Add random jitter to duplicated geographic coordinates to create slight separation between overlapping points.</p>"},{"location":"api/processing/#gigaspatial.processing.geo.add_spatial_jitter--parameters","title":"Parameters:","text":"<p>df : pandas.DataFrame     DataFrame containing geographic coordinates. columns : list of str, optional     Column names containing coordinates to jitter. Default is ['latitude', 'longitude']. amount : float or dict, optional     Amount of jitter to add. If float, same amount used for all columns.     If dict, specify amount per column, e.g., {'lat': 0.0001, 'lon': 0.0002}.     Default is 0.0001 (approximately 11 meters at the equator). seed : int, optional     Random seed for reproducibility. Default is None. copy : bool, optional     Whether to create a copy of the input DataFrame. Default is True.</p>"},{"location":"api/processing/#gigaspatial.processing.geo.add_spatial_jitter--returns","title":"Returns:","text":"<p>pandas.DataFrame     DataFrame with jittered coordinates for previously duplicated points.</p>"},{"location":"api/processing/#gigaspatial.processing.geo.add_spatial_jitter--raises","title":"Raises:","text":"<p>ValueError     If columns don't exist or jitter amount is invalid. TypeError     If input types are incorrect.</p> Source code in <code>gigaspatial/processing/geo.py</code> <pre><code>def add_spatial_jitter(\n    df: pd.DataFrame,\n    columns: List[str] = [\"latitude\", \"longitude\"],\n    amount: float = 0.0001,\n    seed=None,\n    copy=True,\n) -&gt; pd.DataFrame:\n    \"\"\"\n    Add random jitter to duplicated geographic coordinates to create slight separation\n    between overlapping points.\n\n    Parameters:\n    ----------\n    df : pandas.DataFrame\n        DataFrame containing geographic coordinates.\n    columns : list of str, optional\n        Column names containing coordinates to jitter. Default is ['latitude', 'longitude'].\n    amount : float or dict, optional\n        Amount of jitter to add. If float, same amount used for all columns.\n        If dict, specify amount per column, e.g., {'lat': 0.0001, 'lon': 0.0002}.\n        Default is 0.0001 (approximately 11 meters at the equator).\n    seed : int, optional\n        Random seed for reproducibility. Default is None.\n    copy : bool, optional\n        Whether to create a copy of the input DataFrame. Default is True.\n\n    Returns:\n    -------\n    pandas.DataFrame\n        DataFrame with jittered coordinates for previously duplicated points.\n\n    Raises:\n    ------\n    ValueError\n        If columns don't exist or jitter amount is invalid.\n    TypeError\n        If input types are incorrect.\n    \"\"\"\n\n    # Input validation\n    if not isinstance(df, pd.DataFrame):\n        raise TypeError(\"Input must be a pandas DataFrame\")\n\n    if not all(col in df.columns for col in columns):\n        raise ValueError(f\"Not all columns {columns} found in DataFrame\")\n\n    # Handle jitter amounts\n    if isinstance(amount, (int, float)):\n        if amount &lt;= 0:\n            raise ValueError(\"Jitter amount must be positive\")\n        jitter_amounts = {col: amount for col in columns}\n    elif isinstance(amount, dict):\n        if not all(col in amount for col in columns):\n            raise ValueError(\"Must specify jitter amount for each column\")\n        if not all(amt &gt; 0 for amt in amount.values()):\n            raise ValueError(\"All jitter amounts must be positive\")\n        jitter_amounts = amount\n    else:\n        raise TypeError(\"amount must be a number or dictionary\")\n\n    # Create copy if requested\n    df_work = df.copy() if copy else df\n\n    # Set random seed if provided\n    if seed is not None:\n        np.random.seed(seed)\n\n    try:\n        # Find duplicated coordinates\n        duplicate_mask = df_work.duplicated(subset=columns, keep=False)\n        n_duplicates = duplicate_mask.sum()\n\n        if n_duplicates &gt; 0:\n            # Add jitter to each column separately\n            for col in columns:\n                jitter = np.random.uniform(\n                    low=-jitter_amounts[col],\n                    high=jitter_amounts[col],\n                    size=n_duplicates,\n                )\n                df_work.loc[duplicate_mask, col] += jitter\n\n            # Validate results (ensure no remaining duplicates)\n            if df_work.duplicated(subset=columns, keep=False).any():\n                # If duplicates remain, recursively add more jitter\n                df_work = add_spatial_jitter(\n                    df_work,\n                    columns=columns,\n                    amount={col: amt * 2 for col, amt in jitter_amounts.items()},\n                    seed=seed,\n                    copy=False,\n                )\n\n        return df_work\n\n    except Exception as e:\n        raise RuntimeError(f\"Error during jittering operation: {str(e)}\")\n</code></pre>"},{"location":"api/processing/#gigaspatial.processing.geo.aggregate_points_to_zones","title":"<code>aggregate_points_to_zones(points, zones, value_columns=None, aggregation='count', point_zone_predicate='within', zone_id_column='zone_id', output_suffix='', drop_geometry=False)</code>","text":"<p>Aggregate point data to zones with flexible aggregation methods.</p> <p>Parameters:</p> Name Type Description Default <code>points</code> <code>Union[DataFrame, GeoDataFrame]</code> <p>Point data to aggregate</p> required <code>zones</code> <code>GeoDataFrame</code> <p>Zones to aggregate points to</p> required <code>value_columns</code> <code>Optional[Union[str, List[str]]]</code> <p>Column(s) containing values to aggregate If None, only counts will be performed.</p> <code>None</code> <code>aggregation</code> <code>Union[str, Dict[str, str]]</code> <p>Aggregation method(s) to use: - Single string: Use same method for all columns (\"count\", \"mean\", \"sum\", \"min\", \"max\") - Dict: Map column names to aggregation methods</p> <code>'count'</code> <code>point_zone_predicate</code> <code>str</code> <p>Spatial predicate for point-to-zone relationship Options: \"within\", \"intersects\"</p> <code>'within'</code> <code>zone_id_column</code> <code>str</code> <p>Column in zones containing zone identifiers</p> <code>'zone_id'</code> <code>output_suffix</code> <code>str</code> <p>Suffix to add to output column names</p> <code>''</code> <code>drop_geometry</code> <code>bool</code> <p>Whether to drop the geometry column from output</p> <code>False</code> <p>Returns:</p> Type Description <code>GeoDataFrame</code> <p>gpd.GeoDataFrame: Zones with aggregated point values</p> Example <p>poi_counts = aggregate_points_to_zones(pois, zones, aggregation=\"count\") poi_value_mean = aggregate_points_to_zones( ...     pois, zones, value_columns=\"score\", aggregation=\"mean\" ... ) poi_multiple = aggregate_points_to_zones( ...     pois, zones, ...     value_columns=[\"score\", \"visits\"], ...     aggregation={\"score\": \"mean\", \"visits\": \"sum\"} ... )</p> Source code in <code>gigaspatial/processing/geo.py</code> <pre><code>def aggregate_points_to_zones(\n    points: Union[pd.DataFrame, gpd.GeoDataFrame],\n    zones: gpd.GeoDataFrame,\n    value_columns: Optional[Union[str, List[str]]] = None,\n    aggregation: Union[str, Dict[str, str]] = \"count\",\n    point_zone_predicate: str = \"within\",\n    zone_id_column: str = \"zone_id\",\n    output_suffix: str = \"\",\n    drop_geometry: bool = False,\n) -&gt; gpd.GeoDataFrame:\n    \"\"\"\n    Aggregate point data to zones with flexible aggregation methods.\n\n    Args:\n        points (Union[pd.DataFrame, gpd.GeoDataFrame]): Point data to aggregate\n        zones (gpd.GeoDataFrame): Zones to aggregate points to\n        value_columns (Optional[Union[str, List[str]]]): Column(s) containing values to aggregate\n            If None, only counts will be performed.\n        aggregation (Union[str, Dict[str, str]]): Aggregation method(s) to use:\n            - Single string: Use same method for all columns (\"count\", \"mean\", \"sum\", \"min\", \"max\")\n            - Dict: Map column names to aggregation methods\n        point_zone_predicate (str): Spatial predicate for point-to-zone relationship\n            Options: \"within\", \"intersects\"\n        zone_id_column (str): Column in zones containing zone identifiers\n        output_suffix (str): Suffix to add to output column names\n        drop_geometry (bool): Whether to drop the geometry column from output\n\n    Returns:\n        gpd.GeoDataFrame: Zones with aggregated point values\n\n    Example:\n        &gt;&gt;&gt; poi_counts = aggregate_points_to_zones(pois, zones, aggregation=\"count\")\n        &gt;&gt;&gt; poi_value_mean = aggregate_points_to_zones(\n        ...     pois, zones, value_columns=\"score\", aggregation=\"mean\"\n        ... )\n        &gt;&gt;&gt; poi_multiple = aggregate_points_to_zones(\n        ...     pois, zones,\n        ...     value_columns=[\"score\", \"visits\"],\n        ...     aggregation={\"score\": \"mean\", \"visits\": \"sum\"}\n        ... )\n    \"\"\"\n    # Input validation\n    if not isinstance(zones, gpd.GeoDataFrame):\n        raise TypeError(\"zones must be a GeoDataFrame\")\n\n    if zone_id_column not in zones.columns:\n        raise ValueError(f\"Zone ID column '{zone_id_column}' not found in zones\")\n\n    # Convert points to GeoDataFrame if necessary\n    if not isinstance(points, gpd.GeoDataFrame):\n        points_gdf = convert_to_geodataframe(points)\n    else:\n        points_gdf = points.copy()\n\n    # Ensure CRS match\n    if points_gdf.crs != zones.crs:\n        points_gdf = points_gdf.to_crs(zones.crs)\n\n    # Handle value columns\n    if value_columns is not None:\n        if isinstance(value_columns, str):\n            value_columns = [value_columns]\n\n        # Validate that all value columns exist\n        missing_cols = [col for col in value_columns if col not in points_gdf.columns]\n        if missing_cols:\n            raise ValueError(f\"Value columns not found in points data: {missing_cols}\")\n\n    # Handle aggregation method\n    agg_funcs = {}\n\n    if isinstance(aggregation, str):\n        if aggregation == \"count\":\n            # Special case for count (doesn't need value columns)\n            agg_funcs[\"__count\"] = \"count\"\n        elif value_columns is not None:\n            # Apply the same aggregation to all value columns\n            agg_funcs = {col: aggregation for col in value_columns}\n        else:\n            raise ValueError(\n                \"Value columns must be specified for aggregation methods other than 'count'\"\n            )\n    elif isinstance(aggregation, dict):\n        # Validate dictionary keys\n        if value_columns is None:\n            raise ValueError(\n                \"Value columns must be specified when using a dictionary of aggregation methods\"\n            )\n\n        missing_aggs = [col for col in value_columns if col not in aggregation]\n        extra_aggs = [col for col in aggregation if col not in value_columns]\n\n        if missing_aggs:\n            raise ValueError(f\"Missing aggregation methods for columns: {missing_aggs}\")\n        if extra_aggs:\n            raise ValueError(\n                f\"Aggregation methods specified for non-existent columns: {extra_aggs}\"\n            )\n\n        agg_funcs = aggregation\n    else:\n        raise TypeError(\"aggregation must be a string or dictionary\")\n\n    # Create a copy of the zones\n    result = zones.copy()\n\n    # Spatial join\n    joined = gpd.sjoin(points_gdf, zones, how=\"inner\", predicate=point_zone_predicate)\n\n    # Perform aggregation\n    if \"geometry\" in joined.columns and not all(\n        value == \"count\" for value in agg_funcs.values()\n    ):\n        # Drop geometry for non-count aggregations to avoid errors\n        joined = joined.drop(columns=[\"geometry\"])\n\n    if \"__count\" in agg_funcs:\n        # Count points per zone\n        counts = (\n            joined.groupby(zone_id_column)\n            .size()\n            .reset_index(name=f\"point_count{output_suffix}\")\n        )\n        result = result.merge(counts, on=zone_id_column, how=\"left\")\n        result[f\"point_count{output_suffix}\"] = (\n            result[f\"point_count{output_suffix}\"].fillna(0).astype(int)\n        )\n    else:\n        # Aggregate values\n        aggregated = joined.groupby(zone_id_column).agg(agg_funcs).reset_index()\n\n        # Rename columns to include aggregation method\n        if len(value_columns) &gt; 0:\n            # Handle MultiIndex columns from pandas aggregation\n            if isinstance(aggregated.columns, pd.MultiIndex):\n                aggregated.columns = [\n                    (\n                        f\"{col[0]}_{col[1]}{output_suffix}\"\n                        if col[0] != zone_id_column\n                        else zone_id_column\n                    )\n                    for col in aggregated.columns\n                ]\n\n            # Merge back to zones\n            result = result.merge(aggregated, on=zone_id_column, how=\"left\")\n\n            # Fill NaN values with zeros\n            for col in result.columns:\n                if (\n                    col != zone_id_column\n                    and col != \"geometry\"\n                    and pd.api.types.is_numeric_dtype(result[col])\n                ):\n                    result[col] = result[col].fillna(0)\n\n    if drop_geometry:\n        result = result.drop(columns=[\"geometry\"])\n        return result\n\n    return result\n</code></pre>"},{"location":"api/processing/#gigaspatial.processing.geo.aggregate_polygons_to_zones","title":"<code>aggregate_polygons_to_zones(polygons, zones, value_columns, aggregation='sum', predicate='intersects', zone_id_column='zone_id', output_suffix='', drop_geometry=False)</code>","text":"<p>Aggregates polygon data to zones based on a specified spatial relationship.</p> <p>This function performs a spatial join between polygons and zones and then aggregates values from the polygons to their corresponding zones. The aggregation method depends on the <code>predicate</code> parameter, which determines the nature of the spatial relationship.</p> <p>Parameters:</p> Name Type Description Default <code>polygons</code> <code>Union[DataFrame, GeoDataFrame]</code> <p>Polygon data to aggregate. Must be a GeoDataFrame or convertible to one.</p> required <code>zones</code> <code>GeoDataFrame</code> <p>The target zones to which the polygon data will be aggregated.</p> required <code>value_columns</code> <code>Union[str, List[str]]</code> <p>The column(s) in <code>polygons</code> containing the numeric values to aggregate.</p> required <code>aggregation</code> <code>Union[str, Dict[str, str]]</code> <p>The aggregation method(s) to use. Can be a single string (e.g., \"sum\", \"mean\", \"max\") to apply the same method to all columns, or a dictionary mapping column names to aggregation methods (e.g., <code>{'population': 'sum'}</code>). Defaults to \"sum\".</p> <code>'sum'</code> <code>predicate</code> <code>Literal['intersects', 'within', 'fractional']</code> <p>The spatial relationship to use for aggregation: - \"intersects\": Aggregates values for any polygon that intersects a zone. - \"within\": Aggregates values for polygons entirely contained within a zone. - \"fractional\": Performs area-weighted aggregation. The value of a polygon   is distributed proportionally to the area of its overlap with each zone.   This requires calculating a UTM CRS for accurate area measurements. Defaults to \"intersects\".</p> <code>'intersects'</code> <code>zone_id_column</code> <code>str</code> <p>The name of the column in <code>zones</code> that contains the unique zone identifiers. Defaults to \"zone_id\".</p> <code>'zone_id'</code> <code>output_suffix</code> <code>str</code> <p>A suffix to add to the names of the new aggregated columns in the output GeoDataFrame. Defaults to \"\".</p> <code>''</code> <code>drop_geometry</code> <code>bool</code> <p>If True, the geometry column will be dropped from the output GeoDataFrame. Defaults to False.</p> <code>False</code> <p>Returns:</p> Type Description <code>GeoDataFrame</code> <p>gpd.GeoDataFrame: The <code>zones</code> GeoDataFrame with new columns containing the aggregated values. Zones with no intersecting or contained polygons will have <code>0</code> values.</p> <p>Raises:</p> Type Description <code>TypeError</code> <p>If <code>zones</code> is not a GeoDataFrame or <code>polygons</code> cannot be converted.</p> <code>ValueError</code> <p>If <code>zone_id_column</code> or any <code>value_columns</code> are not found, or         if the geometry types in <code>polygons</code> are not polygons.</p> <code>RuntimeError</code> <p>If an error occurs during the area-weighted aggregation process.</p> Example <p>import geopandas as gpd</p> Source code in <code>gigaspatial/processing/geo.py</code> <pre><code>def aggregate_polygons_to_zones(\n    polygons: Union[pd.DataFrame, gpd.GeoDataFrame],\n    zones: gpd.GeoDataFrame,\n    value_columns: Union[str, List[str]],\n    aggregation: Union[str, Dict[str, str]] = \"sum\",\n    predicate: Literal[\"intersects\", \"within\", \"fractional\"] = \"intersects\",\n    zone_id_column: str = \"zone_id\",\n    output_suffix: str = \"\",\n    drop_geometry: bool = False,\n) -&gt; gpd.GeoDataFrame:\n    \"\"\"\n    Aggregates polygon data to zones based on a specified spatial relationship.\n\n    This function performs a spatial join between polygons and zones and then\n    aggregates values from the polygons to their corresponding zones. The aggregation\n    method depends on the `predicate` parameter, which determines the nature of the\n    spatial relationship.\n\n    Args:\n        polygons (Union[pd.DataFrame, gpd.GeoDataFrame]):\n            Polygon data to aggregate. Must be a GeoDataFrame or convertible to one.\n        zones (gpd.GeoDataFrame):\n            The target zones to which the polygon data will be aggregated.\n        value_columns (Union[str, List[str]]):\n            The column(s) in `polygons` containing the numeric values to aggregate.\n        aggregation (Union[str, Dict[str, str]], optional):\n            The aggregation method(s) to use. Can be a single string (e.g., \"sum\",\n            \"mean\", \"max\") to apply the same method to all columns, or a dictionary\n            mapping column names to aggregation methods (e.g., `{'population': 'sum'}`).\n            Defaults to \"sum\".\n        predicate (Literal[\"intersects\", \"within\", \"fractional\"], optional):\n            The spatial relationship to use for aggregation:\n            - \"intersects\": Aggregates values for any polygon that intersects a zone.\n            - \"within\": Aggregates values for polygons entirely contained within a zone.\n            - \"fractional\": Performs area-weighted aggregation. The value of a polygon\n              is distributed proportionally to the area of its overlap with each zone.\n              This requires calculating a UTM CRS for accurate area measurements.\n            Defaults to \"intersects\".\n        zone_id_column (str, optional):\n            The name of the column in `zones` that contains the unique zone identifiers.\n            Defaults to \"zone_id\".\n        output_suffix (str, optional):\n            A suffix to add to the names of the new aggregated columns in the output\n            GeoDataFrame. Defaults to \"\".\n        drop_geometry (bool, optional):\n            If True, the geometry column will be dropped from the output GeoDataFrame.\n            Defaults to False.\n\n    Returns:\n        gpd.GeoDataFrame:\n            The `zones` GeoDataFrame with new columns containing the aggregated values.\n            Zones with no intersecting or contained polygons will have `0` values.\n\n    Raises:\n        TypeError: If `zones` is not a GeoDataFrame or `polygons` cannot be converted.\n        ValueError: If `zone_id_column` or any `value_columns` are not found, or\n                    if the geometry types in `polygons` are not polygons.\n        RuntimeError: If an error occurs during the area-weighted aggregation process.\n\n    Example:\n        &gt;&gt;&gt; import geopandas as gpd\n        &gt;&gt;&gt; # Assuming 'landuse_polygons' and 'grid_zones' are GeoDataFrames\n        &gt;&gt;&gt; # Aggregate total population within each grid zone using area-weighting\n        &gt;&gt;&gt; pop_by_zone = aggregate_polygons_to_zones(\n        ...     landuse_polygons,\n        ...     grid_zones,\n        ...     value_columns=\"population\",\n        ...     predicate=\"fractional\",\n        ...     aggregation=\"sum\",\n        ...     output_suffix=\"_pop\"\n        ... )\n        &gt;&gt;&gt; # Aggregate the count of landuse parcels intersecting each zone\n        &gt;&gt;&gt; count_by_zone = aggregate_polygons_to_zones(\n        ...     landuse_polygons,\n        ...     grid_zones,\n        ...     value_columns=\"parcel_id\",\n        ...     predicate=\"intersects\",\n        ...     aggregation=\"count\"\n        ... )\n    \"\"\"\n    # Input validation\n    if not isinstance(zones, gpd.GeoDataFrame):\n        raise TypeError(\"zones must be a GeoDataFrame\")\n\n    if zones.empty:\n        raise ValueError(\"zones GeoDataFrame is empty\")\n\n    if zone_id_column not in zones.columns:\n        raise ValueError(f\"Zone ID column '{zone_id_column}' not found in zones\")\n\n    if predicate not in [\"intersects\", \"within\", \"fractional\"]:\n        raise ValueError(\n            f\"Unsupported predicate: {predicate}. Predicate can be one of `intersects`, `within`, `fractional`\"\n        )\n\n    # Convert polygons to GeoDataFrame if necessary\n    if not isinstance(polygons, gpd.GeoDataFrame):\n        try:\n            polygons_gdf = convert_to_geodataframe(polygons)\n        except Exception as e:\n            raise TypeError(\n                f\"polygons must be a GeoDataFrame or convertible to one: {e}\"\n            )\n    else:\n        polygons_gdf = polygons.copy()\n\n    if polygons_gdf.empty:\n        LOGGER.warning(\"Empty polygons GeoDataFrame provided\")\n        return zones\n\n    # Validate geometry types\n    non_polygon_geoms = [\n        geom_type\n        for geom_type in polygons_gdf.geometry.geom_type.unique()\n        if geom_type not in [\"Polygon\", \"MultiPolygon\"]\n    ]\n    if non_polygon_geoms:\n        raise ValueError(\n            f\"Input contains non-polygon geometries: {non_polygon_geoms}. \"\n            \"Use aggregate_points_to_zones for point data.\"\n        )\n\n    # Process value columns\n    if isinstance(value_columns, str):\n        value_columns = [value_columns]\n\n    # Validate that all value columns exist\n    missing_cols = [col for col in value_columns if col not in polygons_gdf.columns]\n    if missing_cols:\n        raise ValueError(f\"Value columns not found in polygons data: {missing_cols}\")\n\n    # Check for column name conflicts with zone_id_column\n    if zone_id_column in polygons_gdf.columns:\n        raise ValueError(\n            f\"Column name conflict: polygons DataFrame contains column '{zone_id_column}' \"\n            f\"which conflicts with the zone identifier column. Please rename this column \"\n            f\"in the polygons data to avoid confusion.\"\n        )\n\n    # Ensure CRS match\n    if polygons_gdf.crs != zones.crs:\n        polygons_gdf = polygons_gdf.to_crs(zones.crs)\n\n    # Handle aggregation method\n    agg_funcs = _process_aggregation_methods(aggregation, value_columns)\n\n    # Prepare minimal zones for spatial operations (only zone_id_column and geometry)\n    minimal_zones = zones[[zone_id_column, \"geometry\"]].copy()\n\n    if predicate == \"fractional\":\n        aggregated_data = _fractional_aggregation(\n            polygons_gdf, minimal_zones, value_columns, agg_funcs, zone_id_column\n        )\n    else:\n        aggregated_data = _simple_aggregation(\n            polygons_gdf,\n            minimal_zones,\n            value_columns,\n            agg_funcs,\n            zone_id_column,\n            predicate,\n        )\n\n    # Merge aggregated results back to complete zones data\n    result = zones.merge(\n        aggregated_data[[col for col in aggregated_data.columns if col != \"geometry\"]],\n        on=zone_id_column,\n        how=\"left\",\n    )\n\n    # Fill NaN values with zeros for the newly aggregated columns only\n    aggregated_cols = [col for col in result.columns if col not in zones.columns]\n    for col in aggregated_cols:\n        if pd.api.types.is_numeric_dtype(result[col]):\n            result[col] = result[col].fillna(0)\n\n    # Apply output suffix consistently to result columns only\n    if output_suffix:\n        rename_dict = {col: f\"{col}{output_suffix}\" for col in aggregated_cols}\n        result = result.rename(columns=rename_dict)\n\n    if drop_geometry:\n        result = result.drop(columns=[\"geometry\"])\n\n    return result\n</code></pre>"},{"location":"api/processing/#gigaspatial.processing.geo.aggregate_polygons_to_zones--assuming-landuse_polygons-and-grid_zones-are-geodataframes","title":"Assuming 'landuse_polygons' and 'grid_zones' are GeoDataFrames","text":""},{"location":"api/processing/#gigaspatial.processing.geo.aggregate_polygons_to_zones--aggregate-total-population-within-each-grid-zone-using-area-weighting","title":"Aggregate total population within each grid zone using area-weighting","text":"<p>pop_by_zone = aggregate_polygons_to_zones( ...     landuse_polygons, ...     grid_zones, ...     value_columns=\"population\", ...     predicate=\"fractional\", ...     aggregation=\"sum\", ...     output_suffix=\"_pop\" ... )</p>"},{"location":"api/processing/#gigaspatial.processing.geo.aggregate_polygons_to_zones--aggregate-the-count-of-landuse-parcels-intersecting-each-zone","title":"Aggregate the count of landuse parcels intersecting each zone","text":"<p>count_by_zone = aggregate_polygons_to_zones( ...     landuse_polygons, ...     grid_zones, ...     value_columns=\"parcel_id\", ...     predicate=\"intersects\", ...     aggregation=\"count\" ... )</p>"},{"location":"api/processing/#gigaspatial.processing.geo.annotate_with_admin_regions","title":"<code>annotate_with_admin_regions(gdf, country_code, data_store=None, admin_id_column_suffix='_giga')</code>","text":"<p>Annotate a GeoDataFrame with administrative region information.</p> <p>Performs a spatial join between the input points and administrative boundaries at levels 1 and 2, resolving conflicts when points intersect multiple admin regions.</p> <p>Parameters:</p> Name Type Description Default <code>gdf</code> <code>GeoDataFrame</code> <p>GeoDataFrame containing points to annotate</p> required <code>country_code</code> <code>str</code> <p>Country code for administrative boundaries</p> required <code>data_store</code> <code>Optional[DataStore]</code> <p>Optional DataStore for loading admin boundary data</p> <code>None</code> <p>Returns:</p> Type Description <code>GeoDataFrame</code> <p>GeoDataFrame with added administrative region columns</p> Source code in <code>gigaspatial/processing/geo.py</code> <pre><code>def annotate_with_admin_regions(\n    gdf: gpd.GeoDataFrame,\n    country_code: str,\n    data_store: Optional[DataStore] = None,\n    admin_id_column_suffix=\"_giga\",\n) -&gt; gpd.GeoDataFrame:\n    \"\"\"\n    Annotate a GeoDataFrame with administrative region information.\n\n    Performs a spatial join between the input points and administrative boundaries\n    at levels 1 and 2, resolving conflicts when points intersect multiple admin regions.\n\n    Args:\n        gdf: GeoDataFrame containing points to annotate\n        country_code: Country code for administrative boundaries\n        data_store: Optional DataStore for loading admin boundary data\n\n    Returns:\n        GeoDataFrame with added administrative region columns\n    \"\"\"\n    from gigaspatial.handlers.boundaries import AdminBoundaries\n\n    if not isinstance(gdf, gpd.GeoDataFrame):\n        raise TypeError(\"gdf must be a GeoDataFrame\")\n\n    if gdf.empty:\n        LOGGER.warning(\"Empty GeoDataFrame provided, returning as-is\")\n        return gdf\n\n    # read country admin data\n    admin1_data = AdminBoundaries.create(\n        country_code=country_code, admin_level=1, data_store=data_store\n    ).to_geodataframe()\n\n    admin1_data.rename(\n        columns={\"id\": f\"admin1_id{admin_id_column_suffix}\", \"name\": \"admin1\"},\n        inplace=True,\n    )\n    admin1_data.drop(columns=[\"name_en\", \"parent_id\", \"country_code\"], inplace=True)\n\n    admin2_data = AdminBoundaries.create(\n        country_code=country_code, admin_level=2, data_store=data_store\n    ).to_geodataframe()\n\n    admin2_data.rename(\n        columns={\n            \"id\": f\"admin2_id{admin_id_column_suffix}\",\n            \"parent_id\": f\"admin1_id{admin_id_column_suffix}\",\n            \"name\": \"admin2\",\n        },\n        inplace=True,\n    )\n    admin2_data.drop(columns=[\"name_en\", \"country_code\"], inplace=True)\n\n    # Join dataframes based on 'admin1_id_giga'\n    admin_data = admin2_data.merge(\n        admin1_data[[f\"admin1_id{admin_id_column_suffix}\", \"admin1\", \"geometry\"]],\n        left_on=f\"admin1_id{admin_id_column_suffix}\",\n        right_on=f\"admin1_id{admin_id_column_suffix}\",\n        how=\"outer\",\n    )\n\n    admin_data[\"geometry\"] = admin_data.apply(\n        lambda x: x.geometry_x if x.geometry_x else x.geometry_y, axis=1\n    )\n\n    admin_data = gpd.GeoDataFrame(\n        admin_data.drop(columns=[\"geometry_x\", \"geometry_y\"]),\n        geometry=\"geometry\",\n        crs=4326,\n    )\n\n    admin_data[\"admin2\"].fillna(\"Unknown\", inplace=True)\n    admin_data[f\"admin2_id{admin_id_column_suffix}\"] = admin_data[\n        f\"admin2_id{admin_id_column_suffix}\"\n    ].replace({np.nan: None})\n\n    if gdf.crs is None:\n        LOGGER.warning(\"Input GeoDataFrame has no CRS, assuming EPSG:4326\")\n        gdf.set_crs(epsg=4326, inplace=True)\n    elif gdf.crs != \"EPSG:4326\":\n        LOGGER.info(f\"Reprojecting from {gdf.crs} to EPSG:4326\")\n        gdf = gdf.to_crs(epsg=4326)\n\n    # spatial join gdf to admins\n    gdf_w_admins = gdf.copy().sjoin(\n        admin_data,\n        how=\"left\",\n        predicate=\"intersects\",\n    )\n\n    # Check for duplicates caused by points intersecting multiple polygons\n    if len(gdf_w_admins) != len(gdf):\n        LOGGER.warning(\n            \"Some points intersect multiple administrative boundaries. Resolving conflicts...\"\n        )\n\n        # Group by original index and select the closest admin area for ties\n        gdf_w_admins[\"distance\"] = gdf_w_admins.apply(\n            lambda row: row.geometry.distance(\n                admin_data.loc[row.index_right, \"geometry\"].centroid\n            ),\n            axis=1,\n        )\n\n        # For points with multiple matches, keep the closest polygon\n        gdf_w_admins = gdf_w_admins.loc[\n            gdf_w_admins.groupby(gdf.index)[\"distance\"].idxmin()\n        ].drop(columns=\"distance\")\n\n    # Drop unnecessary columns and reset the index\n    gdf_w_admins = gdf_w_admins.drop(columns=\"index_right\").reset_index(drop=True)\n\n    return gdf_w_admins\n</code></pre>"},{"location":"api/processing/#gigaspatial.processing.geo.buffer_geodataframe","title":"<code>buffer_geodataframe(gdf, buffer_distance_meters, cap_style='round', copy=True)</code>","text":"<p>Buffers a GeoDataFrame with a given buffer distance in meters.</p> <ul> <li>gdf : geopandas.GeoDataFrame     The GeoDataFrame to be buffered.</li> <li>buffer_distance_meters : float     The buffer distance in meters.</li> <li>cap_style : str, optional     The style of caps. round, flat, square. Default is round.</li> </ul> <ul> <li>geopandas.GeoDataFrame     The buffered GeoDataFrame.</li> </ul> Source code in <code>gigaspatial/processing/geo.py</code> <pre><code>def buffer_geodataframe(\n    gdf: gpd.GeoDataFrame,\n    buffer_distance_meters: Union[float, np.array, pd.Series],\n    cap_style: Literal[\"round\", \"square\", \"flat\"] = \"round\",\n    copy=True,\n) -&gt; gpd.GeoDataFrame:\n    \"\"\"\n    Buffers a GeoDataFrame with a given buffer distance in meters.\n\n    Parameters:\n    - gdf : geopandas.GeoDataFrame\n        The GeoDataFrame to be buffered.\n    - buffer_distance_meters : float\n        The buffer distance in meters.\n    - cap_style : str, optional\n        The style of caps. round, flat, square. Default is round.\n\n    Returns:\n    - geopandas.GeoDataFrame\n        The buffered GeoDataFrame.\n    \"\"\"\n\n    # Input validation\n    if not isinstance(gdf, gpd.GeoDataFrame):\n        raise TypeError(\"Input must be a GeoDataFrame\")\n\n    if cap_style not in [\"round\", \"square\", \"flat\"]:\n        raise ValueError(\"cap_style must be round, flat or square.\")\n\n    if gdf.crs is None:\n        raise ValueError(\"Input GeoDataFrame must have a defined CRS\")\n\n    # Create a copy if requested\n    gdf_work = gdf.copy() if copy else gdf\n\n    # Store input CRS\n    input_crs = gdf_work.crs\n\n    try:\n        try:\n            utm_crs = gdf_work.estimate_utm_crs()\n        except Exception as e:\n            LOGGER.warning(\n                f\"Warning: UTM CRS estimation failed, using Web Mercator. Error: {e}\"\n            )\n            utm_crs = \"EPSG:3857\"  # Fallback to Web Mercator\n\n        # Transform to UTM, create buffer, and transform back\n        gdf_work = gdf_work.to_crs(utm_crs)\n        gdf_work[\"geometry\"] = gdf_work[\"geometry\"].buffer(\n            distance=buffer_distance_meters, cap_style=cap_style\n        )\n        gdf_work = gdf_work.to_crs(input_crs)\n\n        return gdf_work\n\n    except Exception as e:\n        raise RuntimeError(f\"Error during buffering operation: {str(e)}\")\n</code></pre>"},{"location":"api/processing/#gigaspatial.processing.geo.convert_to_geodataframe","title":"<code>convert_to_geodataframe(data, lat_col=None, lon_col=None, crs='EPSG:4326')</code>","text":"<p>Convert a pandas DataFrame to a GeoDataFrame, either from latitude/longitude columns or from a WKT geometry column.</p>"},{"location":"api/processing/#gigaspatial.processing.geo.convert_to_geodataframe--parameters","title":"Parameters:","text":"<p>data : pandas.DataFrame     Input DataFrame containing either lat/lon columns or a geometry column. lat_col : str, optional     Name of the latitude column. Default is 'lat'. lon_col : str, optional     Name of the longitude column. Default is 'lon'. crs : str or pyproj.CRS, optional     Coordinate Reference System of the geometry data. Default is 'EPSG:4326'.</p>"},{"location":"api/processing/#gigaspatial.processing.geo.convert_to_geodataframe--returns","title":"Returns:","text":"<p>geopandas.GeoDataFrame     A GeoDataFrame containing the input data with a geometry column.</p>"},{"location":"api/processing/#gigaspatial.processing.geo.convert_to_geodataframe--raises","title":"Raises:","text":"<p>TypeError     If input is not a pandas DataFrame. ValueError     If required columns are missing or contain invalid data.</p> Source code in <code>gigaspatial/processing/geo.py</code> <pre><code>def convert_to_geodataframe(\n    data: pd.DataFrame, lat_col: str = None, lon_col: str = None, crs=\"EPSG:4326\"\n) -&gt; gpd.GeoDataFrame:\n    \"\"\"\n    Convert a pandas DataFrame to a GeoDataFrame, either from latitude/longitude columns\n    or from a WKT geometry column.\n\n    Parameters:\n    ----------\n    data : pandas.DataFrame\n        Input DataFrame containing either lat/lon columns or a geometry column.\n    lat_col : str, optional\n        Name of the latitude column. Default is 'lat'.\n    lon_col : str, optional\n        Name of the longitude column. Default is 'lon'.\n    crs : str or pyproj.CRS, optional\n        Coordinate Reference System of the geometry data. Default is 'EPSG:4326'.\n\n    Returns:\n    -------\n    geopandas.GeoDataFrame\n        A GeoDataFrame containing the input data with a geometry column.\n\n    Raises:\n    ------\n    TypeError\n        If input is not a pandas DataFrame.\n    ValueError\n        If required columns are missing or contain invalid data.\n    \"\"\"\n\n    # Input validation\n    if not isinstance(data, pd.DataFrame):\n        raise TypeError(\"Input 'data' must be a pandas DataFrame\")\n\n    # Create a copy to avoid modifying the input\n    df = data.copy()\n\n    try:\n        if \"geometry\" not in df.columns:\n            # If column names not provided, try to detect them\n            if lat_col is None or lon_col is None:\n                try:\n                    detected_lat, detected_lon = detect_coordinate_columns(df)\n                    lat_col = lat_col or detected_lat\n                    lon_col = lon_col or detected_lon\n                except ValueError as e:\n                    raise ValueError(\n                        f\"Could not automatically detect coordinate columns and no \"\n                        f\"'geometry' column found. Error: {str(e)}\"\n                    )\n\n            # Validate latitude/longitude columns exist\n            if lat_col not in df.columns or lon_col not in df.columns:\n                raise ValueError(\n                    f\"Could not find columns: {lat_col} and/or {lon_col} in the DataFrame\"\n                )\n\n            # Check for missing values\n            if df[lat_col].isna().any() or df[lon_col].isna().any():\n                raise ValueError(\n                    f\"Missing values found in {lat_col} and/or {lon_col} columns\"\n                )\n\n            # Create geometry from lat/lon\n            geometry = gpd.points_from_xy(x=df[lon_col], y=df[lat_col])\n\n        else:\n            # Check if geometry column already contains valid geometries\n            if df[\"geometry\"].apply(lambda x: isinstance(x, base.BaseGeometry)).all():\n                geometry = df[\"geometry\"]\n            elif df[\"geometry\"].apply(lambda x: isinstance(x, str)).all():\n                # Convert WKT strings to geometry objects\n                geometry = df[\"geometry\"].apply(wkt.loads)\n            else:\n                raise ValueError(\n                    \"Invalid geometry format: contains mixed or unsupported types\"\n                )\n\n        # drop the WKT column if conversion was done\n        if (\n            \"geometry\" in df.columns\n            and not df[\"geometry\"]\n            .apply(lambda x: isinstance(x, base.BaseGeometry))\n            .all()\n        ):\n            df = df.drop(columns=[\"geometry\"])\n\n        return gpd.GeoDataFrame(df, geometry=geometry, crs=crs)\n\n    except Exception as e:\n        raise RuntimeError(f\"Error converting to GeoDataFrame: {str(e)}\")\n</code></pre>"},{"location":"api/processing/#gigaspatial.processing.geo.detect_coordinate_columns","title":"<code>detect_coordinate_columns(data, lat_keywords=None, lon_keywords=None, case_sensitive=False)</code>","text":"<p>Detect latitude and longitude columns in a DataFrame using keyword matching.</p>"},{"location":"api/processing/#gigaspatial.processing.geo.detect_coordinate_columns--parameters","title":"Parameters:","text":"<p>data : pandas.DataFrame     DataFrame to search for coordinate columns. lat_keywords : list of str, optional     Keywords for identifying latitude columns. If None, uses default keywords. lon_keywords : list of str, optional     Keywords for identifying longitude columns. If None, uses default keywords. case_sensitive : bool, optional     Whether to perform case-sensitive matching. Default is False.</p>"},{"location":"api/processing/#gigaspatial.processing.geo.detect_coordinate_columns--returns","title":"Returns:","text":"<p>tuple[str, str]     Names of detected (latitude, longitude) columns.</p>"},{"location":"api/processing/#gigaspatial.processing.geo.detect_coordinate_columns--raises","title":"Raises:","text":"<p>ValueError     If no unique pair of latitude/longitude columns can be found. TypeError     If input data is not a pandas DataFrame.</p> Source code in <code>gigaspatial/processing/geo.py</code> <pre><code>def detect_coordinate_columns(\n    data, lat_keywords=None, lon_keywords=None, case_sensitive=False\n) -&gt; Tuple[str, str]:\n    \"\"\"\n    Detect latitude and longitude columns in a DataFrame using keyword matching.\n\n    Parameters:\n    ----------\n    data : pandas.DataFrame\n        DataFrame to search for coordinate columns.\n    lat_keywords : list of str, optional\n        Keywords for identifying latitude columns. If None, uses default keywords.\n    lon_keywords : list of str, optional\n        Keywords for identifying longitude columns. If None, uses default keywords.\n    case_sensitive : bool, optional\n        Whether to perform case-sensitive matching. Default is False.\n\n    Returns:\n    -------\n    tuple[str, str]\n        Names of detected (latitude, longitude) columns.\n\n    Raises:\n    ------\n    ValueError\n        If no unique pair of latitude/longitude columns can be found.\n    TypeError\n        If input data is not a pandas DataFrame.\n    \"\"\"\n\n    # Default keywords if none provided\n    default_lat = [\n        \"latitude\",\n        \"lat\",\n        \"y\",\n        \"lat_\",\n        \"lat(s)\",\n        \"_lat\",\n        \"ylat\",\n        \"latitude_y\",\n    ]\n    default_lon = [\n        \"longitude\",\n        \"lon\",\n        \"long\",\n        \"x\",\n        \"lon_\",\n        \"lon(e)\",\n        \"long(e)\",\n        \"_lon\",\n        \"xlon\",\n        \"longitude_x\",\n    ]\n\n    lat_keywords = lat_keywords or default_lat\n    lon_keywords = lon_keywords or default_lon\n\n    # Input validation\n    if not isinstance(data, pd.DataFrame):\n        raise TypeError(\"Input must be a pandas DataFrame\")\n\n    if not data.columns.is_unique:\n        raise ValueError(\"DataFrame contains duplicate column names\")\n\n    def create_pattern(keywords):\n        \"\"\"Create regex pattern from keywords.\"\"\"\n        return \"|\".join(rf\"\\b{re.escape(keyword)}\\b\" for keyword in keywords)\n\n    def find_matching_columns(columns, pattern, case_sensitive) -&gt; List:\n        \"\"\"Find columns matching the pattern.\"\"\"\n        flags = 0 if case_sensitive else re.IGNORECASE\n        return [col for col in columns if re.search(pattern, col, flags=flags)]\n\n    try:\n        # Create patterns\n        lat_pattern = create_pattern(lat_keywords)\n        lon_pattern = create_pattern(lon_keywords)\n\n        # Find matching columns\n        lat_cols = find_matching_columns(data.columns, lat_pattern, case_sensitive)\n        lon_cols = find_matching_columns(data.columns, lon_pattern, case_sensitive)\n\n        # Remove any longitude matches from latitude columns and vice versa\n        lat_cols = [col for col in lat_cols if col not in lon_cols]\n        lon_cols = [col for col in lon_cols if col not in lat_cols]\n\n        # Detailed error messages based on what was found\n        if not lat_cols and not lon_cols:\n            columns_list = \"\\n\".join(f\"- {col}\" for col in data.columns)\n            raise ValueError(\n                f\"No latitude or longitude columns found. Available columns are:\\n{columns_list}\\n\"\n                f\"Consider adding more keywords or checking column names.\"\n            )\n\n        if not lat_cols:\n            found_lons = \", \".join(lon_cols)\n            raise ValueError(\n                f\"Found longitude columns ({found_lons}) but no latitude columns. \"\n                \"Check latitude keywords or column names.\"\n            )\n\n        if not lon_cols:\n            found_lats = \", \".join(lat_cols)\n            raise ValueError(\n                f\"Found latitude columns ({found_lats}) but no longitude columns. \"\n                \"Check longitude keywords or column names.\"\n            )\n\n        if len(lat_cols) &gt; 1 or len(lon_cols) &gt; 1:\n            raise ValueError(\n                f\"Multiple possible coordinate columns found:\\n\"\n                f\"Latitude candidates: {lat_cols}\\n\"\n                f\"Longitude candidates: {lon_cols}\\n\"\n                \"Please specify more precise keywords.\"\n            )\n\n        return lat_cols[0], lon_cols[0]\n\n    except Exception as e:\n        if isinstance(e, ValueError):\n            raise\n        raise RuntimeError(f\"Error detecting coordinate columns: {str(e)}\")\n</code></pre>"},{"location":"api/processing/#gigaspatial.processing.geo.get_centroids","title":"<code>get_centroids(gdf)</code>","text":"<p>Calculate the centroids of a (Multi)Polygon GeoDataFrame.</p>"},{"location":"api/processing/#gigaspatial.processing.geo.get_centroids--parameters","title":"Parameters:","text":"<p>gdf : geopandas.GeoDataFrame     GeoDataFrame containing (Multi)Polygon geometries.</p>"},{"location":"api/processing/#gigaspatial.processing.geo.get_centroids--returns","title":"Returns:","text":"<p>geopandas.GeoDataFrame     A new GeoDataFrame with Point geometries representing the centroids.</p>"},{"location":"api/processing/#gigaspatial.processing.geo.get_centroids--raises","title":"Raises:","text":"<p>ValueError     If the input GeoDataFrame does not contain (Multi)Polygon geometries.</p> Source code in <code>gigaspatial/processing/geo.py</code> <pre><code>def get_centroids(gdf: gpd.GeoDataFrame) -&gt; gpd.GeoDataFrame:\n    \"\"\"\n    Calculate the centroids of a (Multi)Polygon GeoDataFrame.\n\n    Parameters:\n    ----------\n    gdf : geopandas.GeoDataFrame\n        GeoDataFrame containing (Multi)Polygon geometries.\n\n    Returns:\n    -------\n    geopandas.GeoDataFrame\n        A new GeoDataFrame with Point geometries representing the centroids.\n\n    Raises:\n    ------\n    ValueError\n        If the input GeoDataFrame does not contain (Multi)Polygon geometries.\n    \"\"\"\n    # Validate input geometries\n    if not all(gdf.geometry.geom_type.isin([\"Polygon\", \"MultiPolygon\"])):\n        raise ValueError(\n            \"Input GeoDataFrame must contain only Polygon or MultiPolygon geometries.\"\n        )\n\n    # Calculate centroids\n    centroids = gdf.copy()\n    centroids[\"geometry\"] = centroids.geometry.centroid\n\n    return centroids\n</code></pre>"},{"location":"api/processing/#gigaspatial.processing.geo.map_points_within_polygons","title":"<code>map_points_within_polygons(base_points_gdf, polygon_gdf)</code>","text":"<p>Maps whether each point in <code>base_points_gdf</code> is within any polygon in <code>polygon_gdf</code>.</p>"},{"location":"api/processing/#gigaspatial.processing.geo.map_points_within_polygons--parameters","title":"Parameters:","text":"<p>base_points_gdf : geopandas.GeoDataFrame     GeoDataFrame containing point geometries to check. polygon_gdf : geopandas.GeoDataFrame     GeoDataFrame containing polygon geometries.</p>"},{"location":"api/processing/#gigaspatial.processing.geo.map_points_within_polygons--returns","title":"Returns:","text":"<p>geopandas.GeoDataFrame     The <code>base_points_gdf</code> with an additional column <code>is_within</code> (True/False).</p>"},{"location":"api/processing/#gigaspatial.processing.geo.map_points_within_polygons--raises","title":"Raises:","text":"<p>ValueError     If the geometries in either GeoDataFrame are invalid or not of the expected type.</p> Source code in <code>gigaspatial/processing/geo.py</code> <pre><code>def map_points_within_polygons(base_points_gdf, polygon_gdf):\n    \"\"\"\n    Maps whether each point in `base_points_gdf` is within any polygon in `polygon_gdf`.\n\n    Parameters:\n    ----------\n    base_points_gdf : geopandas.GeoDataFrame\n        GeoDataFrame containing point geometries to check.\n    polygon_gdf : geopandas.GeoDataFrame\n        GeoDataFrame containing polygon geometries.\n\n    Returns:\n    -------\n    geopandas.GeoDataFrame\n        The `base_points_gdf` with an additional column `is_within` (True/False).\n\n    Raises:\n    ------\n    ValueError\n        If the geometries in either GeoDataFrame are invalid or not of the expected type.\n    \"\"\"\n    # Validate input GeoDataFrames\n    if not all(base_points_gdf.geometry.geom_type == \"Point\"):\n        raise ValueError(\"`base_points_gdf` must contain only Point geometries.\")\n    if not all(polygon_gdf.geometry.geom_type.isin([\"Polygon\", \"MultiPolygon\"])):\n        raise ValueError(\n            \"`polygon_gdf` must contain only Polygon or MultiPolygon geometries.\"\n        )\n\n    if not base_points_gdf.crs == polygon_gdf.crs:\n        raise ValueError(\"CRS of `base_points_gdf` and `polygon_gdf` must match.\")\n\n    # Perform spatial join to check if points fall within any polygon\n    joined_gdf = gpd.sjoin(\n        base_points_gdf, polygon_gdf[[\"geometry\"]], how=\"left\", predicate=\"within\"\n    )\n\n    # Add `is_within` column to base_points_gdf\n    base_points_gdf[\"is_within\"] = base_points_gdf.index.isin(\n        set(joined_gdf.index[~joined_gdf.index_right.isna()])\n    )\n\n    return base_points_gdf\n</code></pre>"},{"location":"api/processing/#gigaspatial.processing.geo.simplify_geometries","title":"<code>simplify_geometries(gdf, tolerance=0.01, preserve_topology=True, geometry_column='geometry')</code>","text":"<p>Simplify geometries in a GeoDataFrame to reduce file size and improve visualization performance.</p>"},{"location":"api/processing/#gigaspatial.processing.geo.simplify_geometries--parameters","title":"Parameters","text":"<p>gdf : geopandas.GeoDataFrame     GeoDataFrame containing geometries to simplify. tolerance : float, optional     Tolerance for simplification. Larger values simplify more but reduce detail (default is 0.01). preserve_topology : bool, optional     Whether to preserve topology while simplifying. Preserving topology prevents invalid geometries (default is True). geometry_column : str, optional     Name of the column containing geometries (default is \"geometry\").</p>"},{"location":"api/processing/#gigaspatial.processing.geo.simplify_geometries--returns","title":"Returns","text":"<p>geopandas.GeoDataFrame     A new GeoDataFrame with simplified geometries.</p>"},{"location":"api/processing/#gigaspatial.processing.geo.simplify_geometries--raises","title":"Raises","text":"<p>ValueError     If the specified geometry column does not exist or contains invalid geometries. TypeError     If the geometry column does not contain valid geometries.</p>"},{"location":"api/processing/#gigaspatial.processing.geo.simplify_geometries--examples","title":"Examples","text":"<p>Simplify geometries in a GeoDataFrame:</p> <p>simplified_gdf = simplify_geometries(gdf, tolerance=0.05)</p> Source code in <code>gigaspatial/processing/geo.py</code> <pre><code>def simplify_geometries(\n    gdf: gpd.GeoDataFrame,\n    tolerance: float = 0.01,\n    preserve_topology: bool = True,\n    geometry_column: str = \"geometry\",\n) -&gt; gpd.GeoDataFrame:\n    \"\"\"\n    Simplify geometries in a GeoDataFrame to reduce file size and improve visualization performance.\n\n    Parameters\n    ----------\n    gdf : geopandas.GeoDataFrame\n        GeoDataFrame containing geometries to simplify.\n    tolerance : float, optional\n        Tolerance for simplification. Larger values simplify more but reduce detail (default is 0.01).\n    preserve_topology : bool, optional\n        Whether to preserve topology while simplifying. Preserving topology prevents invalid geometries (default is True).\n    geometry_column : str, optional\n        Name of the column containing geometries (default is \"geometry\").\n\n    Returns\n    -------\n    geopandas.GeoDataFrame\n        A new GeoDataFrame with simplified geometries.\n\n    Raises\n    ------\n    ValueError\n        If the specified geometry column does not exist or contains invalid geometries.\n    TypeError\n        If the geometry column does not contain valid geometries.\n\n    Examples\n    --------\n    Simplify geometries in a GeoDataFrame:\n    &gt;&gt;&gt; simplified_gdf = simplify_geometries(gdf, tolerance=0.05)\n    \"\"\"\n\n    # Check if the specified geometry column exists\n    if geometry_column not in gdf.columns:\n        raise ValueError(\n            f\"Geometry column '{geometry_column}' not found in the GeoDataFrame.\"\n        )\n\n    # Check if the specified column contains geometries\n    if not gpd.GeoSeries(gdf[geometry_column]).is_valid.all():\n        raise TypeError(\n            f\"Geometry column '{geometry_column}' contains invalid geometries.\"\n        )\n\n    # Simplify geometries (non-destructive)\n    gdf_simplified = gdf.copy()\n    gdf_simplified[geometry_column] = gdf_simplified[geometry_column].simplify(\n        tolerance=tolerance, preserve_topology=preserve_topology\n    )\n\n    return gdf_simplified\n</code></pre>"},{"location":"api/processing/#gigaspatial.processing.sat_images","title":"<code>sat_images</code>","text":""},{"location":"api/processing/#gigaspatial.processing.sat_images.calculate_pixels_at_location","title":"<code>calculate_pixels_at_location(gdf, resolution, bbox_size=300, crs='EPSG:3857')</code>","text":"<p>Calculates the number of pixels required to cover a given bounding box around a geographic coordinate, given a resolution in meters per pixel.</p> <p>Parameters:</p> Name Type Description Default <code>gdf</code> <p>a geodataframe with Point geometries that are geographic coordinates</p> required <code>resolution</code> <code>float</code> <p>Desired resolution (meters per pixel).</p> required <code>bbox_size</code> <code>float</code> <p>Bounding box size in meters (default 300m x 300m).</p> <code>300</code> <code>crs</code> <code>str</code> <p>Target projection (default is EPSG:3857).</p> <code>'EPSG:3857'</code> <p>Returns:</p> Name Type Description <code>int</code> <p>Number of pixels per side (width and height).</p> Source code in <code>gigaspatial/processing/sat_images.py</code> <pre><code>def calculate_pixels_at_location(gdf, resolution, bbox_size=300, crs=\"EPSG:3857\"):\n    \"\"\"\n    Calculates the number of pixels required to cover a given bounding box\n    around a geographic coordinate, given a resolution in meters per pixel.\n\n    Parameters:\n        gdf: a geodataframe with Point geometries that are geographic coordinates\n        resolution (float): Desired resolution (meters per pixel).\n        bbox_size (float): Bounding box size in meters (default 300m x 300m).\n        crs (str): Target projection (default is EPSG:3857).\n\n    Returns:\n        int: Number of pixels per side (width and height).\n    \"\"\"\n\n    # Calculate avg lat and lon\n    lon = gdf.geometry.x.mean()\n    lat = gdf.geometry.y.mean()\n\n    # Define projections\n    wgs84 = pyproj.CRS(\"EPSG:4326\")  # Geographic coordinate system\n    mercator = pyproj.CRS(crs)  # Target CRS (EPSG:3857)\n\n    # Transform the center coordinate to EPSG:3857\n    transformer = pyproj.Transformer.from_crs(wgs84, mercator, always_xy=True)\n    x, y = transformer.transform(lon, lat)\n\n    # Calculate scale factor (distortion) at given latitude\n    scale_factor = np.cos(np.radians(lat))  # Mercator scale correction\n\n    # Adjust the effective resolution\n    effective_resolution = resolution * scale_factor\n\n    # Compute number of pixels per side\n    pixels = bbox_size / effective_resolution\n    return int(round(pixels))\n</code></pre>"},{"location":"api/processing/#gigaspatial.processing.tif_processor","title":"<code>tif_processor</code>","text":""},{"location":"api/processing/#gigaspatial.processing.tif_processor.TifProcessor","title":"<code>TifProcessor</code>","text":"<p>A class to handle tif data processing, supporting single-band, RGB, RGBA, and multi-band data. Can merge multiple rasters into one during initialization.</p> Source code in <code>gigaspatial/processing/tif_processor.py</code> <pre><code>@dataclass(config=ConfigDict(arbitrary_types_allowed=True))\nclass TifProcessor:\n    \"\"\"\n    A class to handle tif data processing, supporting single-band, RGB, RGBA, and multi-band data.\n    Can merge multiple rasters into one during initialization.\n    \"\"\"\n\n    dataset_path: Union[Path, str, List[Union[Path, str]]]\n    data_store: Optional[DataStore] = None\n    mode: Literal[\"single\", \"rgb\", \"rgba\", \"multi\"] = \"single\"\n    merge_method: Literal[\"first\", \"last\", \"min\", \"max\", \"mean\"] = \"first\"\n    target_crs: Optional[str] = None  # For reprojection if needed\n    resampling_method: Resampling = Resampling.nearest\n    reprojection_resolution: Optional[Tuple[float, float]] = None\n\n    def __post_init__(self):\n        \"\"\"Validate inputs, merge rasters if needed, and set up logging.\"\"\"\n        self.data_store = self.data_store or LocalDataStore()\n        self.logger = config.get_logger(self.__class__.__name__)\n        self._cache = {}\n        self._temp_dir = tempfile.mkdtemp()\n        self._merged_file_path = None\n        self._reprojected_file_path = None\n        self._clipped_file_path = None\n\n        # Handle multiple dataset paths\n        if isinstance(self.dataset_path, list):\n            if len(self.dataset_path) &gt; 1:\n                self.dataset_paths = [Path(p) for p in self.dataset_path]\n                self._validate_multiple_datasets()\n                self._merge_rasters()\n                self.dataset_path = self._merged_file_path\n        else:\n            self.dataset_paths = [Path(self.dataset_path)]\n            # For absolute paths with LocalDataStore, check file existence directly\n            # to avoid path resolution issues\n            if isinstance(self.data_store, LocalDataStore) and os.path.isabs(\n                str(self.dataset_path)\n            ):\n                if not os.path.exists(str(self.dataset_path)):\n                    raise FileNotFoundError(f\"Dataset not found at {self.dataset_path}\")\n            elif not self.data_store.file_exists(str(self.dataset_path)):\n                raise FileNotFoundError(f\"Dataset not found at {self.dataset_path}\")\n\n            # Reproject single raster during initialization if target_crs is set\n            if self.target_crs:\n                self.logger.info(f\"Reprojecting single raster to {self.target_crs}...\")\n                with self.data_store.open(str(self.dataset_path), \"rb\") as f:\n                    with rasterio.MemoryFile(f.read()) as memfile:\n                        with memfile.open() as src:\n                            self._reprojected_file_path = self._reproject_to_temp_file(\n                                src, self.target_crs\n                            )\n                self.dataset_path = self._reprojected_file_path\n\n        self._load_metadata()\n        self._validate_mode_band_compatibility()\n\n    @contextmanager\n    def open_dataset(self):\n        \"\"\"Context manager for accessing the dataset, handling temporary reprojected files.\"\"\"\n        if self._merged_file_path:\n            with rasterio.open(self._merged_file_path) as src:\n                yield src\n        elif self._reprojected_file_path:\n            with rasterio.open(self._reprojected_file_path) as src:\n                yield src\n        elif self._clipped_file_path:\n            with rasterio.open(self._clipped_file_path) as src:\n                yield src\n        elif isinstance(self.data_store, LocalDataStore):\n            with rasterio.open(str(self.dataset_path)) as src:\n                yield src\n        else:\n            with self.data_store.open(str(self.dataset_path), \"rb\") as f:\n                with rasterio.MemoryFile(f.read()) as memfile:\n                    with memfile.open() as src:\n                        yield src\n\n    def reproject_to(\n        self,\n        target_crs: str,\n        output_path: Optional[Union[str, Path]] = None,\n        resampling_method: Optional[Resampling] = None,\n        resolution: Optional[Tuple[float, float]] = None,\n    ):\n        \"\"\"\n        Reprojects the current raster to a new CRS and optionally saves it.\n\n        Args:\n            target_crs: The CRS to reproject to (e.g., \"EPSG:4326\").\n            output_path: The path to save the reprojected raster. If None,\n                         it is saved to a temporary file.\n            resampling_method: The resampling method to use.\n            resolution: The target resolution (pixel size) in the new CRS.\n        \"\"\"\n        self.logger.info(f\"Reprojecting raster to {target_crs}...\")\n\n        # Use provided or default values\n        resampling_method = resampling_method or self.resampling_method\n        resolution = resolution or self.reprojection_resolution\n\n        with self.open_dataset() as src:\n            if src.crs.to_string() == target_crs:\n                self.logger.info(\n                    \"Raster is already in the target CRS. No reprojection needed.\"\n                )\n                # If output_path is specified, copy the file\n                if output_path:\n                    self.data_store.copy_file(str(self.dataset_path), output_path)\n                return self.dataset_path\n\n            dst_path = output_path or os.path.join(\n                self._temp_dir, f\"reprojected_single_{os.urandom(8).hex()}.tif\"\n            )\n\n            with rasterio.open(\n                dst_path,\n                \"w\",\n                **self._get_reprojection_profile(src, target_crs, resolution),\n            ) as dst:\n                for band_idx in range(1, src.count + 1):\n                    reproject(\n                        source=rasterio.band(src, band_idx),\n                        destination=rasterio.band(dst, band_idx),\n                        src_transform=src.transform,\n                        src_crs=src.crs,\n                        dst_transform=dst.transform,\n                        dst_crs=dst.crs,\n                        resampling=resampling_method,\n                        num_threads=multiprocessing.cpu_count(),\n                    )\n\n            self.logger.info(f\"Reprojection complete. Output saved to {dst_path}\")\n            return Path(dst_path)\n\n    def get_raster_info(self) -&gt; Dict[str, Any]:\n        \"\"\"Get comprehensive raster information.\"\"\"\n        return {\n            \"count\": self.count,\n            \"width\": self.width,\n            \"height\": self.height,\n            \"crs\": self.crs,\n            \"bounds\": self.bounds,\n            \"transform\": self.transform,\n            \"dtypes\": self.dtype,\n            \"nodata\": self.nodata,\n            \"mode\": self.mode,\n            \"is_merged\": self.is_merged,\n            \"source_count\": self.source_count,\n        }\n\n    def _reproject_to_temp_file(\n        self, src: rasterio.DatasetReader, target_crs: str\n    ) -&gt; str:\n        \"\"\"Helper to reproject a raster and save it to a temporary file.\"\"\"\n        dst_path = os.path.join(\n            self._temp_dir, f\"reprojected_temp_{os.urandom(8).hex()}.tif\"\n        )\n        profile = self._get_reprojection_profile(\n            src, target_crs, self.reprojection_resolution\n        )\n\n        with rasterio.open(dst_path, \"w\", **profile) as dst:\n            for band_idx in range(1, src.count + 1):\n                reproject(\n                    source=rasterio.band(src, band_idx),\n                    destination=rasterio.band(dst, band_idx),\n                    src_transform=src.transform,\n                    src_crs=src.crs,\n                    dst_transform=dst.transform,\n                    dst_crs=dst.crs,\n                    resampling=self.resampling_method,\n                )\n        return dst_path\n\n    def _validate_multiple_datasets(self):\n        \"\"\"Validate that all datasets exist and have compatible properties.\"\"\"\n        if len(self.dataset_paths) &lt; 2:\n            raise ValueError(\"Multiple dataset paths required for merging\")\n\n        with self.data_store.open(str(self.dataset_paths[0]), \"rb\") as f:\n            with rasterio.MemoryFile(f.read()) as memfile:\n                with memfile.open() as ref_src:\n                    ref_count = ref_src.count\n                    ref_dtype = ref_src.dtypes[0]\n                    ref_crs = ref_src.crs\n                    ref_transform = ref_src.transform\n                    ref_nodata = ref_src.nodata\n\n        for i, path in enumerate(self.dataset_paths[1:], 1):\n            with self.data_store.open(str(path), \"rb\") as f:\n                with rasterio.MemoryFile(f.read()) as memfile:\n                    with memfile.open() as src:\n                        if src.count != ref_count:\n                            raise ValueError(\n                                f\"Dataset {i} has {src.count} bands, expected {ref_count}\"\n                            )\n                        if src.dtypes[0] != ref_dtype:\n                            raise ValueError(\n                                f\"Dataset {i} has dtype {src.dtypes[0]}, expected {ref_dtype}\"\n                            )\n                        if not self.target_crs and src.crs != ref_crs:\n                            self.logger.warning(\n                                f\"Dataset {i} has CRS {src.crs}, expected {ref_crs}. \"\n                                \"Consider setting target_crs parameter for reprojection before merging.\"\n                            )\n                        if self.target_crs is None and not self._transforms_compatible(\n                            src.transform, ref_transform\n                        ):\n                            self.logger.warning(\n                                f\"Dataset {i} has different resolution. Resampling may be needed.\"\n                            )\n                        if src.nodata != ref_nodata:\n                            self.logger.warning(\n                                f\"Dataset {i} has different nodata value: {src.nodata} vs {ref_nodata}\"\n                            )\n\n    def _get_reprojection_profile(\n        self,\n        src: rasterio.DatasetReader,\n        target_crs: str,\n        resolution: Optional[Tuple[float, float]],\n        compression: str = \"lzw\",\n    ):\n        \"\"\"Calculates and returns the profile for a reprojected raster.\"\"\"\n        if resolution:\n            src_res = (abs(src.transform.a), abs(src.transform.e))\n            self.logger.info(\n                f\"Using target resolution: {resolution}. Source resolution: {src_res}.\"\n            )\n            # Calculate transform and dimensions based on the new resolution\n            dst_transform, width, height = calculate_default_transform(\n                src.crs,\n                target_crs,\n                src.width,\n                src.height,\n                *src.bounds,\n                resolution=resolution,\n            )\n        else:\n            # Keep original resolution but reproject\n            dst_transform, width, height = calculate_default_transform(\n                src.crs, target_crs, src.width, src.height, *src.bounds\n            )\n\n        profile = src.profile.copy()\n        profile.update(\n            {\n                \"crs\": target_crs,\n                \"transform\": dst_transform,\n                \"width\": width,\n                \"height\": height,\n                \"compress\": compression,  # Add compression to save space\n            }\n        )\n        return profile\n\n    def _transforms_compatible(self, transform1, transform2, tolerance=1e-6):\n        \"\"\"Check if two transforms have compatible pixel sizes.\"\"\"\n        return (\n            abs(transform1.a - transform2.a) &lt; tolerance\n            and abs(transform1.e - transform2.e) &lt; tolerance\n        )\n\n    def _merge_rasters(self):\n        \"\"\"Merge multiple rasters into a single raster.\"\"\"\n        self.logger.info(f\"Merging {len(self.dataset_paths)} rasters...\")\n\n        # Open all datasets and handle reprojection if needed\n        datasets_to_merge = []\n        temp_reprojected_files = []\n        try:\n            for path in self.dataset_paths:\n                with self.data_store.open(str(path), \"rb\") as f:\n                    with rasterio.MemoryFile(f.read()) as memfile:\n                        with memfile.open() as src:\n                            if self.target_crs and src.crs != self.target_crs:\n                                self.logger.info(\n                                    f\"Reprojecting {path.name} to {self.target_crs} before merging.\"\n                                )\n                                reprojected_path = self._reproject_to_temp_file(\n                                    src, self.target_crs\n                                )\n                                temp_reprojected_files.append(reprojected_path)\n                                datasets_to_merge.append(\n                                    rasterio.open(reprojected_path)\n                                )\n                            else:\n                                temp_path = os.path.join(\n                                    self._temp_dir,\n                                    f\"temp_{path.stem}_{os.urandom(4).hex()}.tif\",\n                                )\n                                temp_reprojected_files.append(temp_path)\n\n                                profile = src.profile\n                                with rasterio.open(temp_path, \"w\", **profile) as dst:\n                                    dst.write(src.read())\n                                datasets_to_merge.append(rasterio.open(temp_path))\n\n            self._merged_file_path = os.path.join(self._temp_dir, \"merged_raster.tif\")\n\n            if self.merge_method == \"mean\":\n                merged_array, merged_transform = self._merge_with_mean(\n                    datasets_to_merge\n                )\n            else:\n                merged_array, merged_transform = merge(\n                    datasets_to_merge,\n                    method=self.merge_method,\n                    resampling=self.resampling_method,\n                )\n\n            # Get profile from the first file in the list (all should be compatible now)\n            ref_src = datasets_to_merge[0]\n            profile = ref_src.profile.copy()\n            profile.update(\n                {\n                    \"height\": merged_array.shape[-2],\n                    \"width\": merged_array.shape[-1],\n                    \"transform\": merged_transform,\n                    \"crs\": self.target_crs if self.target_crs else ref_src.crs,\n                }\n            )\n\n            with rasterio.open(self._merged_file_path, \"w\", **profile) as dst:\n                dst.write(merged_array)\n        finally:\n            for dataset in datasets_to_merge:\n                if hasattr(dataset, \"close\"):\n                    dataset.close()\n\n            # Clean up temporary files immediately\n            for temp_file in temp_reprojected_files:\n                try:\n                    os.remove(temp_file)\n                except OSError:\n                    pass\n\n        self.logger.info(\"Raster merging completed!\")\n\n    def _merge_with_mean(self, src_files):\n        \"\"\"Merge rasters using mean aggregation.\"\"\"\n        # Get bounds and resolution for merged raster\n        bounds = src_files[0].bounds\n        transform = src_files[0].transform\n\n        for src in src_files[1:]:\n            bounds = rasterio.coords.BoundingBox(\n                min(bounds.left, src.bounds.left),\n                min(bounds.bottom, src.bounds.bottom),\n                max(bounds.right, src.bounds.right),\n                max(bounds.top, src.bounds.top),\n            )\n\n        # Calculate dimensions for merged raster\n        width = int((bounds.right - bounds.left) / abs(transform.a))\n        height = int((bounds.top - bounds.bottom) / abs(transform.e))\n\n        # Create new transform for merged bounds\n        merged_transform = rasterio.transform.from_bounds(\n            bounds.left, bounds.bottom, bounds.right, bounds.top, width, height\n        )\n\n        estimated_memory = height * width * src_files[0].count * 8  # float64\n        if estimated_memory &gt; 1e9:  # 1GB threshold\n            self.logger.warning(\n                f\"Large memory usage expected: {estimated_memory/1e9:.1f}GB\"\n            )\n\n        # Initialize arrays for sum and count\n        sum_array = np.zeros((src_files[0].count, height, width), dtype=np.float64)\n        count_array = np.zeros((height, width), dtype=np.int32)\n\n        # Process each source file\n        for src in src_files:\n            # Read data\n            data = src.read()\n\n            # Calculate offset in merged raster\n            src_bounds = src.bounds\n            col_off = int((src_bounds.left - bounds.left) / abs(transform.a))\n            row_off = int((bounds.top - src_bounds.top) / abs(transform.e))\n\n            # Get valid data mask\n            if src.nodata is not None:\n                valid_mask = data[0] != src.nodata\n            else:\n                valid_mask = np.ones(data[0].shape, dtype=bool)\n\n            # Add to sum and count arrays\n            end_row = row_off + data.shape[1]\n            end_col = col_off + data.shape[2]\n\n            sum_array[:, row_off:end_row, col_off:end_col] += np.where(\n                valid_mask, data, 0\n            )\n            count_array[row_off:end_row, col_off:end_col] += valid_mask.astype(np.int32)\n\n        # Calculate mean\n        mean_array = np.divide(\n            sum_array,\n            count_array,\n            out=np.full_like(\n                sum_array, src_files[0].nodata or 0, dtype=sum_array.dtype\n            ),\n            where=count_array &gt; 0,\n        )\n\n        return mean_array.astype(src_files[0].dtypes[0]), merged_transform\n\n    def _load_metadata(self):\n        \"\"\"Load metadata from the TIF file if not already cached\"\"\"\n        try:\n            with self.open_dataset() as src:\n                self._cache[\"transform\"] = src.transform\n                self._cache[\"crs\"] = src.crs.to_string()\n                self._cache[\"bounds\"] = src.bounds\n                self._cache[\"width\"] = src.width\n                self._cache[\"height\"] = src.height\n                self._cache[\"resolution\"] = (abs(src.transform.a), abs(src.transform.e))\n                self._cache[\"x_transform\"] = src.transform.a\n                self._cache[\"y_transform\"] = src.transform.e\n                self._cache[\"nodata\"] = src.nodata\n                self._cache[\"count\"] = src.count\n                self._cache[\"dtype\"] = src.dtypes[0]\n        except (rasterio.errors.RasterioIOError, FileNotFoundError) as e:\n            raise FileNotFoundError(f\"Could not read raster metadata: {e}\")\n        except Exception as e:\n            raise RuntimeError(f\"Unexpected error loading metadata: {e}\")\n\n    @property\n    def is_merged(self) -&gt; bool:\n        \"\"\"Check if this processor was created from multiple rasters.\"\"\"\n        return len(self.dataset_paths) &gt; 1\n\n    @property\n    def source_count(self) -&gt; int:\n        \"\"\"Get the number of source rasters.\"\"\"\n        return len(self.dataset_paths)\n\n    @property\n    def transform(self):\n        \"\"\"Get the transform from the TIF file\"\"\"\n        return self._cache[\"transform\"]\n\n    @property\n    def crs(self):\n        \"\"\"Get the coordinate reference system from the TIF file\"\"\"\n        return self._cache[\"crs\"]\n\n    @property\n    def bounds(self):\n        \"\"\"Get the bounds of the TIF file\"\"\"\n        return self._cache[\"bounds\"]\n\n    @property\n    def resolution(self) -&gt; Tuple[float, float]:\n        \"\"\"Get the x and y resolution (pixel width and height or pixel size) from the TIF file\"\"\"\n        return self._cache[\"resolution\"]\n\n    @property\n    def x_transform(self) -&gt; float:\n        \"\"\"Get the x transform from the TIF file\"\"\"\n        return self._cache[\"x_transform\"]\n\n    @property\n    def y_transform(self) -&gt; float:\n        \"\"\"Get the y transform from the TIF file\"\"\"\n        return self._cache[\"y_transform\"]\n\n    @property\n    def count(self) -&gt; int:\n        \"\"\"Get the band count from the TIF file\"\"\"\n        return self._cache[\"count\"]\n\n    @property\n    def nodata(self) -&gt; int:\n        \"\"\"Get the value representing no data in the rasters\"\"\"\n        return self._cache[\"nodata\"]\n\n    @property\n    def dtype(self):\n        \"\"\"Get the data types from the TIF file\"\"\"\n        return self._cache.get(\"dtype\", [])\n\n    @property\n    def width(self):\n        return self._cache[\"width\"]\n\n    @property\n    def height(self):\n        return self._cache[\"height\"]\n\n    def to_dataframe(\n        self, drop_nodata=True, check_memory=True, **kwargs\n    ) -&gt; pd.DataFrame:\n        \"\"\"\n        Convert raster to DataFrame.\n\n        Args:\n            drop_nodata: Whether to drop nodata values\n            check_memory: Whether to check memory before operation (default True)\n            **kwargs: Additional arguments\n\n        Returns:\n            pd.DataFrame with raster data\n        \"\"\"\n        # Memory guard check\n        if check_memory:\n            self._memory_guard(\"conversion\", threshold_percent=80.0)\n\n        try:\n            if self.mode == \"single\":\n                return self._to_dataframe(\n                    band_number=kwargs.get(\"band_number\", 1),\n                    drop_nodata=drop_nodata,\n                    band_names=kwargs.get(\"band_names\", None),\n                )\n            else:\n                return self._to_dataframe(\n                    band_number=None,  # All bands\n                    drop_nodata=drop_nodata,\n                    band_names=kwargs.get(\"band_names\", None),\n                )\n        except Exception as e:\n            raise ValueError(\n                f\"Failed to process TIF file in mode '{self.mode}'. \"\n                f\"Please ensure the file is valid and matches the selected mode. \"\n                f\"Original error: {str(e)}\"\n            )\n\n        return df\n\n    def to_geodataframe(self, check_memory=True, **kwargs) -&gt; gpd.GeoDataFrame:\n        \"\"\"\n        Convert the processed TIF data into a GeoDataFrame, where each row represents a pixel zone.\n        Each zone is defined by its bounding box, based on pixel resolution and coordinates.\n\n        Args:\n            check_memory: Whether to check memory before operation\n            **kwargs: Additional arguments passed to to_dataframe()\n\n        Returns:\n            gpd.GeoDataFrame with raster data\n        \"\"\"\n        # Memory guard check\n        if check_memory:\n            self._memory_guard(\"conversion\", threshold_percent=80.0)\n\n        df = self.to_dataframe(check_memory=False, **kwargs)\n\n        x_res, y_res = self.resolution\n\n        # create bounding box for each pixel\n        geometries = [\n            box(lon - x_res / 2, lat - y_res / 2, lon + x_res / 2, lat + y_res / 2)\n            for lon, lat in zip(df[\"lon\"], df[\"lat\"])\n        ]\n\n        gdf = gpd.GeoDataFrame(df, geometry=geometries, crs=self.crs)\n\n        return gdf\n\n    def to_dataframe_chunked(\n        self, drop_nodata=True, chunk_size=None, target_memory_mb=500, **kwargs\n    ):\n        \"\"\"\n        Convert raster to DataFrame using chunked processing for memory efficiency.\n\n        Automatically routes to the appropriate chunked method based on mode.\n        Chunk size is automatically calculated based on target memory usage.\n\n        Args:\n            drop_nodata: Whether to drop nodata values\n            chunk_size: Number of rows per chunk (auto-calculated if None)\n            target_memory_mb: Target memory per chunk in MB (default 500)\n            **kwargs: Additional arguments (band_number, band_names, etc.)\n        \"\"\"\n\n        if chunk_size is None:\n            chunk_size = self._calculate_optimal_chunk_size(\n                \"conversion\", target_memory_mb\n            )\n\n        windows = self._get_chunk_windows(chunk_size)\n\n        # SIMPLE ROUTING\n        if self.mode == \"single\":\n            return self._to_dataframe_chunked(\n                windows,\n                band_number=kwargs.get(\"band_number\", 1),\n                drop_nodata=drop_nodata,\n                band_names=kwargs.get(\"band_names\", None),\n            )\n        else:  # rgb, rgba, multi\n            return self._to_dataframe_chunked(\n                windows,\n                band_number=None,\n                drop_nodata=drop_nodata,\n                band_names=kwargs.get(\"band_names\", None),\n            )\n\n    def clip_to_geometry(\n        self,\n        geometry: Union[\n            Polygon, MultiPolygon, gpd.GeoDataFrame, gpd.GeoSeries, List[dict], dict\n        ],\n        crop: bool = True,\n        all_touched: bool = True,\n        invert: bool = False,\n        nodata: Optional[Union[int, float]] = None,\n        pad: bool = False,\n        pad_width: float = 0.5,\n        return_clipped_processor: bool = True,\n    ) -&gt; Union[\"TifProcessor\", tuple]:\n        \"\"\"\n        Clip raster to geometry boundaries.\n\n        Parameters:\n        -----------\n        geometry : various\n            Geometry to clip to. Can be:\n            - Shapely Polygon or MultiPolygon\n            - GeoDataFrame or GeoSeries\n            - List of GeoJSON-like dicts\n            - Single GeoJSON-like dict\n        crop : bool, default True\n            Whether to crop the raster to the extent of the geometry\n        all_touched : bool, default True\n            Include pixels that touch the geometry boundary\n        invert : bool, default False\n            If True, mask pixels inside geometry instead of outside\n        nodata : int or float, optional\n            Value to use for masked pixels. If None, uses raster's nodata value\n        pad : bool, default False\n            Pad geometry by half pixel before clipping\n        pad_width : float, default 0.5\n            Width of padding in pixels if pad=True\n        return_clipped_processor : bool, default True\n            If True, returns new TifProcessor with clipped data\n            If False, returns (clipped_array, transform, metadata)\n\n        Returns:\n        --------\n        TifProcessor or tuple\n            Either new TifProcessor instance or (array, transform, metadata) tuple\n        \"\"\"\n        # Handle different geometry input types\n        shapes = self._prepare_geometry_for_clipping(geometry)\n\n        # Validate CRS compatibility\n        self._validate_geometry_crs(geometry)\n\n        # Perform the clipping\n        with self.open_dataset() as src:\n            try:\n                clipped_data, clipped_transform = mask(\n                    dataset=src,\n                    shapes=shapes,\n                    crop=crop,\n                    all_touched=all_touched,\n                    invert=invert,\n                    nodata=nodata,\n                    pad=pad,\n                    pad_width=pad_width,\n                    filled=True,\n                )\n\n                # Update metadata for the clipped raster\n                clipped_meta = src.meta.copy()\n                clipped_meta.update(\n                    {\n                        \"height\": clipped_data.shape[1],\n                        \"width\": clipped_data.shape[2],\n                        \"transform\": clipped_transform,\n                        \"nodata\": nodata if nodata is not None else src.nodata,\n                    }\n                )\n\n            except ValueError as e:\n                if \"Input shapes do not overlap raster\" in str(e):\n                    raise ValueError(\n                        \"The geometry does not overlap with the raster. \"\n                        \"Check that both are in the same coordinate reference system.\"\n                    ) from e\n                else:\n                    raise e\n\n        if return_clipped_processor:\n            # Create a new TifProcessor with the clipped data\n            return self._create_clipped_processor(clipped_data, clipped_meta)\n        else:\n            return clipped_data, clipped_transform, clipped_meta\n\n    def clip_to_bounds(\n        self,\n        bounds: tuple,\n        bounds_crs: Optional[str] = None,\n        return_clipped_processor: bool = True,\n    ) -&gt; Union[\"TifProcessor\", tuple]:\n        \"\"\"\n        Clip raster to rectangular bounds.\n\n        Parameters:\n        -----------\n        bounds : tuple\n            Bounding box as (minx, miny, maxx, maxy)\n        bounds_crs : str, optional\n            CRS of the bounds. If None, assumes same as raster CRS\n        return_clipped_processor : bool, default True\n            If True, returns new TifProcessor, else returns (array, transform, metadata)\n\n        Returns:\n        --------\n        TifProcessor or tuple\n            Either new TifProcessor instance or (array, transform, metadata) tuple\n        \"\"\"\n        # Create bounding box geometry\n        bbox_geom = box(*bounds)\n\n        # If bounds_crs is specified and different from raster CRS, create GeoDataFrame for reprojection\n        if bounds_crs is not None:\n            raster_crs = self.crs\n\n            if not self.crs == bounds_crs:\n                # Create GeoDataFrame with bounds CRS and reproject\n                bbox_gdf = gpd.GeoDataFrame([1], geometry=[bbox_geom], crs=bounds_crs)\n                bbox_gdf = bbox_gdf.to_crs(raster_crs)\n                bbox_geom = bbox_gdf.geometry.iloc[0]\n\n        return self.clip_to_geometry(\n            geometry=bbox_geom,\n            crop=True,\n            return_clipped_processor=return_clipped_processor,\n        )\n\n    def to_graph(\n        self,\n        connectivity: Literal[4, 8] = 4,\n        band: Optional[int] = None,\n        include_coordinates: bool = False,\n        graph_type: Literal[\"networkx\", \"sparse\"] = \"networkx\",\n        check_memory: bool = True,\n    ) -&gt; Union[nx.Graph, sp.csr_matrix]:\n        \"\"\"\n        Convert raster to graph based on pixel adjacency.\n\n        Args:\n            connectivity: 4 or 8-connectivity\n            band: Band number (1-indexed)\n            include_coordinates: Include x,y coordinates in nodes\n            graph_type: 'networkx' or 'sparse'\n            check_memory: Whether to check memory before operation\n\n        Returns:\n            Graph representation of raster\n        \"\"\"\n\n        # Memory guard check\n        if check_memory:\n            self._memory_guard(\"graph\", threshold_percent=80.0)\n\n        with self.open_dataset() as src:\n            band_idx = band - 1 if band is not None else 0\n            if band_idx &lt; 0 or band_idx &gt;= src.count:\n                raise ValueError(\n                    f\"Band {band} not available. Raster has {src.count} bands\"\n                )\n\n            data = src.read(band_idx + 1)\n            nodata = src.nodata if src.nodata is not None else self.nodata\n            valid_mask = (\n                data != nodata if nodata is not None else np.ones_like(data, dtype=bool)\n            )\n\n            height, width = data.shape\n\n            # Find all valid pixels\n            valid_rows, valid_cols = np.where(valid_mask)\n            num_valid_pixels = len(valid_rows)\n\n            # Create a sequential mapping from (row, col) to a node ID\n            node_map = np.full(data.shape, -1, dtype=int)\n            node_map[valid_rows, valid_cols] = np.arange(num_valid_pixels)\n\n            # Define neighborhood offsets\n            if connectivity == 4:\n                # von Neumann neighborhood (4-connectivity)\n                offsets = [(-1, 0), (1, 0), (0, -1), (0, 1)]\n            else:  # connectivity == 8\n                # Moore neighborhood (8-connectivity)\n                offsets = [\n                    (-1, -1),\n                    (-1, 0),\n                    (-1, 1),\n                    (0, -1),\n                    (0, 1),\n                    (1, -1),\n                    (1, 0),\n                    (1, 1),\n                ]\n\n            # Collect nodes and edges\n            nodes_to_add = []\n            edges_to_add = []\n\n            for i in range(num_valid_pixels):\n                row, col = valid_rows[i], valid_cols[i]\n                current_node_id = node_map[row, col]\n\n                # Prepare node attributes\n                node_attrs = {\"value\": float(data[row, col])}\n                if include_coordinates:\n                    x, y = src.xy(row, col)\n                    node_attrs[\"x\"] = x\n                    node_attrs[\"y\"] = y\n                nodes_to_add.append((current_node_id, node_attrs))\n\n                # Find neighbors and collect edges\n                for dy, dx in offsets:\n                    neighbor_row, neighbor_col = row + dy, col + dx\n\n                    # Check if neighbor is within bounds and is a valid pixel\n                    if (\n                        0 &lt;= neighbor_row &lt; height\n                        and 0 &lt;= neighbor_col &lt; width\n                        and valid_mask[neighbor_row, neighbor_col]\n                    ):\n                        neighbor_node_id = node_map[neighbor_row, neighbor_col]\n\n                        # Ensure each edge is added only once\n                        if current_node_id &lt; neighbor_node_id:\n                            neighbor_value = float(data[neighbor_row, neighbor_col])\n                            edges_to_add.append(\n                                (current_node_id, neighbor_node_id, neighbor_value)\n                            )\n\n            if graph_type == \"networkx\":\n                G = nx.Graph()\n                G.add_nodes_from(nodes_to_add)\n                G.add_weighted_edges_from(edges_to_add)\n                return G\n            else:  # sparse matrix\n                edges_array = np.array(edges_to_add)\n                row_indices = edges_array[:, 0]\n                col_indices = edges_array[:, 1]\n                weights = edges_array[:, 2]\n\n                # Add reverse edges for symmetric matrix\n                from_idx = np.append(row_indices, col_indices)\n                to_idx = np.append(col_indices, row_indices)\n                weights = np.append(weights, weights)\n\n                return sp.coo_matrix(\n                    (weights, (from_idx, to_idx)),\n                    shape=(num_valid_pixels, num_valid_pixels),\n                ).tocsr()\n\n    def sample_by_coordinates(\n        self, coordinate_list: List[Tuple[float, float]], **kwargs\n    ) -&gt; Union[np.ndarray, dict]:\n        self.logger.info(\"Sampling raster values at the coordinates...\")\n\n        with self.open_dataset() as src:\n            if self.mode == \"rgba\":\n                if self.count != 4:\n                    raise ValueError(\"RGBA mode requires a 4-band TIF file\")\n\n                rgba_values = {\"red\": [], \"green\": [], \"blue\": [], \"alpha\": []}\n\n                for band_idx, color in enumerate([\"red\", \"green\", \"blue\", \"alpha\"], 1):\n                    rgba_values[color] = [\n                        vals[0]\n                        for vals in src.sample(coordinate_list, indexes=band_idx)\n                    ]\n\n                return rgba_values\n\n            elif self.mode == \"rgb\":\n                if self.count != 3:\n                    raise ValueError(\"RGB mode requires a 3-band TIF file\")\n\n                rgb_values = {\"red\": [], \"green\": [], \"blue\": []}\n\n                for band_idx, color in enumerate([\"red\", \"green\", \"blue\"], 1):\n                    rgb_values[color] = [\n                        vals[0]\n                        for vals in src.sample(coordinate_list, indexes=band_idx)\n                    ]\n\n                return rgb_values\n            elif self.count &gt; 1:\n                return np.array(\n                    [vals for vals in src.sample(coordinate_list, **kwargs)]\n                )\n            else:\n                return np.array([vals[0] for vals in src.sample(coordinate_list)])\n\n    def sample_by_polygons(\n        self,\n        polygon_list,\n        stat: Union[str, Callable, List[Union[str, Callable]]] = \"mean\",\n    ):\n        \"\"\"\n        Sample raster values by polygons and compute statistic(s) for each polygon.\n\n        Args:\n            polygon_list: List of shapely Polygon or MultiPolygon objects.\n            stat: Statistic(s) to compute. Can be:\n                - Single string: 'mean', 'median', 'sum', 'min', 'max', 'std', 'count'\n                - Single callable: custom function that takes array and returns scalar\n                - List of strings/callables: multiple statistics to compute\n\n        Returns:\n            If single stat: np.ndarray of computed statistics for each polygon\n            If multiple stats: List of dictionaries with stat names as keys\n        \"\"\"\n        # Determine if single or multiple stats\n        single_stat = not isinstance(stat, list)\n        stats_list = [stat] if single_stat else stat\n\n        # Prepare stat functions\n        stat_funcs = []\n        stat_names = []\n\n        for s in stats_list:\n            if callable(s):\n                stat_funcs.append(s)\n                stat_names.append(\n                    s.__name__\n                    if hasattr(s, \"__name__\")\n                    else f\"custom_{len(stat_names)}\"\n                )\n            else:\n                # Handle string statistics\n                if s == \"count\":\n                    stat_funcs.append(len)\n                else:\n                    stat_funcs.append(getattr(np, s))\n                stat_names.append(s)\n\n        results = []\n\n        with self.open_dataset() as src:\n            for polygon in tqdm(polygon_list):\n                try:\n                    out_image, _ = mask(src, [polygon], crop=True, filled=False)\n\n                    # Use masked arrays for more efficient nodata handling\n                    if hasattr(out_image, \"mask\"):\n                        valid_data = out_image.compressed()\n                    else:\n                        valid_data = (\n                            out_image[out_image != self.nodata]\n                            if self.nodata\n                            else out_image.flatten()\n                        )\n\n                    if len(valid_data) == 0:\n                        if single_stat:\n                            results.append(np.nan)\n                        else:\n                            results.append({name: np.nan for name in stat_names})\n                    else:\n                        if single_stat:\n                            results.append(stat_funcs[0](valid_data))\n                        else:\n                            # Compute all statistics for this polygon\n                            polygon_stats = {}\n                            for func, name in zip(stat_funcs, stat_names):\n                                try:\n                                    polygon_stats[name] = func(valid_data)\n                                except Exception:\n                                    polygon_stats[name] = np.nan\n                            results.append(polygon_stats)\n\n                except Exception:\n                    if single_stat:\n                        results.append(np.nan)\n                    else:\n                        results.append({name: np.nan for name in stat_names})\n\n        return np.array(results) if single_stat else results\n\n    def sample_by_polygons_batched(\n        self,\n        polygon_list: List[Union[Polygon, MultiPolygon]],\n        stat: Union[str, Callable] = \"mean\",\n        batch_size: int = 100,\n        n_workers: int = 4,\n        show_progress: bool = True,\n        check_memory: bool = True,\n        **kwargs,\n    ) -&gt; np.ndarray:\n        \"\"\"\n        Sample raster values by polygons in parallel using batching.\n\n        Args:\n            polygon_list: List of Shapely Polygon or MultiPolygon objects\n            stat: Statistic to compute\n            batch_size: Number of polygons per batch\n            n_workers: Number of worker processes\n            show_progress: Whether to display progress bar\n            check_memory: Whether to check memory before operation\n            **kwargs: Additional arguments\n\n        Returns:\n            np.ndarray of statistics for each polygon\n        \"\"\"\n        import sys\n\n        # Memory guard check with n_workers consideration\n        if check_memory:\n            is_safe = self._memory_guard(\n                \"batched_sampling\",\n                threshold_percent=85.0,\n                n_workers=n_workers,\n                raise_error=False,\n            )\n\n            if not is_safe:\n                # Suggest reducing n_workers\n                memory_info = self._check_available_memory()\n                estimates = self._estimate_memory_usage(\"batched_sampling\", n_workers=1)\n\n                # Calculate optimal workers\n                suggested_workers = max(\n                    1, int(memory_info[\"available\"] * 0.7 / estimates[\"per_worker\"])\n                )\n\n                warnings.warn(\n                    f\"Consider reducing n_workers from {n_workers} to {suggested_workers} \"\n                    f\"to reduce memory pressure.\",\n                    ResourceWarning,\n                )\n\n        # Platform check\n        if sys.platform in [\"win32\", \"darwin\"]:\n            import warnings\n            import multiprocessing as mp\n\n            if mp.get_start_method(allow_none=True) != \"fork\":\n                warnings.warn(\n                    \"Batched sampling may not work on Windows/macOS. \"\n                    \"Use sample_by_polygons() if you encounter errors.\",\n                    RuntimeWarning,\n                )\n\n        def _chunk_list(data_list, chunk_size):\n            \"\"\"Yield successive chunks from data_list.\"\"\"\n            for i in range(0, len(data_list), chunk_size):\n                yield data_list[i : i + chunk_size]\n\n        if len(polygon_list) == 0:\n            return np.array([])\n\n        stat_func = stat if callable(stat) else getattr(np, stat)\n        polygon_chunks = list(_chunk_list(polygon_list, batch_size))\n\n        with multiprocessing.Pool(\n            initializer=self._initializer_worker, processes=n_workers\n        ) as pool:\n            process_func = partial(self._process_polygon_batch, stat_func=stat_func)\n            if show_progress:\n                batched_results = list(\n                    tqdm(\n                        pool.imap(process_func, polygon_chunks),\n                        total=len(polygon_chunks),\n                        desc=f\"Sampling polygons\",\n                    )\n                )\n            else:\n                batched_results = list(pool.imap(process_func, polygon_chunks))\n\n            results = [item for sublist in batched_results for item in sublist]\n\n        return np.array(results)\n\n    def _initializer_worker(self):\n        \"\"\"\n        Initializer function for each worker process.\n        Opens the raster dataset and stores it in a process-local variable.\n        This function runs once per worker, not for every task.\n        \"\"\"\n        global src_handle, memfile_handle\n\n        # Priority: merged &gt; reprojected &gt; original (same as open_dataset)\n        local_file_path = None\n        if self._merged_file_path:\n            # Merged file is a local temp file\n            local_file_path = self._merged_file_path\n        elif self._reprojected_file_path:\n            # Reprojected file is a local temp file\n            local_file_path = self._reprojected_file_path\n        elif isinstance(self.data_store, LocalDataStore):\n            # Local file - can open directly\n            local_file_path = str(self.dataset_path)\n\n        if local_file_path:\n            # Open local file directly\n            with open(local_file_path, \"rb\") as f:\n                memfile_handle = rasterio.MemoryFile(f.read())\n                src_handle = memfile_handle.open()\n        else:\n            # Custom DataStore\n            with self.data_store.open(str(self.dataset_path), \"rb\") as f:\n                memfile_handle = rasterio.MemoryFile(f.read())\n                src_handle = memfile_handle.open()\n\n    def _get_worker_dataset(self):\n        \"\"\"Get dataset handle for worker process.\"\"\"\n        global src_handle\n        if src_handle is None:\n            raise RuntimeError(\"Raster dataset not initialized in this process.\")\n        return src_handle\n\n    def _process_single_polygon(self, polygon, stat_func):\n        \"\"\"\n        Helper function to process a single polygon.\n        This will be run in a separate process.\n        \"\"\"\n        try:\n            src = self._get_worker_dataset()\n            out_image, _ = mask(src, [polygon], crop=True, filled=False)\n\n            if hasattr(out_image, \"mask\"):\n                valid_data = out_image.compressed()\n            else:\n                valid_data = (\n                    out_image[out_image != self.nodata]\n                    if self.nodata\n                    else out_image.flatten()\n                )\n\n            return stat_func(valid_data) if len(valid_data) &gt; 0 else np.nan\n        except RuntimeError as e:\n            self.logger.error(f\"Worker not initialized: {e}\")\n            return np.nan\n        except Exception as e:\n            self.logger.debug(f\"Error processing polygon: {e}\")\n            return np.nan\n\n    def _process_polygon_batch(self, polygon_batch, stat_func):\n        \"\"\"\n        Processes a batch of polygons.\n        \"\"\"\n        return [\n            self._process_single_polygon(polygon, stat_func)\n            for polygon in polygon_batch\n        ]\n\n    def _to_dataframe(\n        self,\n        band_number: Optional[int] = None,\n        drop_nodata: bool = True,\n        band_names: Optional[Union[str, List[str]]] = None,\n    ) -&gt; pd.DataFrame:\n        \"\"\"\n        Process TIF to DataFrame - handles both single-band and multi-band.\n\n        Args:\n            band_number: Specific band to read (1-indexed). If None, reads all bands.\n            drop_no Whether to drop nodata values\n            band_names: Custom names for bands (multi-band only)\n\n        Returns:\n            pd.DataFrame with lon, lat, and band value(s)\n        \"\"\"\n        with self.open_dataset() as src:\n            if band_number is not None:\n                # SINGLE BAND MODE\n                band = src.read(band_number)\n                mask = self._build_data_mask(band, drop_nodata, src.nodata)\n                lons, lats = self._extract_coordinates_with_mask(mask)\n                pixel_values = (\n                    np.extract(mask, band) if mask is not None else band.flatten()\n                )\n                band_name = band_names if isinstance(band_names, str) else \"pixel_value\"\n\n                return pd.DataFrame({\"lon\": lons, \"lat\": lats, band_name: pixel_values})\n            else:\n                # MULTI-BAND MODE (all bands)\n                stack = src.read()\n\n                # Auto-detect band names by mode\n                if band_names is None:\n                    if self.mode == \"rgb\":\n                        band_names = [\"red\", \"green\", \"blue\"]\n                    elif self.mode == \"rgba\":\n                        band_names = [\"red\", \"green\", \"blue\", \"alpha\"]\n                    else:\n                        band_names = [\n                            src.descriptions[i] or f\"band_{i+1}\"\n                            for i in range(self.count)\n                        ]\n\n                # Build mask (checks ALL bands!)\n                mask = self._build_multi_band_mask(stack, drop_nodata, src.nodata)\n\n                # Create DataFrame\n                data_dict = self._bands_to_dict(stack, self.count, band_names, mask)\n                df = pd.DataFrame(data_dict)\n\n                # RGBA: normalize alpha if needed\n                if (\n                    self.mode == \"rgba\"\n                    and \"alpha\" in df.columns\n                    and df[\"alpha\"].max() &gt; 1\n                ):\n                    df[\"alpha\"] = df[\"alpha\"] / 255.0\n\n            return df\n\n    def _to_dataframe_chunked(\n        self,\n        windows: List[rasterio.windows.Window],\n        band_number: Optional[int] = None,\n        drop_nodata: bool = True,\n        band_names: Optional[Union[str, List[str]]] = None,\n        show_progress: bool = True,\n    ) -&gt; pd.DataFrame:\n        \"\"\"Universal chunked converter for ALL modes.\"\"\"\n\n        chunks = []\n        iterator = tqdm(windows, desc=\"Processing chunks\") if show_progress else windows\n\n        with self.open_dataset() as src:\n            # Auto-detect band names ONCE (before loop)\n            if band_number is None and band_names is None:\n                if self.mode == \"rgb\":\n                    band_names = [\"red\", \"green\", \"blue\"]\n                elif self.mode == \"rgba\":\n                    band_names = [\"red\", \"green\", \"blue\", \"alpha\"]\n                else:  # multi\n                    band_names = [\n                        src.descriptions[i] or f\"band_{i+1}\" for i in range(self.count)\n                    ]\n\n            for window in iterator:\n                if band_number is not None:\n                    # SINGLE BAND\n                    band_chunk = src.read(band_number, window=window)\n                    mask = self._build_data_mask(band_chunk, drop_nodata, src.nodata)\n                    lons, lats = self._get_chunk_coordinates(window, src)\n                    band_name = (\n                        band_names if isinstance(band_names, str) else \"pixel_value\"\n                    )\n\n                    # Build chunk DataFrame (could use helper but simple enough)\n                    if mask is not None:\n                        mask_flat = mask.flatten()\n                        chunk_df = pd.DataFrame(\n                            {\n                                \"lon\": lons[mask_flat],\n                                \"lat\": lats[mask_flat],\n                                band_name: band_chunk.flatten()[mask_flat],\n                            }\n                        )\n                    else:\n                        chunk_df = pd.DataFrame(\n                            {\"lon\": lons, \"lat\": lats, band_name: band_chunk.flatten()}\n                        )\n                else:\n                    # MULTI-BAND (includes RGB/RGBA)\n                    stack_chunk = src.read(window=window)\n                    mask = self._build_multi_band_mask(\n                        stack_chunk, drop_nodata, src.nodata\n                    )\n                    lons, lats = self._get_chunk_coordinates(window, src)\n\n                    # Build DataFrame using helper\n                    band_dict = {\n                        band_names[i]: stack_chunk[i] for i in range(self.count)\n                    }\n                    chunk_df = self._build_chunk_dataframe(lons, lats, band_dict, mask)\n\n                    # RGBA: normalize alpha\n                    if self.mode == \"rgba\" and \"alpha\" in chunk_df.columns:\n                        if chunk_df[\"alpha\"].max() &gt; 1:\n                            chunk_df[\"alpha\"] = chunk_df[\"alpha\"] / 255.0\n\n                chunks.append(chunk_df)\n\n        result = pd.concat(chunks, ignore_index=True)\n        return result\n\n    def _prepare_geometry_for_clipping(\n        self,\n        geometry: Union[\n            Polygon,\n            MultiPolygon,\n            MultiPoint,\n            gpd.GeoDataFrame,\n            gpd.GeoSeries,\n            List[dict],\n            dict,\n        ],\n    ) -&gt; List[dict]:\n        \"\"\"Convert various geometry formats to list of GeoJSON-like dicts for rasterio.mask\"\"\"\n\n        if isinstance(geometry, MultiPoint):\n            # Use bounding box of MultiPoint\n            minx, miny, maxx, maxy = geometry.bounds\n            bbox = box(minx, miny, maxx, maxy)\n            return [bbox.__geo_interface__]\n\n        if isinstance(geometry, (Polygon, MultiPolygon)):\n            # Shapely geometry\n            return [geometry.__geo_interface__]\n\n        elif isinstance(geometry, gpd.GeoDataFrame):\n            # GeoDataFrame - use all geometries\n            return [\n                geom.__geo_interface__ for geom in geometry.geometry if geom is not None\n            ]\n\n        elif isinstance(geometry, gpd.GeoSeries):\n            # GeoSeries\n            return [geom.__geo_interface__ for geom in geometry if geom is not None]\n\n        elif isinstance(geometry, dict):\n            # Single GeoJSON-like dict\n            return [geometry]\n\n        elif isinstance(geometry, list):\n            # List of GeoJSON-like dicts\n            return geometry\n\n        else:\n            raise TypeError(\n                f\"Unsupported geometry type: {type(geometry)}. \"\n                \"Supported types: Shapely geometries, GeoDataFrame, GeoSeries, \"\n                \"GeoJSON-like dict, or list of GeoJSON-like dicts.\"\n            )\n\n    def _validate_geometry_crs(\n        self,\n        original_geometry: Any,\n    ) -&gt; None:\n        \"\"\"Validate that geometry CRS matches raster CRS\"\"\"\n\n        # Get raster CRS\n        raster_crs = self.crs\n\n        # Try to get geometry CRS\n        geometry_crs = None\n\n        if isinstance(original_geometry, (gpd.GeoDataFrame, gpd.GeoSeries)):\n            geometry_crs = original_geometry.crs\n        elif hasattr(original_geometry, \"crs\"):\n            geometry_crs = original_geometry.crs\n\n        # Warn if CRS mismatch detected\n        if geometry_crs is not None and raster_crs is not None:\n            if not raster_crs == geometry_crs:\n                self.logger.warning(\n                    f\"CRS mismatch detected! Raster CRS: {raster_crs}, \"\n                    f\"Geometry CRS: {geometry_crs}. \"\n                    \"Consider reprojecting geometry to match raster CRS for accurate clipping.\"\n                )\n\n    def _create_clipped_processor(\n        self, clipped_data: np.ndarray, clipped_meta: dict\n    ) -&gt; \"TifProcessor\":\n        \"\"\"\n        Helper to create a new TifProcessor instance from clipped data.\n        Saves the clipped data to a temporary file and initializes a new TifProcessor.\n        \"\"\"\n        # Create a temporary placeholder file to initialize the processor\n        # This allows us to get the processor's temp_dir\n        placeholder_dir = tempfile.mkdtemp()\n        placeholder_path = os.path.join(\n            placeholder_dir, f\"placeholder_{os.urandom(8).hex()}.tif\"\n        )\n\n        # Create a minimal valid TIF file as placeholder\n        placeholder_transform = rasterio.transform.from_bounds(0, 0, 1, 1, 1, 1)\n        with rasterio.open(\n            placeholder_path,\n            \"w\",\n            driver=\"GTiff\",\n            width=1,\n            height=1,\n            count=1,\n            dtype=\"uint8\",\n            crs=\"EPSG:4326\",\n            transform=placeholder_transform,\n        ) as dst:\n            dst.write(np.zeros((1, 1, 1), dtype=\"uint8\"))\n\n        # Create a new TifProcessor instance with the placeholder\n        # Use LocalDataStore() since the temp file is always a local absolute path\n        new_processor = TifProcessor(\n            dataset_path=placeholder_path,\n            data_store=LocalDataStore(),  # Always use LocalDataStore for temp files\n            mode=self.mode,\n        )\n\n        # Now save the clipped file directly to the new processor's temp directory\n        # Similar to how _reproject_to_temp_file works\n        clipped_file_path = os.path.join(\n            new_processor._temp_dir, f\"clipped_{os.urandom(8).hex()}.tif\"\n        )\n\n        with rasterio.open(clipped_file_path, \"w\", **clipped_meta) as dst:\n            dst.write(clipped_data)\n\n        # Verify file was created successfully\n        if not os.path.exists(clipped_file_path):\n            raise RuntimeError(f\"Failed to create clipped file at {clipped_file_path}\")\n\n        # Set the clipped file path and update processor attributes\n        new_processor._clipped_file_path = clipped_file_path\n        new_processor.dataset_path = clipped_file_path\n        new_processor.dataset_paths = [Path(clipped_file_path)]\n\n        # Clean up placeholder file and directory\n        try:\n            os.remove(placeholder_path)\n            os.rmdir(placeholder_dir)\n        except OSError:\n            pass\n\n        # Reload metadata since the path changed\n        new_processor._load_metadata()\n\n        self.logger.info(f\"Clipped raster saved to temporary file: {clipped_file_path}\")\n\n        return new_processor\n\n    def _get_pixel_coordinates(self):\n        \"\"\"Helper method to generate coordinate arrays for all pixels\"\"\"\n        if \"pixel_coords\" not in self._cache:\n            # use cached values\n            bounds = self._cache[\"bounds\"]\n            width = self._cache[\"width\"]\n            height = self._cache[\"height\"]\n            pixel_size_x = self._cache[\"x_transform\"]\n            pixel_size_y = self._cache[\"y_transform\"]\n\n            self._cache[\"pixel_coords\"] = np.meshgrid(\n                np.linspace(\n                    bounds.left + pixel_size_x / 2,\n                    bounds.right - pixel_size_x / 2,\n                    width,\n                ),\n                np.linspace(\n                    bounds.top + pixel_size_y / 2,\n                    bounds.bottom - pixel_size_y / 2,\n                    height,\n                ),\n            )\n\n        return self._cache[\"pixel_coords\"]\n\n    def _get_chunk_coordinates(self, window, src):\n        \"\"\"Get coordinates for a specific window chunk.\"\"\"\n        transform = src.window_transform(window)\n        rows, cols = np.meshgrid(\n            np.arange(window.height), np.arange(window.width), indexing=\"ij\"\n        )\n        xs, ys = rasterio.transform.xy(transform, rows.flatten(), cols.flatten())\n        return np.array(xs), np.array(ys)\n\n    def _extract_coordinates_with_mask(self, mask=None):\n        \"\"\"Extract flattened coordinates, optionally applying a mask.\"\"\"\n        x_coords, y_coords = self._get_pixel_coordinates()\n\n        if mask is not None:\n            return np.extract(mask, x_coords), np.extract(mask, y_coords)\n\n        return x_coords.flatten(), y_coords.flatten()\n\n    def _build_data_mask(self, data, drop_nodata=True, nodata_value=None):\n        \"\"\"Build a boolean mask for filtering data based on nodata values.\"\"\"\n        if not drop_nodata or nodata_value is None:\n            return None\n\n        return data != nodata_value\n\n    def _build_multi_band_mask(\n        self,\n        bands: np.ndarray,\n        drop_nodata: bool = True,\n        nodata_value: Optional[float] = None,\n    ) -&gt; Optional[np.ndarray]:\n        \"\"\"\n        Build mask for multi-band data - drops pixels where ANY band has nodata.\n\n        Args:\n            bands: 3D array of shape (n_bands, height, width)\n            drop_nodata Whether to drop nodata values\n            nodata_value: The nodata value to check\n\n        Returns:\n            Boolean mask or None if no masking needed\n        \"\"\"\n        if not drop_nodata or nodata_value is None:\n            return None\n\n        # Check if ANY band has nodata at each pixel location\n        has_nodata = np.any(bands == nodata_value, axis=0)\n\n        # Return True where ALL bands are valid\n        valid_mask = ~has_nodata\n\n        return valid_mask if not valid_mask.all() else None\n\n    def _bands_to_dict(self, bands, band_count, band_names, mask=None):\n        \"\"\"Read specified bands and return as a dictionary with optional masking.\"\"\"\n\n        lons, lats = self._extract_coordinates_with_mask(mask)\n        data_dict = {\"lon\": lons, \"lat\": lats}\n\n        for idx, name in enumerate(band_names[:band_count]):\n            band_data = bands[idx]\n            data_dict[name] = (\n                np.extract(mask, band_data) if mask is not None else band_data.flatten()\n            )\n\n        return data_dict\n\n    def _calculate_optimal_chunk_size(\n        self, operation: str = \"conversion\", target_memory_mb: int = 500\n    ) -&gt; int:\n        \"\"\"\n        Calculate optimal chunk size (number of rows) based on target memory usage.\n\n        Args:\n            operation: Type of operation ('conversion', 'graph')\n            target_memory_mb: Target memory per chunk in megabytes\n\n        Returns:\n            Number of rows per chunk\n        \"\"\"\n        bytes_per_element = np.dtype(self.dtype).itemsize\n        n_bands = self.count\n        width = self.width\n\n        # Adjust for operation type\n        if operation == \"conversion\":\n            # DataFrame overhead is roughly 2x\n            bytes_per_row = width * n_bands * bytes_per_element * 2\n        elif operation == \"graph\":\n            # Graph needs additional space for edges\n            bytes_per_row = width * bytes_per_element * 4  # Estimate\n        else:\n            bytes_per_row = width * n_bands * bytes_per_element\n\n        target_bytes = target_memory_mb * 1024 * 1024\n        chunk_rows = max(1, int(target_bytes / bytes_per_row))\n\n        # Ensure chunk size doesn't exceed total height\n        chunk_rows = min(chunk_rows, self.height)\n\n        self.logger.info(\n            f\"Calculated chunk size: {chunk_rows} rows \"\n            f\"(~{self._format_bytes(chunk_rows * bytes_per_row)} per chunk)\"\n        )\n\n        return chunk_rows\n\n    def _get_chunk_windows(self, chunk_size: int) -&gt; List[rasterio.windows.Window]:\n        \"\"\"\n        Generate window objects for chunked reading.\n\n        Args:\n            chunk_size: Number of rows per chunk\n\n        Returns:\n            List of rasterio.windows.Window objects\n        \"\"\"\n        windows = []\n        for row_start in range(0, self.height, chunk_size):\n            row_end = min(row_start + chunk_size, self.height)\n            window = rasterio.windows.Window(\n                col_off=0,\n                row_off=row_start,\n                width=self.width,\n                height=row_end - row_start,\n            )\n            windows.append(window)\n\n        return windows\n\n    def _format_bytes(self, bytes_value: int) -&gt; str:\n        \"\"\"Convert bytes to human-readable format.\"\"\"\n        for unit in [\"B\", \"KB\", \"MB\", \"GB\", \"TB\"]:\n            if bytes_value &lt; 1024.0:\n                return f\"{bytes_value:.2f} {unit}\"\n            bytes_value /= 1024.0\n        return f\"{bytes_value:.2f} PB\"\n\n    def _check_available_memory(self) -&gt; dict:\n        \"\"\"\n        Check available system memory.\n\n        Returns:\n            Dict with total, available, and used memory info\n        \"\"\"\n        import psutil\n\n        memory = psutil.virtual_memory()\n        return {\n            \"total\": memory.total,\n            \"available\": memory.available,\n            \"used\": memory.used,\n            \"percent\": memory.percent,\n            \"available_human\": self._format_bytes(memory.available),\n        }\n\n    def _estimate_memory_usage(\n        self, operation: str = \"conversion\", n_workers: int = 1\n    ) -&gt; dict:\n        \"\"\"\n        Estimate memory usage for various operations.\n\n        Args:\n            operation: Type of operation ('conversion', 'batched_sampling', 'merge', 'graph')\n            n_workers: Number of workers (for batched_sampling)\n\n        Returns:\n            Dict with estimated memory usage in bytes and human-readable format\n        \"\"\"\n        bytes_per_element = np.dtype(self.dtype).itemsize\n        n_pixels = self.width * self.height\n        n_bands = self.count\n\n        estimates = {}\n\n        if operation == \"conversion\":\n            # to_dataframe/to_geodataframe: full raster + DataFrame overhead\n            raster_memory = n_pixels * n_bands * bytes_per_element\n            # DataFrame overhead (roughly 2x for storage + processing)\n            dataframe_memory = (\n                n_pixels * n_bands * 16\n            )  # 16 bytes per value in DataFrame\n            total = raster_memory + dataframe_memory\n            estimates[\"raster\"] = raster_memory\n            estimates[\"dataframe\"] = dataframe_memory\n            estimates[\"total\"] = total\n\n        elif operation == \"batched_sampling\":\n            # Each worker loads full raster into MemoryFile\n            # Need to get file size\n            if self._merged_file_path:\n                file_path = self._merged_file_path\n            elif self._reprojected_file_path:\n                file_path = self._reprojected_file_path\n            else:\n                file_path = str(self.dataset_path)\n\n            try:\n                import os\n\n                file_size = os.path.getsize(file_path)\n            except:\n                # Estimate if can't get file size\n                file_size = n_pixels * n_bands * bytes_per_element * 1.2  # Add overhead\n\n            estimates[\"per_worker\"] = file_size\n            estimates[\"total\"] = file_size * n_workers\n\n        elif operation == \"merge\":\n            # _merge_with_mean uses float64 arrays\n            raster_memory = n_pixels * n_bands * 8  # float64\n            estimates[\"sum_array\"] = raster_memory\n            estimates[\"count_array\"] = n_pixels * 4  # int32\n            estimates[\"total\"] = raster_memory + n_pixels * 4\n\n        elif operation == \"graph\":\n            # to_graph: data + node_map + edges\n            data_memory = n_pixels * bytes_per_element\n            node_map_memory = n_pixels * 4  # int32\n            # Estimate edges (rough: 4-connectivity = 4 edges per pixel)\n            edges_memory = n_pixels * 4 * 3 * 8  # 3 values per edge, float64\n            total = data_memory + node_map_memory + edges_memory\n            estimates[\"data\"] = data_memory\n            estimates[\"node_map\"] = node_map_memory\n            estimates[\"edges\"] = edges_memory\n            estimates[\"total\"] = total\n\n        # Add human-readable format\n        estimates[\"human_readable\"] = self._format_bytes(estimates[\"total\"])\n\n        return estimates\n\n    def _memory_guard(\n        self,\n        operation: str,\n        threshold_percent: float = 80.0,\n        n_workers: Optional[int] = None,\n        raise_error: bool = False,\n    ) -&gt; bool:\n        \"\"\"\n        Check if operation is safe to perform given memory constraints.\n\n        Args:\n            operation: Type of operation to check\n            threshold_percent: Maximum % of available memory to use (default 80%)\n            n_workers: Number of workers (for batched operations)\n            raise_error: If True, raise MemoryError instead of warning\n\n        Returns:\n            True if operation is safe, False otherwise\n\n        Raises:\n            MemoryError: If raise_error=True and memory insufficient\n        \"\"\"\n        import warnings\n\n        estimates = self._estimate_memory_usage(operation, n_workers=n_workers or 1)\n        memory_info = self._check_available_memory()\n\n        estimated_usage = estimates[\"total\"]\n        available = memory_info[\"available\"]\n        threshold = available * (threshold_percent / 100.0)\n\n        is_safe = estimated_usage &lt;= threshold\n\n        if not is_safe:\n            usage_str = self._format_bytes(estimated_usage)\n            available_str = memory_info[\"available_human\"]\n\n            message = (\n                f\"Memory warning: {operation} operation may require {usage_str} \"\n                f\"but only {available_str} is available. \"\n                f\"Current memory usage: {memory_info['percent']:.1f}%\"\n            )\n\n            if raise_error:\n                raise MemoryError(message)\n            else:\n                warnings.warn(message, ResourceWarning)\n                if hasattr(self, \"logger\"):\n                    self.logger.warning(message)\n\n        return is_safe\n\n    def _validate_mode_band_compatibility(self):\n        \"\"\"Validate that mode matches band count.\"\"\"\n        mode_requirements = {\n            \"single\": (1, \"1-band\"),\n            \"rgb\": (3, \"3-band\"),\n            \"rgba\": (4, \"4-band\"),\n        }\n\n        if self.mode in mode_requirements:\n            required_count, description = mode_requirements[self.mode]\n            if self.count != required_count:\n                raise ValueError(\n                    f\"{self.mode.upper()} mode requires a {description} TIF file\"\n                )\n        elif self.mode == \"multi\" and self.count &lt; 2:\n            raise ValueError(\"Multi mode requires a TIF file with 2 or more bands\")\n\n    def __enter__(self):\n        return self\n\n    def __del__(self):\n        \"\"\"Clean up temporary files and directories.\"\"\"\n        if hasattr(self, \"_temp_dir\") and os.path.exists(self._temp_dir):\n            shutil.rmtree(self._temp_dir, ignore_errors=True)\n\n    def cleanup(self):\n        \"\"\"Explicit cleanup method for better control.\"\"\"\n        if hasattr(self, \"_temp_dir\") and os.path.exists(self._temp_dir):\n            shutil.rmtree(self._temp_dir)\n            self.logger.info(\"Cleaned up temporary files\")\n\n    def __exit__(self, exc_type, exc_value, traceback):\n        \"\"\"Proper context manager exit with cleanup.\"\"\"\n        self.cleanup()\n        return False\n</code></pre>"},{"location":"api/processing/#gigaspatial.processing.tif_processor.TifProcessor.bounds","title":"<code>bounds</code>  <code>property</code>","text":"<p>Get the bounds of the TIF file</p>"},{"location":"api/processing/#gigaspatial.processing.tif_processor.TifProcessor.count","title":"<code>count: int</code>  <code>property</code>","text":"<p>Get the band count from the TIF file</p>"},{"location":"api/processing/#gigaspatial.processing.tif_processor.TifProcessor.crs","title":"<code>crs</code>  <code>property</code>","text":"<p>Get the coordinate reference system from the TIF file</p>"},{"location":"api/processing/#gigaspatial.processing.tif_processor.TifProcessor.dtype","title":"<code>dtype</code>  <code>property</code>","text":"<p>Get the data types from the TIF file</p>"},{"location":"api/processing/#gigaspatial.processing.tif_processor.TifProcessor.is_merged","title":"<code>is_merged: bool</code>  <code>property</code>","text":"<p>Check if this processor was created from multiple rasters.</p>"},{"location":"api/processing/#gigaspatial.processing.tif_processor.TifProcessor.nodata","title":"<code>nodata: int</code>  <code>property</code>","text":"<p>Get the value representing no data in the rasters</p>"},{"location":"api/processing/#gigaspatial.processing.tif_processor.TifProcessor.resolution","title":"<code>resolution: Tuple[float, float]</code>  <code>property</code>","text":"<p>Get the x and y resolution (pixel width and height or pixel size) from the TIF file</p>"},{"location":"api/processing/#gigaspatial.processing.tif_processor.TifProcessor.source_count","title":"<code>source_count: int</code>  <code>property</code>","text":"<p>Get the number of source rasters.</p>"},{"location":"api/processing/#gigaspatial.processing.tif_processor.TifProcessor.transform","title":"<code>transform</code>  <code>property</code>","text":"<p>Get the transform from the TIF file</p>"},{"location":"api/processing/#gigaspatial.processing.tif_processor.TifProcessor.x_transform","title":"<code>x_transform: float</code>  <code>property</code>","text":"<p>Get the x transform from the TIF file</p>"},{"location":"api/processing/#gigaspatial.processing.tif_processor.TifProcessor.y_transform","title":"<code>y_transform: float</code>  <code>property</code>","text":"<p>Get the y transform from the TIF file</p>"},{"location":"api/processing/#gigaspatial.processing.tif_processor.TifProcessor.__del__","title":"<code>__del__()</code>","text":"<p>Clean up temporary files and directories.</p> Source code in <code>gigaspatial/processing/tif_processor.py</code> <pre><code>def __del__(self):\n    \"\"\"Clean up temporary files and directories.\"\"\"\n    if hasattr(self, \"_temp_dir\") and os.path.exists(self._temp_dir):\n        shutil.rmtree(self._temp_dir, ignore_errors=True)\n</code></pre>"},{"location":"api/processing/#gigaspatial.processing.tif_processor.TifProcessor.__exit__","title":"<code>__exit__(exc_type, exc_value, traceback)</code>","text":"<p>Proper context manager exit with cleanup.</p> Source code in <code>gigaspatial/processing/tif_processor.py</code> <pre><code>def __exit__(self, exc_type, exc_value, traceback):\n    \"\"\"Proper context manager exit with cleanup.\"\"\"\n    self.cleanup()\n    return False\n</code></pre>"},{"location":"api/processing/#gigaspatial.processing.tif_processor.TifProcessor.__post_init__","title":"<code>__post_init__()</code>","text":"<p>Validate inputs, merge rasters if needed, and set up logging.</p> Source code in <code>gigaspatial/processing/tif_processor.py</code> <pre><code>def __post_init__(self):\n    \"\"\"Validate inputs, merge rasters if needed, and set up logging.\"\"\"\n    self.data_store = self.data_store or LocalDataStore()\n    self.logger = config.get_logger(self.__class__.__name__)\n    self._cache = {}\n    self._temp_dir = tempfile.mkdtemp()\n    self._merged_file_path = None\n    self._reprojected_file_path = None\n    self._clipped_file_path = None\n\n    # Handle multiple dataset paths\n    if isinstance(self.dataset_path, list):\n        if len(self.dataset_path) &gt; 1:\n            self.dataset_paths = [Path(p) for p in self.dataset_path]\n            self._validate_multiple_datasets()\n            self._merge_rasters()\n            self.dataset_path = self._merged_file_path\n    else:\n        self.dataset_paths = [Path(self.dataset_path)]\n        # For absolute paths with LocalDataStore, check file existence directly\n        # to avoid path resolution issues\n        if isinstance(self.data_store, LocalDataStore) and os.path.isabs(\n            str(self.dataset_path)\n        ):\n            if not os.path.exists(str(self.dataset_path)):\n                raise FileNotFoundError(f\"Dataset not found at {self.dataset_path}\")\n        elif not self.data_store.file_exists(str(self.dataset_path)):\n            raise FileNotFoundError(f\"Dataset not found at {self.dataset_path}\")\n\n        # Reproject single raster during initialization if target_crs is set\n        if self.target_crs:\n            self.logger.info(f\"Reprojecting single raster to {self.target_crs}...\")\n            with self.data_store.open(str(self.dataset_path), \"rb\") as f:\n                with rasterio.MemoryFile(f.read()) as memfile:\n                    with memfile.open() as src:\n                        self._reprojected_file_path = self._reproject_to_temp_file(\n                            src, self.target_crs\n                        )\n            self.dataset_path = self._reprojected_file_path\n\n    self._load_metadata()\n    self._validate_mode_band_compatibility()\n</code></pre>"},{"location":"api/processing/#gigaspatial.processing.tif_processor.TifProcessor.cleanup","title":"<code>cleanup()</code>","text":"<p>Explicit cleanup method for better control.</p> Source code in <code>gigaspatial/processing/tif_processor.py</code> <pre><code>def cleanup(self):\n    \"\"\"Explicit cleanup method for better control.\"\"\"\n    if hasattr(self, \"_temp_dir\") and os.path.exists(self._temp_dir):\n        shutil.rmtree(self._temp_dir)\n        self.logger.info(\"Cleaned up temporary files\")\n</code></pre>"},{"location":"api/processing/#gigaspatial.processing.tif_processor.TifProcessor.clip_to_bounds","title":"<code>clip_to_bounds(bounds, bounds_crs=None, return_clipped_processor=True)</code>","text":"<p>Clip raster to rectangular bounds.</p>"},{"location":"api/processing/#gigaspatial.processing.tif_processor.TifProcessor.clip_to_bounds--parameters","title":"Parameters:","text":"<p>bounds : tuple     Bounding box as (minx, miny, maxx, maxy) bounds_crs : str, optional     CRS of the bounds. If None, assumes same as raster CRS return_clipped_processor : bool, default True     If True, returns new TifProcessor, else returns (array, transform, metadata)</p>"},{"location":"api/processing/#gigaspatial.processing.tif_processor.TifProcessor.clip_to_bounds--returns","title":"Returns:","text":"<p>TifProcessor or tuple     Either new TifProcessor instance or (array, transform, metadata) tuple</p> Source code in <code>gigaspatial/processing/tif_processor.py</code> <pre><code>def clip_to_bounds(\n    self,\n    bounds: tuple,\n    bounds_crs: Optional[str] = None,\n    return_clipped_processor: bool = True,\n) -&gt; Union[\"TifProcessor\", tuple]:\n    \"\"\"\n    Clip raster to rectangular bounds.\n\n    Parameters:\n    -----------\n    bounds : tuple\n        Bounding box as (minx, miny, maxx, maxy)\n    bounds_crs : str, optional\n        CRS of the bounds. If None, assumes same as raster CRS\n    return_clipped_processor : bool, default True\n        If True, returns new TifProcessor, else returns (array, transform, metadata)\n\n    Returns:\n    --------\n    TifProcessor or tuple\n        Either new TifProcessor instance or (array, transform, metadata) tuple\n    \"\"\"\n    # Create bounding box geometry\n    bbox_geom = box(*bounds)\n\n    # If bounds_crs is specified and different from raster CRS, create GeoDataFrame for reprojection\n    if bounds_crs is not None:\n        raster_crs = self.crs\n\n        if not self.crs == bounds_crs:\n            # Create GeoDataFrame with bounds CRS and reproject\n            bbox_gdf = gpd.GeoDataFrame([1], geometry=[bbox_geom], crs=bounds_crs)\n            bbox_gdf = bbox_gdf.to_crs(raster_crs)\n            bbox_geom = bbox_gdf.geometry.iloc[0]\n\n    return self.clip_to_geometry(\n        geometry=bbox_geom,\n        crop=True,\n        return_clipped_processor=return_clipped_processor,\n    )\n</code></pre>"},{"location":"api/processing/#gigaspatial.processing.tif_processor.TifProcessor.clip_to_geometry","title":"<code>clip_to_geometry(geometry, crop=True, all_touched=True, invert=False, nodata=None, pad=False, pad_width=0.5, return_clipped_processor=True)</code>","text":"<p>Clip raster to geometry boundaries.</p>"},{"location":"api/processing/#gigaspatial.processing.tif_processor.TifProcessor.clip_to_geometry--parameters","title":"Parameters:","text":"<p>geometry : various     Geometry to clip to. Can be:     - Shapely Polygon or MultiPolygon     - GeoDataFrame or GeoSeries     - List of GeoJSON-like dicts     - Single GeoJSON-like dict crop : bool, default True     Whether to crop the raster to the extent of the geometry all_touched : bool, default True     Include pixels that touch the geometry boundary invert : bool, default False     If True, mask pixels inside geometry instead of outside nodata : int or float, optional     Value to use for masked pixels. If None, uses raster's nodata value pad : bool, default False     Pad geometry by half pixel before clipping pad_width : float, default 0.5     Width of padding in pixels if pad=True return_clipped_processor : bool, default True     If True, returns new TifProcessor with clipped data     If False, returns (clipped_array, transform, metadata)</p>"},{"location":"api/processing/#gigaspatial.processing.tif_processor.TifProcessor.clip_to_geometry--returns","title":"Returns:","text":"<p>TifProcessor or tuple     Either new TifProcessor instance or (array, transform, metadata) tuple</p> Source code in <code>gigaspatial/processing/tif_processor.py</code> <pre><code>def clip_to_geometry(\n    self,\n    geometry: Union[\n        Polygon, MultiPolygon, gpd.GeoDataFrame, gpd.GeoSeries, List[dict], dict\n    ],\n    crop: bool = True,\n    all_touched: bool = True,\n    invert: bool = False,\n    nodata: Optional[Union[int, float]] = None,\n    pad: bool = False,\n    pad_width: float = 0.5,\n    return_clipped_processor: bool = True,\n) -&gt; Union[\"TifProcessor\", tuple]:\n    \"\"\"\n    Clip raster to geometry boundaries.\n\n    Parameters:\n    -----------\n    geometry : various\n        Geometry to clip to. Can be:\n        - Shapely Polygon or MultiPolygon\n        - GeoDataFrame or GeoSeries\n        - List of GeoJSON-like dicts\n        - Single GeoJSON-like dict\n    crop : bool, default True\n        Whether to crop the raster to the extent of the geometry\n    all_touched : bool, default True\n        Include pixels that touch the geometry boundary\n    invert : bool, default False\n        If True, mask pixels inside geometry instead of outside\n    nodata : int or float, optional\n        Value to use for masked pixels. If None, uses raster's nodata value\n    pad : bool, default False\n        Pad geometry by half pixel before clipping\n    pad_width : float, default 0.5\n        Width of padding in pixels if pad=True\n    return_clipped_processor : bool, default True\n        If True, returns new TifProcessor with clipped data\n        If False, returns (clipped_array, transform, metadata)\n\n    Returns:\n    --------\n    TifProcessor or tuple\n        Either new TifProcessor instance or (array, transform, metadata) tuple\n    \"\"\"\n    # Handle different geometry input types\n    shapes = self._prepare_geometry_for_clipping(geometry)\n\n    # Validate CRS compatibility\n    self._validate_geometry_crs(geometry)\n\n    # Perform the clipping\n    with self.open_dataset() as src:\n        try:\n            clipped_data, clipped_transform = mask(\n                dataset=src,\n                shapes=shapes,\n                crop=crop,\n                all_touched=all_touched,\n                invert=invert,\n                nodata=nodata,\n                pad=pad,\n                pad_width=pad_width,\n                filled=True,\n            )\n\n            # Update metadata for the clipped raster\n            clipped_meta = src.meta.copy()\n            clipped_meta.update(\n                {\n                    \"height\": clipped_data.shape[1],\n                    \"width\": clipped_data.shape[2],\n                    \"transform\": clipped_transform,\n                    \"nodata\": nodata if nodata is not None else src.nodata,\n                }\n            )\n\n        except ValueError as e:\n            if \"Input shapes do not overlap raster\" in str(e):\n                raise ValueError(\n                    \"The geometry does not overlap with the raster. \"\n                    \"Check that both are in the same coordinate reference system.\"\n                ) from e\n            else:\n                raise e\n\n    if return_clipped_processor:\n        # Create a new TifProcessor with the clipped data\n        return self._create_clipped_processor(clipped_data, clipped_meta)\n    else:\n        return clipped_data, clipped_transform, clipped_meta\n</code></pre>"},{"location":"api/processing/#gigaspatial.processing.tif_processor.TifProcessor.get_raster_info","title":"<code>get_raster_info()</code>","text":"<p>Get comprehensive raster information.</p> Source code in <code>gigaspatial/processing/tif_processor.py</code> <pre><code>def get_raster_info(self) -&gt; Dict[str, Any]:\n    \"\"\"Get comprehensive raster information.\"\"\"\n    return {\n        \"count\": self.count,\n        \"width\": self.width,\n        \"height\": self.height,\n        \"crs\": self.crs,\n        \"bounds\": self.bounds,\n        \"transform\": self.transform,\n        \"dtypes\": self.dtype,\n        \"nodata\": self.nodata,\n        \"mode\": self.mode,\n        \"is_merged\": self.is_merged,\n        \"source_count\": self.source_count,\n    }\n</code></pre>"},{"location":"api/processing/#gigaspatial.processing.tif_processor.TifProcessor.open_dataset","title":"<code>open_dataset()</code>","text":"<p>Context manager for accessing the dataset, handling temporary reprojected files.</p> Source code in <code>gigaspatial/processing/tif_processor.py</code> <pre><code>@contextmanager\ndef open_dataset(self):\n    \"\"\"Context manager for accessing the dataset, handling temporary reprojected files.\"\"\"\n    if self._merged_file_path:\n        with rasterio.open(self._merged_file_path) as src:\n            yield src\n    elif self._reprojected_file_path:\n        with rasterio.open(self._reprojected_file_path) as src:\n            yield src\n    elif self._clipped_file_path:\n        with rasterio.open(self._clipped_file_path) as src:\n            yield src\n    elif isinstance(self.data_store, LocalDataStore):\n        with rasterio.open(str(self.dataset_path)) as src:\n            yield src\n    else:\n        with self.data_store.open(str(self.dataset_path), \"rb\") as f:\n            with rasterio.MemoryFile(f.read()) as memfile:\n                with memfile.open() as src:\n                    yield src\n</code></pre>"},{"location":"api/processing/#gigaspatial.processing.tif_processor.TifProcessor.reproject_to","title":"<code>reproject_to(target_crs, output_path=None, resampling_method=None, resolution=None)</code>","text":"<p>Reprojects the current raster to a new CRS and optionally saves it.</p> <p>Parameters:</p> Name Type Description Default <code>target_crs</code> <code>str</code> <p>The CRS to reproject to (e.g., \"EPSG:4326\").</p> required <code>output_path</code> <code>Optional[Union[str, Path]]</code> <p>The path to save the reprojected raster. If None,          it is saved to a temporary file.</p> <code>None</code> <code>resampling_method</code> <code>Optional[Resampling]</code> <p>The resampling method to use.</p> <code>None</code> <code>resolution</code> <code>Optional[Tuple[float, float]]</code> <p>The target resolution (pixel size) in the new CRS.</p> <code>None</code> Source code in <code>gigaspatial/processing/tif_processor.py</code> <pre><code>def reproject_to(\n    self,\n    target_crs: str,\n    output_path: Optional[Union[str, Path]] = None,\n    resampling_method: Optional[Resampling] = None,\n    resolution: Optional[Tuple[float, float]] = None,\n):\n    \"\"\"\n    Reprojects the current raster to a new CRS and optionally saves it.\n\n    Args:\n        target_crs: The CRS to reproject to (e.g., \"EPSG:4326\").\n        output_path: The path to save the reprojected raster. If None,\n                     it is saved to a temporary file.\n        resampling_method: The resampling method to use.\n        resolution: The target resolution (pixel size) in the new CRS.\n    \"\"\"\n    self.logger.info(f\"Reprojecting raster to {target_crs}...\")\n\n    # Use provided or default values\n    resampling_method = resampling_method or self.resampling_method\n    resolution = resolution or self.reprojection_resolution\n\n    with self.open_dataset() as src:\n        if src.crs.to_string() == target_crs:\n            self.logger.info(\n                \"Raster is already in the target CRS. No reprojection needed.\"\n            )\n            # If output_path is specified, copy the file\n            if output_path:\n                self.data_store.copy_file(str(self.dataset_path), output_path)\n            return self.dataset_path\n\n        dst_path = output_path or os.path.join(\n            self._temp_dir, f\"reprojected_single_{os.urandom(8).hex()}.tif\"\n        )\n\n        with rasterio.open(\n            dst_path,\n            \"w\",\n            **self._get_reprojection_profile(src, target_crs, resolution),\n        ) as dst:\n            for band_idx in range(1, src.count + 1):\n                reproject(\n                    source=rasterio.band(src, band_idx),\n                    destination=rasterio.band(dst, band_idx),\n                    src_transform=src.transform,\n                    src_crs=src.crs,\n                    dst_transform=dst.transform,\n                    dst_crs=dst.crs,\n                    resampling=resampling_method,\n                    num_threads=multiprocessing.cpu_count(),\n                )\n\n        self.logger.info(f\"Reprojection complete. Output saved to {dst_path}\")\n        return Path(dst_path)\n</code></pre>"},{"location":"api/processing/#gigaspatial.processing.tif_processor.TifProcessor.sample_by_polygons","title":"<code>sample_by_polygons(polygon_list, stat='mean')</code>","text":"<p>Sample raster values by polygons and compute statistic(s) for each polygon.</p> <p>Parameters:</p> Name Type Description Default <code>polygon_list</code> <p>List of shapely Polygon or MultiPolygon objects.</p> required <code>stat</code> <code>Union[str, Callable, List[Union[str, Callable]]]</code> <p>Statistic(s) to compute. Can be: - Single string: 'mean', 'median', 'sum', 'min', 'max', 'std', 'count' - Single callable: custom function that takes array and returns scalar - List of strings/callables: multiple statistics to compute</p> <code>'mean'</code> <p>Returns:</p> Type Description <p>If single stat: np.ndarray of computed statistics for each polygon</p> <p>If multiple stats: List of dictionaries with stat names as keys</p> Source code in <code>gigaspatial/processing/tif_processor.py</code> <pre><code>def sample_by_polygons(\n    self,\n    polygon_list,\n    stat: Union[str, Callable, List[Union[str, Callable]]] = \"mean\",\n):\n    \"\"\"\n    Sample raster values by polygons and compute statistic(s) for each polygon.\n\n    Args:\n        polygon_list: List of shapely Polygon or MultiPolygon objects.\n        stat: Statistic(s) to compute. Can be:\n            - Single string: 'mean', 'median', 'sum', 'min', 'max', 'std', 'count'\n            - Single callable: custom function that takes array and returns scalar\n            - List of strings/callables: multiple statistics to compute\n\n    Returns:\n        If single stat: np.ndarray of computed statistics for each polygon\n        If multiple stats: List of dictionaries with stat names as keys\n    \"\"\"\n    # Determine if single or multiple stats\n    single_stat = not isinstance(stat, list)\n    stats_list = [stat] if single_stat else stat\n\n    # Prepare stat functions\n    stat_funcs = []\n    stat_names = []\n\n    for s in stats_list:\n        if callable(s):\n            stat_funcs.append(s)\n            stat_names.append(\n                s.__name__\n                if hasattr(s, \"__name__\")\n                else f\"custom_{len(stat_names)}\"\n            )\n        else:\n            # Handle string statistics\n            if s == \"count\":\n                stat_funcs.append(len)\n            else:\n                stat_funcs.append(getattr(np, s))\n            stat_names.append(s)\n\n    results = []\n\n    with self.open_dataset() as src:\n        for polygon in tqdm(polygon_list):\n            try:\n                out_image, _ = mask(src, [polygon], crop=True, filled=False)\n\n                # Use masked arrays for more efficient nodata handling\n                if hasattr(out_image, \"mask\"):\n                    valid_data = out_image.compressed()\n                else:\n                    valid_data = (\n                        out_image[out_image != self.nodata]\n                        if self.nodata\n                        else out_image.flatten()\n                    )\n\n                if len(valid_data) == 0:\n                    if single_stat:\n                        results.append(np.nan)\n                    else:\n                        results.append({name: np.nan for name in stat_names})\n                else:\n                    if single_stat:\n                        results.append(stat_funcs[0](valid_data))\n                    else:\n                        # Compute all statistics for this polygon\n                        polygon_stats = {}\n                        for func, name in zip(stat_funcs, stat_names):\n                            try:\n                                polygon_stats[name] = func(valid_data)\n                            except Exception:\n                                polygon_stats[name] = np.nan\n                        results.append(polygon_stats)\n\n            except Exception:\n                if single_stat:\n                    results.append(np.nan)\n                else:\n                    results.append({name: np.nan for name in stat_names})\n\n    return np.array(results) if single_stat else results\n</code></pre>"},{"location":"api/processing/#gigaspatial.processing.tif_processor.TifProcessor.sample_by_polygons_batched","title":"<code>sample_by_polygons_batched(polygon_list, stat='mean', batch_size=100, n_workers=4, show_progress=True, check_memory=True, **kwargs)</code>","text":"<p>Sample raster values by polygons in parallel using batching.</p> <p>Parameters:</p> Name Type Description Default <code>polygon_list</code> <code>List[Union[Polygon, MultiPolygon]]</code> <p>List of Shapely Polygon or MultiPolygon objects</p> required <code>stat</code> <code>Union[str, Callable]</code> <p>Statistic to compute</p> <code>'mean'</code> <code>batch_size</code> <code>int</code> <p>Number of polygons per batch</p> <code>100</code> <code>n_workers</code> <code>int</code> <p>Number of worker processes</p> <code>4</code> <code>show_progress</code> <code>bool</code> <p>Whether to display progress bar</p> <code>True</code> <code>check_memory</code> <code>bool</code> <p>Whether to check memory before operation</p> <code>True</code> <code>**kwargs</code> <p>Additional arguments</p> <code>{}</code> <p>Returns:</p> Type Description <code>ndarray</code> <p>np.ndarray of statistics for each polygon</p> Source code in <code>gigaspatial/processing/tif_processor.py</code> <pre><code>def sample_by_polygons_batched(\n    self,\n    polygon_list: List[Union[Polygon, MultiPolygon]],\n    stat: Union[str, Callable] = \"mean\",\n    batch_size: int = 100,\n    n_workers: int = 4,\n    show_progress: bool = True,\n    check_memory: bool = True,\n    **kwargs,\n) -&gt; np.ndarray:\n    \"\"\"\n    Sample raster values by polygons in parallel using batching.\n\n    Args:\n        polygon_list: List of Shapely Polygon or MultiPolygon objects\n        stat: Statistic to compute\n        batch_size: Number of polygons per batch\n        n_workers: Number of worker processes\n        show_progress: Whether to display progress bar\n        check_memory: Whether to check memory before operation\n        **kwargs: Additional arguments\n\n    Returns:\n        np.ndarray of statistics for each polygon\n    \"\"\"\n    import sys\n\n    # Memory guard check with n_workers consideration\n    if check_memory:\n        is_safe = self._memory_guard(\n            \"batched_sampling\",\n            threshold_percent=85.0,\n            n_workers=n_workers,\n            raise_error=False,\n        )\n\n        if not is_safe:\n            # Suggest reducing n_workers\n            memory_info = self._check_available_memory()\n            estimates = self._estimate_memory_usage(\"batched_sampling\", n_workers=1)\n\n            # Calculate optimal workers\n            suggested_workers = max(\n                1, int(memory_info[\"available\"] * 0.7 / estimates[\"per_worker\"])\n            )\n\n            warnings.warn(\n                f\"Consider reducing n_workers from {n_workers} to {suggested_workers} \"\n                f\"to reduce memory pressure.\",\n                ResourceWarning,\n            )\n\n    # Platform check\n    if sys.platform in [\"win32\", \"darwin\"]:\n        import warnings\n        import multiprocessing as mp\n\n        if mp.get_start_method(allow_none=True) != \"fork\":\n            warnings.warn(\n                \"Batched sampling may not work on Windows/macOS. \"\n                \"Use sample_by_polygons() if you encounter errors.\",\n                RuntimeWarning,\n            )\n\n    def _chunk_list(data_list, chunk_size):\n        \"\"\"Yield successive chunks from data_list.\"\"\"\n        for i in range(0, len(data_list), chunk_size):\n            yield data_list[i : i + chunk_size]\n\n    if len(polygon_list) == 0:\n        return np.array([])\n\n    stat_func = stat if callable(stat) else getattr(np, stat)\n    polygon_chunks = list(_chunk_list(polygon_list, batch_size))\n\n    with multiprocessing.Pool(\n        initializer=self._initializer_worker, processes=n_workers\n    ) as pool:\n        process_func = partial(self._process_polygon_batch, stat_func=stat_func)\n        if show_progress:\n            batched_results = list(\n                tqdm(\n                    pool.imap(process_func, polygon_chunks),\n                    total=len(polygon_chunks),\n                    desc=f\"Sampling polygons\",\n                )\n            )\n        else:\n            batched_results = list(pool.imap(process_func, polygon_chunks))\n\n        results = [item for sublist in batched_results for item in sublist]\n\n    return np.array(results)\n</code></pre>"},{"location":"api/processing/#gigaspatial.processing.tif_processor.TifProcessor.to_dataframe","title":"<code>to_dataframe(drop_nodata=True, check_memory=True, **kwargs)</code>","text":"<p>Convert raster to DataFrame.</p> <p>Parameters:</p> Name Type Description Default <code>drop_nodata</code> <p>Whether to drop nodata values</p> <code>True</code> <code>check_memory</code> <p>Whether to check memory before operation (default True)</p> <code>True</code> <code>**kwargs</code> <p>Additional arguments</p> <code>{}</code> <p>Returns:</p> Type Description <code>DataFrame</code> <p>pd.DataFrame with raster data</p> Source code in <code>gigaspatial/processing/tif_processor.py</code> <pre><code>def to_dataframe(\n    self, drop_nodata=True, check_memory=True, **kwargs\n) -&gt; pd.DataFrame:\n    \"\"\"\n    Convert raster to DataFrame.\n\n    Args:\n        drop_nodata: Whether to drop nodata values\n        check_memory: Whether to check memory before operation (default True)\n        **kwargs: Additional arguments\n\n    Returns:\n        pd.DataFrame with raster data\n    \"\"\"\n    # Memory guard check\n    if check_memory:\n        self._memory_guard(\"conversion\", threshold_percent=80.0)\n\n    try:\n        if self.mode == \"single\":\n            return self._to_dataframe(\n                band_number=kwargs.get(\"band_number\", 1),\n                drop_nodata=drop_nodata,\n                band_names=kwargs.get(\"band_names\", None),\n            )\n        else:\n            return self._to_dataframe(\n                band_number=None,  # All bands\n                drop_nodata=drop_nodata,\n                band_names=kwargs.get(\"band_names\", None),\n            )\n    except Exception as e:\n        raise ValueError(\n            f\"Failed to process TIF file in mode '{self.mode}'. \"\n            f\"Please ensure the file is valid and matches the selected mode. \"\n            f\"Original error: {str(e)}\"\n        )\n\n    return df\n</code></pre>"},{"location":"api/processing/#gigaspatial.processing.tif_processor.TifProcessor.to_dataframe_chunked","title":"<code>to_dataframe_chunked(drop_nodata=True, chunk_size=None, target_memory_mb=500, **kwargs)</code>","text":"<p>Convert raster to DataFrame using chunked processing for memory efficiency.</p> <p>Automatically routes to the appropriate chunked method based on mode. Chunk size is automatically calculated based on target memory usage.</p> <p>Parameters:</p> Name Type Description Default <code>drop_nodata</code> <p>Whether to drop nodata values</p> <code>True</code> <code>chunk_size</code> <p>Number of rows per chunk (auto-calculated if None)</p> <code>None</code> <code>target_memory_mb</code> <p>Target memory per chunk in MB (default 500)</p> <code>500</code> <code>**kwargs</code> <p>Additional arguments (band_number, band_names, etc.)</p> <code>{}</code> Source code in <code>gigaspatial/processing/tif_processor.py</code> <pre><code>def to_dataframe_chunked(\n    self, drop_nodata=True, chunk_size=None, target_memory_mb=500, **kwargs\n):\n    \"\"\"\n    Convert raster to DataFrame using chunked processing for memory efficiency.\n\n    Automatically routes to the appropriate chunked method based on mode.\n    Chunk size is automatically calculated based on target memory usage.\n\n    Args:\n        drop_nodata: Whether to drop nodata values\n        chunk_size: Number of rows per chunk (auto-calculated if None)\n        target_memory_mb: Target memory per chunk in MB (default 500)\n        **kwargs: Additional arguments (band_number, band_names, etc.)\n    \"\"\"\n\n    if chunk_size is None:\n        chunk_size = self._calculate_optimal_chunk_size(\n            \"conversion\", target_memory_mb\n        )\n\n    windows = self._get_chunk_windows(chunk_size)\n\n    # SIMPLE ROUTING\n    if self.mode == \"single\":\n        return self._to_dataframe_chunked(\n            windows,\n            band_number=kwargs.get(\"band_number\", 1),\n            drop_nodata=drop_nodata,\n            band_names=kwargs.get(\"band_names\", None),\n        )\n    else:  # rgb, rgba, multi\n        return self._to_dataframe_chunked(\n            windows,\n            band_number=None,\n            drop_nodata=drop_nodata,\n            band_names=kwargs.get(\"band_names\", None),\n        )\n</code></pre>"},{"location":"api/processing/#gigaspatial.processing.tif_processor.TifProcessor.to_geodataframe","title":"<code>to_geodataframe(check_memory=True, **kwargs)</code>","text":"<p>Convert the processed TIF data into a GeoDataFrame, where each row represents a pixel zone. Each zone is defined by its bounding box, based on pixel resolution and coordinates.</p> <p>Parameters:</p> Name Type Description Default <code>check_memory</code> <p>Whether to check memory before operation</p> <code>True</code> <code>**kwargs</code> <p>Additional arguments passed to to_dataframe()</p> <code>{}</code> <p>Returns:</p> Type Description <code>GeoDataFrame</code> <p>gpd.GeoDataFrame with raster data</p> Source code in <code>gigaspatial/processing/tif_processor.py</code> <pre><code>def to_geodataframe(self, check_memory=True, **kwargs) -&gt; gpd.GeoDataFrame:\n    \"\"\"\n    Convert the processed TIF data into a GeoDataFrame, where each row represents a pixel zone.\n    Each zone is defined by its bounding box, based on pixel resolution and coordinates.\n\n    Args:\n        check_memory: Whether to check memory before operation\n        **kwargs: Additional arguments passed to to_dataframe()\n\n    Returns:\n        gpd.GeoDataFrame with raster data\n    \"\"\"\n    # Memory guard check\n    if check_memory:\n        self._memory_guard(\"conversion\", threshold_percent=80.0)\n\n    df = self.to_dataframe(check_memory=False, **kwargs)\n\n    x_res, y_res = self.resolution\n\n    # create bounding box for each pixel\n    geometries = [\n        box(lon - x_res / 2, lat - y_res / 2, lon + x_res / 2, lat + y_res / 2)\n        for lon, lat in zip(df[\"lon\"], df[\"lat\"])\n    ]\n\n    gdf = gpd.GeoDataFrame(df, geometry=geometries, crs=self.crs)\n\n    return gdf\n</code></pre>"},{"location":"api/processing/#gigaspatial.processing.tif_processor.TifProcessor.to_graph","title":"<code>to_graph(connectivity=4, band=None, include_coordinates=False, graph_type='networkx', check_memory=True)</code>","text":"<p>Convert raster to graph based on pixel adjacency.</p> <p>Parameters:</p> Name Type Description Default <code>connectivity</code> <code>Literal[4, 8]</code> <p>4 or 8-connectivity</p> <code>4</code> <code>band</code> <code>Optional[int]</code> <p>Band number (1-indexed)</p> <code>None</code> <code>include_coordinates</code> <code>bool</code> <p>Include x,y coordinates in nodes</p> <code>False</code> <code>graph_type</code> <code>Literal['networkx', 'sparse']</code> <p>'networkx' or 'sparse'</p> <code>'networkx'</code> <code>check_memory</code> <code>bool</code> <p>Whether to check memory before operation</p> <code>True</code> <p>Returns:</p> Type Description <code>Union[Graph, csr_matrix]</code> <p>Graph representation of raster</p> Source code in <code>gigaspatial/processing/tif_processor.py</code> <pre><code>def to_graph(\n    self,\n    connectivity: Literal[4, 8] = 4,\n    band: Optional[int] = None,\n    include_coordinates: bool = False,\n    graph_type: Literal[\"networkx\", \"sparse\"] = \"networkx\",\n    check_memory: bool = True,\n) -&gt; Union[nx.Graph, sp.csr_matrix]:\n    \"\"\"\n    Convert raster to graph based on pixel adjacency.\n\n    Args:\n        connectivity: 4 or 8-connectivity\n        band: Band number (1-indexed)\n        include_coordinates: Include x,y coordinates in nodes\n        graph_type: 'networkx' or 'sparse'\n        check_memory: Whether to check memory before operation\n\n    Returns:\n        Graph representation of raster\n    \"\"\"\n\n    # Memory guard check\n    if check_memory:\n        self._memory_guard(\"graph\", threshold_percent=80.0)\n\n    with self.open_dataset() as src:\n        band_idx = band - 1 if band is not None else 0\n        if band_idx &lt; 0 or band_idx &gt;= src.count:\n            raise ValueError(\n                f\"Band {band} not available. Raster has {src.count} bands\"\n            )\n\n        data = src.read(band_idx + 1)\n        nodata = src.nodata if src.nodata is not None else self.nodata\n        valid_mask = (\n            data != nodata if nodata is not None else np.ones_like(data, dtype=bool)\n        )\n\n        height, width = data.shape\n\n        # Find all valid pixels\n        valid_rows, valid_cols = np.where(valid_mask)\n        num_valid_pixels = len(valid_rows)\n\n        # Create a sequential mapping from (row, col) to a node ID\n        node_map = np.full(data.shape, -1, dtype=int)\n        node_map[valid_rows, valid_cols] = np.arange(num_valid_pixels)\n\n        # Define neighborhood offsets\n        if connectivity == 4:\n            # von Neumann neighborhood (4-connectivity)\n            offsets = [(-1, 0), (1, 0), (0, -1), (0, 1)]\n        else:  # connectivity == 8\n            # Moore neighborhood (8-connectivity)\n            offsets = [\n                (-1, -1),\n                (-1, 0),\n                (-1, 1),\n                (0, -1),\n                (0, 1),\n                (1, -1),\n                (1, 0),\n                (1, 1),\n            ]\n\n        # Collect nodes and edges\n        nodes_to_add = []\n        edges_to_add = []\n\n        for i in range(num_valid_pixels):\n            row, col = valid_rows[i], valid_cols[i]\n            current_node_id = node_map[row, col]\n\n            # Prepare node attributes\n            node_attrs = {\"value\": float(data[row, col])}\n            if include_coordinates:\n                x, y = src.xy(row, col)\n                node_attrs[\"x\"] = x\n                node_attrs[\"y\"] = y\n            nodes_to_add.append((current_node_id, node_attrs))\n\n            # Find neighbors and collect edges\n            for dy, dx in offsets:\n                neighbor_row, neighbor_col = row + dy, col + dx\n\n                # Check if neighbor is within bounds and is a valid pixel\n                if (\n                    0 &lt;= neighbor_row &lt; height\n                    and 0 &lt;= neighbor_col &lt; width\n                    and valid_mask[neighbor_row, neighbor_col]\n                ):\n                    neighbor_node_id = node_map[neighbor_row, neighbor_col]\n\n                    # Ensure each edge is added only once\n                    if current_node_id &lt; neighbor_node_id:\n                        neighbor_value = float(data[neighbor_row, neighbor_col])\n                        edges_to_add.append(\n                            (current_node_id, neighbor_node_id, neighbor_value)\n                        )\n\n        if graph_type == \"networkx\":\n            G = nx.Graph()\n            G.add_nodes_from(nodes_to_add)\n            G.add_weighted_edges_from(edges_to_add)\n            return G\n        else:  # sparse matrix\n            edges_array = np.array(edges_to_add)\n            row_indices = edges_array[:, 0]\n            col_indices = edges_array[:, 1]\n            weights = edges_array[:, 2]\n\n            # Add reverse edges for symmetric matrix\n            from_idx = np.append(row_indices, col_indices)\n            to_idx = np.append(col_indices, row_indices)\n            weights = np.append(weights, weights)\n\n            return sp.coo_matrix(\n                (weights, (from_idx, to_idx)),\n                shape=(num_valid_pixels, num_valid_pixels),\n            ).tocsr()\n</code></pre>"},{"location":"api/processing/#gigaspatial.processing.utils","title":"<code>utils</code>","text":""},{"location":"api/processing/#gigaspatial.processing.utils.assign_id","title":"<code>assign_id(df, required_columns, id_column='id')</code>","text":"<p>Generate IDs for any entity type in a pandas DataFrame.</p> <p>Parameters:</p> Name Type Description Default <code>df</code> <code>DataFrame</code> <p>Input DataFrame containing entity data</p> required <code>required_columns</code> <code>List[str]</code> <p>List of column names required for ID generation</p> required <code>id_column</code> <code>str</code> <p>Name for the id column that will be generated</p> <code>'id'</code> <p>Returns:</p> Type Description <code>DataFrame</code> <p>pd.DataFrame: DataFrame with generated id column</p> Source code in <code>gigaspatial/processing/utils.py</code> <pre><code>def assign_id(\n    df: pd.DataFrame, required_columns: List[str], id_column: str = \"id\"\n) -&gt; pd.DataFrame:\n    \"\"\"\n    Generate IDs for any entity type in a pandas DataFrame.\n\n    Args:\n        df (pd.DataFrame): Input DataFrame containing entity data\n        required_columns (List[str]): List of column names required for ID generation\n        id_column (str): Name for the id column that will be generated\n\n    Returns:\n        pd.DataFrame: DataFrame with generated id column\n    \"\"\"\n    # Create a copy to avoid modifying the original DataFrame\n    df = df.copy()\n\n    # Check if ID column exists, if not create it with None values\n    if id_column not in df.columns:\n        df[id_column] = None\n\n    # Check required columns exist\n    if not all(col in df.columns for col in required_columns):\n        return df\n\n    # Create identifier concat for UUID generation\n    df[\"identifier_concat\"] = (\n        df[required_columns].astype(str).fillna(\"\").agg(\"\".join, axis=1)\n    )\n\n    # Generate UUIDs only where all required fields are present and no existing ID\n    mask = df[id_column].isna()\n    for col in required_columns:\n        mask &amp;= df[col].notna()\n\n    # Apply UUID generation only where mask is True\n    df.loc[mask, id_column] = df.loc[mask, \"identifier_concat\"].apply(\n        lambda x: str(uuid.uuid3(uuid.NAMESPACE_DNS, x))\n    )\n\n    # Drop temporary column\n    df = df.drop(columns=[\"identifier_concat\"])\n\n    return df\n</code></pre>"},{"location":"examples/","title":"Examples","text":"<p>Welcome to the examples section of the <code>gigaspatial</code> package. Here, you\u2019ll find practical examples that demonstrate how to use the package for various geospatial data tasks.</p>"},{"location":"examples/#getting-started","title":"Getting Started","text":"<p>If you\u2019re new to <code>gigaspatial</code>, start with the Quick Start Guide to learn the basics.</p>"},{"location":"examples/#example-categories","title":"Example Categories","text":""},{"location":"examples/#1-data-downloading","title":"1. Data Downloading","text":"<p>Learn how to download geospatial data from various sources.</p> <ul> <li>Downloading GHSL Data</li> <li>Fetching OSM Data</li> </ul>"},{"location":"examples/#2-data-processing","title":"2. Data Processing","text":"<p>Explore examples of processing geospatial data, such as GeoTIFF files.</p> <ul> <li>Processing GeoTIFF Files</li> </ul>"},{"location":"examples/#3-data-storage-in-progress","title":"3. Data Storage (In Progress)","text":"<p>Discover how to store and retrieve geospatial data in different formats.</p> <ul> <li>Saving Data to GeoJSON (Coming Soon)</li> <li>Storing Data in a Database (Coming Soon)</li> </ul>"},{"location":"examples/#4-data-visualization-in-progress","title":"4. Data Visualization (In Progress)","text":"<p>Learn how to visualize geospatial data using popular libraries.</p> <ul> <li>Plotting Data with Matplotlib (Coming Soon)</li> <li>Creating Interactive Maps with Folium (Coming Soon)</li> </ul>"},{"location":"examples/#5-advanced-use-cases-in-progress","title":"5. Advanced Use Cases (In Progress)","text":"<p>Explore advanced examples that combine multiple functionalities.</p> <ul> <li>Building a Geospatial Pipeline (Coming Soon)</li> <li>Integrating with External APIs (Coming Soon)</li> </ul>"},{"location":"examples/#contributing-examples","title":"Contributing Examples","text":"<p>If you\u2019d like to contribute your own examples, please follow the Contributing Guidelines.</p>"},{"location":"examples/#feedback","title":"Feedback","text":"<p>If you have suggestions for additional examples or encounter any issues, feel free to open an issue or join our Discord community.</p>"},{"location":"examples/advanced/","title":"Advanced Examples","text":"<p>It's in development.</p>"},{"location":"examples/basic/","title":"Data Handler Examples","text":"<p>This guide provides examples of how to use various data handlers in GigaSpatial to access and process different types of spatial data.</p>"},{"location":"examples/basic/#population-data-worldpop","title":"Population Data (WorldPop)","text":"<pre><code>from gigaspatial.handlers import WorldPopHandler\n\n# Get population data for a specific country and year\nconfig = {\n    \"country_code\": \"KEN\",\n    \"year\": 2020,\n}\n\n# Initialize the WorldPop handler\nworldpop = WorldPopDownloader(config = config)\npath_to_data = worldpop.download_dataset()\n</code></pre>"},{"location":"examples/basic/#building-footprints","title":"Building Footprints","text":""},{"location":"examples/basic/#google-open-buildings","title":"Google Open Buildings","text":"<pre><code>from gigaspatial.handlers import GoogleOpenBuildingsHandler\nfrom gigaspatial.generators import PoiViewGenerator\nfrom gigaspatial.core.io import LocalDataStore\nimport geopandas as gpd\n\n# Initialize data store and handlers\ndata_store = LocalDataStore()\ngob_handler = GoogleOpenBuildingsHandler(data_store=data_store)\n\n# Example 1: Load building data for a country\ncountry_code = \"KEN\"  # Kenya\npolygons_gdf = gob_handler.load_polygons(country_code, ensure_available=True)\nprint(f\"Loaded {len(polygons_gdf)} building polygons\")\n\n# Example 2: Load building data for specific points\npoints = [(36.8219, -1.2921), (36.8172, -1.2867)]  # Nairobi coordinates\npoints_gdf = gob_handler.load_polygons(points, ensure_available=True)\nprint(f\"Loaded {len(points_gdf)} building polygons for points\")\n\n# Example 3: Map nearest buildings to points using PoiViewGenerator\n# Initialize POI view generator with points\npoi_generator = PoiViewGenerator(points=points)\n\n# Map nearest Google buildings to POIs\nresult_gdf = poi_generator.map_google_buildings(gob_handler)\nprint(\"\\nPOIs with nearest building information:\")\nprint(result_gdf[[\"poi_id\", \"nearest_google_building_id\", \"nearest_google_building_distance\"]].head())\n\n# Example 4: Save the enriched POI view\noutput_path = poi_generator.save_view(\"nairobi_buildings\", output_format=\"geojson\")\nprint(f\"\\nSaved enriched POI view to: {output_path}\")\n</code></pre> <p>This example demonstrates: 1. Loading building data for a country or specific points 2. Using PoiViewGenerator to map nearest buildings to points of interest 3. Saving the enriched POI view with building information</p> <p>The resulting GeoDataFrame includes: - Original POI information - Nearest building ID - Distance to the nearest building</p>"},{"location":"examples/basic/#microsoft-global-buildings","title":"Microsoft Global Buildings","text":"<pre><code>from gigaspatial.handlers import MSBuildingsDownloader\n\n# Initialize the handler\nmgb = MSBuildingsDownloader()\n\npoints = [(1.25214, 5.5124), (3.45234, 12.51232)]\n\n# Get building footprints\nlist_of_paths = mgb.download(\n    points=points\n)\n</code></pre>"},{"location":"examples/basic/#satellite-imagery","title":"Satellite Imagery","text":""},{"location":"examples/basic/#maxar-imagery","title":"Maxar Imagery","text":"<pre><code>from gigaspatial.handlers import MaxarImageHandler\n\n# Initialize woith default config which reads credentials config from your environment\nmaxar = MaxarImageDownloader()\n\n# Download imagery\nmaxar.download_images_by_coordinates(\n    data=coordinates,\n    res_meters_pixel=0.6,\n    output_dir=\"bronze/maxar\",\n    bbox_size = 300.0,\n    image_prefix = \"maxar_\"\n)\n</code></pre>"},{"location":"examples/basic/#mapbox-imagery","title":"Mapbox Imagery","text":"<pre><code>from gigaspatial.handlers import MapboxImageDownloader\n\n# Initialize with your access token or config will be read from your environment\nmapbox = MapboxImageDownloader(access_token=\"your_access_token\", style_id=\"mapbox/satellite-v9\")\n\n# Get satellite imagery\nmapbox.download_images_by_coordinates(\n    data=coordinates,\n    res_meters_pixel=300.0,\n    output_dir=\"bronze/mapbox\",\n    image_size=(256,256),\n    image_prefix=\"mapbox_\"\n)\n</code></pre>"},{"location":"examples/basic/#internet-speed-data-ookla","title":"Internet Speed Data (Ookla)","text":"<pre><code>from gigaspatial.core.io import LocalDataStore\nfrom gigaspatial.handlers import (\n    OoklaSpeedtestTileConfig, CountryOoklaTiles\n)\n\n# Initialize OoklaSpeedtestTileConfig for a specific quarter and year\nookla_config = OoklaSpeedtestTileConfig(\n    service_type=\"fixed\", year=2023, quarter=3, data_store=LocalDataStore())\n\n# Download and read the Ookla tile data\ndf = ookla_config.read_tile()\nprint(df.head())  # Display the first few rows of the dataset\n\n# Generate country-specific Ookla tiles\ncountry_ookla_tiles = CountryOoklaTiles.from_country(\"KEN\", ookla_config)\n\n# Convert to DataFrame and display\ncountry_df = country_ookla_tiles.to_dataframe()\nprint(country_df.head())\n\n# Convert to GeoDataFrame and display\ncountry_gdf = country_ookla_tiles.to_geodataframe()\nprint(country_gdf.head())\n</code></pre>"},{"location":"examples/basic/#administrative-boundaries","title":"Administrative Boundaries","text":"<pre><code>from gigaspatial.handlers import AdminBoundaries\n\n# Load level-1 administrative boundaries for Kenya\nadmin_boundaries = AdminBoundaries.create(country_code=\"KEN\", admin_level=1)\n\n# Convert to a GeoDataFrame\ngdf = admin_boundaries.to_geodataframe()\n</code></pre>"},{"location":"examples/basic/#openstreetmap-data","title":"OpenStreetMap Data","text":"<pre><code>from gigaspatial.handlers.osm import OSMAmenityFetcher\n\n# Example 1: Fetching school amenities in Kenya\nfetcher = OSMAmenityFetcher(country_iso2=\"KE\", amenity_types=[\"school\"])\nschools_df = fetcher.get_locations()\nprint(schools_df.head())\n\n# Example 2: Fetching hospital and clinic amenities in Tanzania\nfetcher = OSMAmenityFetcher(country_iso2=\"TZ\", amenity_types=[\"hospital\", \"clinic\"])\nhealthcare_df = fetcher.get_locations()\nprint(healthcare_df.head())\n\n# Example 3: Fetching restaurant amenities in Ghana since 2020\nfetcher = OSMAmenityFetcher(country_iso2=\"GH\", amenity_types=[\"restaurant\"])\nrestaurants_df = fetcher.get_locations(since_year=2020)\nprint(restaurants_df.head())\n</code></pre>"},{"location":"examples/use-cases/","title":"Use Cases","text":"<p>It's in development.</p>"},{"location":"examples/downloading/ghsl/","title":"Downloading and Processing GHSL Data","text":"<p>This example demonstrates how to download and process data from the Global Human Settlement Layer (GHSL) using the <code>GHSLDataHandler</code> class.</p>"},{"location":"examples/downloading/ghsl/#prerequisites","title":"Prerequisites","text":"<p>Ensure you have installed the <code>gigaspatial</code> package and set up the necessary configuration. Follow the Installation Guide if you haven't already.</p>"},{"location":"examples/downloading/ghsl/#example-code","title":"Example Code","text":"<pre><code>from gigaspatial.handlers import GHSLDataHandler\n\n# Initialize the handler with desired product and parameters\nghsl_handler = GHSLDataHandler(\n    product=\"GHS_BUILT_S\",  # Built-up surface\n    year=2020,\n    resolution=100,  # 100m resolution\n)\n\n# Download and load data for a specific country\ncountry_code = \"TUR\"\ndownloaded_files = ghsl_handler.load_data(country_code, ensure_available=True)\n\n# Load the data into a DataFrame\ndf = ghsl_handler.load_into_dataframe(country_code, ensure_available=True)\nprint(df.head())\n\n# You can also load data for specific points\npoints = [(38.404581, 27.4816677), (39.8915702, 32.7809618)]  # Example coordinates\ndf_points = ghsl_handler.load_into_dataframe(points, ensure_available=True)\n</code></pre>"},{"location":"examples/downloading/ghsl/#explanation","title":"Explanation","text":"<ul> <li>GHSLDataHandler: This class provides a unified interface for downloading and processing GHSL data.</li> <li>Available Products:</li> <li><code>GHS_BUILT_S</code>: Built-up surface</li> <li><code>GHS_BUILT_H_AGBH</code>: Average building height</li> <li><code>GHS_BUILT_H_ANBH</code>: Average number of building heights</li> <li><code>GHS_BUILT_V</code>: Building volume</li> <li><code>GHS_POP</code>: Population</li> <li><code>GHS_SMOD</code>: Settlement model</li> <li>Parameters:</li> <li><code>product</code>: The GHSL product to use</li> <li><code>year</code>: The year of the data (default: 2020)</li> <li><code>resolution</code>: The resolution in meters (default: 100)</li> <li>Methods:</li> <li><code>load_data()</code>: Downloads and loads the data</li> <li><code>load_into_dataframe()</code>: Loads the data into a pandas DataFrame</li> </ul>"},{"location":"examples/downloading/ghsl/#next-steps","title":"Next Steps","text":"<p>Once the data is downloaded and processed, you can: 1. Store the data using the <code>DataStore</code> class 2. Visualize the data using <code>geopandas</code> and <code>matplotlib</code> 3. Process the data further using the Processing Examples</p> <p>Back to Examples</p>"},{"location":"examples/downloading/osm/","title":"Downloading OSM Data","text":"<p>This example demonstrates how to fetch and process OpenStreetMap (OSM) data using the <code>OSMLocationFetcher</code> class.</p>"},{"location":"examples/downloading/osm/#prerequisites","title":"Prerequisites","text":"<p>Ensure you have installed the <code>gigaspatial</code> package and set up the necessary configuration. Follow the Installation Guide if you haven\u2019t already.</p>"},{"location":"examples/downloading/osm/#example-code","title":"Example Code","text":"<pre><code>from gigaspatial.handlers.osm import OSMLocationFetcher\n\n# Initialize the fetcher\nfetcher = OSMLocationFetcher(\n    country=\"Spain\",\n    location_types=[\"amenity\", \"building\", \"shop\"]\n)\n\n# Fetch and process OSM locations\nlocations = fetcher.fetch_locations(since_year=2020, handle_duplicates=\"combine\")\nprint(locations.head())\n</code></pre>"},{"location":"examples/downloading/osm/#explanation","title":"Explanation","text":"<ul> <li>OSMLocationFetcher: This class fetches and processes location data from OpenStreetMap.</li> <li>fetch_locations: This method fetches and processes OSM data based on the specified criteria.</li> </ul>"},{"location":"examples/downloading/osm/#next-steps","title":"Next Steps","text":"<p>Once the data is fetched, you can process it using the Processing Examples.</p> <p>Back to Examples</p>"},{"location":"examples/processing/tif/","title":"Processing Raster Files","text":"<p>This example demonstrates how to process raster files using the <code>TifProcessor</code> class.</p>"},{"location":"examples/processing/tif/#prerequisites","title":"Prerequisites","text":"<p>Ensure you have installed the <code>gigaspatial</code> package and set up the necessary configuration. Follow the Installation Guide if you haven\u2019t already.</p>"},{"location":"examples/processing/tif/#example-code","title":"Example Code","text":"<pre><code>from gigaspatial.processing import TifProcessor\nfrom gigaspatial.core.io import LocalDataStore\nfrom rasterio.warp import Resampling # For reprojection methods\n\n# NOTE: For these examples, replace \"/path/to/your/file.tif\" with actual paths to your GeoTIFF files.\n# You might need to create dummy files or use existing ones for local testing.\n\n# 1. Initialize with a single TIFF file\nprint(\"--- Single TIFF File Processing ---\")\nsingle_processor = TifProcessor(\n    \"/path/to/single_band.tif\", \n    mode=\"single\" # Can be \"rgb\", \"rgba\", \"multi\"\n)\ndf_single = single_processor.to_dataframe()\nprint(\"Single-band DataFrame head:\")\nprint(df_single.head())\nprint(\"Raster Info for single_processor:\")\nprint(single_processor.get_raster_info())\n\n\n# 2. Initialize with multiple TIFF files for merging and reprojection\nprint(\"\\n--- Multi-raster Merging and Reprojection ---\")\n# Replace with actual paths to your tif files. Ensure they are compatible for merging.\n# Example: two adjacent tiles from a dataset.\ntif_paths = [\n    \"/path/to/raster1.tif\",\n    \"/path/to/raster2.tif\"\n]\nmerged_reprojected_processor = TifProcessor(\n    dataset_path=tif_paths,\n    mode=\"single\", # Or \"multi\", \"rgb\", \"rgba\" depending on your data\n    merge_method=\"mean\", # Options: \"first\", \"last\", \"min\", \"max\", \"mean\"\n    target_crs=\"EPSG:4326\", # Reproject all rasters to WGS84 during initialization\n)\ndf_merged = merged_reprojected_processor.to_dataframe()\nprint(\"Merged and Reprojected DataFrame head:\")\nprint(df_merged.head())\nprint(\"Raster Info for merged_reprojected_processor:\")\nprint(merged_reprojected_processor.get_raster_info())\n\n# 3. Explicit Reprojection after initialization\nprint(\"\\n--- Explicit Reprojection ---\")\n# Reproject the current raster (e.g., the merged one) to a different CRS or resolution\n# In a real scenario, you'd save this to a persistent location.\nreprojected_output_path = \"./temp_reprojected_raster.tif\" \nreprojected_path = merged_reprojected_processor.reproject_to(\n    target_crs=\"EPSG:3857\", # Web Mercator\n    output_path=reprojected_output_path,\n    resampling_method=Resampling.bilinear # Different resampling method\n)\nprint(f\"Raster reprojected to: {reprojected_path}\")\n\n# 4. Convert raster to a graph (NetworkX example)\nprint(\"\\n--- Raster to Graph Conversion ---\")\n# Assuming '/path/to/single_band.tif' is a suitable single-band raster\ngraph_processor = TifProcessor(\n    \"/path/to/single_band.tif\", \n    mode=\"single\" # Graph conversion typically for single-band data\n)\ngraph = graph_processor.to_graph(\n    connectivity=8, # 4-connectivity (von Neumann) or 8-connectivity (Moore)\n    include_coordinates=True, # Include 'x' and 'y' coordinates as node attributes\n    graph_type=\"networkx\" # Or \"sparse\" for scipy.sparse.csr_matrix\n)\nprint(f\"Generated a NetworkX graph with {graph.number_of_nodes()} nodes and {graph.number_of_edges()} edges.\")\n# Example: Access node attributes (first 5 nodes)\n# for node_id, data in list(graph.nodes(data=True))[:5]:\n#    print(f\"Node {node_id}: Value={data['value']:.2f}, X={data.get('x'):.2f}, Y={data.get('y'):.2f}\")\n</code></pre>"},{"location":"examples/processing/tif/#explanation","title":"Explanation","text":"<p>The <code>TifProcessor</code> class provides robust functionalities for handling GeoTIFF files, from single-band to multi-band (RGB, RGBA) datasets, with advanced processing capabilities including:</p> <ul> <li>Initialization:<ul> <li>Can be initialized with a single GeoTIFF file path.</li> <li>Supports a list of GeoTIFF file paths for automatic merging during initialization, configured via <code>merge_method</code> (<code>first</code>, <code>last</code>, <code>min</code>, <code>max</code>, <code>mean</code>).</li> <li>The <code>mode</code> parameter (<code>single</code>, <code>rgb</code>, <code>rgba</code>, <code>multi</code>) dictates how bands are interpreted and validated.</li> <li><code>target_crs</code> and <code>reprojection_resolution</code> can be set during initialization to reproject rasters immediately to a consistent CRS and pixel size.</li> </ul> </li> <li>Data Extraction:<ul> <li><code>to_dataframe()</code>: Converts raster data into a pandas DataFrame, with columns for longitude, latitude, and pixel values (or band-specific values for multi-band modes).</li> <li><code>to_geodataframe()</code>: Extends <code>to_dataframe()</code> by adding a <code>geometry</code> column, converting each pixel into a GeoDataFrame representing its bounding box, with the correct CRS.</li> </ul> </li> <li>Reprojection (<code>reproject_to</code>):<ul> <li>Allows explicit reprojection of the current raster to a new Coordinate Reference System (CRS) and/or resolution, saving the output to a specified path or a temporary file.</li> <li>Supports different <code>resampling_method</code> options (e.g., <code>Resampling.nearest</code>, <code>Resampling.bilinear</code>).</li> </ul> </li> <li>Raster Information (<code>get_raster_info</code>):<ul> <li>Provides a dictionary containing comprehensive metadata about the raster, such as band count, dimensions, CRS, bounds, transform, data types, nodata values, processing mode, and merge status.</li> </ul> </li> <li>Graph Conversion (<code>to_graph</code>):<ul> <li>Converts raster data into a graph (NetworkX graph or SciPy sparse matrix) based on pixel adjacency.</li> <li>Supports <code>connectivity</code> of 4 (von Neumann neighborhood) or 8 (Moore neighborhood).</li> <li>Can include geographic coordinates and pixel values as node attributes.</li> </ul> </li> <li>Sampling:<ul> <li><code>sample_by_coordinates()</code>: Extracts pixel values at specific geographic coordinates.</li> <li><code>sample_by_polygons()</code>: Computes aggregate statistics (e.g., mean, sum, min, max, count) of pixel values within given polygon boundaries, supporting single or multiple statistics.</li> <li><code>sample_by_polygons_batched()</code>: Provides a parallelized version of polygon sampling for performance-intensive tasks.</li> </ul> </li> </ul>"},{"location":"examples/processing/tif/#multi-raster-reprojection","title":"Multi-raster reprojection","text":"<p>The differences in the reprojected metadata are expected and are a direct result of the order of operations: reproject then merge versus merge then reproject. The two processes follow different steps, leading to variations in the final raster's dimensions, bounds, and resolution.</p>"},{"location":"examples/processing/tif/#reproject-then-merge","title":"Reproject then Merge","text":"<p>When you specify <code>target_crs</code> at initialization, the code first reprojects each individual raster to the target CRS (<code>EPSG:4326</code>) and then merges the reprojected outputs.</p> <ul> <li>Step 1: Reprojection: Each input raster is reprojected from <code>ESRI:54009</code> to <code>EPSG:4326</code>. During this step, <code>rasterio</code>'s <code>calculate_default_transform</code> function computes a new transform and pixel dimensions (<code>width</code>, <code>height</code>) for each raster. The reprojected rasters are now in the same CRS with a consistent resolution (e.g., <code>0.00918...</code> degrees).</li> <li>Step 2: Merging: The reprojected rasters, which are now in the same CRS and have similar resolutions, are merged. The <code>rasterio.merge</code> function can combine these aligned rasters seamlessly. The final output's dimensions are calculated by finding the union of all reprojected rasters' bounds and applying the shared resolution, resulting in a single, larger raster.</li> </ul> <p>This process ensures a uniform resolution and grid alignment across all parts of the final merged raster.</p>"},{"location":"examples/processing/tif/#merge-then-reproject","title":"Merge then Reproject","text":"<p>When <code>target_crs</code> is not specified at initialization, the code first merges the two rasters in their original <code>ESRI:54009</code> CRS and then reprojects the single, merged output to <code>EPSG:4326</code>.</p> <ul> <li>Step 1: Merging: The two rasters are merged in <code>ESRI:54009</code>. Since they are in the same CRS and have the same resolution (<code>1000.0</code> meters), <code>rasterio.merge</code> can simply combine them side-by-side. The original raster was <code>1000x1000</code>, so merging a second one next to it likely creates a <code>2000x1000</code> raster, as seen in the metadata. The resolution remains <code>1000.0</code> meters.</li> <li>Step 2: Reprojection: The single <code>2000x1000</code> raster is then reprojected to <code>EPSG:4326</code>. A new transform and pixel dimensions are calculated for this single, larger raster. Since <code>calculate_default_transform</code> is working on a different-shaped input, it will calculate a different output resolution and grid shape. The resulting resolution (<code>0.00973...</code>) and dimensions (<code>2076x832</code>) will be different because the reprojection is performed on a single, larger input rather than two smaller ones.</li> </ul>"},{"location":"examples/processing/tif/#why-the-metadata-is-different","title":"Why the Metadata is Different","text":"<ul> <li>Resolution: The <code>reproject-then-merge</code> approach maintains a consistent resolution that is calculated for a single tile and then applied to all. The <code>merge-then-reproject</code> approach calculates a single resolution for the entire, larger combined area. The process of resampling to a new grid (a core part of reprojection) is inherently sensitive to the input's size and shape.</li> <li>Dimensions (<code>width</code>, <code>height</code>): The final pixel dimensions are a function of the total bounds and the final resolution. Since the resolution is different in the two methods, the width and height must also be different to cover the same geographic area.</li> <li>Bounds: The final bounds are nearly identical in latitude and longitude, which makes sense because both methods represent the same geographic area. Any slight differences are due to rounding and the nuances of resampling.</li> </ul> <p>Conclusion: The differences are normal and reflect the non-commutative nature of these two geospatial operations. The reproject then merge approach is generally preferable as it ensures greater consistency and can be more accurate when dealing with rasters that have slightly different resolutions or alignments, as it creates a single, clean grid before combining the data.</p> <p>Back to Examples</p>"},{"location":"getting-started/installation/","title":"Installation Guide","text":"<p>This guide will walk you through the steps to install the <code>gigaspatial</code> package on your system. The package is compatible with Python 3.10 and above.</p>"},{"location":"getting-started/installation/#prerequisites","title":"Prerequisites","text":"<p>Before installing <code>gigaspatial</code>, ensure you have Python installed on your system. You can check your Python version by running:</p> <pre><code>python --version\n</code></pre> <p>If Python is not installed, you can download it from the official Python website.</p>"},{"location":"getting-started/installation/#installing-from-pypi","title":"Installing from PyPI","text":"<p>The easiest way to install <code>gigaspatial</code> is directly from PyPI using pip:</p> <pre><code>pip install giga-spatial\n</code></pre> <p>This will install the latest stable version of the package along with all its dependencies.</p>"},{"location":"getting-started/installation/#installing-from-source","title":"Installing from Source","text":"<p>If you need to install a specific version or want to contribute to the development, you can install from the source:</p> <ol> <li> <p>Clone the Repository:    <pre><code>git clone https://github.com/unicef/giga-spatial.git\ncd giga-spatial\n</code></pre></p> </li> <li> <p>Install the Package:    <pre><code>pip install .\n</code></pre></p> </li> </ol>"},{"location":"getting-started/installation/#installing-in-development-mode","title":"Installing in Development Mode","text":"<p>If you plan to contribute to the package or modify the source code, you can install it in development mode. This allows you to make changes to the code without reinstalling the package:</p> <pre><code>pip install -e .\n</code></pre>"},{"location":"getting-started/installation/#installing-dependencies","title":"Installing Dependencies","text":"<p>The package dependencies are automatically installed when you install <code>gigaspatial</code>. However, if you need to install them manually, you can use:</p> <pre><code>pip install -r requirements.txt\n</code></pre>"},{"location":"getting-started/installation/#verifying-the-installation","title":"Verifying the Installation","text":"<p>After installation, you can verify that the package is installed correctly by running:</p> <pre><code>python -c \"import gigaspatial; print(gigaspatial.__version__)\"\n</code></pre> <p>This should print the version of the installed package.</p>"},{"location":"getting-started/installation/#troubleshooting","title":"Troubleshooting","text":"<p>If you encounter any issues during installation, consider the following:</p> <ul> <li> <p>Ensure <code>pip</code> is up-to-date:   <pre><code>pip install --upgrade pip\n</code></pre></p> </li> <li> <p>Check for conflicting dependencies: If you have other Python packages installed that might conflict with <code>gigaspatial</code>, consider using a virtual environment.</p> </li> <li> <p>Use a Virtual Environment: To avoid conflicts with other Python packages, you can create a virtual environment:   <pre><code>python -m venv venv\nsource venv/bin/activate  # On Windows, use `venv\\Scripts\\activate`\npip install giga-spatial  # or pip install . if installing from source\n</code></pre></p> </li> </ul>"},{"location":"getting-started/installation/#next-steps","title":"Next Steps","text":"<p>Once the installation is complete, you can proceed to the Quick Start Guide to begin using the <code>gigaspatial</code> package.</p>"},{"location":"getting-started/quickstart/","title":"Quick Start Guide","text":"<p>This guide will walk you through the basic usage of the <code>gigaspatial</code> package. By the end of this guide, you will be able to download, process, and store geospatial data using the package.</p>"},{"location":"getting-started/quickstart/#prerequisites","title":"Prerequisites","text":"<p>Before you begin, ensure that you have installed the <code>gigaspatial</code> package. If you haven't installed it yet, follow the Installation Guide.</p>"},{"location":"getting-started/quickstart/#importing-the-package","title":"Importing the Package","text":"<p>Start by importing the <code>gigaspatial</code> package:</p> <pre><code>import gigaspatial as gs\n</code></pre>"},{"location":"getting-started/quickstart/#setting-up-configuration","title":"Setting Up Configuration","text":"<p>The <code>gigaspatial</code> package uses a unified configuration system to manage paths, API keys, and other settings.</p> <ul> <li>Environment Variables: Most configuration is handled via environment variables, which can be set in a <code>.env</code> file at the project root. For a full list of supported variables and their descriptions, see the Configuration Guide.</li> <li>Defaults: If not set, sensible defaults are used for all paths and keys.</li> <li>Manual Overrides: You can override data directory paths in your code using <code>config.set_path</code>.</li> </ul>"},{"location":"getting-started/quickstart/#example-env-file","title":"Example <code>.env</code> File","text":"<pre><code>BRONZE_DIR=/path/to/your/bronze_tier_data\nSILVER_DIR=/path/to/your/silver_tier_data\nGOLD_DIR=/path/to/your/gold_tier_data\nVIEWS_DIR=/path/to/your/views_data\nCACHE_DIR=/path/to/your/cache\nADMIN_BOUNDARIES_DIR=/path/to/your/admin_boundaries\nMAPBOX_ACCESS_TOKEN=your_mapbox_token_here\n# ... other keys ...\n</code></pre>"},{"location":"getting-started/quickstart/#setting-paths-programmatically","title":"Setting Paths Programmatically","text":"<pre><code>from gigaspatial.config import config\n\nconfig.set_path(\"bronze\", \"/path/to/your/bronze_tier_data\")\nconfig.set_path(\"gold\", \"/path/to/your/gold_tier_data\")\nconfig.set_path(\"views\", \"/path/to/your/views_data\")\n</code></pre> <p>For more details and troubleshooting, see the full configuration guide.</p>"},{"location":"getting-started/quickstart/#downloading-and-processing-geospatial-data","title":"Downloading and Processing Geospatial Data","text":"<p>The <code>gigaspatial</code> package provides several handlers for different types of geospatial data. Here are examples for two commonly used handlers:</p>"},{"location":"getting-started/quickstart/#ghsl-global-human-settlement-layer-data","title":"GHSL (Global Human Settlement Layer) Data","text":"<p>The <code>GHSLDataHandler</code> provides access to various GHSL products including built-up surface, building height, population, and settlement model data:</p> <pre><code>from gigaspatial.handlers import GHSLDataHandler\n\n# Initialize the handler with desired product and parameters\nghsl_handler = GHSLDataHandler(\n    product=\"GHS_BUILT_S\",  # Built-up surface\n    year=2020,\n    resolution=100,  # 100m resolution\n)\n\n# Download data for a specific country\ncountry_code = \"TUR\"\ndownloaded_files = ghsl_handler.load_data(country_code, ensure_available = True)\n\n# Load the data into a DataFrame\ndf = ghsl_handler.load_into_dataframe(country_code, ensure_available = True)\nprint(df.head())\n\n# You can also load data for specific points or geometries\npoints = [(38.404581,27.4816677), (39.8915702, 32.7809618)]\ndf_points = ghsl_handler.load_into_dataframe(points, ensure_available = True)\n</code></pre>"},{"location":"getting-started/quickstart/#google-open-buildings-data","title":"Google Open Buildings Data","text":"<p>The <code>GoogleOpenBuildingsHandler</code> provides access to Google's Open Buildings dataset, which includes building footprints and points:</p> <pre><code>from gigaspatial.handlers import GoogleOpenBuildingsHandler\n\n# Initialize the handler\ngob_handler = GoogleOpenBuildingsHandler()\n\n# Download and load building polygons for a country\ncountry_code = \"TUR\"\npolygons_gdf = gob_handler.load_polygons(country_code, ensure_available = True)\n\n# Download and load building points for a country\npoints_gdf = gob_handler.load_points(country_code, ensure_available = True)\n\n# You can also load data for specific points or geometries\npoints = [(38.404581, 27.4816677), (39.8915702, 32.7809618)]\npolygons_gdf = gob_handler.load_polygons(points, ensure_available = True)\n</code></pre>"},{"location":"getting-started/quickstart/#storing-geospatial-data","title":"Storing Geospatial Data","text":"<p>You can store the processed data in various formats using the <code>DataStore</code> class from the <code>core.io</code> module. Here's an example of saving data to a parquet file:</p> <pre><code>from gigaspatial.core.io import LocalDataStore\n\n# Initialize the data store\ndata_store = LocalDataStore()\n\n# Save the processed data to a parquet file\nwith data_store.open(\"/path/to/your/output/processed_data.parquet\", \"rb\") as f:\n    processed_data.to_parquet(f)\n</code></pre> <p>If your dataset is already a <code>pandas.DataFrame</code> or <code>geopandas.GeoDataFrame</code>, <code>write_dataset</code> method from the <code>core.io.writers</code> module can be used to write the dataset in various formats. </p> <pre><code>from gigaspatial.core.io.writers import write_dataset\n\n# Save the processed data to a GeoJSON file\nwrite_dataset(data=processed_data, data_store=data_store, path=\"/path/to/your/output/processed_data.geojson\")\n</code></pre>"},{"location":"getting-started/quickstart/#visualizing-geospatial-data","title":"Visualizing Geospatial Data","text":"<p>To visualize the geospatial data, you can use libraries like <code>geopandas</code> and <code>matplotlib</code>. Here's an example of plotting the processed data on a map:</p> <pre><code>import geopandas as gpd\nimport matplotlib.pyplot as plt\n\n# Load the GeoJSON file\ngdf = gpd.read_file(\"/path/to/your/output/processed_data.geojson\")\n\n# Plot the data\ngdf.plot()\nplt.show()\n</code></pre> <p><code>geopandas.GeoDataFrame.explore</code> can also be used to visualise the data on interactive map based on <code>GeoPandas</code> and <code>folium/leaflet.js</code>: <pre><code># Visualize the data\ngdf.explore(\"population\", cmap=\"Blues\")\n</code></pre></p>"},{"location":"getting-started/quickstart/#next-steps","title":"Next Steps","text":"<p>Now that you have a basic understanding of how to use the <code>gigaspatial</code> package, you can explore more advanced features and configurations. Check out the User Guide for detailed documentation and examples.</p>"},{"location":"getting-started/quickstart/#additional-resources","title":"Additional Resources","text":"<ul> <li>API Documentation: Detailed documentation of all classes and functions.</li> <li>Examples: Real-world examples and use cases.</li> <li>Changelog: Information about the latest updates and changes.</li> </ul>"},{"location":"user-guide/","title":"User Guide","text":"<p>Welcome to the User Guide for the <code>gigaspatial</code> package. This guide provides detailed documentation on how to use the package for various geospatial data tasks.</p>"},{"location":"user-guide/#getting-started","title":"Getting Started","text":"<p>If you\u2019re new to <code>gigaspatial</code>, start with the Quick Start Guide to learn the basics.</p>"},{"location":"user-guide/#core-concepts","title":"Core Concepts","text":""},{"location":"user-guide/#1-configuration","title":"1. Configuration","text":"<p>Learn how to configure the package, including setting paths, API keys, and other settings.</p> <ul> <li>Configuration Overview</li> <li>Using Environment Variables</li> <li>Setting Paths and Keys Manually</li> </ul>"},{"location":"user-guide/#additional-resources","title":"Additional Resources","text":"<ul> <li>Examples: Real-world examples and use cases.</li> <li>API Reference: Detailed documentation of all classes and functions.</li> <li>Changelog: Information about the latest updates and changes.</li> <li>Contributing: Guidelines for contributing to the project.</li> </ul>"},{"location":"user-guide/#support","title":"Support","text":"<p>If you encounter any issues or have questions, feel free to open an issue or join our Discord community.</p>"},{"location":"user-guide/configuration/","title":"Configuration","text":"<p>The <code>gigaspatial</code> package uses a unified configuration system, managed by the <code>config.py</code> file, to handle paths, API keys, and other settings. This guide explains how to configure the package for your environment.</p>"},{"location":"user-guide/configuration/#environment-variables-overview","title":"Environment Variables Overview","text":"<p>Configuration is primarily managed via environment variables, which can be set in a <code>.env</code> file at the project root. Below is a table of all supported environment variables, their defaults, and descriptions:</p> Variable Default Description ADLS_CONNECTION_STRING \"\" Azure Data Lake connection string ADLS_CONTAINER_NAME \"\" Azure Data Lake container name GOOGLE_SERVICE_ACCOUNT \"\" Google service account credentials API_PROFILE_FILE_PATH profile.share Path to API profile file API_SHARE_NAME \"\" API share name API_SCHEMA_NAME \"\" API schema name MAPBOX_ACCESS_TOKEN \"\" Mapbox API access token MAXAR_USERNAME \"\" Maxar API username MAXAR_PASSWORD \"\" Maxar API password MAXAR_CONNECTION_STRING \"\" Maxar API connection string/key OPENCELLID_ACCESS_TOKEN \"\" OpenCellID API access token GEOREPO_API_KEY \"\" UNICEF GeoRepo API key GEOREPO_USER_EMAIL \"\" UNICEF GeoRepo user email GIGA_SCHOOL_LOCATION_API_KEY \"\" GIGA School Location API key GIGA_SCHOOL_PROFILE_API_KEY \"\" GIGA School Profile API key GIGA_SCHOOL_MEASUREMENTS_API_KEY \"\" GIGA School Measurements API key ROOT_DATA_DIR . Root directory for all data tiers BRONZE_DIR bronze Directory for raw/bronze tier data SILVER_DIR silver Directory for processed/silver tier data GOLD_DIR gold Directory for final/gold tier data VIEWS_DIR views Directory for views data CACHE_DIR cache Directory for cache/temp files ADMIN_BOUNDARIES_DIR admin_boundaries Directory for admin boundary data <p>Tip: You can copy <code>.env_sample</code> to <code>.env</code> and fill in your values.</p>"},{"location":"user-guide/configuration/#example-env-file","title":"Example <code>.env</code> File","text":"<pre><code># Data directories\nBRONZE_DIR=/path/to/your/bronze_tier_data\nSILVER_DIR=/path/to/your/silver_tier_data\nGOLD_DIR=/path/to/your/gold_tier_data\nVIEWS_DIR=/path/to/your/views_data\nCACHE_DIR=/path/to/your/cache\nADMIN_BOUNDARIES_DIR=/path/to/your/admin_boundaries\n\n# API keys and credentials\nMAPBOX_ACCESS_TOKEN=your_mapbox_token_here\nMAXAR_USERNAME=your_maxar_username_here\nMAXAR_PASSWORD=your_maxar_password_here\nMAXAR_CONNECTION_STRING=your_maxar_key_here\n# ... other keys ...\n</code></pre>"},{"location":"user-guide/configuration/#how-configuration-is-loaded","title":"How Configuration is Loaded","text":"<ul> <li>The <code>config.py</code> file uses pydantic-settings to load environment variables from <code>.env</code> (if present) or the system environment.</li> <li>All directory paths are resolved as <code>Path</code> objects. If a path is relative, it is resolved relative to the current working directory.</li> <li>Defaults are used if environment variables are not set.</li> </ul>"},{"location":"user-guide/configuration/#setting-paths-and-keys-programmatically","title":"Setting Paths and Keys Programmatically","text":"<p>You can override directory paths in your code using the <code>set_path</code> method:</p> <pre><code>from gigaspatial.config import config\n\n# Set custom data storage paths\nconfig.set_path(\"bronze\", \"/path/to/your/bronze_tier_data\")\nconfig.set_path(\"gold\", \"/path/to/your/gold_tier_data\")\nconfig.set_path(\"views\", \"/path/to/your/views_data\")\n</code></pre> <p>Note: API keys and credentials should be set via environment variables for security.</p>"},{"location":"user-guide/configuration/#ensuring-directories-exist","title":"Ensuring Directories Exist","text":"<p>To ensure all configured directories exist (and optionally create them if missing):</p> <pre><code>from gigaspatial.config import config\n\n# Raise error if any directory does not exist\nconfig.ensure_directories_exist(create=False)\n\n# Or, create missing directories automatically\nconfig.ensure_directories_exist(create=True)\n</code></pre>"},{"location":"user-guide/configuration/#verifying-the-configuration","title":"Verifying the Configuration","text":"<p>You can print the current configuration for debugging:</p> <pre><code>from gigaspatial.config import config\nprint(config)\n</code></pre>"},{"location":"user-guide/configuration/#troubleshooting","title":"Troubleshooting","text":"<ul> <li>.env File Location: Ensure <code>.env</code> is in your project root.</li> <li>Absolute Paths: Use absolute paths for directories to avoid confusion.</li> <li>Environment Variable Precedence: Values in <code>.env</code> override defaults, but can be overridden by system environment variables.</li> <li>Missing Directories: Use <code>config.ensure_directories_exist(create=True)</code> to create missing directories.</li> <li>API Keys: Double-check that all required API keys are set for the services you use.</li> </ul>"},{"location":"user-guide/configuration/#next-steps","title":"Next Steps","text":"<p>Once configuration is set up, proceed to the Data Handling Guide (Coming Soon) to start using <code>gigaspatial</code>.</p> <p>Back to User Guide</p>"}]}